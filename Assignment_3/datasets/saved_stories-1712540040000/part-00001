{"aid": "39963140", "title": "Evil staging and gauntlet programs, tools to increase your software reliability", "url": "https://www.rubick.com/evil-staging-gauntlet-programs/", "domain": "rubick.com", "votes": 2, "user": "gtirloni", "posted_at": "2024-04-07 19:32:08", "comments": 0, "source_title": "Evil staging and gauntlet programs, tools to increase your software reliability", "source_text": "Jade Rubick - Evil staging and gauntlet programs, tools to increase your\nsoftware reliability\n\njade rubick\n\n# Evil staging and gauntlet programs, tools to increase your software\nreliability\n\n2024-04-04platform-engineeringsoftware-designstandardsreliability\n\nI\u2019m excited to share some innovative but experimental practices today. You can\nuse them to improve the reliability of your engineering organization. They are\nbased on ideas from chaos engineering.\n\n## What is chaos engineering?\n\nThe idea behind chaos engineering is to test your systems by deliberately\ninjecting failure into those systems. By doing so, you inoculate your software\nto those classes of failure. For example, you might fill up the disk on a\nserver, and see if anything breaks. If it does, you learn how it fails, and\nyou fix it. If it doesn\u2019t break, then you\u2019ve learned your software is\ncurrently resilient to that class of failure.\n\nThe way I remember it, the techniques of chaos engineering largely emerged\nwith cloud computing. Early cloud computing systems were not reliable. And\nthey encouraged an architecture which was more distributed and microservice\nbased. Companies like Netflix and Amazon were essentially trying to create\nreliable systems on a substrate of unreliable systems.\n\nTypically, chaos engineering uses an approach where you use gamedays and\ndeliberately test your systems for failure. Gamedays are a wonderful practice.\nI encourage you to use them! But I\u2019m going to talk about some chaos\nengineering approaches that are more systemic. They are also more\nexperimental, and less common. If executed well, they should guarantee good\nresults. And, you might use gamedays as a regular practice while you roll them\nout.\n\n## \ud83d\ude08 Evil Staging\n\nThe first technique is \u201cevil staging\u201d. What is this diabolical practice?\n\nIn software engineering, the idea of a deployment pipeline is to take software\nand gradually release it into more and more realistic and rigorous stages,\nuntil you release it to customers. Thus, a common practice is to have\nengineering teams...\n\n  1. Work locally (on dev).\n  2. Then push their work into a shared environment (staging).\n  3. And then finally to deploy it to production (prod).\n\nTypically, each stage is more and more \u201clife-like\u201d and rigorous. For example,\ntypically your staging environment will have more realistic data in it than\ndev. It will have more people using it. It will be closer to what is happening\nin production.\n\nThe key idea behind Evil Staging is that you make your staging environment\nmore evil \ud83d\ude08. For example, it might have no free disk space. Or the processes\nmight be restarted every hour. Or it might frequently run out of memory. All\nof these are realistic scenarios, that will happen to your application in\nproduction. But Evil Staging makes them happen all the time.\n\nThere are a million ways you could do Evil Staging:\n\n  * You can do it as a staging environment, or do it as a separate production environment.\n  * You could organize the failures with some sort of regular schedule, or do them randomly.\n\nBut the basic idea is to make these failures happen more often.\n\nWill this improve reliability? It might not. If people respond to failures in\nstaging by investigating them and fixing them, then this might work for you!\nIf alerts on staging don\u2019t cause any response, then it\u2019s probably not a good\nfit.\n\nIf it\u2019s not a good fit, read on for Gauntlet programs.\n\n## Gauntlet Programs\n\nThe second technique is what I call a Gauntlet Program.\n\nA Gauntlet Program is where you gradually introduce faults into your staging\nand then production environments. But, you do it on a gradual schedule. It\nmight take a year or two to fully ramp up the gauntlet.\n\nThe idea is that you are gradually inoculating your software from entire\nclasses of failure. So a gauntlet program might look like this:\n\n  * Month 1: Introduce the Gauntlet Program to the engineering team. Train engineers on using failure injection tools (it could be Gremlin, or any of the alternative tools available). Communicate it in a fun way, give teams time to prepare their systems, and then they can get back to whatever other work they were doing. Then, you start the challenge. The initial challenge is to make systems resilient to unexpected host restarts within 30 days. First this will happen on staging, then it will happen in production (yes, truly!). You give teams a playbook that includes observability, gamedays, and a lot of support, as they prepare their systems for the first stage of the Gauntlet. Then the time comes, and there are some minor issues, but they are resolved, and fixed. Congratulations, you\u2019ve just eliminated an entire category of failure from your system. And since it\u2019s ongoing, it won\u2019t reoccur.\n  * Month 3: Focus shifts to preparing systems to withstand hosts running out of disk space. The failures start in staging, and two weeks later, move to production. (Yes, truly!)\n  * Month 5: The challenge involves making systems resilient against running out of memory. The failures start in staging, and two weeks later, move to production.\n  * Month 7: Teams work to ensure their systems can handle maximum disk I/O consumption. With the same sequence of staging and production failures.\n  * Month 9: The focus shifts to time being out of sync. Various time offsets are introduced, following the standard pattern.\n  * Month 11: The challenge for this period is that latency is added to some or all network traffic.\n  * Month 13: The teams now face the fact that a percentage of all network traffic fails.\n  * Month 15: Next, the teams face the challenge of dealing with making their systems resilient to network partitions.\n  * Month 17: The challenge is to run out of file handles. Sneaky one that!\n  * Month 19: The final challenge involves making systems resilient to DNS failures. They should ensure that things degrade in a reasonable way, until DNS resumes.\n\nEach challenge is designed to innoculate your software from a new category of\ncommon failure. As you progress through the Gauntlet, your systems should\nbecome gradually more reliable. They should also learn to be more proactive\nabout reliability issues, and think about their software in an adversarial and\nproactive way. This builds a culture of proactive and systematic reliability.\n\nYou can make this as aggressive or gradual as you like. Adding a challenge per\nquarter might be a more reasonable approach in some environments. Or, if\nreliability is truly a concern, you might make it happen even faster!\n\nNote that you can add additional types of challenges to the gauntlet. You\nmight add some security related challenges, for example. You could even add\nsome usability challenges, to guarantee that basic workflows in your\napplication are never broken (this would pair nicely with training on\nSynthetic monitoring). And, there are a lot of limits I didn\u2019t include in the\nlist above.\n\n## Make the gauntlet into a Reliability Race\n\nYou could design a gauntlet as a Reliability Race, where you have a maturity\nmodel, and each team turns up the difficulty level until they reach MAX LEVEL.\n\nIt could look like this:\n\n  * Newbie: have an on-call set up. Get a basic level of monitoring in place for the team.\n  * Level 1: have run a game day.\n  * Level 2: added the \u201chost restart\u201d failure to their software (in staging and production), and it\u2019s running fine.\n  * Level 3: added the \u201cdisk is full\u201d failure to their software, and it\u2019s running fine.\n  * Level 4: added the \u201cmemory full\u201d failure to their software, and it\u2019s running fine.\n  * Level 5: added \u201cmaxed out disk I/O\u201d failure to their software, and it\u2019s running fine.\n  * Level 6: added the \u201ctime drift\u201d failure to their software, and it\u2019s running fine.\n  * Level 7: added the \u201cnetwork latency\u201d failure to their software, and it\u2019s running fine.\n  * Level 8: added the \u201cpacket loss\u201d failure to their software, and it\u2019s running fine.\n  * Level 9: added the \u201cnetwork down\u201d failure to their software, and it\u2019s running fine.\n  * Level 10: added the \u201cfile handles full\u201d failure to their software, and it\u2019s running fine.\n  * Max level, you win! Added the \u201cDNS is down\u201d failure to their software, and it\u2019s running fine.\n\nBy the way, running fine can mean whatever makes sense for your application.\nIt may mean graceful failure, it might mean you\u2019ve decided the failure is\nacceptable, or it might mean doing extraordinary things to make the experience\nokay. That\u2019s for you to decide. Leadership should probably provide some\nguidance on how to reason about these things, so every team doesn\u2019t interpret\nit differently.\n\nMaking it a Reliability Race could make it more enjoyable for participants. It\ncould also make it easier for some teams to reason about their local\npriorities versus the Reliability Race. For example, a team could already be\ndoing some important reliability work, and decide to prioritize that before\ndealing with \u201chost restart\u201d failures. Every team will also have a different\namount of work they have to do to meet the Gauntlet, so this makes that more\nflexible.\n\n## The Newbie Gauntlet\n\nA variation of the Gauntlet Program is a Newbie Gauntlet. This is easier to\nintroduce because you only do it for new things.\n\nWhenever you spin up a new service, you start with all the failures in your\nstaging and production environment. This makes it so you have to build things\nto be resilient at every point. It forces good reliability and engineering\npractices in everything you do.\n\nThe advantage of this is that it makes all your new services reliable by\ndefault. But you would have to be careful you don\u2019t make it so onerous that\npeople want to avoid creating new services!\n\n## Why I like these approaches\n\nI like these approaches because they can guarantee good results. They are\nsystemic in nature, so they make it necessary to build your software in a\ndifferent way.\n\nYou\u2019ll notice that they are analogous to test suites. They are always present,\nalways defending you against making the mistakes you are certain to make. They\nmake the expectations of reliability explicit in your software, and give you\nsafeguards to ensure you don\u2019t violate them.\n\n## The objections you\u2019ll hear\n\nPeople will say you shouldn\u2019t do this because it will cause failures in\nproduction. But those failures will happen anyway \u2013 they will just come at an\ninconvenient time. This method gives teams time to prepare. It also\ncommunicates that the expectation is that you build things in a reliable way.\nOften, teams do not get this type of explicit guidance, and they can\nironically be punished for building things in a reliable way.\n\nI hope more companies experiment with variations like this! Let me know your\nexperiences, and what you come up with!\n\n## Prereqs\n\nFor this to be successful:\n\n  * Your leadership team needs to value reliability.\n  * You probably need to have an on call rotation.\n  * You probably also need observability tooling (something like Honeycomb, New Relic, or Datadog).\n  * The rollout should ideally be communicated well, and by leadership that has empathy and can do good change management.\n  * I will note that a lot of SRE organizations do not have this level of support. It is similar in some ways to error budgets or SLAs in that it needs a high degree of support to be successful.\n\n## Thank yous\n\nEvil staging was inspired by some of the practices I heard from Netflix. I\nremember it being talked about at New Relic when I was there. I don\u2019t remember\nwho coined the term. I believe Gauntlet programs are my own invention, but of\ncourse I\u2019ve been in several environments where a lot of these ideas were\nfloating around. And I\u2019ve worked with people who have been at Netflix, Amazon,\nand other places where reliability practices were pioneered. I developed\nGauntlet Programs as a potential product strategy for Gremlin, when I was VP\nof Engineering and Product there.\n\nThank you to Tim Tischler for sharing with me the list of limits.\n\nImages were generated by Dalle. I\u2019m sure they\u2019ll look very dated in a year.\nVery 2024 AI generated. It will seem vintage, I\u2019m sure.\n\nSubscribe Engineering Leadership Weekly & Frontline Management & Decoding\nleadership podcast\n\nContact Point me at your organizational problems. I advise startups and help\nin a variety of ways.\n\nComments powered by Talkyard.\n\n#### The dangers of unreliable platforms (on the SRE Path podcast) 2024-03-18\n\nLast updated Apr 05, 2024 21:20.\n\n", "frontpage": false}
