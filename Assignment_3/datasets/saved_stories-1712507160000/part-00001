{"aid": "39959790", "title": "SentenceTransformers: Python framework for sentence, text and image embeddings", "url": "https://www.sbert.net/index.html", "domain": "sbert.net", "votes": 5, "user": "tosh", "posted_at": "2024-04-07 10:23:51", "comments": 0, "source_title": "SentenceTransformers Documentation \u2014 Sentence-Transformers documentation", "source_text": "SentenceTransformers Documentation \u2014 Sentence-Transformers documentation\n\n  * \u00bb\n  * SentenceTransformers Documentation\n  * Edit on GitHub\n\n# SentenceTransformers Documentation\u00b6\n\nSentenceTransformers is a Python framework for state-of-the-art sentence, text\nand image embeddings. The initial work is described in our paper Sentence-\nBERT: Sentence Embeddings using Siamese BERT-Networks.\n\nYou can use this framework to compute sentence / text embeddings for more than\n100 languages. These embeddings can then be compared e.g. with cosine-\nsimilarity to find sentences with a similar meaning. This can be useful for\nsemantic textual similarity, semantic search, or paraphrase mining.\n\nThe framework is based on PyTorch and Transformers and offers a large\ncollection of pre-trained models tuned for various tasks. Further, it is easy\nto fine-tune your own models.\n\n# Installation\u00b6\n\nYou can install it using pip:\n\n    \n    \n    pip install -U sentence-transformers\n\nWe recommend Python 3.8 or higher, and at least PyTorch 1.11.0. See\ninstallation for further installation options, especially if you want to use a\nGPU.\n\n# Usage\u00b6\n\nThe usage is as simple as:\n\n    \n    \n    from sentence_transformers import SentenceTransformer model = SentenceTransformer(\"all-MiniLM-L6-v2\") # Our sentences to encode sentences = [ \"This framework generates embeddings for each input sentence\", \"Sentences are passed as a list of string.\", \"The quick brown fox jumps over the lazy dog.\" ] # Sentences are encoded by calling model.encode() embeddings = model.encode(sentences) # Print the embeddings for sentence, embedding in zip(sentences, embeddings): print(\"Sentence:\", sentence) print(\"Embedding:\", embedding) print(\"\")\n\n# Performance\u00b6\n\nOur models are evaluated extensively and achieve state-of-the-art performance\non various tasks. Further, the code is tuned to provide the highest possible\nspeed. Have a look at Pre-Trained Models for an overview of available models\nand the respective performance on different tasks.\n\n# Contact\u00b6\n\nContact person: Tom Aarsen, tom.aarsen@huggingface.co\n\nDon\u2019t hesitate to open an issue on the repository if something is broken (and\nit shouldn\u2019t be) or if you have further questions.\n\nThis repository contains experimental software and is published for the sole\npurpose of giving additional background details on the respective publication.\n\n# Citing & Authors\u00b6\n\nIf you find this repository helpful, feel free to cite our publication\nSentence-BERT: Sentence Embeddings using Siamese BERT-Networks:\n\n>\n>     @inproceedings{reimers-2019-sentence-bert, title = \"Sentence-BERT:\n> Sentence Embeddings using Siamese BERT-Networks\", author = \"Reimers, Nils\n> and Gurevych, Iryna\", booktitle = \"Proceedings of the 2019 Conference on\n> Empirical Methods in Natural Language Processing\", month = \"11\", year =\n> \"2019\", publisher = \"Association for Computational Linguistics\", url =\n> \"https://arxiv.org/abs/1908.10084\", }\n\nIf you use one of the multilingual models, feel free to cite our publication\nMaking Monolingual Sentence Embeddings Multilingual using Knowledge\nDistillation:\n\n>\n>     @inproceedings{reimers-2020-multilingual-sentence-bert, title = \"Making\n> Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\",\n> author = \"Reimers, Nils and Gurevych, Iryna\", booktitle = \"Proceedings of\n> the 2020 Conference on Empirical Methods in Natural Language Processing\",\n> month = \"11\", year = \"2020\", publisher = \"Association for Computational\n> Linguistics\", url = \"https://arxiv.org/abs/2004.09813\", }\n\nIf you use the code for data augmentation, feel free to cite our publication\nAugmented SBERT: Data Augmentation Method for Improving Bi-Encoders for\nPairwise Sentence Scoring Tasks:\n\n>\n>     @inproceedings{thakur-2020-AugSBERT, title = \"Augmented {SBERT}: Data\n> Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring\n> Tasks\", author = \"Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes\n> and Gurevych, Iryna\", booktitle = \"Proceedings of the 2021 Conference of the\n> North American Chapter of the Association for Computational Linguistics:\n> Human Language Technologies\", month = jun, year = \"2021\", address =\n> \"Online\", publisher = \"Association for Computational Linguistics\", url =\n> \"https://www.aclweb.org/anthology/2021.naacl-main.28\", pages = \"296--310\", }\n\nOverview\n\n  * Installation\n\n    * Install SentenceTransformers\n    * Install PyTorch with CUDA support\n  * Quickstart\n\n    * Comparing Sentence Similarities\n    * Pre-Trained Models\n    * Training your own Embeddings\n  * Pretrained Models\n\n    * Model Overview\n    * Semantic Search\n    * Multi-Lingual Models\n    * Image & Text-Models\n    * Other Models\n  * Pretrained Cross-Encoders\n\n    * MS MARCO\n    * SQuAD (QNLI)\n    * STSbenchmark\n    * Quora Duplicate Questions\n    * NLI\n  * Publications\n  * Hugging Face \ud83e\udd17\n\n    * The Hugging Face Hub\n    * Using Hugging Face models\n    * Sharing your models\n    * Sharing your embeddings\n    * Additional resources\n\nUsage\n\n  * Computing Sentence Embeddings\n\n    * Prompt Templates\n    * Input Sequence Length\n    * Storing & Loading Embeddings\n    * Multi-Process / Multi-GPU Encoding\n    * Sentence Embeddings with Transformers\n  * Semantic Textual Similarity\n  * Embedding Quantization\n\n    * Binary Quantization\n    * Scalar (int8) Quantization\n    * Additional extensions\n    * Demo\n    * Try it yourself\n  * Semantic Search\n\n    * Background\n    * Symmetric vs. Asymmetric Semantic Search\n    * Python\n    * util.semantic_search\n    * Speed Optimization\n    * Elasticsearch\n    * Approximate Nearest Neighbor\n    * Retrieve & Re-Rank\n    * Examples\n  * Retrieve & Re-Rank\n\n    * Retrieve & Re-Rank Pipeline\n    * Retrieval: Bi-Encoder\n    * Re-Ranker: Cross-Encoder\n    * Example Scripts\n    * Pre-trained Bi-Encoders (Retrieval)\n    * Pre-trained Cross-Encoders (Re-Ranker)\n  * Clustering\n\n    * k-Means\n    * Agglomerative Clustering\n    * Fast Clustering\n    * Topic Modeling\n  * Paraphrase Mining\n  * Translated Sentence Mining\n\n    * Marging Based Mining\n    * Examples\n  * Cross-Encoders\n\n    * Bi-Encoder vs. Cross-Encoder\n    * When to use Cross- / Bi-Encoders?\n    * Cross-Encoders Usage\n    * Combining Bi- and Cross-Encoders\n    * Training Cross-Encoders\n  * Image Search\n\n    * Installation\n    * Usage\n    * Examples\n\nTraining\n\n  * Training Overview\n\n    * Network Architecture\n    * Creating Networks from Scratch\n    * Training Data\n    * Loss Functions\n    * Evaluators\n    * Loading Custom SentenceTransformer Models\n    * Multitask Training\n    * Adding Special Tokens\n    * Best Transformer Model\n  * Loss Overview\n\n    * Loss modifiers\n    * Distillation\n    * Commonly used Loss Functions\n  * Matryoshka Embeddings\n\n    * Use Cases\n    * Results\n    * Training\n    * Inference\n    * Code Examples\n  * Adaptive Layers\n\n    * Use Cases\n    * Results\n    * Training\n    * Inference\n    * Code Examples\n  * Multilingual-Models\n\n    * Available Pre-trained Models\n    * Usage\n    * Performance\n    * Extend your own models\n    * Training\n    * Data Format\n    * Loading Training Datasets\n    * Sources for Training Data\n    * Evaluation\n    * Citation\n  * Model Distillation\n\n    * Knowledge Distillation\n    * Speed - Performance Trade-Off\n    * Dimensionality Reduction\n    * Quantization\n  * Cross-Encoders\n\n    * Examples\n    * Training CrossEncoders\n  * Augmented SBERT\n\n    * Motivation\n    * Extend to your own datasets\n    * Methodology\n    * Scenario 1: Limited or small annotated datasets (few labeled sentence-pairs)\n    * Scenario 2: No annotated datasets (Only unlabeled sentence-pairs)\n    * Training\n    * Citation\n  * Training Datasets\n\n    * Datasets on the Hugging Face Hub\n\nTraining Examples\n\n  * Semantic Textual Similarity\n\n    * Training data\n    * Loss Function\n  * Natural Language Inference\n\n    * Data\n    * SoftmaxLoss\n    * MultipleNegativesRankingLoss\n  * Paraphrase Data\n\n    * Datasets\n    * Training\n    * Pre-Trained Models\n    * Work in Progress\n  * Quora Duplicate Questions\n\n    * Pretrained Models\n    * Dataset\n    * Usage\n    * Training\n    * MultipleNegativesRankingLoss\n  * MS MARCO\n\n    * Bi-Encoder\n    * Cross-Encoder\n    * Cross-Encoder Knowledge Distillation\n\nUnsupervised Learning\n\n  * Unsupervised Learning\n\n    * TSDAE\n    * SimCSE\n    * CT\n    * CT (In-Batch Negative Sampling)\n    * Masked Language Model (MLM)\n    * GenQ\n    * GPL\n    * Performance Comparison\n  * Domain Adaptation\n\n    * Domain Adaptation vs. Unsupervised Learning\n    * Adaptive Pre-Training\n    * GPL: Generative Pseudo-Labeling\n\nPackage Reference\n\n  * SentenceTransformer\n  * util\n  * quantization\n  * Models\n  * Losses\n  * Evaluation\n  * Datasets\n  * cross_encoder\n\nNext\n\n\u00a9 Copyright 2024, Nils Reimers \u2022 Contact\n\nBuilt with Sphinx using a theme provided by Read the Docs.\n\n", "frontpage": true}
