{"aid": "40025481", "title": "Learning Python with a little help from AI (2023)", "url": "https://about.gitlab.com/blog/2023/11/09/learning-python-with-a-little-help-from-ai-code-suggestions/", "domain": "about.gitlab.com", "votes": 1, "user": "danboarder", "posted_at": "2024-04-13 19:47:15", "comments": 0, "source_title": "Learning Python with a little help from AI", "source_text": "Learning Python with a little help from AI\n\nBlog\n\nAI/ML\n\nLearning Python with a little help from AI\n\nNovember 9, 2023\n\n31 min read\n\n# Learning Python with a little help from AI\n\nUse this guided tutorial, along with GitLab Duo Code Suggestions, to learn a\nnew programming language.\n\nMichael Friedrich\n\nDevSecOps platformtutorialworkflowAI/ML\n\nLearning a new programming language can help broaden your software development\nexpertise, open career opportunities, or create fun challenges. However, it\ncan be difficult to decide on one specific approach to learning a new\nlanguage. Artificial intelligence (AI) can help. In this tutorial, you'll\nlearn how to leverage AI-powered GitLab Duo Code Suggestions for a guided\nexperience in learning the Python programming language with a pratical hands-\non example.\n\n  * Preparations\n\n    * VS Code\n    * Code Suggestions\n  * Learning a new programming language: Python\n\n    * Development environment for Python\n    * Hello, World\n  * Start learning Python with a practical example\n\n    * Define variables and print them\n    * Explore variable types\n  * File I/O: Read and print a log file\n  * Flow control\n\n    * Loops and lists to collect files\n    * Conditionally collect files\n  * Functions\n\n    * Start with a simple log format\n    * String and data structure operations\n    * Parse log files using regular expressions\n    * Advanced log format: auth.log\n    * Parsing more types: Structured logging\n  * Printing results and formatting\n  * Dependency management and continuous verification\n\n    * Pip and pyenv: Bringing structure into Python\n    * Automation: Configure CI/CD pipeline for Python\n  * What is next\n\n    * Async learning exercises\n    * Share your feedback\n\n## Preparations\n\nChoose your preferred and supported IDE, and follow the documentation to\nenable Code Suggestions for GitLab.com SaaS or GitLab self-managed instances.\n\nProgramming languages can require installing the language interpreter command-\nline tools or compilers that generate binaries from source code to build and\nrun the application.\n\nTip: You can also use GitLab Remote Development workspaces to create your own\ncloud development environments, instead of local development environments.\nThis blog post focuses on using VS Code and the GitLab Web IDE.\n\n### VS Code\n\nInstall VS Code on your client, and open it. Navigate to the Extensions menu\nand search for gitlab workflow. Install the GitLab Workflow extension for VS\nCode. VS Code will also detect the programming languages, and offer to install\nadditional plugins for syntax highlighting and development experience. For\nexample, install the Python extension.\n\n### Code Suggestions\n\nFamiliarize yourself with suggestions before actually verifying the\nsuggestions. GitLab Duo Code Suggestions are provided as you type, so you do\nnot need use specific keyboard shortcuts. To accept a code suggestion, press\nthe tab key. Also note that writing new code works more reliably than\nrefactoring existing code. AI is non-deterministic, which means that the same\nsuggestion may not be repeated after deleting the code suggestion. While Code\nSuggestions is in Beta, we are working on improving the accuracy of generated\ncontent overall. Please review the known limitations, as this could affect\nyour learning experience.\n\nTip: The latest release of Code Suggestions supports multiline instructions.\nYou can refine the specifications to your needs to get better suggestions. We\nwill practice this method throughout the blog post.\n\n## Learning a new programming language: Python\n\nNow, let's dig into learning Python, which is one of the supported languages\nin Code Suggestions.\n\nBefore diving into the source code, make sure to set up your development\nenvironment.\n\n### Development environment for Python\n\n  1. Create a new project learn-python-ai in GitLab, and clone the project into your development environment. All code snippets are available in this \"Learn Python with AI\" project.\n\n    \n    \n    git clone https://gitlab.com/NAMESPACE/learn-python-ai.git cd learn-python-ai git status\n\n  2. Install Python and the build toolchain. Example on macOS using Homebrew:\n\n    \n    \n    brew install python\n\n  3. Consider adding a .gitignore file for Python, for example this .gitignore template for Python.\n\nYou are all set to learn Python!\n\n### Hello, World\n\nStart your learning journey in the official documentation, and review the\nlinked resources, for example, the Python tutorial. The library and language\nreference documentation can be helpful, too.\n\nTip: When I touched base with Python in 2005, I did not have many use cases\nexcept as a framework to test Windows 2000 drivers. Later, in 2016, I\nrefreshed my knowledge with the book \"Head First Python, 2nd Edition,\"\nproviding great practical examples for the best learning experience \u2013 two\nweeks later, I could explain the differences between Python 2 and 3. You do\nnot need to worry about Python 2 \u2013 it has been deprecated some years ago, and\nwe will focus only on Python 3 in this blog post. In August 2023, \"Head First\nPython, 3rd Edition\" was published. The book provides a great learning\nresource, along with the exercises shared in this blog post.\n\nCreate a new file hello.py in the root directory of the project and start with\na comment saying # Hello world. Review and accept the suggestion by pressing\nthe tab key and save the file (keyboard shortcut: cmd s).\n\n    \n    \n    # Hello world\n\nCommit the change to the Git repository. In VS Code, use the keyboard shortcut\nctrl shift G, add a commit message, and hit cmd enter to submit.\n\nUse the command palette (cmd shift p) and search for create terminal to open a\nnew terminal. Run the code with the Python interpreter. On macOS, the binary\nfrom Homebrew is called python3, other operating systems and distributions\nmight use python without the version.\n\n    \n    \n    python3 hello.py\n\nTip: Adding code comments in Python starting with the # character before you\nstart writing a function or algorithm will help Code Suggestions with more\ncontext to provide better suggestions. In the example above, we did that with\n# Hello world, and will continue doing so in the next exercises.\n\nAdd hello.py to Git, commit all changes and push them to your GitLab project.\n\n    \n    \n    git add hello.py git commit -avm \"Initialize Python\" git push\n\nThe source code for all exercises in this blog post is available in this\n\"Learn Python with AI\" project.\n\n## Start learning Python with a practical example\n\nThe learning goal in the following sections involves diving into the language\ndatatypes, variables, flow control, and functions. We will also look into file\noperations, string parsing, and data structure operations for printing the\nresults. The exercises will help build a command-line application that reads\ndifferent log formats, works with the data, and provides a summary. This will\nbe the foundation for future projects that fetch logs from REST APIs, and\ninspire more ideas such as rendering images, creating a web server, or adding\nObservability metrics.\n\nAs an experienced admin, you can put the script into production and use real-\nworld log format exmples. Parsing and analyzing logs in stressful production\nincidents can be time-consuming. A local CLI tool is sometimes faster than a\nlog management tool.\n\nLet's get started: Create a new file called log_reader.py in the directory\nroot, add it to Git, and create a Git commit.\n\n### Define variables and print them\n\nAs a first step, we need to define the log files location, and the expected\nfile suffix. Therefore, let's create two variables and print them. Actually,\nask Code Suggestions to do that for you by writing only the code comments and\naccepting the suggestions. Sometimes, you need to experiment with suggestions\nand delete already accepted code blocks. Do not worry \u2013 the quality of the\nsuggestions will improve over time as the model generates better suggestions\nwith more context.\n\n    \n    \n    # Specify the path and file suffix in variables path = '/var/log/' file_suffix = '.log' # Print the variables print(path) print(file_suffix)\n\nNavigate into the VS Code terminal and run the Python script:\n\n    \n    \n    python3 log_reader.py\n\nPython supports many different types in the standard library. Most common\ntypes are: Numeric (int, float, complex), Boolean (True, False), and String\n(str). Data structures include support for lists, tuples, and dictionaries.\n\n### Explore variable types\n\nTo practice different variable types, let's define a limit of log files to\nread as a variable with the integer type.\n\n    \n    \n    # Define log file limit variable log_file_limit = 1024\n\nCreate a Boolean variable that forces to read all files in the directory, no\nmatter the log file suffix.\n\n    \n    \n    # Define boolean variable whether to read all files recursively read_all_files_recursively = True\n\n## File I/O: Read and print a log file\n\nCreate a directory called log-data in your project tree. You can copy all file\nexamples from the log-data directory in the example project.\n\nCreate a new file sample.log with the following content, or any other two\nlines that provide a different message at the end.\n\n    \n    \n    Oct 17 00:00:04 ebpf-chaos systemd[1]: dpkg-db-backup.service: Deactivated successfully. Oct 17 00:00:04 ebpf-chaos systemd[1]: Finished Daily dpkg database backup service.\n\nInstruct Code Suggestions to read the file log-data/sample.log and print the\ncontent.\n\n    \n    \n    # Read the file in log-data/sample.log and print its content with open('log-data/sample.log', 'r') as f: print(f.read())\n\nTip: You will notice the indent here. The with open() as f: statement opens a\nnew scope where f is available as stream. This flow requires indenting )tab)\nthe code block, and perform actions in this scope, calling f.read() to read\nthe file contents, and passing the immediate value as parameter into the\nprint() function.\n\nNavigate into the terminal, and run the script again with python3\nlog_reader.py. You will see the file content shown in the VS Code editor, also\nprinted into the terminal.\n\n## Flow control\n\nReading one log file is not enough \u2013 we want to analyze all files in a given\ndirectory recursively. For the next exercise, we instruct Code Suggestions to\ncreate an index of all files.\n\nPrepare the log-data directory with more example files from the log-data\ndirectory in the example project. The directory tree should look as follows:\n\n    \n    \n    tree log-data \u2500\u256f log-data \u251c\u2500\u2500 sample.log \u2514\u2500\u2500 var \u2514\u2500\u2500 log \u251c\u2500\u2500 auth.log \u251c\u2500\u2500 syslog.log \u2514\u2500\u2500 syslog_structured.log 3 directories, 4 files\n\n### Loops and lists to collect files\n\nModify the path variable to use the value log-data/.\n\n    \n    \n    # Specify the path and file suffix in variables path = 'log-data/' file_suffix = '.log'\n\nTell Code Suggestions to read all file paths in the directory into a list.\nAfter the collection loop, print the list of file paths.\n\n    \n    \n    # Read all file paths in the directory into a list # Print the list of log file paths\n\nAn example of a suggestion could look like this:\n\n    \n    \n    # Read all file paths in the directory into a list import os # Read all file paths in the directory into a list log_files = [] for root, directories, files in os.walk(path): for file in files: if file.endswith(file_suffix): log_files.append(os.path.join(root, file)) # Print the list of log file paths print(log_files)\n\nLet's analyze what happens here: The import os statement is required to bring\nthe os library into the current scope, and we are able to call os.walk()\nlater. The log_files list is initialized as an empty list using empty\nbrackets. os.walk() expects a path as directory tree to start searching for\nfiles. The function returns a generator object, which will walk the directory\ntree recursively. The returned tuples include the collected files, which we\nwant to access in a loop iteraor, using the for loop notation. When the file\nends with the defined suffix, its full path is appended to the list. This is\nan optimized and complex example. Do not worry about understanding everything\njust yet - we will revisit this later.\n\n### Conditionally collect files\n\nModify the instructions and add the Boolean flag evaluation for\nread_all_files_recursively as an input to the glob() function.\n\n    \n    \n    # Read all file paths in the top level directory into a list # If read_all_files_recursively is True, read all files recursively\n    \n    \n    # Read all file paths in the top level directory import os # Read all file paths in the top level directory into a list # If read_all_files_recursively is True, read all files recursively log_files = [] for file in os.listdir(path): if file.endswith(file_suffix): log_files.append(os.path.join(path, file)) if read_all_files_recursively: for root, directories, files in os.walk(path): for file in files: if file.endswith(file_suffix): log_files.append(os.path.join(root, file)) # Print log_files print(log_files)\n\nThe result is not optimal yet because it always executes the first loop, and\noptionally the second loop. This flow leads to duplicated results when the\nscript is executed.\n\n    \n    \n    python3 log_reader.py ['log-data/sample.log', 'log-data/sample.log', 'log-data/var/log/auth.log']\n\nExperiment with Code Suggestions instructions to get a solution for the\nproblem. There are different approaches you can take:\n\n  1. A potential solution is to wrap the source code into an if-then-else block, and move the os.listdir() loop into the else-block.\n\n    \n    \n    if read_all_files_recursively: for root, directories, files in os.walk(path): for file in files: if file.endswith(file_suffix): log_files.append(os.path.join(root, file)) else: for file in os.listdir(path): if file.endswith(file_suffix): log_files.append(os.path.join(path, file))\n\n  2. Alternatively, do not use append() to always add a new list entry, but check if the item exists in the list first.\n\n    \n    \n    for file in os.listdir(path): if file.endswith(file_suffix): # check if the entry exists in the list already if os.path.isfile(os.path.join(path, file)): log_files.append(os.path.join(path, file)) if read_all_files_recursively: for root, directories, files in os.walk(path): for file in files: if file.endswith(file_suffix): # check if the entry exists in the list already if file not in log_files: log_files.append(os.path.join(root, file))\n\n  3. Or, we could eliminate duplicate entries after collecting all items. Python allows converting lists into sets, which hold unique entries. After applying set(), you can again convert the set back into a list. Code Suggestions knows about this possibility, and will help with the comment # Ensure that only unique file paths are in the list\n\n    \n    \n    # Ensure that only unique file paths are in the list log_files = list(set(log_files))\n\n  4. Take a step back and evaluate whether the variable read_all_files_recursively makes sense. Maybe the default behavior should just be reading all files recursively?\n\nTip for testing different paths in VS Code: Select the code blocks, and press\ncmd / on macOS to comment out the code.\n\n## Functions\n\nLet's create a function called parse_log_file that parses a log file, and\nreturns the extracted data. We will define the expected log format and columns\nto extract, following the syslog format specification. There are different log\nformat types and also customized formats by developers that need to be taken\ninto account \u2013 exercise for later.\n\n### Start with a simple log format\n\nInspect a running Linux VM, or use the following example log file example for\nadditional implementation.\n\n    \n    \n    less /var/log/syslog | grep -v docker Oct 17 00:00:04 ebpf-chaos systemd[1]: Starting Daily dpkg database backup service... Oct 17 00:00:04 ebpf-chaos systemd[1]: Starting Rotate log files... Oct 17 00:00:04 ebpf-chaos systemd[1]: dpkg-db-backup.service: Deactivated successfully. Oct 17 00:00:04 ebpf-chaos systemd[1]: Finished Daily dpkg database backup service. Oct 17 00:00:04 ebpf-chaos systemd[1]: logrotate.service: Deactivated successfully. Oct 17 00:00:04 ebpf-chaos systemd[1]: Finished Rotate log files. Oct 17 00:17:01 ebpf-chaos CRON[727495]: (root) CMD ( cd / && run-parts --report /etc/cron.hourly)\n\nWe can create an algorithm to split each log line by whitespaces, and then\njoin the results again. Let's ask Code Suggestions for help.\n\n    \n    \n    # Split log line \"Oct 17 00:00:04 ebpf-chaos systemd[1]: Finished Rotate log files.\" by whitespaces and save in a list log_line = \"Oct 17 00:00:04 ebpf-chaos systemd[1]: Finished Rotate log files.\" log_line_split = log_line.split(\" \") print(log_line_split)\n\nRun the script again to verify the result.\n\n    \n    \n    python3 log_reader.py ['Oct', '17', '00:00:04', 'ebpf-chaos', 'systemd[1]:', 'Finished', 'Rotate', 'log', 'files.']\n\nThe first three items are part of the datetime string, followed by the host,\nservice, and remaining log message items. Let's practice string operations in\nPython as the next step.\n\n### String and data structure operations\n\nLet's ask Code Suggestions for help with learning to join strings, and perform\nlist operations.\n\n  1. Join the first three items with a whitespace again.\n  2. Keep host and service.\n  3. Join the remaining variable item count into a string, separated with whitespaces, again.\n  4. Store the identified column keys, and their respective values in a new data structure: dictionary.\n\n    \n    \n    python3 log_reader.py # Array ['Oct', '17', '00:00:04', 'ebpf-chaos', 'systemd[1]:', 'Finished', 'Rotate', 'log', 'files.'] # Dictionary {'datetime': 'Oct 17 00:00:04', 'host': 'ebpf-chaos', 'service': 'systemd[1]:', 'message': ' ebpf-chaos systemd[1]: Finished Rotate log files.'}\n\nA working suggestion can look like the following:\n\n    \n    \n    # Initialize results dictionary with empty values for datetime, host, service, message # Loop over log line split # Join the first three list items as date string # Item 4: host # Item 5: service # Join the remaining items into a string, separated with whitespaces # Print the results after the loop results = {'datetime': '', 'host': '', 'service': '', 'message': ''} for item in log_line_split: if results['datetime'] == '': results['datetime'] = ' '.join(log_line_split[0:3]) elif results['host'] == '': results['host'] = log_line_split[3] elif results['service'] == '': results['service'] = log_line_split[4] else: results['message'] += ' ' + item print(results)\n\nThe suggested algorithm loops over all log line items, and applies the same\noperation for the first three items. log_line_split[0:3] extracts a slice of\nthree items into a new list. Calling join() on a separator character and\npassing the array as an argument joins the items into a string. The algorithm\ncontinues to check for not initialized values for host (Item 4) and service\n(Item 5)and concludes with the remaining list items appended into the message\nstring. To be honest, I would have used a slightly different algorithm, but it\nis a great learning curve to see other algorithms, and ways to implement them.\nPractice with different instructions, and data structures, and continue\nprinting the data sets.\n\nTip: If you need to terminate a script early, you can use sys.exit(). The\nremaining code will not be executed.\n\n    \n    \n    import sys sys.exit(1)\n\nImagine doing these operations for different log formats, and message types \u2013\nit can get complicated and error-prone very quickly. Maybe there is another\napproach.\n\n### Parse log files using regular expressions\n\nThere are different syslog format RFCs \u2013 RFC 3164 is obsolete but still found\nin the wild as default configuration (matching the pattern above), while RFC\n5424 is more modern, including datetime with timezone information. Parsing\nthis format can be tricky, so let's ask Code Suggestions for advice.\n\nIn some cases, the suggestions include regular expressions. They might not\nmatch immediately, making the code more complex to debug, with trial and\nerrors. A good standalone resource to text and explain regular expressions is\nregex101.com.\n\nTip: You can skip diving deep into regular expressions using the following\ncode snippet as a quick cheat. The next step involves instructing Code\nSuggestions to use these log patterns, and help us extract all valuable\ncolumns.\n\n    \n    \n    # Define the syslog log format regex in a dictionary # Add entries for RFC3164, RFC5424 regex_log_pattern = { 'rfc3164': '([A-Z][a-z][a-z]\\s{1,2}\\d{1,2}\\s\\d{2}[:]\\d{2}[:]\\d{2})\\s([\\w][\\w\\d\\.@-]*)\\s(.*)$', 'rfc5424': '(?:(\\d{4}[-]\\d{2}[-]\\d{2}[T]\\d{2}[:]\\d{2}[:]\\d{2}(?:\\.\\d{1,6})?(?:[+-]\\d{2}[:]\\d{2}|Z)?)|-)\\s(?:([\\w][\\w\\d\\.@-]*)|-)\\s(.*)$;' }\n\nWe know what the function should do, and its input parameters \u2013 the file name,\nand a log pattern to match. The log lines should be split by this regular\nexpression, returning a key-value dictionary for each log line. The function\nshould return a list of dictionaries.\n\n    \n    \n    # Create a function that parses a log file # Input parameter: file path # Match log line against regex_log_pattern # Return the results as dictionary list: log line, pattern, extracted columns\n\nRemember the indent for opening a new scope? The same applies for functions in\nPython. The def identifier requires a function name, and a list of parameters,\nfollowed by an opening colon. The next lines of code require the indent. VS\nCode will help with live-linting wrong indent, before the script execution\nfails, or the CI/CD pipelines.\n\nContinue with Code Suggestions \u2013 it might already know that you want to parse\nall log files, and parse them using the newly created function.\n\nA full working example can look like this:\n\n    \n    \n    import os # Specify the path and file suffix in variables path = 'log-data/' file_suffix = '.log' # Read all file paths in the directory into a list log_files = [] for root, directories, files in os.walk(path): for file in files: if file.endswith(file_suffix): log_files.append(os.path.join(root, file)) # Define the syslog log format regex in a dictionary # Add entries for RFC3164, RFC5424 regex_log_pattern = { 'rfc3164': '([A-Z][a-z][a-z]\\s{1,2}\\d{1,2}\\s\\d{2}[:]\\d{2}[:]\\d{2})\\s([\\w][\\w\\d\\.@-]*)\\s(.*)$', 'rfc5424': '(?:(\\d{4}[-]\\d{2}[-]\\d{2}[T]\\d{2}[:]\\d{2}[:]\\d{2}(?:\\.\\d{1,6})?(?:[+-]\\d{2}[:]\\d{2}|Z)?)|-)\\s(?:([\\w][\\w\\d\\.@-]*)|-)\\s(.*)$;' } # Create a function that parses a log file # Input parameter: file path # Match log line against regex_log_pattern # Return the results as dictionary list: log line, pattern name, extracted columns import re def parse_log_file(file_path): # Read the log file with open(file_path, 'r') as f: log_lines = f.readlines() # Create a list to store the results results = [] # Iterate over the log lines for log_line in log_lines: # Match the log line against the regex pattern for pattern_name, pattern in regex_log_pattern.items(): match = re.match(pattern, log_line) # If the log line matches the pattern, add the results to the list if match: extracted_columns = match.groups() results.append({ 'log_line': log_line, 'pattern_name': pattern_name, 'extracted_columns': extracted_columns, 'source_file': file_path }) # Return the results return results # Parse all files and print results for log_file in log_files: results = parse_log_file(log_file) print(results)\n\nLet's unpack what the parse_log_file() function does:\n\n  1. Opens the file from file_path parameter.\n  2. Reads all lines into a new variable log_lines.\n  3. Creates a results list to store all items.\n  4. Iterates over the log lines.\n  5. Matches against all regex patterns configured in regex_log_pattern.\n  6. If a match is found, extracts the matching column values.\n  7. Creates a results item, including the values for the keys log_line, pattern_name, extracted_colums, source_file.\n  8. Appends the results item to the results list.\n  9. Returns the results list.\n\nThere are different variations to this \u2013 especially for the returned result\ndata structure. For this specific case, log lines come as list already. Adding\na dictionary object instead of a raw log line allows function callers to\nextract the desired information in the next step. Once a working example has\nbeen implemented, you can refactor the code later, too.\n\n### Advanced log format: auth.log\n\nParsing the syslog on a Linux distribution might not unveil the necessary data\nto analyze. On a virtual machine that exposes port 22 (SSH) to the world, the\nauthentication log is much more interesting \u2013 plenty of bots and malicious\nactors testing default password combinations and often brute force attacks.\n\nThe following snippet from /var/log/auth.log on one of my private servers\nshows the authentication log format and the random attempts from bots using\ndifferent usernames, etc.\n\n    \n    \n    Oct 15 00:00:19 ebpf-chaos sshd[3967944]: Failed password for invalid user ubuntu from 93.254.246.194 port 48840 ssh2 Oct 15 00:00:20 ebpf-chaos sshd[3967916]: Failed password for root from 180.101.88.227 port 44397 ssh2 Oct 15 00:00:21 ebpf-chaos sshd[3967944]: Received disconnect from 93.254.246.194 port 48840:11: Bye Bye [preauth] Oct 15 00:00:21 ebpf-chaos sshd[3967944]: Disconnected from invalid user ubuntu 93.254.246.194 port 48840 [preauth] Oct 15 00:00:24 ebpf-chaos sshd[3967916]: Failed password for root from 180.101.88.227 port 44397 ssh2 Oct 15 00:00:25 ebpf-chaos sshd[3967916]: Received disconnect from 180.101.88.227 port 44397:11: [preauth] Oct 15 00:00:25 ebpf-chaos sshd[3967916]: Disconnected from authenticating user root 180.101.88.227 port 44397 [preauth] Oct 15 00:00:25 ebpf-chaos sshd[3967916]: PAM 2 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=180.101.88.227 user=root Oct 15 00:00:25 ebpf-chaos sshd[3967998]: Invalid user teamspeak from 185.218.20.10 port 33436\n\nTip for intrusion prevention: Add a firewall setup, and use fail2ban to block\ninvalid auth logins.\n\nThe next exercise is to extend the logic to understand the free form log\nmessage parts, for example Failed password for invalid user ubuntu from\n93.254.246.194 port 48840 ssh2. The task is to store the data in an optional\ndictionary with key value pairs.\n\nCreate a new function that takes the previously parsed log line results as\ninput, and specifically parses the last list item for each line.\n\n  1. Count the number of Failed password and Invalid user messages.\n  2. Return the results with count, log file, pattern\n\nA working suggestion can look like the following code:\n\n    \n    \n    # Create a function that parses a log file message from the last extracted_columns entry # Input: Parsed log lines results list # Loop over all log lines in the list, and extract the last list item as message # Count failure strings in the message: Failed password, Invalid user # Return the results if failure count greater 0: log_file, count, failure string def parse_log_file_message(results): failure_results = [] # Iterate over the log lines for result in results: # Extract the message from the last list item message = result['extracted_columns'][-1] # Count the number of failure strings in the message failure_count = message.count('Failed password') + message.count('Invalid user') # If the failure count is greater than 0, add the results to the list if failure_count > 0: failure_results.append({ 'log_file': result['source_file'], 'count': failure_count, 'failure_string': message }) # Return the results return failure_results # Parse all files and print results for log_file in log_files: results = parse_log_file(log_file) failure_results = parse_log_file_message(results) print(failure_results)\n\nThe algorithm follows the previous implementations: First, create a results\narray to store matching data. Then, iterate over the already parsed log_lines\nin the list. Each log line contains the extracted_columns key, which holds the\nfree-form message string at the end. The next step is to call the string\nobject function count() to count how many times a given character sequence is\ncontained in a string. The returned numbers are added up to the failure_count\nvariable. If it is greater than zero, the result is added to the results list,\nincluding the log_file, count and failure_string key-value pairs. After\nreturning the parsed log message results, loop through all log files, parse\nthem, and print the results again.\n\nExecute the script to inspect the detected matches. Note that the data\nstructure can be optimized in future learning steps.\n\n    \n    \n    python3 log_reader.py [{'log_file': 'log-data/var/log/auth.log', 'count': 1, 'failure_string': 'sshd[3967944]: Failed password for invalid user ubuntu from 93.254.246.194 port 48840 ssh2'}, {'log_file': 'log-data/var/log/auth.log', 'count': 1, 'failure_string': 'sshd[3967916]: Failed password for root from 180.101.88.227 port 44397 ssh2'}, {'log_file': 'log-data/var/log/auth.log', 'count': 1, 'failure_string': 'sshd[3967916]: Failed password for root from 180.101.88.227 port 44397 ssh2'}, {'log_file': 'log-data/var/log/auth.log', 'count': 1, 'failure_string': 'sshd[3967998]: Invalid user teamspeak from 185.218.20.10 port 33436'}, {'log_file': 'log-data/var/log/auth.log', 'count': 1, 'failure_string': 'sshd[3967998]: Failed password for invalid user teamspeak from 185.218.20.10 port 33436 ssh2'}, {'log_file': 'log-data/var/log/auth.log', 'count': 1, 'failure_string': 'sshd[3968077]: Invalid user mcserver from 218.211.33.146 port 50950'}]\n\n### Parsing more types: Structured logging\n\nApplication developers can use the structured logging format to help machine\nparsers to extract the key value pairs. Prometheus provides this information\nin the following structure in syslog:\n\n    \n    \n    Oct 17 19:00:10 ebpf-chaos prometheus[594]: ts=2023-10-17T19:00:10.425Z caller=compact.go:519 level=info component=tsdb m sg=\"write block\" mint=1697558404661 maxt=1697565600000 ulid=01HCZG4ZX51GTH8H7PVBYDF4N6 duration=148.675854ms Oct 17 19:00:10 ebpf-chaos prometheus[594]: ts=2023-10-17T19:00:10.464Z caller=head.go:1213 level=info component=tsdb msg =\"Head GC completed\" caller=truncateMemory duration=6.845245ms Oct 17 19:00:10 ebpf-chaos prometheus[594]: ts=2023-10-17T19:00:10.467Z caller=checkpoint.go:100 level=info component=tsd b msg=\"Creating checkpoint\" from_segment=2308 to_segment=2309 mint=1697565600000 Oct 17 19:00:10 ebpf-chaos prometheus[594]: ts=2023-10-17T19:00:10.517Z caller=head.go:1185 level=info component=tsdb msg =\"WAL checkpoint complete\" first=2308 last=2309 duration=50.052621ms\n\nThis format is easier to parse for scripts, because the message part can be\nsplit by whitespaces, and the assignment character =. Strings that contain\nwhitespaces are guaranteed to be enclosed with quotes. The downside is that\nnot all programming language libraries provide ready-to-use structured logging\nlibraries, making it harder for developers to adopt this format.\n\nPractice following the previous example to parse the auth.log format with\nadditional information. Tell Code Suggestions that you are expecting\nstructured logging format with key-value pairs, and which returned data\nstructure would be great:\n\n    \n    \n    # Create a function that parses a log file message from the last extracted_columns entry # Input: Parsed log lines results list # Loop over all log lines in the list, and extract the last list item as message # Parse structured logging key-value pairs into a dictionary # Return results: log_file, dictionary\n\n### Printing results and formatting\n\nMany of the examples used the print() statement to print the content on the\nterminal. Python objects in the standard library support text representation,\nand for some types it makes more sense (string, numbers), others cannot\nprovide much details (functions, etc.).\n\nYou can also pretty-print almost any data structure (lists, sets,\ndictionaries) in Python. The JSON library can format data structures in a\nreadable format, and use a given spaces indent to draw the JSON structure on\nthe terminal. Note that we use the import statement here to bring libraries\ninto the current scope, and access their methods, for example json.dumps.\n\n    \n    \n    import json print(json.dumps(structured_results, indent=4))\n\nPractice with modifying the existing source code, and replace the code\nsnippets where appropriate. Alternatively, create a new function that\nimplements pretty printing.\n\n    \n    \n    # Create a pretty print function with indent 4\n\nThis idea works in a similar fashion with creating your own logger\nfunctions...but we have to stop learning and take a break. Before we conclude\nthe first blog post in the learning series, let's ensure that CI/CD and\ndependencies are set up properly for future exercises and async practice.\n\n## Dependency management and continuous verification\n\n### Pip and pyenv: Bringing structure into Python\n\nDependencies can be managed in the requirements.txt file, including optional\nversion dependencies. Using requirements.txt file also has the advantage of\nbeing the single source of truth for local development environments and\nrunning continuous builds with GitLab CI/CD. They can use the same\ninstallation command:\n\n    \n    \n    pip install -r requirements.txt\n\nSome Linux distributions do not install the pip package manager by default,\nfor example, Ubuntu/Debian require to install the python3-pip package.\n\nYou can manage different virtual environments using venv. This workflow can be\nbeneficial to install Python dependencies into the virtual environment,\ninstead of globally into the OS path which might break on upgrades.\n\n    \n    \n    pip install virtualenv virtualenv venv source venv/bin/activate\n\n### Automation: Configure CI/CD pipeline for Python\n\nThe CI/CD pipeline should continuously lint, test, and build the code. You can\nmimic the steps from the local development, and add testing more environments\nand versions:\n\n  1. Lint the source code and check for formatting errors. The example uses Pyflakes, a mature linter, and Ruff, a fast linter written in Rust.\n  2. Cache dependencies installed using the pip package manager, following the documentation for Python caching in GitLab CI/CD. This saves time and resources on repeated CI/CD pipeline runs.\n  3. Use parallel matrix builds to test different Python versions, based on the available container images on Docker Hub and their tags.\n\n    \n    \n    stages: - lint - test default: image: python:latest cache: # Pip's cache doesn't store the python packages paths: # https://pip.pypa.io/en/stable/topics/caching/ - .cache/pip before_script: - python -V # Print out python version for debugging - pip install virtualenv - virtualenv venv - source venv/bin/activate variables: # Change pip's cache directory to be inside the project directory since we can only cache local items. PIP_CACHE_DIR: \"$CI_PROJECT_DIR/.cache/pip\" # lint template .lint-tmpl: script: - echo \"Linting Python version $VERSION\" parallel: matrix: - VERSION: ['3.9', '3.10', '3.11', '3.12'] # https://hub.docker.com/_/python # Lint, using Pyflakes: https://pypi.org/project/pyflakes/ lint-pyflakes: extends: [.lint-tmpl] script: - pip install -r requirements.txt - find . -not -path './venv' -type f -name '*.py' -exec sh -c 'pyflakes {}' \\; # Lint, using Ruff (Rust): https://docs.astral.sh/ruff/ lint-ruff: extends: [.lint-tmpl] script: - pip install -r requirements.txt - ruff .\n\n## What is next\n\nFun fact: GitLab Duo Code Suggestions also helped writing this blog post in VS\nCode, knowing about the context. In the screenshot, I just wanted to add a tip\nabout regex101, and GitLab Duo already knew.\n\nIn an upcoming blog, we will look into advanced learning examples with more\npractical (log) filtering and parallel operations, how to fetch logs from API\nendpoints (CI/CD job logs for example), and more data analytics and\nobservability. Until then, here are a few recommendations for practicing\nasync.\n\n### Async learning exercises\n\n  * Implement the missing log_file_limit variable check.\n  * Print a summary of the results in Markdown, not only JSON format.\n  * Extend the script to accept a search filter as environment variable. Print/count only filtered results.\n  * Extend the script to accept a date range. It might require parsing the datetime column in a time object to compare the range.\n  * Inspect a GitLab CI/CD pipeline job log, and download the raw format. Extend the log parser to parse this specific format, and print a summary.\n\n### Share your feedback\n\nWhich programming language are you learning or considering learning? Start a\nnew topic on our community forum or Discord and share your experience.\n\nWhen you use GitLab Duo Code Suggestions, please share your thoughts and\nfeedback in the feedback issue.\n\nShare this article\n\n## Sign up for GitLab\u2019s newsletter\n\nAll fields required\n\n## More to explore\n\nView all blog posts\n\nAI/ML\n\n#### Introducing the GitLab AI Transparency Center\n\nAI/ML\n\n#### 10 best practices for using AI-powered GitLab Duo Chat\n\nAI/ML\n\n#### How to put generative AI to work in your DevSecOps environment\n\n### We want to hear from you\n\nEnjoyed reading this blog post or have questions or feedback? Share your\nthoughts by creating a new topic in the GitLab community forum. Share your\nfeedback\n\n#### Ready to get started?\n\nSee what your team could do with a unified DevSecOps Platform.\n\nGet free trial\n\nNew to GitLab and not sure where to start?\n\nGet started guide\n\nLearn about what GitLab can do for your team\n\nTalk to an expert\n\n\u00ae\n\n### Platform\n\n  * DevSecOps platform\n\n### Pricing\n\n  * View plans\n  * Why Premium?\n  * Why Ultimate?\n\n### Solutions\n\n  * Digital transformation\n  * Security & Compliance\n  * Automated software delivery\n  * Agile Delivery\n  * Cloud transformation\n  * SCM\n  * CI/CD\n  * Value stream management\n  * GitOps\n  * Enterprise\n  * Small business\n  * Startups\n  * Nonprofits\n  * Public sector\n  * Education\n  * Financial services\n\n### Resources\n\n  * Install\n  * Quick start guides\n  * Learn\n  * Product documentation\n  * Blog\n  * Customer success stories\n  * Remote\n  * TeamOps\n  * Community\n  * Forum\n  * Events\n  * Partners\n\n### Company\n\n  * About\n  * Jobs\n  * Leadership\n  * Team\n  * Handbook\n  * Investor relations\n  * Environmental, social and governance (ESG)\n  * Diversity, inclusion and belonging (DIB)\n  * Trust Center\n  * AI Transparency Center\n  * Newsletter\n  * Press\n\n### Contact us\n\n  * Contact an expert\n  * Get help\n  * Customer portal\n  * Status\n  * Terms of use\n  * Privacy statement (Updated 3/19/24)\n\nGit is a trademark of Software Freedom Conservancy and our use of 'GitLab' is\nunder license\n\nView page source Please contribute\n\n\u00a9 2024 GitLab B.V.\n\n## This website uses cookies\n\nWe use cookies to make our websites and services operate correctly, to\nunderstand how visitors engage with us and to improve our product and\nmarketing efforts. See our cookie policy for more information.Cookie Policy\n\n## Privacy Preference Center\n\n  * ### Your Privacy\n\n  * ### Strictly Necessary Cookies\n\n  * ### Functionality Cookies\n\n  * ### Performance and Analytics Cookies\n\n  * ### Targeting and Advertising Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your\nbrowser, mostly in the form of cookies. This information might be about you,\nyour preferences or your device and is mostly used to make the site work as\nyou expect it to. The information does not usually directly identify you, but\nit can give you a more personalized web experience. Because we respect your\nright to privacy, you can choose not to allow some types of cookies. Click on\nthe different category headings to find out more and change our default\nsettings. However, blocking some types of cookies may impact your experience\nof the site and the services we are able to offer. Cookie Policy\n\nUser ID: 498e51ee-7c80-40fc-a611-a8016c87772c\n\nThis User ID will be used as a unique identifier while storing and accessing\nyour preferences for future.\n\nTimestamp: --\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched\noff in our systems. They are usually only set in response to actions made by\nyou which amount to a request for services, such as setting your privacy\npreferences, enabling you to securely log into the site, filling in forms, or\nusing the customer checkout. GitLab processes any personal data collected\nthrough these cookies on the basis of our legitimate interest.\n\n#### Functionality Cookies\n\nThese cookies enable helpful but non-essential website functions that improve\nyour website experience. By recognizing you when you return to our website,\nthey may, for example, allow us to personalize our content for you or remember\nyour preferences. If you do not allow these cookies then some or all of these\nservices may not function properly. GitLab processes any personal data\ncollected through these cookies on the basis of your consent\n\n#### Performance and Analytics Cookies\n\nThese cookies allow us and our third-party service providers to recognize and\ncount the number of visitors on our websites and to see how visitors move\naround our websites when they are using it. This helps us improve our products\nand ensures that users can easily find what they need on our websites. These\ncookies usually generate aggregate statistics that are not associated with an\nindividual. To the extent any personal data is collected through these\ncookies, GitLab processes that data on the basis of your consent.\n\n#### Targeting and Advertising Cookies\n\nThese cookies enable different advertising related functions. They may allow\nus to record information about your visit to our websites, such as pages\nvisited, links followed, and videos viewed so we can make our websites and the\nadvertising displayed on it more relevant to your interests. They may be set\nthrough our website by our advertising partners. They may be used by those\ncompanies to build a profile of your interests and show you relevant\nadvertisements on other websites. GitLab processes any personal data collected\nthrough these cookies on the basis of your consent.\n\n### Cookie List\n\nConsent Leg.Interest\n\nlabel\n\nlabel\n\nlabel\n\nlabel\n\nClose\n\nsuggested results\n\n", "frontpage": false}
