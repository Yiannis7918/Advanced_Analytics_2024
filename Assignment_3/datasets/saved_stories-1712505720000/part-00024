{"aid": "39959380", "title": "The lifecycle of a code AI completion", "url": "https://sourcegraph.com/blog/the-lifecycle-of-a-code-ai-completion", "domain": "sourcegraph.com", "votes": 1, "user": "tosh", "posted_at": "2024-04-07 08:55:03", "comments": 0, "source_title": "The lifecycle of a code AI completion", "source_text": "The lifecycle of a code AI completion\n\n  * Consent\n  * Details\n  * [#IABV2SETTINGS#]\n  * About\n\n## This website uses cookies \ud83c\udf6a\n\nWe use cookies to offer you a better browsing experience, analyze site traffic\nand personalize content. Learn about how we use cookies and how you can\ncontrol them in our Cookie Policy. By using our site, you consent to our use\nof cookies.\n\nShow details\n\n  * Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies.\n\n    * Cloudflare\n\n1\n\nLearn more about this provider\n\ntsNecessary for the website security.\n\nExpiry: SessionType: HTML\n\n    * Cookiebot\n\n1\n\nLearn more about this provider\n\n1.gifUsed to count the number of sessions to the website, necessary for\noptimizing CMP product delivery.\n\nExpiry: SessionType: Pixel\n\n    * Github\n\n2\n\nLearn more about this provider\n\nAI_bufferUsed in context with the \"AI_sentBuffer\" in order to limit the number\nof data-server-updates (Azure). This synergy also allows the website to detect\nany duplicate data-server-updates.\n\nExpiry: SessionType: HTML\n\nAI_sentBufferUsed in context with the \"AI_buffer\" in order to limit the number\nof data-server-updates (Azure). This synergy also allows the website to detect\nany duplicate data-server-updates.\n\nExpiry: SessionType: HTML\n\n    * Google\n\n1\n\nLearn more about this provider\n\ntest_cookieUsed to check if the user's browser supports cookies.\n\nExpiry: 1 dayType: HTTP\n\n    * LinkedIn\n\n3\n\nLearn more about this provider\n\nli_gcStores the user's cookie consent state for the current domain\n\nExpiry: 180 daysType: HTTP\n\nbscookie [x2]This cookie is used to identify the visitor through an\napplication. This allows the visitor to login to a website through their\nLinkedIn application for example.\n\nExpiry: 1 yearType: HTTP\n\n    * Loom\n\n4\n\nLearn more about this provider\n\nloom_anon_commentAllows user to leave comments on the website's video-content\nanonymously.\n\nExpiry: 400 daysType: HTTP\n\nreplayStorage#chunkUploadsControl cookie used in connection to the website\u2019s\nContent Delivery Network (CDN).\n\nExpiry: PersistentType: IDB\n\nreplayStorage#eventsControl cookie used in connection to the website\u2019s Content\nDelivery Network (CDN).\n\nExpiry: PersistentType: IDB\n\nreplayStorage#pendingCapturesControl cookie used in connection to the\nwebsite\u2019s Content Delivery Network (CDN).\n\nExpiry: PersistentType: IDB\n\n    * New Relic\n\n1\n\nLearn more about this provider\n\nJSESSIONIDPreserves users states across page requests.\n\nExpiry: SessionType: HTTP\n\n    * Swiftype\n\n1\n\nLearn more about this provider\n\nlogged_inRegisters whether the user is logged in. This allows the website\nowner to make parts of the website inaccessible, based on the user's log-in\nstatus.\n\nExpiry: 1 yearType: HTTP\n\n    * Twitch\n\n3\n\nLearn more about this provider\n\nserver_session_idRegisters which server-cluster is serving the visitor. This\nis used in context with load balancing, in order to optimize user experience.\n\nExpiry: SessionType: HTTP\n\nunique_idPreserves users states across page requests.\n\nExpiry: 13 monthsType: HTTP\n\nunique_id_durableRegisters whether the user is logged in. This allows the\nwebsite owner to make parts of the website inaccessible, based on the user's\nlog-in status.\n\nExpiry: 13 monthsType: HTTP\n\n    * YouTube\n\n2\n\nLearn more about this provider\n\nCONSENT [x2]Used to detect if the visitor has accepted the marketing category\nin the cookie banner. This cookie is necessary for GDPR-compliance of the\nwebsite.\n\nExpiry: 2 yearsType: HTTP\n\n    * about.sourcegraph.com sourcegraph.com\n\n2\n\ncid [x2]This cookie is necessary for making credit card transactions on the\nwebsite. The service is provided by Stripe.com which allows online\ntransactions without storing any credit card information.\n\nExpiry: 1 dayType: HTTP\n\n    * about.sourcegraph.com zenodo.org\n\n2\n\nSESS# [x2]Preserves users states across page requests.\n\nExpiry: 1 dayType: HTTP\n\n    * cookiebot.com handbook.sourcegraph.com\n\n5\n\nCookieConsent [x5]Stores the user's cookie consent state for the current\ndomain\n\nExpiry: 1 yearType: HTTP\n\n    * gitlab.com hsforms.com sourcegraph.com zoominfo.com\n\n5\n\n_cfuvid [x5]This cookie is a part of the services provided by Cloudflare -\nIncluding load-balancing, deliverance of website content and serving DNS\nconnection for website operators.\n\nExpiry: SessionType: HTTP\n\n    * link.excalidraw.com track.hubspot.com radar.cloudflare.com sgdev.org sourcegraph.com vimeo.com zoominfo.com\n\n7\n\n__cf_bm [x7]This cookie is used to distinguish between humans and bots. This\nis beneficial for the website, in order to make valid reports on the use of\ntheir website.\n\nExpiry: 1 dayType: HTTP\n\n    * s.swiftypecdn.com github.com\n\n3\n\n_gh_sess [x3]Preserves users states across page requests.\n\nExpiry: SessionType: HTTP\n\n    * sourcegraph.com\n\n1\n\nthemeThis cookie is part of a bundle of cookies which serve the purpose of\ncontent delivery and presentation. The cookies keep the correct state of font,\nblog/picture sliders, color themes and other website settings.\n\nExpiry: PersistentType: HTML\n\n    * static.twitchcdn.net\n\n2\n\nsession_storage_unique_idThis cookie is necessary for the login function on\nthe website.\n\nExpiry: SessionType: HTML\n\napi_tokenNecessary for the implementation of video-content on the website.\n\nExpiry: 13 monthsType: HTTP\n\n    * zenodo.org\n\n1\n\nhex (32)Used to manage server calls to the website's backend systems.\n\nExpiry: SessionType: HTTP\n\n  * Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n    * Cookiebot\n\n1\n\nLearn more about this provider\n\nCookieConsentBulkSetting-#Enables cookie consent across multiple websites\n\nExpiry: PersistentType: HTML\n\n    * Vimeo\n\n2\n\nLearn more about this provider\n\nplayerSaves the user's preferences when playing embedded videos from Vimeo.\n\nExpiry: 1 yearType: HTTP\n\nsync_activeContains data on visitor's video-content preferences - This allows\nthe website to remember parameters such as preferred volume or video quality.\nThe service is provided by Vimeo.com.\n\nExpiry: PersistentType: HTML\n\n    * link.excalidraw.com\n\n1\n\nfirebaseLocalStorageDb#firebaseLocalStorageFacilitates the notification\nfunction within the chatbox, allowing the website\u2019s support team to notify the\nuser, when a reply has been given in the chatbox.\n\nExpiry: PersistentType: IDB\n\n    * static.twitchcdn.net\n\n1\n\nlocal_copy_unique_idContains data on visitor's video-content preferences -\nThis allows the website to remember parameters such as preferred volume or\nvideo quality. The service is provided by Vimeo.com.\n\nExpiry: PersistentType: HTML\n\n  * Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously.\n\n    * Google\n\n13\n\nLearn more about this provider\n\n_ga [x4]Registers a unique ID that is used to generate statistical data on how\nthe visitor uses the website.\n\nExpiry: 2 yearsType: HTTP\n\n_gat [x2]Used by Google Analytics to throttle request rate\n\nExpiry: 1 dayType: HTTP\n\n_gid [x2]Registers a unique ID that is used to generate statistical data on\nhow the visitor uses the website.\n\nExpiry: 1 dayType: HTTP\n\ncollectUsed to send data to Google Analytics about the visitor's device and\nbehavior. Tracks the visitor across devices and marketing channels.\n\nExpiry: SessionType: Pixel\n\n_ga_# [x3]Used by Google Analytics to collect data on the number of times a\nuser has visited the website as well as dates for the first and most recent\nvisit.\n\nExpiry: 2 yearsType: HTTP\n\ntdRegisters statistical data on users' behaviour on the website. Used for\ninternal analytics by the website operator.\n\nExpiry: SessionType: Pixel\n\n    * LinkedIn\n\n2\n\nLearn more about this provider\n\nln_orRegisters statistical data on users' behaviour on the website. Used for\ninternal analytics by the website operator.\n\nExpiry: 1 dayType: HTTP\n\nAnalyticsSyncHistoryUsed in connection with data-synchronization with third-\nparty analysis service.\n\nExpiry: 30 daysType: HTTP\n\n    * Loom\n\n7\n\nLearn more about this provider\n\nsentryReplaySessionRegisters data on visitors' website-behaviour. This is used\nfor internal analysis and website optimization.\n\nExpiry: SessionType: HTML\n\nviewer_session_dataUsed in connection with End User Experience Monitoring\n(EUEM). The cookie collects information on application/device performance,\nwhich allows support/website staff to optimize their products and services.\n\nExpiry: PersistentType: HTML\n\nuserleap.idsThis cookie is set to make split-tests on the website, which\noptimizes the website's relevance towards the visitor \u2013 the cookie can also be\nset to improve the visitor's experience on a website.\n\nExpiry: PersistentType: HTML\n\najs_anonymous_idThis cookie is used to count how many times a website has been\nvisited by different visitors - this is done by assigning the visitor an ID,\nso the visitor does not get registered twice.\n\nExpiry: PersistentType: HTML\n\n__Host-psifi.analyticsTraceRegisters statistical data on users' behaviour on\nthe website. Used for internal analytics by the website operator.\n\nExpiry: 1 dayType: HTTP\n\n__Host-psifi.analyticsTraceV2Registers statistical data on users' behaviour on\nthe website. Used for internal analytics by the website operator.\n\nExpiry: 1 dayType: HTTP\n\ndd_cookie_test_#Registers data on visitors' website-behaviour. This is used\nfor internal analysis and website optimization.\n\nExpiry: 1 dayType: HTTP\n\n    * Swiftype\n\n1\n\nLearn more about this provider\n\n_octoPending\n\nExpiry: 1 yearType: HTTP\n\n    * Twitch\n\n1\n\nLearn more about this provider\n\nga_#Registers a unique ID that is used to generate statistical data on how the\nvisitor uses the website.\n\nExpiry: 1 dayType: HTTP\n\n    * Vimeo\n\n1\n\nLearn more about this provider\n\nvuidCollects data on the user's visits to the website, such as which pages\nhave been read.\n\nExpiry: 2 yearsType: HTTP\n\n    * about.sourcegraph.com\n\n1\n\noriginalReferrerDetects how the user reached the website by registering their\nlast URL-address.\n\nExpiry: 1 yearType: HTTP\n\n    * cdn.loom.com cl.ly\n\n3\n\najs_anonymous_id [x3]This cookie is used to identify a specific visitor - this\ninformation is used to identify the number of specific visitors on a website.\n\nExpiry: 1 yearType: HTTP\n\n    * js.hs-banner.com\n\n4\n\n__hsscIdentifies if the cookie data needs to be updated in the visitor's\nbrowser.\n\nExpiry: 1 dayType: HTTP\n\n__hssrcUsed to recognise the visitor's browser upon reentry on the website.\n\nExpiry: SessionType: HTTP\n\n__hstcSets a unique ID for the session. This allows the website to obtain data\non visitor behaviour for statistical purposes.\n\nExpiry: 180 daysType: HTTP\n\nhubspotutkSets a unique ID for the session. This allows the website to obtain\ndata on visitor behaviour for statistical purposes.\n\nExpiry: 180 daysType: HTTP\n\n    * sourcegraph.com\n\n2\n\nziwsSessionCollects statistics on the user's visits to the website, such as\nthe number of visits, average time spent on the website and what pages have\nbeen read.\n\nExpiry: SessionType: HTML\n\nziwsSessionIdCollects statistics on the user's visits to the website, such as\nthe number of visits, average time spent on the website and what pages have\nbeen read.\n\nExpiry: SessionType: HTML\n\n    * static.twitchcdn.net\n\n1\n\nreferrer_urlDetects how the user reached the website by registering their last\nURL-address.\n\nExpiry: 1 dayType: HTTP\n\n  * Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\n\n    * 6sc.co\n\n1\n\n6suuidRegisters user behaviour and navigation on the website, and any\ninteraction with active campaigns. This is used for optimizing advertisement\nand for efficient retargeting.\n\nExpiry: 400 daysType: HTTP\n\n    * Appnexus\n\n1\n\nLearn more about this provider\n\nreceive-cookie-deprecationPending\n\nExpiry: 400 daysType: HTTP\n\n    * Clearbit\n\n1\n\nLearn more about this provider\n\nrack.sessionUsed in context with video-advertisement. The cookie limits the\nnumber of times a user is shown the same advertisement. The cookie is also\nused to ensure relevance of the video-advertisement to the specific user.\n\nExpiry: 7 daysType: HTTP\n\n    * Google\n\n9\n\nLearn more about this provider\n\n__Host-GAPSCollects data on the user's visits to the website, such as the\nnumber of visits, average time spent on the website and what pages have been\nloaded with the purpose of generating reports for optimising the website\ncontent.\n\nExpiry: 2 yearsType: HTTP\n\nIDEUsed by Google DoubleClick to register and report the website user's\nactions after viewing or clicking one of the advertiser's ads with the purpose\nof measuring the efficacy of an ad and to present targeted ads to the user.\n\nExpiry: 1 yearType: HTTP\n\npagead/landing [x3]Collects data on visitor behaviour from multiple websites,\nin order to present more relevant advertisement - This also allows the website\nto limit the number of times that they are shown the same advertisement.\n\nExpiry: SessionType: Pixel\n\nads/ga-audiencesUsed by Google AdWords to re-engage visitors that are likely\nto convert to customers based on the visitor's online behaviour across\nwebsites.\n\nExpiry: SessionType: Pixel\n\nNIDRegisters a unique ID that identifies a returning user's device. The ID is\nused for targeted ads.\n\nExpiry: 6 monthsType: HTTP\n\npagead/1p-user-list/#Tracks if the user has shown interest in specific\nproducts or events across multiple websites and detects how the user navigates\nbetween sites. This is used for measurement of advertisement efforts and\nfacilitates payment of referral-fees between websites.\n\nExpiry: SessionType: Pixel\n\n_gcl_auUsed by Google AdSense for experimenting with advertisement efficiency\nacross websites using their services.\n\nExpiry: 3 monthsType: HTTP\n\n    * Hubspot\n\n1\n\nLearn more about this provider\n\n__ptq.gifSends data to the marketing platform Hubspot about the visitor's\ndevice and behaviour. Tracks the visitor across devices and marketing\nchannels.\n\nExpiry: SessionType: Pixel\n\n    * LinkedIn\n\n4\n\nLearn more about this provider\n\nbcookieUsed by the social networking service, LinkedIn, for tracking the use\nof embedded services.\n\nExpiry: 1 yearType: HTTP\n\nli_sugrCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: 3 monthsType: HTTP\n\nlidcUsed by the social networking service, LinkedIn, for tracking the use of\nembedded services.\n\nExpiry: 1 dayType: HTTP\n\nUserMatchHistoryEnsures visitor browsing-security by preventing cross-site\nrequest forgery. This cookie is essential for the security of the website and\nvisitor.\n\nExpiry: 30 daysType: HTTP\n\n    * Loom\n\n5\n\nLearn more about this provider\n\najs_user_idCollects data on visitors' preferences and behaviour on the website\n- This information is used make content and advertisement more relevant to the\nspecific visitor.\n\nExpiry: PersistentType: HTML\n\n__tld__ [x2]Used to track visitors on multiple websites, in order to present\nrelevant advertisement based on the visitor's preferences.\n\nExpiry: SessionType: HTTP\n\najs_user_idThis cookie is used to collect data on the visitor's behavior on\nthe website - this information can be used to assign the visitor to a visitor\nsegment, based on common preferences.\n\nExpiry: SessionType: HTTP\n\nloom_referral_videoUsed to detect referrals in relation to website's embedded\nvideo-player.\n\nExpiry: SessionType: HTTP\n\n    * Microsoft\n\n12\n\nLearn more about this provider\n\n_uetsid [x2]Used to track visitors on multiple websites, in order to present\nrelevant advertisement based on the visitor's preferences.\n\nExpiry: PersistentType: HTML\n\n_uetsid_exp [x2]Contains the expiry-date for the cookie with corresponding\nname.\n\nExpiry: PersistentType: HTML\n\n_uetvid [x2]Used to track visitors on multiple websites, in order to present\nrelevant advertisement based on the visitor's preferences.\n\nExpiry: PersistentType: HTML\n\n_uetvid_exp [x2]Contains the expiry-date for the cookie with corresponding\nname.\n\nExpiry: PersistentType: HTML\n\nMSPTCThis cookie registers data on the visitor. The information is used to\noptimize advertisement relevance.\n\nExpiry: 1 yearType: HTTP\n\nMUIDUsed widely by Microsoft as a unique user ID. The cookie enables user\ntracking by synchronising the ID across many Microsoft domains.\n\nExpiry: 1 yearType: HTTP\n\n_uetsidCollects data on visitor behaviour from multiple websites, in order to\npresent more relevant advertisement - This also allows the website to limit\nthe number of times that they are shown the same advertisement.\n\nExpiry: 1 dayType: HTTP\n\n_uetvidUsed to track visitors on multiple websites, in order to present\nrelevant advertisement based on the visitor's preferences.\n\nExpiry: 1 yearType: HTTP\n\n    * Reddit\n\n2\n\nLearn more about this provider\n\nrp.gifNecessary for the implementation of the Reddit.com's share-button\nfunction.\n\nExpiry: SessionType: Pixel\n\n_rdt_uuidUsed to track visitors on multiple websites, in order to present\nrelevant advertisement based on the visitor's preferences.\n\nExpiry: 3 monthsType: HTTP\n\n    * StackAdapt\n\n12\n\nLearn more about this provider\n\nsa-user-id [x3]Used to track visitors on multiple websites, in order to\npresent relevant advertisement based on the visitor's preferences.\n\nExpiry: 1 yearType: HTTP\n\nsa-user-id-v2 [x3]Used to track visitors on multiple websites, in order to\npresent relevant advertisement based on the visitor's preferences.\n\nExpiry: 1 yearType: HTTP\n\nsa-user-idUsed to track visitors on multiple websites, in order to present\nrelevant advertisement based on the visitor's preferences.\n\nExpiry: PersistentType: HTML\n\nsa-user-id-v2Used to track visitors on multiple websites, in order to present\nrelevant advertisement based on the visitor's preferences.\n\nExpiry: PersistentType: HTML\n\nsa-user-id-v3 [x3]Pending\n\nExpiry: 1 yearType: HTTP\n\nsa-user-id-v3Pending\n\nExpiry: PersistentType: HTML\n\n    * Twitch\n\n4\n\nLearn more about this provider\n\nKP_UIDzThis cookie registers data on the visitor. The information is used to\noptimize advertisement relevance.\n\nExpiry: 1 dayType: HTTP\n\nKP_UIDZThis cookie registers data on the visitor. The information is used to\noptimize advertisement relevance.\n\nExpiry: 1 dayType: HTTP\n\nKP_UIDz-ssnNecessary for the implementation and functionality of Twitch video-\ncontent on the website.\n\nExpiry: 1 dayType: HTTP\n\nKP_UIDZ-ssnNecessary for the implementation and functionality of Twitch video-\ncontent on the website.\n\nExpiry: 1 dayType: HTTP\n\n    * YouTube\n\n37\n\nLearn more about this provider\n\nLAST_RESULT_ENTRY_KEY [x2]Used to track user\u2019s interaction with embedded\ncontent.\n\nExpiry: SessionType: HTTP\n\nnextId [x2]Used to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nrequests [x2]Used to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nyt.innertube::nextId [x2]Registers a unique ID to keep statistics of what\nvideos from YouTube the user has seen.\n\nExpiry: PersistentType: HTML\n\nyt.innertube::requests [x2]Registers a unique ID to keep statistics of what\nvideos from YouTube the user has seen.\n\nExpiry: PersistentType: HTML\n\nytidb::LAST_RESULT_ENTRY_KEY [x2]Used to track user\u2019s interaction with\nembedded content.\n\nExpiry: PersistentType: HTML\n\nYtIdbMeta#databases [x2]Used to track user\u2019s interaction with embedded\ncontent.\n\nExpiry: PersistentType: IDB\n\nyt-remote-cast-available [x2]Stores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-cast-installed [x2]Stores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-connected-devices [x2]Stores the user's video player preferences\nusing embedded YouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-device-id [x2]Stores the user's video player preferences using\nembedded YouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-fast-check-period [x2]Stores the user's video player preferences\nusing embedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-app [x2]Stores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-name [x2]Stores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\n#-#Pending\n\nExpiry: SessionType: HTML\n\niU5q-!O9@$Registers a unique ID to keep statistics of what videos from YouTube\nthe user has seen.\n\nExpiry: SessionType: HTML\n\nLogsDatabaseV2:V#||LogsRequestsStorePending\n\nExpiry: PersistentType: IDB\n\nremote_sidNecessary for the implementation and functionality of YouTube video-\ncontent on the website.\n\nExpiry: SessionType: HTTP\n\nServiceWorkerLogsDatabase#SWHealthLogNecessary for the implementation and\nfunctionality of YouTube video-content on the website.\n\nExpiry: PersistentType: IDB\n\nTESTCOOKIESENABLEDUsed to track user\u2019s interaction with embedded content.\n\nExpiry: 1 dayType: HTTP\n\nVISITOR_INFO1_LIVETries to estimate the users' bandwidth on pages with\nintegrated YouTube videos.\n\nExpiry: 180 daysType: HTTP\n\nVISITOR_PRIVACY_METADATAStores the user's cookie consent state for the current\ndomain\n\nExpiry: 180 daysType: HTTP\n\nYSCRegisters a unique ID to keep statistics of what videos from YouTube the\nuser has seen.\n\nExpiry: SessionType: HTTP\n\n    * about.sourcegraph.com\n\n1\n\n#GUID#23Pending\n\nExpiry: 1 yearType: HTTP\n\n    * b.6sc.co\n\n1\n\nv1/beacon/img.gifUsed in context with Account-Based-Marketing (ABM). The\ncookie registers data such as IP-addresses, time spent on the website and page\nrequests for the visit. This is used for retargeting of multiple users rooting\nfrom the same IP-addresses. ABM usually facilitates B2B marketing purposes.\n\nExpiry: SessionType: Pixel\n\n    * cl.ly share.zight.com\n\n2\n\n_session_id [x2]Stores visitors' navigation by registering landing pages -\nThis allows the website to present relevant products and/or measure their\nadvertisement efficiency on other websites.\n\nExpiry: 14 daysType: HTTP\n\n    * j.6sc.co\n\n5\n\n_6senseCompanyDetailsUsed in context with Account-Based-Marketing (ABM). The\ncookie registers data such as IP-addresses, time spent on the website and page\nrequests for the visit. This is used for retargeting of multiple users rooting\nfrom the same IP-addresses. ABM usually facilitates B2B marketing purposes.\n\nExpiry: PersistentType: HTML\n\n_an_uidPresents the user with relevant content and advertisement. The service\nis provided by third-party advertisement hubs, which facilitate real-time\nbidding for advertisers.\n\nExpiry: 7 daysType: HTTP\n\n_gd_sessionCollects visitor data related to the user's visits to the website,\nsuch as the number of visits, average time spent on the website and what pages\nhave been loaded, with the purpose of displaying targeted ads.\n\nExpiry: 1 dayType: HTTP\n\n_gd_svisitorCollects visitor data related to the user's visits to the website,\nsuch as the number of visits, average time spent on the website and what pages\nhave been loaded, with the purpose of displaying targeted ads.\n\nExpiry: 2 yearsType: HTTP\n\n_gd_visitorCollects visitor data related to the user's visits to the website,\nsuch as the number of visits, average time spent on the website and what pages\nhave been loaded, with the purpose of displaying targeted ads.\n\nExpiry: 2 yearsType: HTTP\n\n    * static.twitchcdn.net\n\n5\n\nlocal_storage_app_session_idNecessary for the implementation and functionality\nof Twitch video-content on the website.\n\nExpiry: PersistentType: HTML\n\nlocal_storage_device_idNecessary for the implementation and functionality of\nTwitch video-content on the website.\n\nExpiry: PersistentType: HTML\n\nsentry_device_idRequired for the embedded media player to operate.\n\nExpiry: PersistentType: HTML\n\nsession_storage_last_visited_twitch_urlNecessary for the implementation and\nfunctionality of Twitch video-content on the website.\n\nExpiry: SessionType: HTML\n\nexperiment_overridesCollects data on the user across websites - This data is\nused to make advertisement more relevant.\n\nExpiry: 13 monthsType: HTTP\n\n    * tribl.io\n\n1\n\nti_Used in context with Account-Based-Marketing (ABM). The cookie registers\ndata such as IP-addresses, time spent on the website and page requests for the\nvisit. This is used for retargeting of multiple users rooting from the same\nIP-addresses. ABM usually facilitates B2B marketing purposes.\n\nExpiry: 400 daysType: HTTP\n\n  * Unclassified cookies are cookies that we are in the process of classifying, together with the providers of individual cookies.\n\n    * Azure\n\n1\n\nLearn more about this provider\n\nVstsSessionPending\n\nExpiry: 1 yearType: HTTP\n\n    * CloudApp\n\n1\n\nLearn more about this provider\n\nca_anonymous_idPending\n\nExpiry: SessionType: HTTP\n\n    * Fewer &amp; Faster\n\n1\n\nLearn more about this provider\n\n_secure_speakerd_sessionPending\n\nExpiry: 14 daysType: HTTP\n\n    * Github\n\n2\n\nLearn more about this provider\n\naccess.token.keyPending\n\nExpiry: PersistentType: HTML\n\nnavigationsPending\n\nExpiry: SessionType: HTML\n\n    * Google\n\n4\n\nLearn more about this provider\n\nsg_datetimePending\n\nExpiry: 30 daysType: HTTP\n\nsg_website_referrerPending\n\nExpiry: 30 daysType: HTTP\n\nsourcegraphRecentSourceUrlPending\n\nExpiry: 400 daysType: HTTP\n\nsourcegraphSourceUrlPending\n\nExpiry: 400 daysType: HTTP\n\n    * LinkedIn\n\n1\n\nLearn more about this provider\n\nli_adsIdPending\n\nExpiry: PersistentType: HTML\n\n    * about.sourcegraph.com\n\n4\n\nsourcegraphCohortIdPending\n\nExpiry: 1 yearType: HTTP\n\nsourcegraphDeviceIdPending\n\nExpiry: 1 yearType: HTTP\n\nsourcegraphInsertIdPending\n\nExpiry: 1 yearType: HTTP\n\nsourcegraphSessionIdPending\n\nExpiry: 1 dayType: HTTP\n\n    * about.sourcegraph.com sourcegraph.com\n\n2\n\nlandingSource [x2]Pending\n\nExpiry: SessionType: HTML\n\n    * ai.uni-bremen.de\n\n3\n\nDokuWikiPending\n\nExpiry: SessionType: HTTP\n\nDW7fa065a06cb74b536c124cfbe56ac6d3Pending\n\nExpiry: SessionType: HTTP\n\nDWd6fcb57a725757b22fe830cccebe05e6Pending\n\nExpiry: SessionType: HTTP\n\n    * app.fossa.io\n\n1\n\nfossa.sidPending\n\nExpiry: 7 daysType: HTTP\n\n    * circleci.com\n\n1\n\nring-sessionPending\n\nExpiry: 1 yearType: HTTP\n\n    * cl.ly share.zight.com\n\n2\n\nfirst_visit [x2]Pending\n\nExpiry: SessionType: HTTP\n\n    * gitlab.com\n\n1\n\n_gitlab_sessionPending\n\nExpiry: 1 dayType: HTTP\n\n    * js.zi-scripts.com\n\n2\n\n_zitok [x2]Pending\n\nExpiry: 1 yearType: HTTP\n\n    * prow.k8s.io\n\n1\n\n_gorilla_csrfPending\n\nExpiry: 1 dayType: HTTP\n\n    * sourcegraph.com\n\n9\n\nunifiedScriptVerifiedPending\n\nExpiry: SessionType: HTML\n\napollo-cache-persist-anonymousPending\n\nExpiry: PersistentType: HTML\n\ncody.chat.history:anonymousPending\n\nExpiry: PersistentType: HTML\n\ndeveloperSettingsDialogPending\n\nExpiry: PersistentType: HTML\n\ngraphiql:docExplorerOpenPending\n\nExpiry: PersistentType: HTML\n\ngraphiql:queryPending\n\nExpiry: PersistentType: HTML\n\ngraphiql:tabStatePending\n\nExpiry: PersistentType: HTML\n\ntemporarySettingsPending\n\nExpiry: PersistentType: HTML\n\nuser-history:anonymousPending\n\nExpiry: PersistentType: HTML\n\n    * static.twitchcdn.net\n\n1\n\ncontent-classification-labels-acknowledgedPending\n\nExpiry: PersistentType: HTML\n\n    * tribl.io\n\n1\n\n_t.gifPending\n\nExpiry: SessionType: Pixel\n\nCross-domain consent5 Your consent applies to the following domains:\n\nList of domains your consent applies to:\n\nhandbook.sourcegraph.com\n\nchooseyoursearch.com\n\ninfo.sourcegraph.com\n\nsourcegraph.com\n\nabout.sourcegraph.com\n\nCookie declaration last updated on 3/16/24 by Cookiebot\n\n## [#IABV2_TITLE#]\n\n[#IABV2_BODY_INTRO#]\n\n[#IABV2_BODY_LEGITIMATE_INTEREST_INTRO#]\n\n[#IABV2_BODY_PREFERENCE_INTRO#]\n\n[#IABV2_BODY_PURPOSES_INTRO#]\n\n[#IABV2_BODY_PURPOSES#]\n\n[#IABV2_BODY_FEATURES_INTRO#]\n\n[#IABV2_BODY_FEATURES#]\n\n[#IABV2_BODY_PARTNERS_INTRO#]\n\n[#IABV2_BODY_PARTNERS#]\n\nWe use cookies to personalize content and ads, to provide social media\nfeatures and to analyze our traffic. We also share information about your use\nof our site with our social media, advertising and analytics partners who may\ncombine it with other information that you\u2019ve provided to them or that they\u2019ve\ncollected from your use of their services. You consent to our cookies if you\ncontinue to use our website.\n\nCookies are small text files that can be used by websites to make a user's\nexperience more efficient.\n\nThe law states that we can store cookies on your device if they are strictly\nnecessary for the operation of this site. For all other types of cookies we\nneed your permission.\n\nThis site uses different types of cookies. Some cookies are placed by third\nparty services that appear on our pages.\n\nYou can at any time change or withdraw your consent from the Cookie\nDeclaration on our website.\n\nLearn more about who we are, how you can contact us and how we process\npersonal data in our Privacy Policy.\n\nPlease state your consent ID and date when you contact us regarding your\nconsent.\n\nPowered by Cookiebot by Usercentrics\n\n# The lifecycle of a code AI completion\n\nPhilipp Spiess October 31, 2023\n\nGenerative AI, whether for code, text, images, or other use cases, appears as\na magic black box to many users. Users typically navigate to a website,\ninstall an app, or set up an extension and start seeing the results of the AI\ntool. But, have you ever wondered what goes into this magic black box or how\nit really works?\n\nIn this post, we want to demystify what goes into a code AI completion for\nCody, our code AI assistant that knows your entire codebase. Leveraging a\nLarge Language Model (LLM) to generate a code AI response is fairly trivial,\nbut doing so in a production-grade application that serves many different use\ncases, coding languages, workflows, and other variables while achieving a\nhigh-level of completion acceptance and developer happiness is a whole other\nthing. We\u2019ll cover the importance of the underlying LLM but also expand the\nimplementation to a fully featured AI engineering system that features various\npre and post processing steps, discuss the role of context and how to retrieve\nit, and more as we explore the lifecycle of a code AI completion. Let\u2019s dive\nin!\n\n## Code completions 101\n\nIn its minimal form, a code autocomplete request takes the current code inside\nthe editor and asks an LLM to complete it. You can do this with ChatGPT too!\nConsider the following example:\n\n    \n    \n    // sort.js function bubbleSort(array) { | }\n\nFrom this limited file we already have a lot of information to work with:\n\n  * The cursor is inside the function body so the user is most likely going to continue writing code at this position\n  * We know that the file is called sort.js\n  * The code before the cursor (which we refer to as prefix) has strong hints about what code we want to write\n  * The code after the cursor (postfix) is empty and consists only of a closing bracket.\n\nWe can easily convert this into a prompt for ChatGPT and have it generate the\nimplementation for us:\nhttps://chat.openai.com/share/27aeb581-2d68-4ac8-94eb-3c64af91f0c6\n\nCongratulations, you just wrote a code completion AI!\n\nIn fact, this is pretty much how we started out with Cody autocomplete back in\nMarch! All you need to make this into a full-blown VS Code extension, is to\nimplement this API interface:\n\n    \n    \n    /** * Provides inline completion items for the given position and document. * If inline completions are enabled, this method will be called whenever the user stopped typing. * It will also be called when the user explicitly triggers inline completions or explicitly asks for the next or previous inline completion. * In that case, all available inline completions should be returned. * `context.triggerKind` can be used to distinguish between these scenarios. * * @param document The document inline completions are requested for. * @param position The position inline completions are requested for. * @param context A context object with additional information. * @param token A cancellation token. * @return An array of completion items or a thenable that resolves to an array of completion items. */ provideInlineCompletionItems(document: TextDocument, position: Position, context: InlineCompletionContext, token: CancellationToken): ProviderResult<InlineCompletionItem[] | InlineCompletionList>;\n\nHowever, our trivial implementation has a few shortcomings: In a real world\napplication, this would be too slow, it would not have understanding of the\nright syntactic boundaries, and it would lack contextual awareness of your\ncodebase. The interaction with the LLM is important, but only a small piece of\na much larger AI engineering system. Let\u2019s dig a bit deeper and see what it\ntakes to make Cody, a production ready AI application.\n\n## How to get great AI completions\n\nBefore we dive into the specifics, let\u2019s outline a few basics principles for\ngetting great AI completions. In fact, the principles are the same as if\nyou\u2019re asking someone new on the team to do great work! In order to do their\nwork, the new dev (or the AI assistant) needs to have an understanding of the\ntask at hand. We refer to this knowledge as context. The more context you\nhave, the more effective you\u2019ll be in a project.\n\nFor code completions, we can use the current code file as the basis for our\ncontext. When writing code, you start by pointing the cursor at a specific\nposition inside the document. From that position, we can define the prefix as\nthe text before and the suffix as the text below that cursor. When coding,\nyour lowest level task is to insert code between the prefix and the suffix.\n\nHowever, a developer will also look at other files in the project and try to\nunderstand relationships between them: Some of this extended context might\ncome from introduction material during their onboarding, their own mental\nmodel, existing code and API interfaces, and so much more.\n\nTo get great AI completions, we need to think along the same lines and must be\nable to extract relevant context for the current problem. Modern LLMs already\ncome with a lot of context from the data they were trained on. They know the\nprogramming language and are familiar with a lot of the open source libraries\nthat are commonly associated with it. So our task is to fill in the gaps and\nadd context that is specific to the project at hand.\n\nIn AI engineering, we call this process RAG (retrieval augmented generation).\nWe retrieve specific knowledge, like code snippets and documentation, from any\nexternal knowledge source (which may or may not be included in the model\ntraining set) and use it to guide the generative process. If I point you to an\narbitrary file in an arbitrary codebase and ask you to \u201cwrite some code\u201d,\nyou\u2019d also appreciate some context about that codebase. RAG is about\nautomating this process.\n\nWhen working on code completions inside the editor, we can use APIs available\nin the editor to get as much context as possible. For example: What repo are\nyou working on? What are other files that you have recently edited? Are you\ntrying to write a docstring, implement a function body, or work out the right\narguments for a method call?\n\nWith Cody, we use a two step process for retrieving context. We first have a\nplanning step that is packed with heuristics to categorize the type of code\ncompletion that is required and then, based on that, retrieve context that\nworks best for the problem at hand.\n\nOnce we have a collection of context, we build a prompt that is optimized for\nthe underlying LLM. In our ChatGPT example we would ask it to \u201ccomplete the\nfollowing code\u201d. Then it\u2019s up for the GPUs to roll the dice and give you some\ntext back. This step is usually referred to as the generation.\n\nLastly, we want to do some processing on the generated content. In the ChatGPT\nexample above, there is a lot of text that we do not want in the text editor,\nfor example. We refer to this step as post-processing.\n\nTo summarize, every Cody code completion currently goes through these four\nsteps:\n\n## Planning\n\nThe first step is all about preparing the best possible execution plan for the\nautocomplete request. We must decide on what context we believe would work\nbest and what parameters to use for the generation process. At the moment, all\nof these steps are rule based (that is, they do not invoke any AI system yet\nand are usually very fast to complete) and based on heuristics that we\u2019ve\ngathered over time. You can compare this a lot to a database that does a query\nplanning step before it does any of the heavier work. It allows us to divide\nthe problem space into different categories and optimize for them\nindividually, instead of trying to create a one-size-fits-all solution.\n\nLet\u2019s dive into some of the heuristics we currently use in production during\nthis step:\n\n### Single-line vs. Multi-line requests\n\nThe first learning we had is that there are situations where a user would only\nexpect the current line to be completed and situations where users are willing\nto wait longer in order to receive a completion that fills out a whole\nfunction definition. To detect which type of request is needed, we use a\nmixture of language heuristics (by looking at indentation and specific\nsymbols) and precise language information (guided by Tree-sitter, more on that\nlater).\n\nMulti-line requests run through the same pipeline but have additional logic\nduring post-processing to make sure the response fits well into the existing\ndocument. One interesting learning was that if a user is willing to wait\nlonger for a multi-line request, it usually is worth it to increase latency\nslightly in favor of quality. For our production setup this means we use a\nmore complex language model for multi-line completions than we do for single-\nline completions.\n\nBecause of the language-specific nature of this heuristic, we generally do not\nsupport multi-line completions for all languages. However, we\u2019re always happy\nto extend our list of supported languages and, since Cody is open-source, you\ncan also contribute and improve the list.\n\n### Syntactic triggers\n\nThe position of the cursor relative to elements of code like the beginning of\nan expression or the current block scope offers insight into the user's intent\nand desired completion behavior. The first version of Cody used regular\nexpressions to approximate these syntactic clues, but there is only so much\ninformation that you can extract from plaintext pattern matching. The current\nversion of Cody uses a great tool to obtain concrete syntax trees for each\nfile: Tree-sitter.\n\nAt Sourcegraph, we are long-time users of Tree-sitter for improving our code\nsearch experience and it felt natural to extend the usage for our autocomplete\npipeline. More specifically we use custom-built WASM bindings to parse the\ncurrent document state and use that to trigger syntax-specific branches\u2013For\nexample, to detect if the cursor is currently within a comment.\n\nTree-sitter is great for this use case because it is extremely fast, supports\nincremental parsing (so after a document is parsed, changes can be applied\nwith very low latency) and it\u2019s robustness allows us to use it even when the\ndocument is currently being worked on and contains syntax errors.\n\nDuring the planning step, we use Tree-sitter to categorize the autocomplete\nrequest into different syntactic actions like implementing a function body,\nwriting a docstring, or implementing a method call. We can then use this\ninformation to focus on different types of contexts or modify the parameters\nfor the generation phase.\n\n### Suggestion widget interaction\n\nIf you\u2019ve worked with VS Code you\u2019re probably familiar with the suggestion\nwidget. It pops up when you\u2019re trying to call a method on a class and will\nlist you all of the methods that the class implements and is powered by the\nmighty IntelliSense system. In the context of autocomplete, VS Code gives us\nsome hints to create better interoperability between AI suggestions and the\nsuggest widget as part of the InlineCompletionContext, the range of the\ndocument that is going to be replaced with the suggestion and the currently\nselected suggestion.\n\nUsing the suggest widget to steer the LLM results is absolutely magical:\n\n## Retrieval\n\nDepending on the model being used, there are varying limitations for how long\nsuch a context window can be but regardless of these limitations, finding the\nright code examples and prompting them in the correct way will have a huge\nimpact on the quality of the autocomplete result, as outlined above.\n\nOne of the biggest constraints on the retrieval implementation is latency:\nRetrieval happens before any of the generation work can start and is thus in\nthe hot path of the life cycle. We generally want the end to end latency (that\nis, the time between the keystroke and the autocomplete becoming visible) to\nbe as fast as possible, definitely under one second and since this must\naccount for network latency and inference speed, there\u2019s not a lot of room for\nexpensive retrieval.\n\nFrom the first version on, Cody\u2019s main retrieval mechanism was to look at\neditor context. This takes into account other tabs you have open or files that\nyou recently looked at. The result of such retrieval processes are a few\nexample code snippets that are sorted by relevance. We currently use a sliding\nwindow Jaccard similarity search to do that: We take a few lines above the\ncurrent cursor position as the \u201creference\u201d and then start a sliding window\nover relevant files to find the best possible matches.\n\nIn order to reduce client CPU pressure, we limit the files to the most\nrelevant ones. These are usually the files you looked at very recently and are\ngenerally written in the same programming language.\n\nOver the past few months we\u2019ve experimented with various other context\nimprovements. One thing that seemed very promising was to reuse our existing\nembeddings index that we already use for other Cody features. We\u2019ve started to\nmove away from this approach as we\u2019re working on improving the accuracy of\nembeddings responses and removing the need to do extensive caching to make\nthis work.\n\nEditor context is only one possible source for information though and having\none of the world\u2019s largest code graphs, there\u2019s a lot more that we can do. One\noverarching problem that we\u2019re working on right now is how do we rank\ninformation from different sources and only include the relevant information\n(we have learned from internal experimentation that adding irrelevant context\ncan make the response quality worse).\n\n## Generation\n\nAs we move to the next stage, let\u2019s dive into the heart of the autocomplete\nprocess: the Large Language Model (LLM). The LLM is responsible for taking the\nprompt and generating a completion that is relevant, accurate, and fast.\n\nSourcegraph has been a vivid early adopter of Anthropic\u2019s Claude. Because of\nthis, our Autocomplete journey started with early experiments in prompting\nClaude Instant (for its faster response times) to create code completions\nsimilar to the ChatGPT example we explored above. We quickly learned that a\nsimple prompt resulted in a lot of frustration for our users:\n\n  * No Fill in the Middle support: Without adding information from the document suffix, the LLM would often repeat code that is already in the next line. In the terminology of LLMs, this use case is often described as fill in the middle (FITM) or infilling, as the problem is to insert text in the middle of existing text.\n  * Latency: We measured that a significant number of requests came back with no response at all (so the LLM decided to terminate the request early).\n  * Quality: Slight variants in the prompt could have a huge impact on quality. E.g. When we ran an experiment with a prompt that tried to improve the accuracy of comments, we learned that mentioning the term comment caused an increase in comments being generated rather than actual code.\n\nOver the past months, we have made a lot of improvements to the Claude Instant\nprompt, let me highlight some in particular:\n\n  * The first major update to the prompt changed three things which caused the quality, and more specifically the number of no responses to improve dramatically:\n\n    * We moved from markdown backtick tags for code segments to XML tags as suggested by Anthropic. Since Claude has been fine tuned to pay special attention to the structure created by XML tags, we found an improvement in response quality with this easy change. It pays off to read the docs!\n\n    * We found that including whitespace at the end of the prompt would cause significantly worse responses. In the bubbleSort example above, we would end the prompt in all of the whitespace that lead to the cursor so it would end in \\n followed by four spaces. In real world applications, the indentation would often be higher resulting in even more whitespace. We achieved a significant reduction of empty responses by trimming the prompt and accounting for the whitespace differences in post-processing.\n\n    * We also started to lay words in Claude\u2019s mouth by omitting information in the initial question and then leading with this in the assistant prompt. An example for this could be a completion for these two lines:\n        \n                const array = [1, 2, 3]; console.log(|\n\nWhich would translate into a prompt like this:\n\n        \n                Human: Complete the following code <code> const array = [1, 2, 3]; </code> Assistant: Sure! Here is the completion: <code> console.log(\n\n  * The second major update was to add support for Fill in the Middle: Instead of only quoting the prefix, we also added information about the code after the cursor into the prompt. This was not trivial since simple implementations often caused the LLM to simply repeat from the suffix without generating new code. We ended up using a combination of XML tags and the extended reasoning capabilities of Claude Instant 1.2 to our advantage here.\n\n### The strive for faster latencies\n\nA general purpose model like Claude Instant is great as it allows you to\nextend the capabilities of the system by writing better instructions. There\nis, however, a catch: These advanced reasoning capabilities require a much\nlarger model to work and as a result, end-to-end latencies (as measured from\nthe keystroke until the completion is visible) have not been great which\nsignificantly impacted the UX of our service. This was also reflected in a lot\nof early adopter feedback so it\u2019s become an obsession for us to try and\nimprove the status quo.\n\nLatencies apply throughout every step in the autocomplete lifecycle but the\ngeneration part is the definitely the slowest since it also requires routing a\nrequest to the backend and the LLM provider. In our quest to improve the UX,\nwe had to be pedantic about every step in this process. This, of course, meant\nthat we had to add tracing to every step in the pipeline and then critically\nthink about how we can improve all of these segments. Oh and what interesting\nthings you\u2019ll find when you do that!\n\nHere\u2019s a number of improvements that we applied in order to reduce 75th\npercentile of end-to-end latencies for single line completion from 1.8 seconds\nto under 900 milliseconds over the past months:\n\n  * Token limits and stop words: The most time in our request was spent waiting for the LLM to respond (that's somewhat expected). The number of tokens in the prompt and in the output makes a huge difference in these delays, though. After some tweaking and especially the addition of stop-words, we were able to speed up inference times by a ton.\n\n  * Streaming: Later, we'd even start to use streaming (so the LLM can return the response token-by-token) and our client could implement more advanced mechanisms to terminate a completion request early.\n\nFor example if you are looking to complete a function definition and the LLM\nresponse starts to define another function after the current one is finished,\nchances are you don't even want to show the second function--So why block the\nresponse until the request is finished?\n\n  * TCP connection reusing: Autocomplete requires a lot of requests. Roughly a request for every few keystrokes. We don't think about this often, but every new request requires a handshake between the client and server which adds latency.\n\nLuckily there is a solution here and that is to keep the TCP connection open.\nWhat we didn't know: Different HTTP clients have different defaults here and\nsince a Cody autocomplete request is routed from the Client to the Sourcegraph\nserver and then the LLM, we needed to make sure that TCP connections are\nreused for every step in this pipeline.\n\n  * Backend improvements: The story wouldn't be complete with a few obvious improvements. Once we found out that our logging on the Sourcegraph server does a synchronous write to BigQuery for example, it didn't take long for us to notice that this is probably not the best way. Safe to say our server side logging steps are now no longer blocking the critical paths. Whoops!\n\n  * Parallel request limits: Early on, Cody autocomplete was triggering multiple generations for every request. This was added in order to mitigate shortcomings of the initial prompts: If we have a sample of two or three completions to use, we can improve the quality by picking the best one. The catch for this though: The latency is now defined as the longest duration of any of the three requests. We were able to reduce this level and currently only request multiple variants for multi-line completions (which are generally more error prone and less latency sensitive).\n\n  * Recycling prior completion requests: This is a client level improvement that was able to improve latencies in some cases quite dramatically. Imagine you're trying to write console.log(. However, while typing, you make a short break between the console. and the log(. This happens all the time as devs think about how to proceed.\n\nThe small delay would cause Cody to make an autocomplete request, however if\nyou're quick in resuming the typing, that result might not make it to the\nscreen yet as the document state keeps changing.\n\nHowever, chances are that the initial request (the one with console.) would\nalready be enough information for the LLM to generate the desired completion.\nIn practice we have measured this to be the case in quite a few cases (about\nevery tenth request). We have added additional bookkeeping to the clients to\ndetect these cases and recycle such prior completion requests.\n\n  * Following up on downstream performance regressions: Our extensive latency logging setup was also helpful when the downstream inference provider introduced latency regressions. We're proud of our collaboration with Anthropic on these and the data we could share with them was always helpful to fix the regression quickly.\n\nThis is not the end of the journey though and we know there\u2019s still a lot of\nroom for improvement left on the table. One limitation right now is that our\nbackends (the Sourcegraph server and the inference endpoint) are only hosted\nin one region which is not ideal for users of other parts of the world.\nThere\u2019s also the possibility to improve the raw inference speed, especially as\nnew hardware and algorithms become available.\n\n### A use-case specific LLM\n\nRegardless of how fast we make our Claude Instant implementation, we still\nhave to deal with the fact that it's a general purpose model and is thus a lot\nlarger than it needs to be. To avoid falling into a local maxima, we started\nevaluations of use-case specific LLMs that are only helpful for generating\ncode. Our hypothesis was that:\n\n  * Use-case specific LLMs can be better at the their trained use case while having a reduced size (so they are faster to run)\n\n  * We can take advantage of state-of-the-art models that are trained specifically for the Fill in the Middle use case to further improve our response quality.\n\n  * Tokenization for a coder model is likely going to be in our favor more which means we will be able to generate more characters using smaller token counts.\n\n  * Being able to leverage open source LLMs is going to help us futureproof the system while allowing us to have more control over the deployment (if we want to spin up an inference end point at a specific location, we'll need to be in control of this).\n\nStarCoder has always been a model that we found particularly interesting given\nthat it is built especially for our use case, it has multiple variants (based\non the parameters size) so we can run faster models for use cases where we do\nnot need the full accuracy. We can even rely on quantized versions (the name\nof a technique to reduce the precision of a model to reduce its size), that\nhave almost no visible quality difference while being even faster to run.\n\nAfter a long evaluation period against other models, we began a broad A/B test\non our community user group and, after a few bug fixes and improvements, have\nrecently finished the rollout for community users to this model, resulting in\nmuch reduced latencies and an increase in acceptance rate for our users.\n\nAt Sourcegraph, we've always believed that our strength does not come from\nbeing tied and hyper-optimized around a specific LLM (heck, the one that you\noptimize for can be outdated in months anyways!) but that we need to be\nflexible to use the best tooling available and feed it the most relevant\ncontext. This unlocks quite a few opportunities where we can easily move to a\nbetter model for our users and even support local-only inference with tools\nlike Ollama. After all, the AI journey has only just started!\n\n## Post-processing\n\nOnce we have a string from the backend we're done, ...right? Well, almost. The\nreality is that sometimes responses aren't quite what you expect them to be\nbut since we've gone through all of this effort to create these strings, we'll\ngo to lengths to salvage whatever we got back.\n\nWith Cody, this step is called post-processing and we employ a number of\ntricks to make sure the text that is being displayed at the screen is as\nrelevant as possible:\n\n  * Avoiding repeated content: If there's one quality that LLMs have it's that they're really good at repeating content. Unfortunately this sometimes leads to undesired results when the completion contains a line that was already written above or below the cursor. There's only so much we can do with instruction tuning to avoid this so we're also employing rule based systems to guard against this failure case (via algorithms like Levenshtein edit distances).\n\n  * Truncating multi-line completions: Ever since the first implementation of multi-line completions, we identified the need to have language specific rules to know 1) when to trigger a multi-line completion and 2) when to cut it off.\n\nLLMs are really good at continuing to produce output so when you ask it to\nfill out a function body, chances are that the LLM continues to implement\nanother function and another function and another.... \ud83d\ude43\n\nTo prevent this from happening, we use a combination of two techniques to find\nout exactly when we want to truncate the completion:\n\n    * Indentation based: The idea for indentation based systems is to leverage code indentations to find out when the response leaves the current indentation level. The handy bit about this is that it's mostly language-agnostic, we only need to handle a few special cases like closing brackets to get usable results.\n\n    * Syntactical: I've already touched on our use of Tree-sitter above but this is another problem area where syntactic knowledge of the code provides us with a great opportunity. Insteading having to guess where a block ends, Tree-sitter can be used to be precise about it. We've seen great improvements in truncation quality by moving to this approach and will likely extend this to support more programming languages in the near future.\n\n  * Estimating the relevance of a completion: Once we have a completion, it is handy to be able to score how relevant it is. We employ techniques for this mostly for multi-line completions at the moment (where we have more than one candidate completion available during post-processing and can use this information to select the most probable):\n\n    * Using syntactic parsing: Leveraging Tree-sitter again, we can automatically devalue completions with syntax errors.\n\n    * Using probabilities returned by the LLM: One additional benefit of moving away from Claude Instant is that we now have access to the underlying probabilities that the LLM used to generate the completion. We can sum up probabilities to understand how certain the model is about a specific generation.\n\n  * Filtering out obvious bad suggestions: While this is not as big of a problem anymore than it was in our early days, we also have a regex that highlights obvious bad completions. One such example is that in our initial Claude prompt, we'd sometimes get git diff style patches back.\n\nOne overarching learning from this step is that we do not want to filter out\ntoo many completions. If we err on the side of not showing completions, our\nusers have given us the feedback that the product does not work and it's\nreally unclear for a user as to why. Hence, it's better to focus on generating\nrelevant completions.\n\n## Data, data, data...\n\nAt Sourcegraph, we\u2019re strong believers in the saying that \u201cIf you cannot\nmeasure it, you can't improve it\u201d and as a result of this, analytics has\nalways played a key role in how we improve Cody autocomplete. Over time, this\nsystem has become quite advanced as there\u2019s a ton of additional bookkeeping\nneeded to account for all of the VS Code APIs oddities and growing demands.\nLet\u2019s dive into some specifics.\n\n### What metrics do we track?\n\n  * Suggestions: At the heard of our telemetry is an event for every completion that was suggested to a user\n\n    * This includes the number of lines and characters of that completion in addition to the execution plan so we know what's being suggested. We also attach latency information and other debugging information to this event.\n\n    * Every completion has a unique UUID so we can combine various data sources to get a more complete picture of the completion.\n\n    * Knowing when VS Code decides to show a completion is unfortunately a hard problem (that is, unless you are GitHub and can implement specific VS Code APIs that no one else can use in production). So in order to understand when VS Code is deciding to show a completion we have a user-space implementation of their display criteria. Even then, another completion provider could be fast to respond to a completion, in which case we don't know if theirs or our completion is shown. We try to guard against this by logging if the user also has one of a list of known completion extensions enabled for their VS Code instance.\n\n    * We also measure for how long a completion was visible on screen. This, however, is not precise as there are also no VS Code APIs so we approximate this based on a few VS Code specific heuristics.\n\n  * Acceptances: A straightforward success criteria: If a user uses tab to insert a completion, this is a strong signal that a completion is indeed helpful.\n\n  * Partial acceptances: VS Code specifically has UI to only accept one word or one line from a completion. For a partial acceptance we also log how much (in number of characters) of the completion was added and we only log a partial acceptance when at least one full word of the completion was inserted.\n\n  * Completion retention: To better understand how useful the completions are, we also track how completions were changed over time after they were inserted. For this, we have bookkeeping that detects document changes to update the initial range that a completion was inserted at and then uses Levenshtein edit distances at specific polling intervals to capture how much of the initial completion is still present.\n\nBased on these events, we can compute our most important metric and that is\ncompletion acceptance rate. A metric that combines a lot of criteria like\nlatency and quality into a single number.\n\nThe good news is that our users use Cody autocomplete a lot and that we can\nuse this telemetry to get rapid feedback for improvements and use that to run\nA/B tests. To showcase how sensitive our logging is: We noticed a 50ms\nregression to latency in only a few hours of logging. In fact our logging was\nso advanced that we were able to provide valuable insight and fix performance\nregressions caused by Anthropic for a while.\n\nBy adding a lot of metadata from the previous steps to every autocomplete\nevent, we're able to categorize requests into areas that work well and areas\nthat need more improvement. The combination of Tree-sitter syntax information\nhas been really helpful to identify issues in this category.\n\nOne such example is to reduce the frequency of completions on positions where\nwe know that they are unhelpful. One example is if you're at the end of a line\nbut the statement on that line is already complete:\n\n    \n    \n    console.log();| // ^ showing an autocomplete at this point is likely not very useful \ud83d\ude05\n\n## Reliability\n\nThere is one area that, in my opinion, is often overlooked in software\ndevelopment: Reliability. More specifically, we need to ensure that our system\nnot only works on paper, but that it also does not regress in functionality\nover time (this can happen by us pushing faulty updates or by the\ninfrastructure failing us in production).\n\nThere are a few basics for reliability like unit testing and tracking\nproduction errors that every project should implement. Since we\u2019re working\nwith a very flaky environment though (the LLMs indeterministic nature), we\u2019ve\nhad to add a lot more safeguards though.\n\n### Autocomplete tests\n\nI won\u2019t go into detail about this and I don\u2019t think this is controversial\nanymore but automating your tests allows you to move faster. Most of the day\nto day improvements on heuristics outlined above rely on a large integration\ntest suite that calls directly into the provideInlineCompletionItems API that\nVSCode uses. By running through the whole autocomplete architecture, we can\nwrite tests by defining a document and potential LLM responses and make\nassertions on all steps along the way. Here\u2019s an example of such a test:\n\n    \n    \n    it('properly truncates multi-line responses for python', async () => { const items = await getInlineCompletionsInsertText( params( dedent` for i in range(11): if i % 2 == 0: \u2588 `, [ completion` \u251cprint(i) elif i % 3 == 0: print(f\"Multiple of 3: {i}\") else: print(f\"ODD {i}\") for i in range(12): print(\"unrelated\")\u2524`, ], { languageId: 'python', } ) ) expect(items[0]).toMatchInlineSnapshot(` \"print(i) elif i % 3 == 0: print(f\\\\\"Multiple of 3: {i}\\\\\") else: print(f\\\\\"ODD {i}\\\\\")\" `) })\n\nIn addition to a broad suite of integration tests, we also have E2E tests for\nour VS Code extensions that fires up a headless version of VS Code and\ninstruments it via Playwright to ensure it\u2019s working properly.\n\n### LLM inference test suite\n\nSo we know that our implementation works for statically defined LLM responses.\nBut how do we evaluate that changes we make actually have a positive impact on\nthe overall user experience? One way of thinking about this is by looking at\nyour production metric, but even in scenarios where you have lots of data,\nthis usually results in a slow feedback cycle since you need to push a change,\nrun an experiment and wait for it to conclude, evaluate it, and start again...\n\nTo improve our feedback cycles, we started very early to collect static\nexamples of specific document states to automatically run our whole\nautocomplete stack against it. These only consisted of prefix and suffix pairs\nand were mostly evaluated manually using a small web UI that we built. It\u2019s\nbeen super helpful to hook up new models, work on prompt changes, and tweak\nthe generation parameters.\n\nOver time, the manual evaluation became more and more work as we\u2019ve added more\nexamples and focusing only on one file was not a good replication of how a\nuser works in their IDE. Testing LLMs created for code generation is a known\nproblem and so we looked at existing solutions like the famous HumanEval\ntests. Those tests are usually also constrained to a single input file but\nthey do have tests associated that can be run to validate the solution for\ncorrectness. These tests are great to validate the underlying LLM but they\nstill do not capture the big picture of a user using their IDE to write code.\n\nWe knew we had to do more to build the best autocomplete experience and so\nwe\u2019ve recently overhauled our LLM inference test suite to document more and\nmore cases of how code completion is used in the editor. Examples that\nencapsulate a whole workspace configuration like when you are writing a class,\nand then move to a different file and try to write a unit test for this class.\nWe also added a system to run automated tests against the generated completion\nto measure its correctness. This allows us to test changes across the whole\nautocomplete stack quickly and without the need to deploy them, and get a\nsense of whether they improve the experience or not... And it\u2019s only the\nbeginning!\n\n## Summary\n\nIn this post we looked at the lifecycle of a code AI completion for Cody. To\nsummarize, every Cody completion goes through four steps:\n\n  * Planning - analyzing the code context to determine the best approach for generating completions, such as using single vs. multi-line completions.\n\n  * Retrieval - finding relevant code examples from the codebase to provide the best possible context for the LLM.\n\n  * Generation - using the LLM to generate code completions based on the prompt and context provided.\n\n  * Post-processing - refining and filtering the raw AI-generated completions to deliver the most relevant suggestions.\n\nThe goal of Cody is to provide high-quality completions that integrate\nseamlessly into a developer's workflow. Creating an effective code AI\nassistant requires the right context, prompt, and LLM. Through syntactic\nanalysis, smart prompt engineering, proper LLM selection, and the right\ntelemetry we are continuously iterating and improving code completion quality\nand acceptance rate for Cody. Latest numbers show Cody completion acceptance\nrate to be as high as 30%.\n\nCurious to see Cody in action for yourself? Get started for free today.\n\n### Get Cody, the AI coding assistant\n\nCody makes it easy to write, fix, and maintain code.\n\nLearn more\n\n## Products\n\n  * Cody\n  * Code Search\n\n## Solutions\n\n  * Enterprise\n\n## Resources\n\n  * Blog\n  * Library\n  * Customer stories\n  * Changelog\n  * Documentation\n  * Community\n  * Support forum\n\n## Company\n\n  * About\n  * Careers - We're hiring!\n  * Contact\n  * Handbook\n  * Sourcegraph strategy\n\n  * \u00a9 2024 Sourcegraph, Inc.\n  * Terms\n  * Security\n  * Privacy\n\n", "frontpage": false}
