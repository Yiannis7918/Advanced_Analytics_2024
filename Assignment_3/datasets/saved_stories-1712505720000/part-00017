{"aid": "39959228", "title": "Sophia: Scalable Stochastic 2nd-Order Optimizer for Language Model Pre-Training", "url": "https://arxiv.org/abs/2305.14342", "domain": "arxiv.org", "votes": 6, "user": "tosh", "posted_at": "2024-04-07 08:23:26", "comments": 0, "source_title": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training", "source_text": "[2305.14342] Sophia: A Scalable Stochastic Second-order Optimizer for Language\nModel Pre-training\n\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member\ninstitutions, and all contributors. Donate\n\n> cs > arXiv:2305.14342\n\n# Computer Science > Machine Learning\n\narXiv:2305.14342 (cs)\n\n[Submitted on 23 May 2023 (v1), last revised 5 Mar 2024 (this version, v4)]\n\n# Title:Sophia: A Scalable Stochastic Second-order Optimizer for Language\nModel Pre-training\n\nAuthors:Hong Liu, Zhiyuan Li, David Hall, Percy Liang, Tengyu Ma\n\nView a PDF of the paper titled Sophia: A Scalable Stochastic Second-order\nOptimizer for Language Model Pre-training, by Hong Liu and 4 other authors\n\nView PDF HTML (experimental)\n\n> Abstract:Given the massive cost of language model pre-training, a non-\n> trivial improvement of the optimization algorithm would lead to a material\n> reduction on the time and cost of training. Adam and its variants have been\n> state-of-the-art for years, and more sophisticated second-order (Hessian-\n> based) optimizers often incur too much per-step overhead. In this paper, we\n> propose Sophia, Second-order Clipped Stochastic Optimization, a simple\n> scalable second-order optimizer that uses a light-weight estimate of the\n> diagonal Hessian as the pre-conditioner. The update is the moving average of\n> the gradients divided by the moving average of the estimated Hessian,\n> followed by element-wise clipping. The clipping controls the worst-case\n> update size and tames the negative impact of non-convexity and rapid change\n> of Hessian along the trajectory. Sophia only estimates the diagonal Hessian\n> every handful of iterations, which has negligible average per-step time and\n> memory overhead. On language modeling with GPT models of sizes ranging from\n> 125M to 1.5B, Sophia achieves a 2x speed-up compared to Adam in the number\n> of steps, total compute, and wall-clock time, achieving the same perplexity\n> with 50% fewer steps, less total compute, and reduced wall-clock time.\n> Theoretically, we show that Sophia, in a much simplified setting, adapts to\n> the heterogeneous curvatures in different parameter dimensions, and thus has\n> a run-time bound that does not depend on the condition number of the loss.\n\nSubjects:| Machine Learning (cs.LG); Computation and Language (cs.CL);\nOptimization and Control (math.OC)  \n---|---  \nCite as:| arXiv:2305.14342 [cs.LG]  \n(or arXiv:2305.14342v4 [cs.LG] for this version)  \nhttps://doi.org/10.48550/arXiv.2305.14342arXiv-issued DOI via DataCite  \n  \n## Submission history\n\nFrom: Hong Liu [view email] [v1] Tue, 23 May 2023 17:59:21 UTC (2,993 KB) [v2]\nMon, 9 Oct 2023 19:54:09 UTC (2,777 KB) [v3] Tue, 17 Oct 2023 07:44:16 UTC\n(2,777 KB) [v4] Tue, 5 Mar 2024 17:07:16 UTC (6,090 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Sophia: A Scalable Stochastic Second-order\nOptimizer for Language Model Pre-training, by Hong Liu and 4 other authors\n\n  * View PDF\n  * HTML (experimental)\n  * TeX Source\n  * Other Formats\n\nview license\n\nCurrent browse context:\n\ncs.LG\n\n< prev | next >\n\nnew | recent | 2305\n\nChange to browse by:\n\ncs cs.CL math math.OC\n\n### References & Citations\n\n  * NASA ADS\n  * Google Scholar\n  * Semantic Scholar\n\na export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer (What is the Explorer?)\n\nLitmaps (What is Litmaps?)\n\nscite Smart Citations (What are Smart Citations?)\n\n# Code, Data and Media Associated with this Article\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\nDagsHub (What is DagsHub?)\n\nGotit.pub (What is GotitPub?)\n\nPapers with Code (What is Papers with Code?)\n\nScienceCast (What is ScienceCast?)\n\n# Demos\n\nReplicate (What is Replicate?)\n\nHugging Face Spaces (What is Spaces?)\n\nTXYZ.AI (What is TXYZ.AI?)\n\n# Recommenders and Search Tools\n\nInfluence Flower (What are Influence Flowers?)\n\nConnected Papers (What is Connected Papers?)\n\nCORE Recommender (What is CORE?)\n\nIArxiv Recommender (What is IArxiv?)\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new\narXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and\naccepted our values of openness, community, excellence, and user data privacy.\narXiv is committed to these values and only works with partners that adhere to\nthem.\n\nHave an idea for a project that will add value for arXiv's community? Learn\nmore about arXivLabs.\n\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n\n  * About\n  * Help\n\n  * Contact\n  * Subscribe\n\n  * Copyright\n  * Privacy Policy\n\n  * Web Accessibility Assistance\n  * arXiv Operational Status Get status notifications via email or slack\n\n", "frontpage": true}
