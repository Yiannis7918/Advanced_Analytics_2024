{"aid": "39959168", "title": "Little Languages Are the Future of Programming (2022)", "url": "https://chreke.com/little-languages", "domain": "chreke.com", "votes": 2, "user": "mpweiher", "posted_at": "2024-04-07 08:13:47", "comments": 0, "source_title": "Little Languages Are The Future Of Programming", "source_text": "chreke's blog - Little Languages Are The Future Of Programming\n\n# chreke's blog\n\n## Little Languages Are The Future Of Programming\n\nSun 20 November 2022\n\nI\u2019ve become convinced that \u201clittle languages\u201d\u2014small languages designed to\nsolve very specific problems\u2014are the future of programming, particularly after\nreading Gabriella Gonzalez\u2019s The end of history for programming and watching\nAlan Kay\u2019s Programming and Scaling talk. You should go check them out because\nthey\u2019re both excellent, but if you stick around I\u2019ll explain just what I mean\nby \u201clittle languages\u201d and why they\u2019re important.\n\n## What is a \u201clittle language\u201d?\n\nI believe Jon Bentley coined the term \u201clittle language\u201d in his eponymous\nLittle Languages article, where he gave the following definition:\n\n> [...] a little language is specialized to a particular problem domain and\n> does not include many features found in conventional languages.\n\nFor example, SQL is a little language for describing database operations.\nRegular expressions is a little language for text matching. Dhall is a little\nlanguage for configuration management, and so on.\n\nThere are a few other names for these languages: Domain-specific languages\n(DSL:s), problem-oriented languages, etc. However, I like the term \u201clittle\nlanguages\u201d, partially because the term \u201cDSL\u201d has become overloaded to mean\nanything from a library with a fluent interface to a full-blown query language\nlike SQL, but also because \u201clittle languages\u201d emphasizes their diminutive\nnature.\n\n## Why do we need little languages?\n\n> If you look at software today, through the lens of the history of\n> engineering, it\u2019s certainly engineering of a sort\u2014but it\u2019s the kind of\n> engineering that people without the concept of the arch did. Most software\n> today is very much like an Egyptian pyramid with millions of bricks piled on\n> top of each other, with no structural integrity, but just done by brute\n> force and thousands of slaves.\n>\n> \u2014Alan Kay, from A Conversation with Alan Kay\n\nWe have a real problem in the software engineering community: As an\napplication grows in complexity its source code also grows in size. However,\nour capacity for understanding large code bases remains largely fixed.\nAccording to The Emergence of Big Code, a 2020 survey by Sourcegraph, the\nmajority of respondents said that the size of their code base caused one or\nmore of the following problems:\n\n  * Hard to onboard new hires\n  * Code breaks because of lack of understanding of dependencies\n  * Code changes become harder to manage\n\nWhat\u2019s worse, applications seem to grow at an alarming rate: Most respondents\nin the Sourcegraph survey estimated that their code base had grown between 100\nto 500 times in the last ten years. As a concrete example, the Linux kernel\nstarted out at about 10 000 lines of code in 1992. Twenty years later, it\nweighs in at about 30 million lines [1].\n\nWhere does all this code come from? I don\u2019t believe \u201cmore features\u201d is\nsufficient to explain these increases in code volume; rather, I think it has\nto do with the way we build software. The general approach for adding new\nfeatures to a program is to stack them on top of what you already have, not\nunlike how you would construct a pyramid. The problem is that\u2014just like a\npyramid\u2014each subsequent layer will require more bricks than the last one.\n\n### Bucking the trend\n\nDo you really need millions of lines of code to make a modern operating\nsystem? In 2006, Alan Kay and his collaborators in the STEPS program set out\nto challenge that assumption:\n\n> Science progresses by intertwining empirical investigations and theoretical\n> models, so our first question as scientists is: if we made a working model\n> of the personal computing phenomena could it collapse down to something as\n> simple as Maxwell\u2019s Equations for all of the electromagnetic spectrum, or\n> the US Constitution that can be carried in a shirt pocket, or is it so\n> disorganized (or actually complex) to require \u201c3 cubic miles of case law\u201d,\n> as in the US legal system (or perhaps current software practice)? The answer\n> is almost certainly in between, and if so, it would be very interesting if\n> it could be shown to be closer to the simple end than the huge chaotic other\n> extreme.\n>\n> So we ask: is the personal computing experience (counting the equivalent of\n> the OS, apps, and other supporting software) intrinsically 2 billion lines\n> of code, 200 million, 20 million, 2 million, 200,000, 20,000, 2,000?\n>\n> STEPS 2007 Progress Report, p. 4-5\n\nMaxwell\u2019s Equations, which Dr. Kay was referring to, is a set of equations\nthat describe the foundations of electromagnetism, optics and electric\ncircuits. A cool thing about them is that even though they have such a wide\nscope, they\u2019re compact enough to fit on a t-shirt:\n\n\\begin{equation*} \\nabla\\cdot\\mathbf{D}= \\rho \\end{equation*}\n\n\\begin{equation*} \\nabla\\cdot\\mathbf{B}=0 \\end{equation*}\n\n\\begin{equation*} \\nabla\\times\\mathbf{E}=-\\frac{\\partial\\mathbf{B} }{\\partial\nt} \\end{equation*}\n\n\\begin{equation*} \\nabla\\times\\mathbf{H}= \\mathbf{J}+\\frac{\\partial\\mathbf{D}\n}{\\partial t} \\end{equation*}\n\nOne reason they\u2019re so terse is the use of the Del notation (e.g. \\\\(\\nabla\\\\))\nto describe vector calculus operations. Important to note is that Del isn\u2019t\nreally an operator\u2014it\u2019s more like a shorthand to make some equations in vector\ncalculus easier to work with.\n\nWhat if it\u2019s possible to create the equivalent of Del notation for\nprogramming? Just as Del could help make vector calculus more manageable, are\nthere notations that could help us reason about programs in much the same way?\nThis question was one of the \u201cpowerful ideas\u201d that powered the STEPS project:\n\n> We also think that creating languages that fit the problems to be solved\n> makes solving the problems easier, makes the solutions more understandable\n> and smaller, and is directly in the spirit of our \u201cactive-math\u201d approach.\n> These \u201cproblem-oriented languages\u201d will be created and used for large and\n> small problems, and at different levels of abstraction and detail.\n>\n> STEPS 2007 Progress Report, p. 6\n\nThe idea is that as you start to find patterns in your application, you can\nencode them in a little language\u2014this language would then allow you to express\nthese patterns in a more compact manner than would be possible by other means\nof abstraction. Not only could this buck the trend of ever-growing\napplications, it would actually allow the code base to shrink during the\ncourse of development!\n\nOne result from the STEPS program that I find particularly impressive was\nNile, a little language for describing graphics rendering and compositing. The\ngoal was to use Nile to reach feature parity with Cairo\u2014an open-source\nrenderer used in various free software projects\u2014which weighs in at about 44\n000 lines of code. The Nile equivalent ended up being about 300 lines. [2]\n\n## Why not high-level languages?\n\n> Nevertheless, Ada will not prove to be the silver bullet that slays the\n> software productivity monster. It is, after all, just another high-level\n> language, and the biggest payoff from such languages came from the first\n> transition, up from the accidental complexities of the machine into the more\n> abstract statement of step-by-step solutions. Once those accidents have been\n> removed, the remaining ones are smaller, and the payoff from their removal\n> will surely be less.\n>\n> \u2014Frederick P. Brooks, No Silver Bullet\n\n\u201cHey, wait a minute\u201d you might say \u201cwhy can\u2019t we just invent a higher-level,\ngeneral-purpose language?\u201d Personally, I believe we have reached diminishing\nreturns for the expressiveness of general-purpose languages. If there is a\nhigher level, what would it even look like? Take Python, for example\u2014it\u2019s so\nhigh-level it pretty much looks like pseudocode already. [3]\n\nThe problem with general-purpose languages is that you still have to translate\nyour problem to an algorithm, and then express the algorithm in your target\nlanguage. Now, high-level languages are great at describing algorithms, but\nunless the goal was to implement the algorithm then it\u2019s just accidental\ncomplexity.\n\nWriting this post reminded me of a story about Donald Knuth: Knuth had been\nasked to demonstrate his literate programming style in Jon Bentley\u2019s\nProgramming Pearls column; Doug McIlroy was also invited to provide critique\nof Knuth\u2019s program. [4] The task at hand was to count word frequencies in a\ngiven text.\n\nKnuth\u2019s solution was meticulously written in WEB, his own literate programming\nvariant of Pascal. He had even included a purpose-built data structure just\nfor keeping track of word counts, and it all fit within ten pages of code.\nWhile McIlroy was quick to praise the craftsmanship of Knuth\u2019s solution, he\nwas not very impressed with the program itself. As part of his critique, he\nwrote his own solution in a creole of shell script, Unix commands and little\nlanguages:\n\n    \n    \n    tr -cs A-Za-z '\\n' | tr A-Z a-z | sort | uniq -c | sort -rn | sed ${1}q\n\nWhile it might not the most readable code for non-Unix hackers\u2014and McIlroy\nwould probably admit as much, as he saw fit to include an annotated version\u2014\nthis summary response is arguably easier to understand than a ten-page\nprogram.\n\nUnix commands are designed for manipulating text, which is why it\u2019s possible\nto write such a compact word-counting program\u2014maybe shell script could be\nconsidered the \u201cDel notation\u201d of text manipulation?\n\n## Less is more\n\nThe Unix command example above illustrates another characteristic of little\nlanguages: Less powerful languages and more powerful runtimes. Gonzalez had\nnoted the following trend in The end of history for programming:\n\n> A common pattern emerges when we study the above trends:\n>\n>   * Push a userland concern into a runtime concern, which:\n>   * ... makes programs more closely resemble pure mathematical expressions,\n> and:\n>   * ... significantly increases the complexity of the runtime.\n>\n\nRegular expressions and SQL won\u2019t let you express anything but text search and\ndatabase operations, respectively. This is in contrast to a language like C\nwhere there is no runtime and you can express anything that\u2019s possible on a\nvon Neumann architecture. High-level languages like Python and Haskell fall\nsomewhere in between: Memory management is handled for you, but you still have\nthe full power of a Turing-complete language at your disposal, which means you\ncan express any computation possible.\n\nLittle languages are at the opposite end of the power spectrum from C: Not\nonly is the architecture of the computer abstracted away, but some of these\nlanguages also limit the kinds of program you can express\u2014they\u2019re Turing-\nincomplete by design. This might sound awfully limiting, but in fact it opens\nup a whole new dimension of possibilities for optimization and static\nanalysis. And, like abstracting away memory management erases a whole class of\nbugs, it might be possible to erase yet more bugs by abstracting away as much\nof the algorithmic work as possible.\n\n### Static analysis\n\nLess powerful languages are easier to reason about, and can provide stronger\nguarantees than general-purpose languages. For example, Dhall is a total\nfunctional programming language for generating configuration files. Since you\ndon\u2019t want to risk crashing your deployment scripts or putting them into an\ninfinte loop, Dhall programs are guaranteed to:\n\n  1. Not crash, and\n  2. Terminate in a finite amount of time.\n\nThe first point is accomplished by not throwing exceptions; any operation that\ncan fail (e.g. getting the first element of a potentially empty list) returns\nan Optional result, which may or may not contain a value. The second\npoint\u2014guaranteed termination\u2014is accomplished by not allowing recursive\ndefinitions [6]. In other functional programming languages recursion is the\nprimary way you would express loops, but in Dhall you have to rely on the\nbuilt-in fold function instead. The lack of a general loop construct also\nmeans Dhall is not Turing-complete; but since it\u2019s not a general-purpose\nprogramming language it doesn\u2019t need to be (unlike CSS, apparently [7]).\n\nIf the languages are small, reasoning about them becomes even easier. For\nexample, to determine whether an arbitrary Python program is free of side-\neffects is hard, but in SQL it\u2019s trivial\u2014just check if the query starts with\nSELECT [5].\n\nFor Nile, the STEPS team saw the need for a graphical debugger [9]. Bret\nVictor (yes, the same Bret Victor who did the Inventing on Principle talk)\ncame up with a tool that would tell you the exact lines of code that was\ninvolved in drawing a specific pixel on the screen. You can watch Alan Kay\ndemo it on YouTube, but you can also try it yourself. Tools like these are\npossible because Nile is a small language that\u2019s easy to reason about\u2014imagine\ntrying to do the same thing with graphics code written in C++!\n\n### The need for speed\n\nMore powerful programming languages not only increase the potential for bugs,\nit can also be detrimental to performance. For example, if a program isn\u2019t\nexpressed in terms of an algorithm, the runtime is free to choose its own;\nslow expressions can be substituted for faster ones if we can prove that they\nproduce the same result.\n\nFor example, a SQL query doesn\u2019t dictate how a query should be executed\u2014the\ndatabase engine is free to use whichever query plan it deems most appropriate,\ne.g. whether it should use an index, a combination of indices, or just scan\nthe entire database table. Modern database engines also collect statistics on\nthe value distributions of its columns, so they can choose a statistically\noptimal query plan on the fly. This wouldn\u2019t be possible if the query was\ndescribed by the way of an algorithm.\n\nOne part of the \u201csecret sauce\u201d that allowed the Nile language to be so compact\nwas Jitblt, a just-in-time compiler for graphics rendering. From discussions\nbetween the STEPS and Cairo teams it became clear that a lot of the Cairo code\nwas dedicated to hand-optimization of pixel compositing operations; work that\ncould, in theory, be offloaded to a compiler. Dan Amelang from the Cairo team\nvolunteered to implement such a compiler, and the result was Jitblt. This\nmeant that the optimization work in the graphics pipeline could be decoupled\nfrom the purely mathematical descriptions of what to render, which allowed\nNile to run about as fast as the original, hand-optimized Cairo code. [8]\n\n## Small languages, big potential\n\nSo what happened with the STEPS project? Did they end up with the code\nequivalent of \u201c3 cubic miles of case law\u201d, or did they manage to create an\noperating system small enough to fit on a t-shirt? The end result of STEPS was\nKSWorld, a complete operating system including both a document editor and\nspreadsheet editor, which ended up being about 17 000 lines of code [10].\nWhile you would need a really big t-shirt to fit all of that in, I would still\ncall it a success.\n\nThe creation of KSWorld seems to indicate that there\u2019s big potential in little\nlanguages. However, there are still many unanswered questions, such as: How\nshould these little languages talk to each other? Should they compile to a\ncommon intermediate representation? Or should different runtimes exist in\nparallel and communicate with each other via a common protocol (e.g. UNIX\npipes or TCP/IP)? Or maybe each language is small enough to be re-implemented\nin a variety of different host languages (like regular expressions)? Maybe the\nway forward is a combination of all of these? In any case, I\u2019m convinced that\nwe need to come up with a different way of building software. Maybe little\nlanguages will be part of that story, or maybe they won\u2019t\u2014the important thing\nis that we stop trying to pile bricks on top of each other for long enough to\ncome up with something better.\n\n## Further reading\n\n  * Connexion is an open-source API framework from Zazzle; what makes it stand out is that it can generate endpoints automatically from an OpenAPI spec; usually you would use OpenAPI to describe the endpoints of an existing HTTP service, but Connexion does it the other way around: Given an OpenAPI schema it will set up an API server with endpoints, validation logic and live documentation.\n  * Catala is a declarative language for translating law text into an executable specification. Because it supports non-monotonic reasoning (i.e. a later statement can cancel out or further qualify a previous statement) it allows expressing programs in much the same way legal texts are written, e.g. as a set of statements that can be amended or extended by adding new statements.\n  * Racket is a Lisp dialect that\u2019s specifically designed for creating new languages (a technique sometimes referred to as language-oriented programming). I haven\u2019t had time to play around much with Racket myself, but it looks like a very suitable tool for creating \u201clittle languages\u201d. If you\u2019re curious, you can check out the Creating Languages in Racket tutorial.\n  * While the STEPS project wrapped up in 2018, all the results are available online at the VPRI Writings page.\n\n[1]| See https://www.phoronix.com/news/Linux-Git-Stats-EOY2019  \n---|---  \n[2]| STEPS 2009 Progress Report, p. 4-6  \n---|---  \n[3]| To quote Peter Norvig: \u201cI came to Python not because I thought it was a\nbetter / acceptable / pragmatic Lisp, but because it was better pseudocode.\u201d\nSee: https://news.ycombinator.com/item?id=1803815  \n---|---  \n[4]| Programming Pearls, in Communications of the ACM June 1986  \n---|---  \n[5]| Warranty void if you don\u2019t stick to ISO SQL  \n---|---  \n[6]| There\u2019s an interesting Hacker News thread about recursion in Dhall here:\nhttps://news.ycombinator.com/item?id=15187150  \n---|---  \n[7]| Yes, it does look like CSS is Turing-complete  \n---|---  \n[8]| STEPS First Year Progress Report, Dec 2007, p. 12  \n---|---  \n[9]| STEPS 2012 Final Report, p. 12  \n---|---  \n[10]| STEPS 2012 Final Report, p. 32  \n---|---  \n  \nTagged in: theorycrafting little languages\n\nWant more hot takes on programming? Find me on Twitter (@therealchreke) or\nsubscribe to the RSS Feed\n\n", "frontpage": false}
