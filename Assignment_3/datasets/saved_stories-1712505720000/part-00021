{"aid": "39959327", "title": "Create a local rag development using rag-up", "url": "https://github.com/binchenX/rag-up", "domain": "github.com/binchenx", "votes": 2, "user": "devicu", "posted_at": "2024-04-07 08:41:43", "comments": 0, "source_title": "GitHub - binchenX/rag-up", "source_text": "GitHub - binchenX/rag-up\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nbinchenX / rag-up Public\n\n  * Notifications\n  * Fork 0\n  * Star 0\n\n0 stars 0 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# binchenX/rag-up\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nbinchenXadd READMEfd15b7b \u00b7\n\n## History\n\n12 Commits  \n  \n### app\n\n|\n\n### app\n\n| add README  \n  \n### assets\n\n|\n\n### assets\n\n| add README  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| done ollama  \n  \n### Makefile\n\n|\n\n### Makefile\n\n| add README  \n  \n### README.md\n\n|\n\n### README.md\n\n| add README  \n  \n### docker-compose.yml\n\n|\n\n### docker-compose.yml\n\n| add README  \n  \n## Repository files navigation\n\n## README\n\nRag-up set up the local RAG development with following structure.\n\nThe code is a clone of this, with improvement on UX.\n\n  * README\n  * Step 1: Download model\n\n    * Using docker\n    * Using local install\n  * Step 2: Start the services\n  * Step3: Let's chat\n  * Appendix\n\n    * Debug\n    * Orignial answer of the LLM before the RAG\n\n## Step 1: Download model\n\nThis will be one off action. It is to pull the llm model. We will use gemma:2b\nsince it is the smallest.\n\n### Using docker\n\n    \n    \n    \u279c \u2717 docker run -d -v $(pwd)/data/ollama:/root/.ollama -p 11434:11434 --name ollama-init ollama/ollama c8399c92c8ad411efa3bf7c540c8a4dcfaa943642689264ab80cdb19e9c2bea3 \u279c \u2717 docker exec -it ollama-init ollama run gemma:2b success >>> Send a message (/? for help)\n\nIt takes a while to pull the model. Exit the container by typing /bye; then\nstop and kill the ollama container to avoid conflict with the model service we\nwill start later.\n\n    \n    \n    docker stop ollama-init docker rm ollama-init\n\nNow, you have pull the model needed. Export the model path.\n\n    \n    \n    export OLLAMA_MODEL_DATA=$(pwd)/data/ollama\n\n### Using local install\n\nOptionally, you can also download the model by installing ollama directly to\nyour local host and then ollama run gemma:2b.\n\nThe exact location of the model download will depend on your machine, on Mac\nit is ~/.ollama.\n\nExport the model path.\n\n    \n    \n    export OLLAMA_MODEL_DATA=/Users/binchen/.ollama/\n\n## Step 2: Start the services\n\n\u2757 make sure the OLLAMA_MODEL_DATA has been set. see step 1 above.  \n---  \n      \n    \n    docker composer up\n\nIt will start the llm model service and the chat service. The chat using\nlangchain to chain a bunch of steps together. The chat service will also index\nthe external data source and use it to \"argument\" the query. That is where the\nRAG comes from.\n\n## Step3: Let's chat\n\nIt the data source, there is an article about how to build a platform product.\nLet's ask to see if the response will include this new information:\n\n    \n    \n    \u279c \u2717 make chat message=\"tips for build platform product\" curl -i -XPOST \"http://localhost:7654/api/question\" \\ --header \"Content-Type: application/json\" \\ --data '{\"question\": \"tips for build platform product\", \"user_id\": \"koala\"}' { \"answer\": \"The provided text suggests the following tips for building a platform product:\\n\\n- **Improve operational excellence:** Focus on reducing toil and enhancing user satisfaction.\\n- **Enhance system reliability:** Increase scalability and reduce time to move.\\n- **Balance customer needs and platform requirements:** Optimize features and non-functional characteristics.\\n- **Utilize time and resources:** Allocate accordingly based on priorities (25% Features Development, 25% Platform Improvement, 25% Platform Operations, 25% Customer Support).\" }\n\nThis is a great summary of the article and compare with the unargumented\nanswer. It is much relevent and indeed is using new informration to answer the\nquestion. That is exactly what we needed.\n\n## Appendix\n\n### Debug\n\nIf your docker composer failed to start due to the port 11434 is already being\nused, check who is using it and kill it. If you used the desktop version of\nollama in step 1, you will have to quit the application from the icon from the\ntop right of your desktop; otherwise, it will keep restarting automatically.\n\n    \n    \n    lsof -i :11434\n\n### Orignial answer of the LLM before the RAG\n\n## About\n\nNo description, website, or topics provided.\n\n### Resources\n\nReadme\n\nActivity\n\n### Stars\n\n0 stars\n\n### Watchers\n\n1 watching\n\n### Forks\n\n0 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 2\n\n  * binchenX Bin Chen\n  * erangaeb \u03bb.eranga\n\n## Languages\n\n  * Python 88.5%\n  * Makefile 6.1%\n  * Dockerfile 5.4%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
