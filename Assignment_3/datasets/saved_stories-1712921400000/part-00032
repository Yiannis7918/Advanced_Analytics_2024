{"aid": "40008780", "title": "Why ETL Becomes ELT or Even Let\uff1f", "url": "https://github.com/SPLWare/esProc/wiki/Why-ETL-Becomes-ELT-or-Even-LET%EF%BC%9F", "domain": "github.com/splware", "votes": 2, "user": "Judyrabbit", "posted_at": "2024-04-12 02:09:57", "comments": 0, "source_title": "Why ETL Becomes ELT or Even LET\uff1f", "source_text": "Why ETL Becomes ELT or Even LET? \u00b7 SPLWare/esProc Wiki \u00b7 GitHub\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nSPLWare / esProc Public\n\n  * Notifications\n  * Fork 308\n  * Star 4.4k\n\n# Why ETL Becomes ELT or Even LET?\n\nJump to bottom\n\nesProcSPL edited this page Mar 1, 2024 \u00b7 1 revision\n\nETL, the abbreviation of extract, transform and load, is the process of moving\ndata from one or more sources to a target system through extraction,\ntransformation and loading. A regular ETL process should stick to the order of\nextracting data, transforming data and then loading data into the target\nsystem (usually a database). The data finally loaded into the database should\nbe the desired result. However, this reasonable order is often changed in the\nactual execution of the process. ETL becomes ELT or LET, where the source data\nis loaded to the target database before it is transformed or even extracted.\n\nThe source data may come from different sources, such as database, file and\nweb, and has different levels of data quality. Both E and T are computation-\nintensive, and among a variety of data sources, databases have the best\ncomputing ability while others have a little or none. Loading source data into\nthe database has become the first and convenient choice for accomplishing the\ncomputations. This leads to LET. Sometimes data originates from multiple\ndatabases. Cross-database data extraction and transformation is not as\nconvenient as loading data to the target database for further handling. This\nalso results in LET, or ELT.\n\nBut ELT/LET can bring about a series of problems.\n\nOne aspect is the time cost increase. It is already extremely time-consuming\nto load a huge amount of non-extracted and non-transformed original (or\nuseless) data into the database, where computing resources are limited. Then\nit will be sure to take a long time to handle the extra E and T computations\nin the database, adding more time to the whole process. ETL is time-limited.\nIt is generally performed during supposedly idle time per day, which is the\ncommonly known ETL time window (from 22 o\u2019clock to 5 o\u2019clock in the next\nmorning, for instance). If the ETL job cannot be finished within the specified\nperiod, the next day\u2019s business will be affected. The time window becomes\nnarrow if ETL process is too long, and business will be damaged.\n\nThe other is the database load increase. Storing a massive amount of non-\nextracted and non-transformed original data will take up too much database\nspace, resulting in heavy database load and thus scalability pressure. To load\nthe multilevel JSON or XML data much used by today\u2019s applications, multiple\nrelated data tables need to be created in the database, further exacerbating\nthe database capacity issue. As computations increase, database resources\ndecrease and the time window becomes narrower. This gets the system caught in\na vicious circle.\n\nBut why do we need to load data to the database before we can perform E and T?\nAs previously mentioned, it is because non-database data sources have poor\ncomputing abilities and people want to make use of databases\u2019 strong ability\nby first loading data into them. If we can provide satisfactory outside-\ndatabase computing ability, we do not need to do the data loading anymore and\nthe reasonable ETL process can be restored.\n\nThe open-source esProc SPL is an ideal tool for achieving outside-database\ncomputing ability.\n\nSPL is an open-source, stand-alone data computing engine. It has a database-\nindependent computational capability for connecting to and accessing various\ndata sources and performing data processing. Its rich set of class libraries,\nagile syntax and support of procedural programming make it convenient to\nhandle complex data computing tasks in the original order of the ETL process.\nIt performs extraction (E) and transformation (T) outside the database and\nthen loads the prepared data to the target database, completing the true ETL.\n\n## Outside-database computing engine enables true ETL\n\n### Support of diverse data sources and mixed computations\n\nSPL can connect to and access various data sources, empowering each source \u2013\nno matter it has or hasn\u2019t any computing capacity \u2013 to deal with data\nextraction and transformation conveniently and efficiently through its methods\nand syntax.\n\nOne specific feature of SPL is its ability to achieve mixed computations\nbetween diverse data sources. It extracts data from different sources,\ntransforms it and loads it into the database, without the need of database\ncomputing ability anymore. It provides convenient and efficient support\nparticularly for multilevel data like JSON and XML. One simple function is\nenough for parsing them.\n\nHere\u2019s a simple example. To perform a specific operation on data coming from a\nJSON file and a database table:\n\nA  \n---  \n1| =json(file(\"/data/EO.json\").read())| Parse JSON data  \n2| =A1.conj(Orders)  \n3| =A2.select(orderdate>=date(now()))| Get data of the current date  \n4| =connect(\u201csourcedb\u201d).query@x(\u201cselect ID,Name,Area from Client\u201d)| Retrieve\ndata from the database  \n5| =join(A3:o,Client;A4:c,ID)| Perform a join between JSON data and database\ndata  \n6| =A5.new(o.orderID:orderID,...)  \n7| =connect(\u201ctargetdb\u201d).update(A6,orders)| Load result data to the target\ndatabase  \n  \n### Excellent computational ability and procedural control\n\nSPL offers specialized structured data objects and numerous operations on\nthem. It gives direct support for basic computations such as grouping &\naggregation, loop and branch, sorting, filtering, and set-operations, as well\nas for complex computations like getting positions, order-based ranking and\nirregular grouping.\n\nSPL supports cursor specifically for big data computing. It can handle a large\namount of data that exceeds the memory capacity through the cursor using\nalmost the same way of computing data that can fit into the memory. To\nretrieve data from a file and group and summarize it using the cursor, for\ninstance:\n\n    \n    \n    =file(\u201cpersons.txt\u201d).cursor@t(sex,age).groups(sex;avg(age))\n\nBelow is the code for retrieving memory data:\n\n    \n    \n    =file(\u201cpersons.txt\u201d).import@t(sex,age).groups(sex;avg(age))\n\nIn addition, SPL supports procedural programming that enables coding a\ncomputation step by step according to familiar ways of thinking. This makes it\nsuitable for handling complex ETL computations that are conventionally\nachieved with stored procedures in databases. The outside-database SPL\ncomputations are flexible and high-performance and as effective as stored\nprocedures while imposing no workload on databases. The \u201coutside-database\nstored procedure\u201d can be the ideal replacement for the conventional stored\nprocedure. Reducing database load as much as possible is what SPL computations\nare good at. Both E and T computations traditionally handled in databases are\nnow performed outside databases. This avoids consuming database computing\nresources, and storing a large volume of non-extracted and non-transformed\noriginal data, which helps reduce space usage. With SPL, there will be no such\nproblems as database resource and capacity shortages.\n\nSPL has more agile syntax than both SQL and Java. It achieves E and T\ncomputations, especially complex computations, with more concise algorithms\nand much shorter code. This considerably increases development efficiency. We\nhave a case where SPL is used to perform ETL to implement vehicle insurance\npolicy for an insurance company. SPL achieves the computation with less than\n500 cells of code (in SPL grid-style code) while it is originally handled with\n2000 lines of code using the stored procedure. The workload is reduced by over\none-third (See Open-source SPL optimizes batch operating of an insurance\ncompany from 2 hours to 17 minutes).\n\nFrom the angle of technology stack, SPL uses a consistent style of syntax that\nenables a uniform computational ability for diverse data sources during ETL\nhandling. Besides the universal technology approach, SPL enables convenient to\ndevelop and maintain programs. The cost of learning is reduced as programmers\ndo not need to know different ways of handling data coming from diverse\nsources. One important thing is that the uniform technology approach makes SPL\ncode very easy to migrate. Programmers only need to change the data retrieval\ncode when trying to switch to a different source during ETL computations,\nwhile maintaining the essential computing logic.\n\n### Sufficient ETL time window thanks to high-performance computations\n\nIt is convenient to retrieve data from a source with parallel processing in\nSPL, giving full play to the advantages of multiple CPUs for speeding up data\nretrieval and computations. To retrieve data with multiple threads, for\ninstance:\n\nA| B  \n---|---  \n1| fork to(n=12)| =connect(\"sourcedb\")  \n2| =B1.query@x(\"SELECT * FROM ORDERS WHERE MOD(ORDERID,?)=?\", n, A3-1)  \n3| =A1.conj()  \n  \nYou can also use multithreaded processing to retrieve data from a huge file:\n\n    \n    \n    =file(\u201corders.txt\u201d).cursor@tm(area,amount;4)\n\nSPL offers @m option to create a parallel multicursor, automatically handles\nthe parallel processing and summarizes the result set. Many SPL functions,\nsuch as A.select() for filtering and A.sort() for sorting, support working\nwith @m option to enable an automated parallel processing.\n\nAn ETL process often involves data persistence \u2013 by this we mean storage \u2013 of\nintermediate results or the final result. SPL provides two binary storage\nformats. They store data types to avoid data parsing and increase efficiency,\nuse reasonable and appropriate compression mechanisms to balance CPU load and\ndisk access time, support both row-wise storage and column-wise storage to\ncater to more diverse scenarios, and offers unique double increment\nsegmentation technique to implement segmentation on an append-able single file\nfor facilitating parallel processing. These high-performance storage\nmechanisms are basic guarantees of computing performance \u2013 the right storage\nmechanism and efficient algorithm are cornerstones of high-performance\ncomputing, as we know.\n\nSPL supplies a large number of high-performance algorithms. In the previously\nmentioned article, Open-source SPL optimizes batch operating of an insurance\ncompany from 2 hours to 17 minutes, SPL not only reduces code amount to the\noriginal\u2019s one-third, but shortens computing time from 2 hours to 17 minutes\nusing its exclusive multi-purpose traversal technique. The technique enables\nmultiple operations during one traversal of a huge data table, effectively\ndecreasing external storage accesses. RDBs, however, cannot do the same thing\nin SQL. They need multiple traversals for multiple operations. In the above\narticle, there is a computation involving three joins and aggregates on one\nlarge table. SQL will traverse the table three times, but SPL needs one\ntraversal only. That\u2019s why SPL can enhance performance significantly.\n\nOften there are joins between huge primary table and subtable during an ETL\nprocess, like the one between the Orders table and the OrdersDetail table. For\nsuch a join, tables are associated through primary keys (or the key in the\nprimary table and part of the key in the subtable) and they have a one-to-many\nrelationship. If we order tables by their primary keys in advance, we can use\norder-based merge algorithm to perform the join. Compared with the traditional\nHASH JOIN algorithm, order-based merge can reduce the computation\u2019s degree of\ncomplexity from O(N*M) to O(M+N) and greatly boost performance. Databases are\nbased on the theory of unordered sets and it is hard for SQL to exploit\nordered data to increase performance. In the same article, SPL remarkably\nimproves the performance a primary-sub table join computation using the order-\nbased merge algorithm.\n\nSo, using the computing expert SPL as the ET engine to separate and decouple\nthe data computing part from both the source end and the target end during the\nETL process enables highly flexible and easy to migrate code and moderate\npressure on both source and target. The conveniences of outside-database\ncomputations help achieve the ETL process in its original order. SPL also\nensures efficient ETL processes with its high-performance storage formats and\nalgorithms and parallel processing techniques, completing as many ETL tasks as\npossible within a limited time window.\n\nSPL Resource: SPL Official Website | SPL Blog | Download esProc SPL | SPL Source Code\n\n  * Learn performance optimization skills from TPCH tests\n  * Ordered storage of SPL\n  * The impasse of SQL performance optimizing\n  * The Open-source SPL Redefines OLAP Server\n\n##### Clone this wiki locally\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
