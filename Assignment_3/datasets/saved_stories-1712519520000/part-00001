{"aid": "39960773", "title": "Survey of Distributed File System Design Choices", "url": "https://dl.acm.org/doi/10.1145/3465405", "domain": "acm.org", "votes": 3, "user": "PaulHoule", "posted_at": "2024-04-07 13:51:08", "comments": 0, "source_title": "Survey of Distributed File System Design Choices | ACM Transactions on Storage", "source_text": "Survey of Distributed File System Design Choices | ACM Transactions on Storage\n\n## This website uses cookies\n\nWe occasionally run membership recruitment campaigns on social media channels\nand use cookies to track post-clicks. We also share information about your use\nof our site with our social media, advertising and analytics partners who may\ncombine it with other information that you\u2019ve provided to them or that they\u2019ve\ncollected from your use of their services. Use the check boxes below to choose\nthe types of cookies you consent to have stored on your device.\n\nUse necessary cookies only Allow all cookies Show details\n\nOK\n\nUse necessary cookies only Allow selected cookies Allow all cookies\n\nShow details\n\nCookie declaration [#IABV2SETTINGS#] About\n\nNecessary (7) Preferences (5) Statistics (16) Marketing (24) Unclassified (0)\n\nNecessary cookies help make a website usable by enabling basic functions like\npage navigation and access to secure areas of the website. The website cannot\nfunction properly without these cookies. These cookies do not gather\ninformation about you that could be used for marketing purposes and do not\nremember where you have been on the internet.\n\nName| Provider| Purpose| Expiry| Type  \n---|---|---|---|---  \n__jid| c.disquscdn.com| Used to add comments to the website and remember the\nuser's Disqus login credentials across websites that use said service.|\nSession| HTTP  \ndisqusauth| c.disquscdn.com| Registers whether the user is logged in. This\nallows the website owner to make parts of the website inaccessible, based on\nthe user's log-in status.| Session| HTTP  \n__cf_bm| ACM| This cookie is used to distinguish between humans and bots. This\nis beneficial for the website, in order to make valid reports on the use of\ntheir website.| 1 day| HTTP  \n_cfuvid| ACM| This cookie is a part of the services provided by Cloudflare -\nIncluding load-balancing, deliverance of website content and serving DNS\nconnection for website operators.| Session| HTTP  \nCookieConsent| Cookiebot| Stores the user's cookie consent state for the\ncurrent domain| 1 year| HTTP  \n1.gif| Cookiebot| Used to count the number of sessions to the website,\nnecessary for optimizing CMP product delivery.| Session| Pixel  \nVISITOR_PRIVACY_METADATA| YouTube| Stores the user's cookie consent state for\nthe current domain| 180 days| HTTP  \n  \nPreference cookies enable a website to remember information that changes the\nway the website behaves or looks, like your preferred language or the region\nthat you are in.\n\nName| Provider| Purpose| Expiry| Type  \n---|---|---|---|---  \naet-dismiss| c.disquscdn.com| Necessary for the functionality of the website's\ncomment-system.| Persistent| HTML  \ndrafts.queue| c.disquscdn.com| Necessary for the functionality of the\nwebsite's comment-system.| Persistent| HTML  \nsubmitted_posts_cache| c.disquscdn.com| Necessary for the functionality of the\nwebsite's comment-system.| Persistent| HTML  \nmopDeploy| Mopinion| Pending| Session| HTML  \nMACHINE_LAST_SEEN| ACM| Pending| 300 days| HTTP  \n  \nStatistic cookies help website owners understand how visitors interact with\nwebsites by collecting and reporting information anonymously.\n\nName| Provider| Purpose| Expiry| Type  \n---|---|---|---|---  \n_ga| Google| Registers a unique ID that is used to generate statistical data\non how the visitor uses the website.| 2 years| HTTP  \n_ga_#| Google| Used by Google Analytics to collect data on the number of times\na user has visited the website as well as dates for the first and most recent\nvisit.| 2 years| HTTP  \n_gat| Google| Used by Google Analytics to throttle request rate| 1 day| HTTP  \n_gid| Google| Registers a unique ID that is used to generate statistical data\non how the visitor uses the website.| 1 day| HTTP  \n_hjSession_#| Hotjar| Collects statistics on the visitor's visits to the\nwebsite, such as the number of visits, average time spent on the website and\nwhat pages have been read.| 1 day| HTTP  \n_hjSessionUser_#| Hotjar| Collects statistics on the visitor's visits to the\nwebsite, such as the number of visits, average time spent on the website and\nwhat pages have been read.| 1 year| HTTP  \n_hjTLDTest| Hotjar| Registers statistical data on users' behaviour on the\nwebsite. Used for internal analytics by the website operator.| Session| HTTP  \n_hp2_#| Heap Analytics| Collects data on the user\u2019s navigation and behavior on\nthe website. This is used to compile statistical reports and heatmaps for the\nwebsite owner.| 1 day| HTTP  \n_hp2_hld#.#| Heap Analytics| Collects data on the user\u2019s navigation and\nbehavior on the website. This is used to compile statistical reports and\nheatmaps for the website owner.| 1 day| HTTP  \n_hp2_id.#| Heap Analytics| Collects data on the user\u2019s navigation and behavior\non the website. This is used to compile statistical reports and heatmaps for\nthe website owner.| 1 year| HTTP  \n_hp2_ses_props.#| Heap Analytics| Collects data on the user\u2019s navigation and\nbehavior on the website. This is used to compile statistical reports and\nheatmaps for the website owner.| 1 day| HTTP  \ndisqus_unique| c.disquscdn.com| Collects statistics related to the user's\nvisits to the website, such as number of visits, average time spent on the\nwebsite and loaded pages.| Session| HTTP  \ncollect| Google| Used to send data to Google Analytics about the visitor's\ndevice and behavior. Tracks the visitor across devices and marketing\nchannels.| Session| Pixel  \nhjActiveViewportIds| Hotjar| This cookie contains an ID string on the current\nsession. This contains non-personal information on what subpages the visitor\nenters \u2013 this information is used to optimize the visitor's experience.|\nPersistent| HTML  \nhjViewportId| Hotjar| Saves the user's screen size in order to adjust the size\nof images on the website.| Session| HTML  \ntd| Google| Registers statistical data on users' behaviour on the website.\nUsed for internal analytics by the website operator.| Session| Pixel  \n  \nMarketing cookies are used to track visitors across websites. The intention is\nto display ads that are relevant and engaging for the individual user and\nthereby more valuable for publishers and third party advertisers.\n\nName| Provider| Purpose| Expiry| Type  \n---|---|---|---|---  \nbadges-message| c.disquscdn.com| Collects data on the visitor\u2019s use of the\ncomment system on the website, and what blogs/articles the visitor has read.\nThis can be used for marketing purposes.| Persistent| HTML  \napi/telemetry| Heap Analytics| Collects data on user behaviour and interaction\nin order to optimize the website and make advertisement on the website more\nrelevant.| Session| Pixel  \nh| Heap Analytics| Collects data on user behaviour and interaction in order to\noptimize the website and make advertisement on the website more relevant.|\nSession| Pixel  \n#-#| YouTube| Pending| Session| HTML  \niU5q-!O9@$| YouTube| Registers a unique ID to keep statistics of what videos\nfrom YouTube the user has seen.| Session| HTML  \nLAST_RESULT_ENTRY_KEY| YouTube| Used to track user\u2019s interaction with embedded\ncontent.| Session| HTTP  \nLogsDatabaseV2:V#||LogsRequestsStore| YouTube| Pending| Persistent| IDB  \nnextId| YouTube| Used to track user\u2019s interaction with embedded content.|\nSession| HTTP  \nremote_sid| YouTube| Necessary for the implementation and functionality of\nYouTube video-content on the website.| Session| HTTP  \nrequests| YouTube| Used to track user\u2019s interaction with embedded content.|\nSession| HTTP  \nServiceWorkerLogsDatabase#SWHealthLog| YouTube| Necessary for the\nimplementation and functionality of YouTube video-content on the website.|\nPersistent| IDB  \nTESTCOOKIESENABLED| YouTube| Used to track user\u2019s interaction with embedded\ncontent.| 1 day| HTTP  \nVISITOR_INFO1_LIVE| YouTube| Pending| 180 days| HTTP  \nYSC| YouTube| Pending| Session| HTTP  \nyt.innertube::nextId| YouTube| Registers a unique ID to keep statistics of\nwhat videos from YouTube the user has seen.| Persistent| HTML  \nytidb::LAST_RESULT_ENTRY_KEY| YouTube| Used to track user\u2019s interaction with\nembedded content.| Persistent| HTML  \nYtIdbMeta#databases| YouTube| Used to track user\u2019s interaction with embedded\ncontent.| Persistent| IDB  \nyt-remote-cast-available| YouTube| Stores the user's video player preferences\nusing embedded YouTube video| Session| HTML  \nyt-remote-cast-installed| YouTube| Stores the user's video player preferences\nusing embedded YouTube video| Session| HTML  \nyt-remote-connected-devices| YouTube| Stores the user's video player\npreferences using embedded YouTube video| Persistent| HTML  \nyt-remote-device-id| YouTube| Stores the user's video player preferences using\nembedded YouTube video| Persistent| HTML  \nyt-remote-fast-check-period| YouTube| Stores the user's video player\npreferences using embedded YouTube video| Session| HTML  \nyt-remote-session-app| YouTube| Stores the user's video player preferences\nusing embedded YouTube video| Session| HTML  \nyt-remote-session-name| YouTube| Stores the user's video player preferences\nusing embedded YouTube video| Session| HTML  \n  \nUnclassified cookies are cookies that we are in the process of classifying,\ntogether with the providers of individual cookies.\n\nWe do not use cookies of this type.  \n---  \n  \n[#IABV2_LABEL_PURPOSES#] [#IABV2_LABEL_FEATURES#] [#IABV2_LABEL_PARTNERS#]\n\n[#IABV2_BODY_PURPOSES#]\n\n[#IABV2_BODY_FEATURES#]\n\n[#IABV2_BODY_PARTNERS#]\n\nCookies are small text files that can be used by websites to make a user's\nexperience more efficient. Other than those strictly necessary for the\noperation of the site, we need your permission to store any type of cookies on\nyour device. Learn more about ACM, how you can contact us, and how we process\npersonal data in our Privacy Policy. Also please consult our Cookie Notice.\n\nYou can change or withdraw your consent from the Cookie Declaration on our\nwebsite at any time by visiting the Cookie Declaration page. If contacting us\nregarding your consent, please state your consent ID and date from that page.\n\nYour consent applies to the following domains: dl.acm.org\n\nCookie declaration last updated on 3/31/24 by Cookiebot\n\nskip to main content\n\n  * Advanced Search\n  * Browse\n  * About\n  *     * Sign in\n    * Register\n\nAdvanced Search\n\nACM Transactions on Storage\n\ntutorial\n\nOpen Access\n\nShare on\n\n# Survey of Distributed File System Design Choices\n\n  * Authors:\n  * Peter Macko\n\nNetApp, Inc., Waltham, Massachusetts, USA\n\nNetApp, Inc., Waltham, Massachusetts, USA\n\nView Profile\n\n,\n\n  * Jason Hennessey\n\nNetApp, Inc., Waltham, Massachusetts, USA\n\nNetApp, Inc., Waltham, Massachusetts, USA\n\nView Profile\n\nAuthors Info & Claims\n\nACM Transactions on StorageVolume 18Issue 1Article No.: 4pp\n1\u201334https://doi.org/10.1145/3465405\n\nPublished:02 March 2022Publication History\n\n  * 2citation\n  * 9,615\n  * Downloads\n\nMetrics\n\nTotal Citations2\n\nTotal Downloads9,615\n\nLast 12 Months5,257\n\nLast 6 weeks2,697\n\n  * Get Citation Alerts\n  * Publisher Site\n\n  * PDF\n\n## ACM Transactions on Storage\n\nVolume 18, Issue 1\n\nPreviousArticleNextArticle\n\n  *     * Abstract\n    * 1 INTRODUCTION\n    * 2 INSIDE A DISTRIBUTED FILE SYSTEM\n    * 3 POINTERS\n\n      * 3.1 Pointer Types\n\n        * 3.1.1 Local Pointers.\n        * 3.1.2 Explicit, Direct Pointers.\n        * 3.1.3 Explicit, Indirect Pointers.\n        * 3.1.4 Computed (Implicit) Pointers.\n        * 3.1.5 Overlay Networks.\n      * 3.2 Pointer Granularities\n      * 3.3 Multipointers\n    * 4 PATH-TO-INODE TRANSLATION\n\n      * 4.1 Single Node\n      * 4.2 Replicated Namespace\n      * 4.3 Replicated Prefix Table\n      * 4.4 Remote Links\n\n        * 4.4.1 CephFS Implied Remote Links.\n      * 4.5 Shared-Disk-Like File Systems\n      * 4.6 Computed (Using Cluster Membership List)\n\n        * 4.6.1 Probabilistic Computed Location.\n      * 4.7 Overlay Network\n      * 4.8 Distributed Database\n      * 4.9 Other (Orthogonal) Techniques\n\n        * 4.9.1 File System Middleware.\n        * 4.9.2 Scalable Directory Implementations.\n      * 4.10 Discussion\n    * 5 INODE-TO-DATA TRANSLATION\n\n      * 5.1 Inode-to-Data Pointer Types\n\n        * 5.1.1 Separation or Colocation of Data and Metadata.\n        * 5.1.2 Explicit, Hybrid, or Computed Data Location.\n      * 5.2 Data Placement Granularities\n\n        * 5.2.1 Parallel File Systems.\n      * 5.3 Mutable or Immutable Data\n      * 5.4 Client Library or Gateway\n    * 6 REDUNDANCY AND COORDINATION\n\n      * 6.1 Durability\n\n        * 6.1.1 Replication.\n        * 6.1.2 Erasure Coding.\n        * 6.1.3 Placement.\n        * 6.1.4 An Alternative to Shared-Nothing Replication.\n      * 6.2 Internal Consistency Mechanisms\n      * 6.3 Multiclient Coordination\n\n        * 6.3.1 Consistent Mechanisms.\n        * 6.3.2 Available Mechanisms.\n      * 6.4 Leases\n      * 6.5 POSIX Semantics\n    * 7 CASE STUDIES\n\n      * 7.1 AFS and Coda\n      * 7.2 CephFS\n      * 7.3 IPFS\n      * 7.4 A Design Exercise\n\n        * 7.4.1 Path-to-Inode and Inode-to-Data Translation.\n        * 7.4.2 Redundancy and Coordination.\n    * 8 CONCLUSION\n    * ACKNOWLEDGMENTS\n    * REFERENCES\n    * Cited By\n    * Index Terms\n    * Recommendations\n    * Comments\n\nSkip Abstract Section\n\n## Abstract\n\nDecades of research on distributed file systems and storage systems exists.\nNew researchers and engineers have a lot of literature to study, but only a\ncomparatively small number of high-level design choices are available when\ncreating a distributed file system. And within each aspect of the system,\ntypically several common approaches are used. So, rather than surveying\ndistributed file systems, this article presents a survey of important design\ndecisions and, within those decisions, the most commonly used options. It also\npresents a qualitative exploration of their tradeoffs. We include several\nrelatively recent designs and their variations that illustrate other tradeoff\nchoices in the design space, despite being underexplored. In doing so, we\nprovide a primer on distributed file systems, and we also show areas that are\noverexplored and underexplored, in the hopes of inspiring new research.\n\nSkip 1INTRODUCTION Section\n\n## 1 INTRODUCTION\n\nCongratulations, you have just been asked to design the next distributed file\nsystem. But how should you go about it? You can find decades of research on\ndistributed file systems and object stores. And the pace of research is\naccelerating as the world continues to move away from scale-up to scale-out\nsolutions and as the desire increases to provide data management across\nmultiple storage endpoints instead of treating them as isolated silos.\n\nThere are so many research and production systems to learn about\u2014and to learn\nfrom. But comparably far fewer high-level design options are available when\ndesigning various aspects of a distributed file system, and each of these\noptions results in a different set of tradeoffs. For example, there are only\nso many high-level approaches for stitching together a file system namespace\nthat is spread over multiple servers, which then affects the system\u2019s\nperformance and its ability to distribute load or to handle partitions. In\nthis article, we identify several key design decisions, examine the most\ncommonly used options and some of their variations, and provide a qualitative\nanalysis of the relevant tradeoffs. Quantitative evaluation is beyond the\nscope of this work.\n\nWhen it comes to designing a new distributed file system, defining a new\nresearch project, or learning about these file systems for any other reason,\nit is important to be aware of the choices and their implications. None of the\nexisting designs is obviously the best for every situation, so a file system\ndesigner should be aware of the various options and make careful choices that\nfit best with the requirements, such as read and write performance,\nconsistency, partition tolerance, and redundancy. New researchers should also\nbe well aware of this design space so that they can focus their efforts on\nunderexplored areas or on eliminating undesirable properties of existing\ntechniques, rather than simply designing a new file system that does not\nadvance the state of the art.\n\nOur aim is not to present an exhaustive survey of all the relevant literature\nbut just to focus on the options for high-level approaches to designing three\nkey parts of the distributed file system stack: locating an inode (or\nequivalent) given a path, locating data given an inode, and choosing various\nconsistency and synchronization options, which can be\u2014with some\nlimitations\u2014mixed and matched. Even though we do not claim to explore all the\ndesign choices and tradeoffs exhaustively, our goal is to survey the most\nimportant ones. We mention specific systems and publications only where they\nare relevant, drawing examples from both the old and the new and from both\nacademia and industry. We do not aim to list or to examine all the important\ndistributed file systems in history, of which a good starting point for\nexploration includes prior surveys of distributed file systems [56, 67], as\nwell as topic-specific surveys on peer-to-peer file systems [22] and\ndecentralized access control in distributed file systems [39].\n\nFor the purposes of this article\u2014so that we can capture a wider set of\ninteresting design options\u2014we define a distributed file system quite broadly\nas a system that stores files across more than one node and supports basic\noperations such as read, write, create, lookup, and listing a directory. We\nalso generally assume a hierarchical namespace, regardless of whether it has\none or several roots. Many of the design options also apply to distributed\nobject stores, which have several requirements and constraints in common with\ndistributed file systems.\n\nAfter we describe a high-level, archetypical file system architecture in\nSection 2, we survey the common types of pointers that are used in file\nsystems, such as those that point from directory entries to inodes or from\ninodes to data, in Section 3. Sections 4, 5, and 6 then survey the key parts\nof the file system design: locating inodes given a path, locating data given\nan inode, and choosing options to handle coordination and to ensure file\nsystem consistency. Finally, we present several case studies and a design\nexercise in Section 7. In the case studies, we examine the tradeoffs or\ncharacteristics of several known file systems through the lens of those design\nchoices. We then use these tradeoffs to design a hypothetical file system. We\nthen conclude in Section 8.\n\nSkip 2INSIDE A DISTRIBUTED FILE SYSTEM Section\n\n## 2 INSIDE A DISTRIBUTED FILE SYSTEM\n\nAt a very abstract level, a distributed file system\u2014just like any other\nstorage system\u2014maps pathnames to data so that it can perform reads and\nupdates. However, its distributed nature makes it harder to coordinate\noperations across all nodes and to ensure enough redundancy for the system to\nsufficiently tolerate node failures and, if applicable, also to tolerate\nnetwork partitions. In addition, use of distributed file system techniques can\nbe advantageous when scaling out the performance of local file systems [28],\neven though the set of challenges for the system is smaller; for example, we\nwould not expect a network partition between file system processes that run on\ndifferent cores.\n\nFigure 1 is an example of a distributed file system architecture (not of any\nspecific file system) from a mash-up of several systems that, for didactic\npurposes, illustrates several important concepts, including:\n\nFig. 1.\n\nView Figure\n\nFig. 1. An example of a distributed file system architecture data path,\nillustrating a path-to-inode translation, followed by inode-to-data\ntranslation.\n\n  * Pointer types: Distributed file systems use different kinds of what we call pointers to reference metadata and data. For example, the list of block locations in the inode in this figure uses an explicit, indirect pointer type, because the inode stores only volume IDs and block numbers, needing to consult an external volume-to-node map to determine the actual locations.\n\n  * Data and metadata management granularity: Data and metadata are managed at different granularities at the different levels. Figure 1 illustrates partitioning the metadata by subtrees (i.e., subtree granularity), while allowing different parts of files to be physically stored on different nodes (i.e., subfile granularity).\n\nWe discuss pointer types and granularities in more detail in Section 3.\n\nAnd, at a high level, we can divide the data path into two parts:\n\n  * Path-to-inode translation: Starting from the pathname, how does a system locate the inode of the file or directory, which contains its key metadata, such as the location of the data? The example file system in Figure 1 uses prefix tables (Section 4.3) to determine which metadata server stores which part of the namespace.\n\n  * Inode-to-data translation: Starting from the inode, how does a system locate the data? In this example, the inodes reference data at subfile granularity, and the pointers specify volume IDs and offsets within volumes. The volume IDs are then translated to node IDs.\n\nWe discuss the various options for the above layers of the file system stack\nin Sections 4 and 5, respectively.\n\nAnd unlike local file systems, distributed file systems must also make\nadditional design choices:\n\n  * Redundancy and coordination: How does a system ensure that there is enough redundancy\u2014whether replication or erasure coding\u2014to handle failures, and how does it then coordinate accesses and updates across all those replicas and/or erasure-coded fragments?\n\nWe discuss these aspects of the distributed system designs in Section 6.\n\nFor the purposes of this article, we use the terms node and server\ninterchangeably to refer to a node that hosts a part of the file system, such\nas its data, metadata, or both. A file system client accesses the file system\nby sending requests to one or more of the servers, through either a standard\nprotocol such as NFS, a client-side library, or an agent. A client does not\npermanently host any of the file system\u2019s data or metadata, though it may need\nto retain small amounts of state such as cryptographic keys or file handles\nfor access. In some file systems, participants can be both clients and servers\nat the same time. Cluster refers to a collection of all the nodes that host\nthe file system but not its clients unless stated otherwise.\n\nSkip 3POINTERS Section\n\n## 3 POINTERS\n\nBefore we discuss the overall architecture of distributed file systems, we\nneed to understand three important concepts: pointer types, pointer\ngranularities, and multipointers.\n\n### 3.1 Pointer Types\n\nDistributed file systems use several kinds of pointers, or lookup mechanisms,\nfor pointing to metadata or data. The types are summarized in Table 1 and are\nexplained further in the rest of this section, from the simplest to the most\ncomplicated type.\n\nTable 1.\n\nType| Network Hops| Examples  \n---|---|---  \nLocal| 0| Inode to data in AFS [25]  \nExplicit, direct| 1| Multiaddresses in libp2p [34]  \nExplicit, indirect| 1| Remote links in NetApp\u00ae ONTAP\u00ae FlexGroup volumes [30]  \nComputed (implicit)| 1| CephFS [73], distributed volumes in GlusterFS [21]  \nOverlay network| \\\\( O(\\log n) \\\\)| Name to inode in IPFS [9]  \n  \nView Table\n\nTable 1. Pointer Types\n\n#### 3.1.1 Local Pointers.\n\nThis is the simplest kind of pointer, which always references metadata or data\non the same node of the file system. For example, this kind of pointer is used\nin inodes of file systems that store an inode and the corresponding data on\nthe same node, such as AFS [25], Coda [57], and NetApp Data ONTAP\u00ae GX [17] and\nin some GlusterFS configurations [21]. In this case, these pointers are\nperfectly analogous to the local pointers that the underlying local file\nsystem uses to reference data from the inodes.\n\n#### 3.1.2 Explicit, Direct Pointers.\n\nThe simplest distributed pointer explicitly names a node that it references,\nsuch as by specifying its IP address. The main disadvantage for these pointers\nis that they are inflexible. For example, if a node\u2019s IP address changes\n(e.g., due to the use of DHCP), then the pointers suddenly stop working. Or,\nmore importantly, if a node fails and a different node assumes its role, then\nall the pointers have to be updated.\n\nThis kind of pointer is surprisingly rare in distributed systems, but one\nexample of a well-designed usage is in libp2p [34], which is used by IPFS [9]:\nInstead of just using opaque peer IDs, it can also augment them with the\npeer\u2019s IP address and port in its multiaddress format. So, as long as the IP\naddress does not change, others can contact the peer directly without needing\nto be routed through the overlay network.\n\n#### 3.1.3 Explicit, Indirect Pointers.\n\nThis is perhaps the most common kind of pointer in distributed file systems,\nbecause it is both simple and flexible. Explicit, indirect pointers reference\na node, a local volume, or a subdirectory on the node without actually naming\nthe node, such as by referencing a virtual node ID or a local volume ID. These\nIDs are then mapped by using a table, which is typically replicated on all the\nnodes in a cluster, and\u2014depending on the type of pointer and the\nimplementation of the file system\u2014is also cached or replicated on the clients.\nThese tables are usually small and infrequently updated; updates are usually\ncoordinated through a fault-tolerant mechanism such as Paxos.\n\nA specific kind of this pointer type is a prefix table (Section 4.3), which\nmaps path prefixes to nodes that manage the corresponding subtrees. An example\nof such usage is the volume location databases in AFS [25], Coda [57], and\nData ONTAP GX [17].\n\n#### 3.1.4 Computed (Implicit) Pointers.\n\nInstead of storing the pointers explicitly, several distributed file systems\ncompute the target of the pointer by using a key, such as a file ID or a\ncontent hash of the data, and a cluster membership list. Perhaps the best\nknown example is the CRUSH [74] function used in CephFS [73] to place and to\nlook up data on the nodes in the cluster. Computed (implicit) pointers are\nusually implemented by using consistent hashing [29] to minimize data movement\nwhen a node joins or leaves a cluster, but other approaches are possible. For\nexample, Flat Datacenter Storage [45] uses a hash function to locate the first\nblock (called the tract) of the file, but the rest of the blocks are located\nrelative to it in a manner that distributes load and capacity more evenly\nacross the cluster.\n\nThis approach can be combined with the explicit, indirect pointer type by\nletting the hash function reference a virtual node ID or a local volume ID,\nwhich is then resolved by using a table replicated on all the nodes in the\nsystem.\n\n#### 3.1.5 Overlay Networks.\n\nMany peer-to-peer systems use overlay networks based on distributed hash\ntables (DHTs), for example, Chord-style DHTs [61]. One advantage of an overlay\nnetwork is that a request can be routed to the appropriate node just by using\na key, without knowing the node\u2019s address, location, or identity\u2014and even\nwithout knowing the full membership list. In fact, only a small fraction of\nthe membership nodes must be known for the routing to function, which makes it\nadvantageous for large-scale clusters with high node churn. Such overlay\nnetworks are thus highly robust to node failures, and many (but not all) are\nalso robust to partitions.\n\nA disadvantage of DHT-based overlay networks is their cost in the number of\nnetwork round-trips: Contacting a node based on the ID of an object that it\nhosts costs \\\\( O(\\log n) \\\\) round-trips, where \\\\( n \\\\) is the total number\nof nodes in the network. At each step, a node forwards the request to another\nnode whose ID is closer to the requested ID by some metric, such as the number\nof shared least significant digits as illustrated in Figure 2. A common\noptimization is to cache seen node IDs and their IP addresses.\n\nFig. 2.\n\nView Figure\n\nFig. 2. An example of a DHT-style overlay network from OceanStore [31] showing\na sample path of routing from node 2207 when searching for 4598. Each node\nroutes the request to the next node that it knows about, which has at least\none more shared least significant digit with the search key (underlined).\n\nExamples of such networks are Kademlia [38] (a variant of which is used in\nIPFS [9]), Tapestry [23] (used in OceanStore [31]), and DHash (a part of CFS\n[13]). These networks should not be confused with Dynamo-style DHTs [14],\nwhich are also distributed hash tables. The difference is that Dynamo-style\nDHTs require each node to have full knowledge of node membership and are thus\ncorrespondingly faster, while DHT-based overlay networks do not require such\nknowledge and are correspondingly slower.\n\n### 3.2 Pointer Granularities\n\nA target that a pointer references can be one of the following four\ngranularities:\n\n  * Subtree (sometimes called volume): An entire subtree.\n\n  * Bucket (not to be confused with Amazon S3 buckets): A collection of potentially unrelated files or parts of files.\n\n  * File: An entire file, either its inode or all of its contents.\n\n  * Subfile: A part of a file, either fixed-length or variable-length.\n\nThese granularities often become the granularities of metadata or data\nmanagement, because most file systems cannot switch between them. For example,\nprefix tables in AFS [25] have the subtree granularity, which means that it is\nimpossible to change the location of one file without changing its name; we\ndiscuss this issue further in Section 4.3. IPFS [9] is a notable exception,\nwhich can switch between referencing file contents by using whole-file and\nsubfile granularities.\n\n### 3.3 Multipointers\n\nTypically a pointer points to a single target; multipointer generalizes the\nconcept of pointers to specify potentially multiple targets. It can be done\nimplicitly or explicitly.\n\nFor example, the master server in Google File System (GFS) [20] has an\nexplicit list of servers that store each chunk (even though it is just\nmaintained in memory); another example is IPFS [9], which explicitly stores\nthe list of replicas. An example of an implicit multipointer in CephFS [73] is\nthe mapping of placement group IDs to data nodes (called Object Storage\nDevices), which determines the list of the nodes from a single key by using\nthe CRUSH [74] hash function.\n\nSkip 4PATH-TO-INODE TRANSLATION Section\n\n## 4 PATH-TO-INODE TRANSLATION\n\nThe first part of the data path is path-to-inode translation, in which the\nfile system starts with a path, or alternatively a cached directory object and\na file name, and finds the corresponding inode. This inode contains important\nmetadata such as size, owner, permissions, checksum, and location of data.\nMany distributed file systems call this set of information a metadata object;\nin this article, we refer to it as an inode and metadata interchangeably.\n\nMost distributed file systems resolve paths one component at a time, just like\ntheir single-node counterparts. Only a few file systems, such as CalvinFS\n[68], employ whole-path indexing. Resolving one component at a time makes many\noperations on directories faster, such as renaming or changing permissions.\nHowever, certain other operations are more complicated; for example, moving a\ndirectory must be properly synchronized to avoid creating directory loops [7].\n\nTable 2 summarizes the common design options and assesses them based on the\nfollowing characteristics:\n\nTable 2.\n\nMethod| Lookup Latency(Round-trips)| FlexibleInodeLocation|\nMetadataGranularity| PartitionTolerance| Examples  \n---|---|---|---|---|---  \nSingle Node| \\\\( O(1) \\\\)| No| Subtree| No| HDFS 1.x [60], Orion [78]  \nReplicated Namespace| \\\\( O(1) \\\\)| No| Subtree| Depends| GlusterFS [21]  \nReplicated Prefix Table| \\\\( O(1) \\\\)| No| Subtree| Depends| AFS [25], Data\nONTAP GX [17]  \nRemote Links| \\\\( O(1) \\\\)| Yes| Subtree or File| No| Farsite [7], FlexGroup\nvolumes [30]  \nShared-Disk Like| Usually \\\\( O(1) \\\\)| No| File| No| GPFS [58], Oracle FSS\n[32]  \nComputed| \\\\( O(1) \\\\)| No| File| Depends| GlusterFS [21], Tahoe-LAFS [64]  \nOverlay Network| \\\\( O(\\log n) \\\\)| No| File| Depends| IPFS [9], OceanStore\n[31, 54]  \nDistributed Database| Depends| No| Depends| Depends| ADLS [51], CalvinFS [68]  \n  \nView Table\n\nTable 2. Path-to-Inode Translation\n\n  * Lookup latency (round-trips): The latency of path component lookups and updates, measured as the number of nodes that need to be contacted via a network round-trip (\\\\( n \\\\) refers to the number of nodes in the cluster).\n\n  * Flexible inode location: Whether it is possible to change the location of the inode (not necessarily data) in the network without changing either the pathname or an internal file ID; that is, whether the file system supports location independence of naming. Even if an approach is marked as \u201cNo,\u201d it may be possible to achieve some flexibility through clever solutions, such as Coral Distributed Sloppy Hash Table (Coral DSHT) [19] for overlay networks (we briefly discuss Coral in Section 4.7).\n\n  * Metadata granularity: The granularity of metadata management; that is, whether the metadata for files is grouped by subtrees, or whether each file\u2019s metadata are distributed independently.\n\n  * Partition tolerance: Whether the approach (not necessarily all specific implementations) supports file system availability in minority network partitions; that is, when less than half of the nodes are reachable. There are several possible definitions of availability, but in this article, we generally concern ourselves with the stronger definition, which describes the system as available only if it can continue to perform updates even in the minority partition. Many systems support the weaker form of availability, in which nodes can read any data that they can navigate to on their side of the partition but may not be able to continue to write.\n\nWe will now discuss each of these approaches. In the following subsections, we\nfirst explain the approach by using an exemplar system, then we discuss\ntradeoffs, and then\u2014for interested readers\u2014we discuss variations and other\nnotable technical details. We start by discussing the simplest but less common\napproaches and then proceed to the more common approaches.\n\nSeveral systems combine two or more approaches, and we point out several such\nexamples throughout this section. Finally, in Section 4.9, we present several\nother important design approaches. We conclude Section 4 with a brief\ndiscussion in Section 4.10 of several interesting, underexplored problems.\n\n### 4.1 Single Node\n\nThe simplest case of path-to-inode translation is to store the entire\nnamespace (both directories and files) on a single node of a cluster,\npotentially being replicated on another node in an active-passive manner. This\napproach may be sufficient for small clusters, but because of its scalability\nproblems, it is not common. It is, however, important historically; for\nexample, HDFS [60] used to support only one active name server before HDFS\nFederation was introduced in version 2.0. Since version 2.0, HDFS supports\nmultiple name nodes that manage independent namespaces, even though they still\nshare the same set of data nodes. More recent examples include Orion [78] and\nMooseFS [40], which manage the metadata of the entire namespace on a single\nmetadata server, which then can be replicated.\n\nOne advantage of storing the entire namespace on a single node is that\nresolving a path requires only a single round-trip to that node and back.\nHowever, the node can easily become the bottleneck, so this approach is viable\nonly for small clusters or for cases of large files. One option to alleviate\nthis problem is to use shadow masters, which can serve read-only traffic, such\nas in GFS [20]. Depending on the file system, the version of the metadata that\nis served by the shadow master may or may not slightly lag behind the version\nthat is served by the master. Another advantage of this approach is that the\nsingle server becomes the central point of coordination.\n\n### 4.2 Replicated Namespace\n\nPerhaps the second simplest case of path-to-inode translation is to have the\nfile system replicate the directory structure or even the entire namespace\n(including directory entries for files) on all nodes in the cluster. This\napproach can scale better than the single-node method but is still limited to\nsmaller clusters, because, although metadata reads can be performed quickly,\nupdates are slower. This approach, although not common, is used by one\nprominent distributed file system\u2014GlusterFS [21].\n\nThe cleanest examples of replicated namespace are the striped and dispersed\nvolumes in GlusterFS, illustrated in Figure 3, which replicate the entire\nnamespace on all nodes of the cluster. In the striped case, the file system\nstripes the file data across the cluster; in the dispersed case, it uses\nerasure coding. Unlike many other file systems, this process is driven by\nsoftware running on the clients. Metadata are smaller than data, so it is\nfeasible for the entire namespace to be stored on each node, especially in\nlarge-file workloads.\n\nFig. 3.\n\nView Figure\n\nFig. 3. A simplified example of a striped volume in GlusterFS [21], which\nreplicates the entire namespace on all nodes, striping the data across them.\nIn this example, the distribution is entirely driven by the client software.\n\nIn the distributed configuration, GlusterFS replicates only the directory\nstructure onto all nodes, which allows it to scale to a larger number of nodes\nbut stores each file on a single node or replicates it onto a subset of nodes.\nFiles are placed according to the consistent hash of their name by using a\nmechanism that is reminiscent of a Dynamo-style DHT [14]. It can be thought of\nas a combination of the replicated namespace and hash-based file placement,\nwhich we describe in Section 4.6. This approach is more scalable, because\nthere are usually many fewer directories than files.\n\nAn advantage of this approach is that a path lookup can be satisfied by using\na single network round-trip if the entire namespace is replicated. At most, it\ncan take two round-trips if only the directories are replicated (one to get\nthe containing directory, the other to get the file if it is on a different\nnode). The obvious drawbacks of this approach are the cost of managing\nreplication across a potentially large number of replicas and the resulting\nimpact on scalability, storage overhead, and the latency of metadata updates.\n\nPartition tolerance depends on the implementation and/or the configuration of\nthe system. For example, GlusterFS can be configured to turn replicas on the\nminority side of the network partition into read-only replicas, but it is also\npossible to let the two sides of the partition operate independently, creating\na split-brain problem, and heal the differences later. However, many kinds of\ndifferences may need to be resolved manually.\n\n### 4.3 Replicated Prefix Table\n\nArguably the most popular approach is to partition the file system namespace\nbased on prefixes. The canonical example is AFS [25], which is illustrated in\nFigure 4. The file system maintains a small volume location database (VLDB),\nwhich maps prefixes to nodes that store the corresponding subtrees, which AFS\ncalls volumes. More generically, we call such a database a prefix table for\nclarity and because it is a very similar concept to Sprite-style prefix tables\n[75]; the word \u201cvolume\u201d is also often overused to name different concepts. The\nconcept of prefix tables can be thought of as a special case of explicit,\nindirect pointers, as already mentioned in Section 3.1.3. Most file systems\nallow the path prefixes to nest, just as illustrated in the example in Figure\n4. This approach is often combined with replicated namespaces for redundancy.\nFor example, Coda [57] extends AFS to allow each volume (subtree) to be\nreplicated on multiple nodes.\n\nFig. 4.\n\nView Figure\n\nFig. 4. An example of a replicated prefix table, similarly to the VLDB in AFS,\nwhich maps path prefixes to volumes or nodes.\n\nThe prefix table is usually small, infrequently updated, and replicated on all\nnodes in the cluster. Most file systems update it by using a reliable, fault-\ntolerant mechanism, such as Paxos. The table is highly replicated, and\nupdating it is expensive. This cost creates pressure to keep the table small\nand infrequently updated, which consequently means that the subtrees should be\nrelatively large.\n\nMany implementations also replicate the prefix table onto the clients, so that\na client can determine which node stores the inode for which file and can\naccess it in a single network round-trip. The file system may allow the copies\nof the table on the clients to become stale, in which case a client learns\nabout the new version after attempting to access a file or a directory on a\nnode that does not host it anymore.\n\nPartitioning the namespace thus preserves low-latency access, while naturally\nresulting in a more scalable approach. Depending on the implementation, this\napproach can be tolerant to partitions as long as clients can continue to\naccess the subtrees that are available on their side of the partition;\nhowever, updating the prefix table is generally not safe during partitions.\n\nIf data are colocated with the metadata, then a major disadvantage of prefix\ntables is that moving (renaming) files between prefixes results in data\nmovement. Some systems, such as GlobalFS [46], solve this problem by creating\ntemporary remote links. After a client requests a file or a directory to be\nmoved, the system creates a remote link on the target volume to the original\nlocation of the file or directory, while the data movement completes in the\nbackground. File systems generally avoid making these links permanent, because\nit increases complexity and lowers performance when resolving paths.\nConversely, it is generally not possible to change the location of a file\nwithout changing its name in replicated prefix table systems.\n\n### 4.4 Remote Links\n\nThe most flexible method for partitioning the file system namespace across\nmultiple nodes uses permanent remote links (as opposed to temporary remote\nlinks, as mentioned earlier), which are hard links that point from a directory\nentry on one node to an inode on a different node. An example of this approach\nis FlexGroup volumes [30], which is a distributed file system that is built on\ntop of NetApp WAFL\u00ae [24] and is illustrated in Figure 5. Another example of a\ndistributed file system with remote links is Slice [8].\n\nFig. 5.\n\nView Figure\n\nFig. 5. A simplified example of remote links as used by FlexGroup volumes\n[30].\n\nRemote links thus avoid the need to share a prefix table across the nodes; the\nnodes need to agree only on which node hosts the root of the file system.\nThese links are typically explicit, indirect pointers (see Section 3.1.3):\nThey do not encode the IP addresses of the target nodes directly but instead\nspecify them as node IDs. Or in the case of FlexGroup volumes, they encode\nlocal volume IDs, since a node can host more than one participating local\nvolume, which allows coarse-grained rebalancing without the need to update the\nlinks.\n\nRemote links enable fine-grained file placement, where any file or directory\ncan be placed on any node, or in the case of FlexGroup volumes, on any local\nfile system on any node. Data placement can be influenced by the potentially\nconflicting goals of balancing capacity and load while ideally also preserving\nlocality [30]. And unlike the prefix tables, moving files and directories does\nnot actually result in data movement. Fine-grained rebalancing may be\npossible, if supported by the implementation, by allowing the movement of\nfiles and directories across nodes and by updating the links accordingly.\n\nUnlike partitioning the namespace via replicated prefix tables, traversing a\npath can result in several network round-trips, potentially even one or more\nround-trips per path component because of traversing remote links. This\napproach is particularly costly if the links cross WAN-level distances,\nespecially if the system is laid out such that parsing a path encounters WAN-\nlevel latencies more than once.\n\nEncountering several network round-trips in a single file system request is\nfortunately a rare occurrence, because most practical systems maintain caches\nthat map prefixes to the locations where those path prefixes are actually\nstored, similar to Sprite-style prefix tables [75]. A client or a node\ntypically starts with an empty cache, and it builds the cache over time. An\nefficient way to detect stale mappings is when a client sends a request to a\nnode that does not store the file or a directory any longer, the node replies\nwith an error [7]. The client then removes the mapping from the cache and\nrestarts the path traversal by using the second-longest prefix match from its\ncache.\n\nAnother disadvantage of this approach is its lack of partition tolerance, in\nboth its weaker and stronger forms. If part of the path traverses nodes that\nare hosted on the other side of a partition, then everything on that subtree\nmay become inaccessible\u2014even if large portions of that subtree are ultimately\nstored on the same side of the partition as the client. Partitions can further\ncause problems with making sure that remote links and their caches are\nconsistent.\n\nRetrieving attributes of multiple files or subdirectories in a given directory\ncan incur high overhead, such as when running ls -l, which fetches the inode\nacross every single remote link that is found in the directory. One way to\naddress this issue is to cache remote inodes (i.e., the inodes on the other\nside of remote links) locally, which then incurs the overhead of making these\ncaches consistent. Another source of overhead from remote links is their\nupdates, which often require executing multinode transactions, potentially\neven up to three nodes in certain cases of atomically moving a file between\ndirectories on different nodes. Because of these overheads, systems like\nFlexGroup volumes choose to minimize both the number and the update frequency\nof remote links.\n\nFarsite [7] uses remote links to stitch together a relatively small number of\nlarge subtrees. The namespace on each subtree is managed by a group of 7 to 10\nnodes, in which the nodes replicate the same metadata and use Paxos to\ncoordinate.\n\nLustre [37] is another example of a distributed file system that uses remote\nlinks to stitch together a relatively small number of large subtrees. Lustre\nDistributed Namespace Phase I [15] allows the administrator to create a new\nsubdirectory as a remote directory on a different metadata node from its\nparent. All new files and directories under that new subdirectory are then\ncreated in the new location.\n\n#### 4.4.1 CephFS Implied Remote Links.\n\nA unique variation of remote links is CephFS [73], which partitions subtrees\nacross nodes, then uses an in-memory cache (backed by a journal) to get the\ndistributed benefits of remote links without actually having remote links.\nEach CephFS metadata server manages a different set of local subtrees and\ncaches information about adjacent directories (including which server hosts\nthem authoritatively) on each of its local subtrees: both parents, recursively\nall the way to the root, and direct children.\n\nPath resolution can thus start at any node, and at each step, if the node\u2019s\ncache does not have information about the requested subdirectory, then path\ntraversal can continue at the appropriate authoritative node until it reaches\nthe subtree boundary. At that point, the traversal switches to another node\n[72]. A path traversal can thus involve several nodes, just like with remote\nlinks. The metadata server\u2019s in-memory cache\u2014and by extension, its journal\ncontents for persistence\u2014thus contains equivalent information to remote links,\neven though they are not stored explicitly.\n\nAnother difference between CephFS and file systems that use remote links is\nthat the directory inode is stored with its parent directory, not on the node\nthat manages its contents.\n\n### 4.5 Shared-Disk-Like File Systems\n\nAn important class of distributed file systems is built around a shared-disk\nabstraction, where every node in the cluster has relatively low-level\naccess\u2014i.e., block- or chunk-level access\u2014to all disks in the cluster. This\ntypically assumes a form of networking in which optimizing for locality is not\nas important (e.g., flat data center networking [45]), and it also assumes\nthat there are no network partitions, which limits the environments in which\nthese file systems can be deployed. One of the best-known examples is GPFS\n[58], in which nodes access shared storage at a chunk granularity and\ncoordinate among themselves by using a distributed locking manager.\n\nSuch file systems often resemble local file systems, except with distributed\nshared storage and distributed locking. Directory entries simply reference\ninode IDs as in a local file system. But the inodes may be actually stored on\nanother node in the cluster, and the system generally does not make any effort\nto keep related items colocated. In fact, to maximize parallelism, it is\ncommon for chunks of the same file to be evenly spread throughout the cluster.\n\nA more recent example of a shared-disk-like file system with a somewhat\ndifferent architecture is Oracle FSS [32], which builds a file system on top\nof a distributed direct-access storage device that supports conditional\nmultipage writes. FSS uses this feature to implement a B+ tree, which stores\nall of the file system\u2019s metadata.\n\nShared-disk-like file systems share some characteristics of remote links,\nbecause many directory entry-to-inode links are remote. However, because these\n\u201clinks\u201d are so commonplace, these systems are designed so that updating them\ndoes not have to be as limited. Shared-disk-like file systems typically spread\nboth data and metadata across a large number of nodes in a very scalable\nmanner (e.g., consider the way that GPFS [58] uses extendible hashing for\ndirectories, discussed in Section 4.9.2), which provides good load\ndistribution and allows large clusters. Striping data across multiple nodes\nallows a file system to offer high aggregate throughput even for a single\nfile, which is why shared-disk-like file systems tend to also be parallel file\nsystems. (We talk more about parallel file systems in Section 5.2.) As a\nresult, many shared-disk-like file systems are used for serving high-\nperformance computing (HPC) workloads.\n\nThese kinds of file systems are different from systems such as Lustre [37],\nwhich support shared (e.g., dual-ported) storage for reliability but normally\nallow shared storage to be accessed only by a single server at a time. (We\ndescribe this architecture in Section 6.1.4.) Similarly, these shared-disk-\nlike systems should not be confused with file systems that use a shared-disk\nabstraction only for data but not for metadata. For example, Orion [78], which\nis a distributed nonvolatile memory file system based on NOVA [77], manages\nmetadata on a separate metadata server but allows clients to directly access\nstorage on all data nodes by using RDMA.\n\n### 4.6 Computed (Using Cluster Membership List)\n\nInstead of storing the location of file and directory inodes explicitly (or\neven not so explicitly through prefix tables), several distributed file\nsystems compute the location based on the file name or an internal inode or\nfile ID, in a manner similar to computed (implicit) pointers (Section 3.1.4).\nPerhaps the simplest examples are distributed volumes in GlusterFS [21],\nillustrated in Figure 6. This mechanism replicates the directory structure on\nall nodes as already mentioned in Section 4.2 but places files on the\nindividual nodes based on a hash, similarly to Dynamo-style DHTs [14]. Each\nsuch directory stores a map of hash ranges to nodes, and the file system\nplaces files according to the appropriate directories. GlusterFS can rebalance\ndata as necessary by modifying the hash ranges and by moving data accordingly,\nsuch as in response to the addition and removal of nodes.\n\nFig. 6.\n\nView Figure\n\nFig. 6. A simplified example of a distributed volume in GlusterFS [21], which\nreplicates the directory structure but places files on individual nodes based\non a hash of the file name.\n\nIn the case of GlusterFS, this process is entirely client driven, which means\nthat a client can determine and access the given node with a single network\nround-trip. The file system supports two mechanisms for dealing with stale\nmappings: a temporary remote link and a lookup request broadcast. If a user\nrenames a file and the new file name hash falls within a range that is\nassigned to another node, then GlusterFS has to migrate the file to that other\nnode. However, so that the operation can be acknowledged to the user quickly,\nGlusterFS creates a temporary remote link in the new location while the data\nmigrates in the background. In the absence of a link file, such as during\nrebalancing, if a client does not find the file where expected, then the\nclient broadcasts the lookup request to all the nodes in the cluster.\n\nObviously, this approach depends on the file system knowing the list of\nmembers of the cluster; it does not deal gracefully with membership changes,\nand it is not partition tolerant.\n\n#### 4.6.1 Probabilistic Computed Location.\n\nOne unique approach for overcoming these problems is used by Tahoe-LAFS [64],\na decentralized peer-to-peer file system designed specifically for high\nmembership churn and low trust in nodes. It uses an algorithm similar to\nRendezvous hashing, also known as Highest Random Weight hashing [65, 66], to\nprobabilistically compute file locations. To determine where to store a file\nwith the unique key \\\\( g \\\\), a Tahoe-LAFS client first computes \\\\(\n\\mathrm{Hash}(g, s) \\\\) for every possible server node \\\\( s \\\\), then it\nsorts the values. The client then stores the erasure-coded fragments on the\nfirst \\\\( N \\\\) nodes, but requires only \\\\( k \\lt N \\\\) fragments to\nreconstruct the file; typically \\\\( N = 10 \\\\) and \\\\( k = 3 \\\\). When looking\nup a file, it repeats this process, but checks the top \\\\( 2N \\\\) nodes in\nparallel, determines the highest version number of the file that it\nencounters, and fetches the corresponding fragments from those nodes. If not\nenough fragments are available, then it checks further down the list.\n\nThe system periodically checks each file to make sure that enough erasure-\ncoded fragments are available within the top \\\\( 2N \\\\) nodes, and if not, it\nuploads additional fragments. This approach, although somewhat expensive,\nallows the system to handle high membership churn and network partitions. If a\nnetwork partition occurs, then each file that is available within the\npartition is periodically republished, and Tahoe-LAFS can always create new\nfiles. Because of the codes that the system uses to achieve its high\nreliability, a particularly significant source of overhead in Tahoe-LAFS is\nthe CPU cost of erasure coding.\n\n### 4.7 Overlay Network\n\nA more common approach that bases file or inode location on file names or IDs\nuses overlay networks, typically based on DHTs as explained in Section 3.1.5.\nOverlay networks are decentralized, robust, and often partition-tolerant\nmechanisms for finding nodes based on the IDs or hashes of data that they\nhost. The nodes are not required to be aware of the entire membership list,\nalbeit at the cost of \\\\( O(\\log n) \\\\) network round-trips, where \\\\( n \\\\)\nis the number of nodes, as explained previously.\n\nA recent example of a distributed file system that uses an overlay network is\nIPFS [9], which stores a mapping from file IDs to the corresponding lists of\npeers in a DHT overlay network based on Kademlia [38]. When retrieving a file,\na client retrieves the list of peers, after which it can contact any of them\nto read the contents. Many other peer-to-peer file systems, such as OceanStore\n[31, 54], address both inodes and file contents by using the overlay network.\n\nAs an optimization, file systems such as IPFS that are designed to operate on\na global scale sometimes use multiple nested clusters inspired by Coral DSHT\n[19], which makes retrieval of keys in nested clusters very efficient. Figure\n7 illustrates Coral with three levels of nested clusters organized by ping\nlatency; by default, level 2 requires round-trips between nodes to be less\nthan 20 ms, level 1 must be less than 60 ms, and level 0 contains all the\nnodes. A node publishes information to all the clusters in which it\nparticipates, starting from the innermost one. During lookup, a node searches\nfirst within its innermost nested cluster, then progresses to the next level,\nuntil it ultimately reaches the global cluster, stopping as soon as it finds\nthe key.\n\nFig. 7.\n\nView Figure\n\nFig. 7. Coral DSHT [19] organizes nodes into nested clusters based on pairwise\nping latency. This method improves read latency for information that is stored\non nodes closer in the network. IPFS [9] is an examle of Coral DSHT usage.\n\nOne example of how such file systems can be made partition tolerant (because\nnot all of them are) is by letting nodes periodically republish the data they\nhost to the DHT. The data are then accessible to other nodes on the same side\nof the partition. When the partition heals, the ID-to-peer mappings are stored\non the nodes whose IDs are now closest to the file ID. Entries stored in such\nDHTs typically have expiration times (e.g., a Time to Live or TTL) to\nperiodically eliminate stale entries.\n\n### 4.8 Distributed Database\n\nSeveral file systems use an existing distributed, sharded database management\nsystem (DBMS) to store the file system metadata, often both inodes and\ndirectories\u2014in fact, the entire namespace. For example, ADLS [51] uses parts\nof Microsoft SQL Server\u2019s Hekaton engine [16]; CalvinFS [68] is built on top\nof Calvin [69] (a fast geo-distributed database); and Magic Pocket [12]\nheavily uses sharded MySQL [43]. Although most file systems are designed with\na specific database engine in mind, HopsFS [44] can use any compatible NewSQL\ndatabase engine, such as MySQL Cluster.\n\nThe use of existing, mature DBMS solutions greatly simplifies a file system\u2019s\ndesign by allowing it to reuse a lot of the DBMS\u2019 functionality, especially\npertaining to consistency; coordination; replication; and, if applicable,\ntransactions. Most of the properties of these systems, such as performance or\npartition tolerance, depend on the corresponding properties of the underlying\ndatabase system.\n\n### 4.9 Other (Orthogonal) Techniques\n\nIn addition to the aforementioned design options, several orthogonal\ntechniques are also noteworthy.\n\n#### 4.9.1 File System Middleware.\n\nSome file systems, such as IndexFS [53], BatchFS [79], and DeltaFS [80], can\nbe categorized as file system middleware. They do not store any data on their\nown, but they provide an abstraction or a value-add over one or more\nunderlying distributed file systems or object stores.\n\nFor example, IndexFS [53] is a scalable metadata layer on top of existing\nstorage, and BatchFS [79] extends it by allowing clients to check out a part\nof the namespace, modify it, and then efficiently check it back in. This\noptimization is useful for bulk-oriented workloads. DeltaFS [80] takes it a\nstep further by providing decentralized namespace management with no global\nnamespace; clients can almost arbitrarily import and combine existing\nnamespaces and export their own namespaces.\n\n#### 4.9.2 Scalable Directory Implementations.\n\nIf a large directory is stored on a single node, then it can quickly become a\nscalability bottleneck. This problem is particularly pronounced in workloads\nsuch as HPC, in which a large number of compute nodes generate and/or access\nmany files in a single directory [53]. At the same time, it is important to\nmake small directories efficient. Several systems, such as GPFS [58], GIGA+\n[48], and IndexFS [53], provide directory implementations that are efficient\nfor small directories and yet very scalable.\n\nFor example, directories in GPFS [58] use extendible hashing [18]: A directory\nstarts as a hash table with one bucket, and if it exceeds a certain size, then\nGPFS splits it; also, buckets can be split recursively. A lookup of a specific\ndirectory entry can thus be accomplished by using a single network round-trip.\n\nIndexFS [53] represents directories by using sorted string tables that are\nstored on underlying shared storage, but the basic principle is similar: A\ndirectory starts as a single partition, hosted by a single file system node,\nbut when the partition size exceeds a threshold, IndexFS splits it by using\nhash partitioning and lets another node manage the other half. IndexFS can\nsplit partitions recursively as necessary, resulting in near-linear scaling.\nIndexFS also supports a very efficient batch import of changes to the\ndirectory.\n\n### 4.10 Discussion\n\nOne of the underexplored areas in the research literature seems to be the\nability to provide fine-grained inode placement that is flexible without\nchanging the file paths. The only major approach that is flexible enough to do\nthis is the use of remote links, but that approach has several major\nchallenges, such as not being partition tolerant and not being suitable for\nWAN-level latencies.\n\nAnother example of an underexplored research area is how to overcome the\ndisadvantages of decentralized path-to-inode translations. For example, it\nwould be advantageous to reduce the lookup latencies in DHT-based overlay\nnetworks from \\\\( O(\\log n) \\\\) in the common case, or to make it easier to\nreason about data availability during partitions.\n\nOther possible areas for research are novel combinations of the various\napproaches and development of new approaches for path-to-inode translation\nthat are better than any of the existing solutions (or proving that such a\nthing does not exist).\n\nSkip 5INODE-TO-DATA TRANSLATION Section\n\n## 5 INODE-TO-DATA TRANSLATION\n\nAfter the file system translates a path to an inode and loads it, the file\ndata can be stored locally on the same node as the inode, such as in AFS [25],\nCoda [57], and FlexGroup volumes [30]. Alternatively, the lookup process has\nto go through one or more levels of mapping\u2014typically by using just one kind\nof pointer type or a combination of two pointer types from inodes\u2014until the\nright data node is identified.\n\nUnless the file system uses an overlay network (Section 3.1.5), identifying\nthe node that houses the data is performed with only a single network round-\ntrip. Even if the file system uses an explicit, indirect pointer or a computed\npointer (Sections 3.1.3 and 3.1.4, respectively), the table or membership list\nthat is used to resolve the pointer is typically already replicated on all the\nnodes in the cluster. For example, CephFS uses a two-level mapping as follows:\n\n(1)| CephFS maps different parts of a file, called objects, to placement\ngroups by using a hash function.  \n---|---  \n(2)| Placement groups are then mapped to storage nodes by using the CRUSH\nfunction [74], which takes as an input the placement group ID and the\nmembership list.  \n  \nBecause the membership list is already replicated on all participants, the\ndata can be accessed by using a single network round-trip.\n\nIn the rest of this section, we discuss the relevant design decisions and\ntradeoffs.\n\n### 5.1 Inode-to-Data Pointer Types\n\nOne of the key design choices is the mechanism that the file system uses in\nthe inodes to point to data.\n\n#### 5.1.1 Separation or Colocation of Data and Metadata.\n\nThe first consideration is whether to store metadata (inodes) and data\nseparately or to colocate them; the tradeoffs for both are summarized in Table\n3. Inodes and data are separated not only when the file system uses separate\nmetadata nodes, such as in CephFS [73] or Farsite [7], but also when the\nsystem uses a distributed metadata service, such as in IPFS [9] or OceanStore\n[31].\n\nTable 3.\n\nType| Latency| Throughput| MetadataOperations| Examples  \n---|---|---|---|---  \nSeparate| Medium| Medium\u2013Fast| Medium\u2013Fast| ADLS [51], CalvinFS [68], Farsite\n[7]  \nColocated| Fast| Medium| Slow\u2013Medium| AFS [25], FlexGroup volumes [30]  \n  \nView Table\n\nTable 3. Separation or Colocation of Data and Metadata\n\nStoring metadata on separate nodes can be advantageous especially in the\nfollowing cases:\n\n  * When building a distributed file system on top of existing distributed storage, such as with CephFS [73] and HopsFS [44].\n\n  * When such architecture makes it possible to scale the metadata layer independently from the data layer, such as with IndexFS [53], DeltaFS [80], and CephFS [73].\n\n  * When an inode is allowed to reference parts or replicas of a file on different nodes, clients can achieve higher throughput by fetching data from the different nodes in parallel (analogous to striping in RAID), such as with Lustre [37] and HDFS [60].\n\n  * When the entire namespace can be stored on a single node, such as with GFS [20] and MooseFS [40].\n\n  * When reusing an existing DBMS or a DHT, such as with Magic Pocket [12] and IPFS [9].\n\nGenerally speaking, the latter two cases have the potential to implement\nmultifile transactions more efficiently, or at least more simply, such as\nmoving a file between directories that are stored on different nodes. The last\ncase is particularly intriguing if the DBMS includes features such as\ndistributed, atomic transactions.\n\nIn contrast, distributed systems that colocate metadata and data on the same\nnodes spread metadata across a larger number of nodes; systems that separate\nmetadata and data can generally store more metadata on a single node, because\nmetadata are smaller than data. Consequently, colocating data and metadata is\nlikely to have a larger fraction of multinode transactions, with the\ncorresponding negative effect on performance.\n\nDisadvantages of separating metadata from data include:\n\n  * The metadata server can become a scalability bottleneck.\n\n  * Even after the system has found and read an inode, it has to perform more network round-trips to read or to update the data, resulting in higher latency.\n\nThe latter issue can be overcome by using a hybrid approach (although it is\nnot common): The first part of a file can be stored on the same node as the\ninode, so that no extra network round-trips are needed to start reading the\ndata, while other parts of the file can be stored on different nodes to enable\nhigh parallel throughput.\n\n#### 5.1.2 Explicit, Hybrid, or Computed Data Location.\n\nIf metadata and data are not colocated, then data can be referenced in one of\nthe following three ways:\n\n  * Explicit: The data location is stored explicitly by using either a direct pointer (such as by specifying the node\u2019s IP address and the inode number) or an indirect pointer (such as by using a logical node ID or a volume ID instead of the IP address).\n\n  * Hybrid: The inode stores IDs of the data blocks, such as GUIDs (also called UUIDs) or content hashes, instead of their locations. The actual location is then determined from those IDs, either by using a computed pointer or by routing a request through an overlay network.\n\n  * Computed/Overlay: The location of data blocks is fully determined from the metadata, such as an internal file ID, or from the file name itself; the inode does not contain any information about the blocks. The final mapping can be performed with computed pointers or an overlay network.\n\nTable 4 summarizes several tradeoffs, although the tradeoffs are somewhat more\nnuanced than can be expressed in a simple table. The main practical difference\nbetween the three approaches is their ability to evenly distribute load across\ndifferent nodes in the cluster.\n\nTable 4.\n\nType| Latency| Throughput| DataLocation| Examples  \n---|---|---|---|---  \nExplicit| Fast| Fast| Flexible| Farsite [7], IPFS [9]  \nHybrid| Slow\u2013Medium| Medium| Inflexible| CalvinFS [68], Tahoe-LAFS [64]  \nComputed/Overlay| Slow\u2013Fast| Medium\u2013Fast| Inflexible| CephFS [73], OceanStore\n[31, 54]  \n  \nView Table\n\nTable 4. Data Location (Separate Data and Metadata Only)\n\nReferencing nodes (or virtual nodes or similar concepts that then directly map\nonto nodes) by using the explicit approach allows the system to distribute the\nload and capacity evenly, even taking into account up-to-date metrics, such as\nCPU utilization or used storage capacity [30]. To keep the load and capacity\nbalanced, some file systems may even allow the ability to migrate data after\nthe initial data placement. This level of flexibility makes the file system\nextremely well positioned to provide the best possible latency and throughput.\n\nComputing the location of data from the file ID or file name (i.e., the third\ncase) does not allow such flexibility, but it is usually still possible to\nachieve good throughput and\u2014unless overlay networks are used\u2014good latency.\nHowever, data cannot be placed naively. A common pitfall is to place different\nparts of a file (typically identified as a combination of file ID and a block\nnumber) by using a hash function. Even though the hash function produces a\nuniform distribution, ultimately one node receives a significantly higher\nnumber of blocks than others do [45]. This result is known as the balls-into-\nbins problem.\n\nA possible approach to overcome this problem is to pick the first data node\nfor the first block of the file in a sensible way, such as by using a uniform\nhash function, and then to place the rest of the blocks relative to that. For\nexample, if nodes are numbered \\\\( 0 \\ldots (n-1) \\\\) and the file ID is \\\\( g\n\\\\), then block \\\\( i \\\\) of the file should be placed on node \\\\(\n\\left(\\textrm {Hash}(g) + i \\right) \\bmod n \\\\). And to further even out the\nload, Flat Datacenter Storage employs more techniques on top of this method\n[45]. However, because this approach requires knowing the full list of nodes,\nit does not work well with overlay networks.\n\nOne advantage of computed location is potentially simplified reader/writer\ncoordination: Readers can immediately see the newly updated (overwritten) data\nwithout needing to reread the inode. Depending on the file system\u2019s\nimplementation, this may or may not apply to data that are appended to (or\notherwise written beyond the end of) the file. Another advantage of computed\nlocation is that writing data requires smaller updates to inodes, or possibly\neven no updates at all. CephFS [73] cites the latter as an important\nadvantage, because it significantly decreases the load on its metadata\nservers.\n\nThe hybrid case, when the inode contains block IDs whose location is then\ncomputed or determined by an overlay network, results in a generally\ninflexible system. Even if the data are placed by using a uniform hash\nfunction, it may still overload a single node due to the aforementioned balls-\ninto-bins problem, but without having an effective way to spread the load more\nevenly as in the computed case. However, other techniques may be able to\nmitigate the load on the hot nodes, such as caching data along common routing\npaths in the case of DHT-based overlay networks. The hybrid case is, however,\nparticularly useful when implementing deduplication.\n\n### 5.2 Data Placement Granularities\n\nAnother important consideration for a distributed file system is the\ngranularity of data placement\u2014how much data must be stored together on a\nsingle node. For example, flexible, fine-grained data placement is important\nfor systems that dynamically move data or add replicas depending on the usage,\nsuch as IPFS [9], or for enabling fine-grained rebalancing. Table 5 summarizes\nthe options and a few tradeoffs.\n\nTable 5.\n\nGranularity| Throughput| Flexibility| Examples  \n---|---|---|---  \nSubtree/Bucket| Medium| Inflexible| AFS [25], Data ONTAP GX [17]  \nFile| Medium| Flexible| Farsite [7], FlexGroup volumes [30]  \nSubfile| Fast| Flexible| CephFS [73], GFS [20], HDFS [60]  \n  \nView Table\n\nTable 5. Data Placement Granularities\n\nFor distributed file systems that store inodes with their data, the\ngranularity at which data are managed is identical to the granularity of\nmetadata management. For example, AFS [25] employs prefix tables (Section\n4.3), so it inherits subtree granularity. FlexGroup volumes [30] use remote\nlinks, so the whole system inherits file granularity. Storing inodes with\ntheir data generally implies that such systems can have only subtree, bucket,\nor file granularities.\n\nFile systems with subtree granularity store data under a subtree together,\nsuch as in AFS [25], and bucket granularity extends the concept by allowing\nthe system to store potentially unrelated data together, such as in Magic\nPocket [12]. The problem with subtree and bucket granularities is that they\nare inflexible, because a potentially large amount of file data must be stored\ntogether on a single node. This inflexibility can cause problems with\nimbalance in node capacity and load, without an easy way to rebalance the data\nto fix the problem. Fixing the problem likely involves significant data\nmovement, such as splitting a subtree onto multiple nodes.\n\nHowever, file granularity does not necessarily guarantee file-granular\nrebalancing either. For example, FlexGroup volumes [30] make data placement\ndecisions only on ingest but do not move data afterward due to interplay with\nthe NFSv3 protocol. However, an analysis of existing deployments showed that\nsuch imbalance is not a problem in practice.\n\nIn systems that colocate inodes and data as explained previously, managing\ndata placement at a subtree/bucket level or a file level, despite being\nseemingly inferior in isolation, is often mandated by other parts of the\nsystem design, and the advantages of those other design decisions are often\nsignificant. For example, AFS [25] uses subtree granularity because of its use\nof prefix tables, which gives the system its low latency. Coarser data\nplacement granularity, generally speaking, simplifies the system design. For\nmany use cases, the improvement in throughput or flexibility that a finer\ngranularity would provide is not important enough to warrant the extra\ncomplexity.\n\nFrom the perspective of throughput, subtree and file granularities are\nsimilar, because the file\u2019s inode references data on a single node either way.\nIn contrast, subfile granularity can improve the aggregate file throughput,\nbecause the data can be fetched from multiple storage nodes in parallel,\npossibly by multiple clients. Many such file systems also allow large files to\nbe written concurrently by multiple clients in parallel, with different\nclients writing to different storage nodes to achieve high parallel\nthroughput.\n\n#### 5.2.1 Parallel File Systems.\n\nFile systems with subfile granularity match the definition of parallel file\nsystems (sometimes called parallel architecture file systems). The definition\nspecifies that a parallel file system is a file system in which \u201cdata blocks\nare striped, in parallel, across multiple storage devices on multiple storage\nservers\u201d [67], typically to provide high performance. Informally, parallel\nfile systems are often equated with file systems that are suitable for HPC\nenvironments, and in fact, most file systems that are used in HPC fall into\nthat category.\n\nA well-known example of a highly scalable parallel file system is Lustre [37],\nwhich stripes large files across a potentially large number of storage nodes.\nIt is thus possible to achieve high write throughput if the clients are\ncoordinated enough to write to different nodes. GPFS [58] is another example\nof a scalable parallel file system that is designed for HPC workloads.\n\nHowever, the literal interpretation of the definition of parallel file systems\nallows for other file systems with subfile granularity, even if they are not\ntypically used in HPC workloads, such as GFS [20] and even IPFS [9].\n\n### 5.3 Mutable or Immutable Data\n\nAnother fundamental design decision is whether to treat data and possibly even\nmetadata as immutable. Immutability has many benefits, such as simpler\ncoordination, versioning, and snapshots, but it also has a significant\nnegative consequence: The resultant high degree of data and metadata sharing\nmakes data deletion extremely difficult, to the point that many such systems\nnever delete old blocks of data. Examples of such systems are CYRUS [11] and\nIvy [42]. Development of efficient, fault-tolerant, and partition-tolerant\nmethods for garbage-collecting of unneeded immutable data would be a\nworthwhile area for research.\n\n### 5.4 Client Library or Gateway\n\nAnother common difference between distributed file systems is whether the file\nsystem code lives in the client libraries, or whether a client accesses a\nstorage node via some standard protocol, such as NFS or CIFS. An advantage of\nthe former is an additional opportunity for optimization; for example, if a\nclient knows the location of data, such as with CephFS [73], then it can\naccess the data directly without going through a central gateway. This\napproach eliminates the need for gateway nodes, through which all requests\nmust be processed and which can be scalability bottlenecks. By reducing the\nnumber of network round-trips, it also decreases latency and potentially\nincreases throughput.\n\nAn advantage of the latter is a lower barrier to adoption, because many IT\norganizations have strict vetting of which code runs on the clients. An option\nin the middle is a protocol like pNFS [59]; among other features, pNFS allows\nthe metadata server to directly specify which parts of the file are located\nwhere.\n\nSkip 6REDUNDANCY AND COORDINATION Section\n\n## 6 REDUNDANCY AND COORDINATION\n\nDistributed file systems are trusted to store data and to make that data\navailable upon request. Being distributed, however, means that there are more\nways for things to go wrong than with a local file system; not only can disks\nfail, but so can nodes, networks, racks, and even data centers. How different\nsystems account for these failures varies, reflecting diverse goals and\ntradeoffs based on the requirements of the file system. Some systems replicate\ndata to increase availability (or use erasure coding to reduce storage\noverhead), and others offer eventual consistency to speed things up when it is\nnot critical to have the most recent data. But complexity always comes at a\nprice, and in distributed file systems, that price is some combination of\nadditional overhead for storage, CPU, network bandwidth, and network latency;\ntolerance for stale data; and the durability of data.\n\nThe parameters discussed next need not be static for a particular system or\ndata. They can be adjusted dynamically (e.g., by policy, API call, or machine\nlearning), even on a per-I/O basis [47]. For example, highly read data and\nimportant metadata can have more copies for durability or performance reasons,\nor durability parameters can be tuned to match the measured reliability of the\nunderlying media [27].\n\nIn this section, we delve into deeper detail on these techniques, discuss\ntheir overheads and offer some examples. Several of these techniques have been\nused in database products for some time. For a detailed examination, refer to\nViotti and Vukolic [70].\n\n### 6.1 Durability\n\n#### 6.1.1 Replication.\n\nMost distributed file systems use replication to ensure data availability and\ndurability if failures occur. Often, we can characterize replication with\nthree constants:\n\n  * \\\\( n \\\\) = the number of replicas\n\n  * \\\\( r \\\\) = the number of replicas that a reader contacts\n\n  * \\\\( w \\\\) = the number of replicas to which a writer writes synchronously (i.e., the number of replicas that must acknowledge the write before it is acknowledged to the client)\n\nIn general, if we have \\\\( r + w \\gt n \\\\), then the read set and the write\nset of each data item overlap, so the system offers strong consistency [71].\nThe reader thus sees all the writes, but it may have to resolve any\nconflicting updates that the file system has not yet resolved in a consistent\nmanner.\n\nA more common method of ensuring strong consistency is to have a designated\nprimary node for each piece of data and to require it to be one of the \\\\( w\n\\\\) nodes that must acknowledge the write synchronously. Then the reader can\ncontact the primary node whenever strong consistency is necessary, or it can\ncontact any node if it is acceptable to read slightly stale data.\n\nIn most distributed file systems, \\\\( r = 1 \\\\) and \\\\( w = n \\\\), which\nresults in a strongly consistent file system. But in some cases, \\\\( w \\lt n\n\\\\) or even \\\\( w = 1 \\\\), which describes asynchronous replication when \\\\( n\n\\gt 1 \\\\). Only a few systems have \\\\( r \\gt 1 \\\\) or \\\\( n \\gt w \\gt 1 \\\\),\ndespite its being a more common occurrence in NoSQL systems.\n\n#### 6.1.2 Erasure Coding.\n\nReplication meets the needs of many systems; however, a family of techniques\ncalled erasure coding (EC) or forward error correction can produce comparable\nprotection (e.g., tolerating two disk or node failures) while significantly\nreducing the storage overhead. It comes at the expense of additional network\nand computational overhead.\n\nXOR is the fastest and most basic EC technique; it computes a simple parity\nblock from \\\\( n \\\\) blocks using their bitwise eXclusive OR. This technique\noffers a useful property: If one of the original \\\\( n \\\\) blocks is lost,\nthen, to recompute the missing block, just XOR the remaining \\\\( n-1 \\\\)\nblocks with the computed parity block. For example, to be able to withstand a\nsingle data center failure, Facebook\u2019s f4 [41] uses XOR to erasure-code data\nacross data centers. Another example of a system that supports XOR erasure\ncoding is MooseFS [40].\n\nIn practice, many distributed systems that implement EC use more complex Reed-\nSolomon codes [52] to allow an arbitrary number of parity blocks to be\ngenerated at the expense of higher computational overhead. \\\\( RS(N,k) \\\\)\nrefers to a Reed-Solomon code that encodes an object into \\\\( N \\\\) blocks, of\nwhich any \\\\( k \\\\) can produce the original object; the space overhead is \\\\(\nN/k \\\\). As an example, because Tahoe-LAFS [64] is a distributed file system\nthat is designed for use with less reliable servers, it implements \\\\(\nRS(10,3) \\\\) by default. Data can be recovered even if 7 of the original 10\nservers are no longer available, while having a space cost of only \\\\(\n3.\\overline{3} \\\\) times the original data. Other examples of distributed\nsystems that support such EC are LizardFS [35] and HDFS 3.x [60], which\nsupport both Reed-Solomon and XOR codes.\n\nFor high performance, many systems keep the first \\\\( k \\\\) EC blocks\nunmodified from the original data, so the blocks can be accessed without\nadditional network requests or computation. The additional parity blocks are\nthen needed only during failure/repair scenarios. EC algorithms that include\nthe original data in the output blocks are systematic, an example of which is\nReed-Solomon with appropriate parameters.\n\nOther EC algorithms can improve upon Reed-Solomon\u2019s computational or I/O\noverheads [49]. For example, Microsoft Windows Azure Storage [26] and HDFS-\nXorbas [55] use Local Reconstruction Codes or Locally Repairable Codes, which\nreduce disk and network I/O during reconstruction but are not as space\nefficient as Reed-Solomon codes are.\n\nSome storage systems combine different erasure coding methods to capture the\nbenefits of two or more methods. For example, HACFS [76] uses a fast but less\nspace-efficient EC for hot data and uses a slower but more compact EC for cold\ndata. Facebook\u2019s f4 [41] uses a Reed-Solomon code within a data center but\nuses XOR across data centers, which minimizes the number of data centers that\nare involved in writes and degraded reads.\n\n#### 6.1.3 Placement.\n\nThe placement policies for \\\\( n \\\\) replicas\u2014and in the case of \\\\( w \\lt n\n\\\\), also for where the various writes must land\u2014often take into account\nfailure domains so that a single network or power failure is less likely to\naffect the availability of data. Domains can include fate-shared components\nwithin a data center (such as nodes and racks) as well as data centers\nthemselves. For example, HDFS [60] addresses node and rack availability by\nplacing at least one replica on a different rack, and Dropbox\u2019s Magic Pocket\n[33] and Facebook\u2019s f4 [41] erasure-code data across data centers. We can\ngeneralize such data placement policies as having \\\\( d \\\\) failure domains\n(e.g., racks or data centers) with the three parameters separated into \\\\(\nn_{0 \\ldots d-1} \\\\), \\\\( r_{0 \\ldots d-1} \\\\), and \\\\( w_{0 \\ldots d-1} \\\\).\n\nA higher number of data centers, and replicas in general, improves durability\nbut increases the bandwidth consumption of writes. Requiring writes to be\nsynchronously acknowledged from several failure domains has an adverse effect\non write latency, and performing them asynchronously primarily affects the\npeak write bandwidth. However, a higher number of replicas and failure domains\nimproves read latency, because reads can be satisfied from closer replicas,\nand/or (depending on the system) read bandwidth improves if the reads can be\nsatisfied from several replicas in parallel.\n\n#### 6.1.4 An Alternative to Shared-Nothing Replication.\n\nEven though most distributed systems implement redundancy in a shared-nothing\nmanner, where memory and storage of a node are not accessible to other nodes\n[62], several file systems support the use of shared, dual-ported storage to\nprovide redundancy. Depending on the failure scenario, this approach obviates\nor lessens the need for redundancy above the storage layer.\n\nFigure 8 illustrates an example of a building block in a highly available\nobject storage configuration in Lustre [37], which consists of two dual-ported\nRAID arrays and two object storage servers, with both RAID arrays connected to\nboth servers. The RAID arrays in this example also feature redundant\ncontrollers. In this way, Lustre is protected against individual server\nfailures, individual disk failures, and individual RAID controller failures.\n\nFig. 8.\n\nView Figure\n\nFig. 8. An example of a redundant, highly available building block of Lustre\u2019s\nobject storage service [37], which consists of two servers and two RAID arrays\nwith redundant controllers.\n\nAnother example with a similar architecture is FlexGroup volumes [30], which\nis built on top of a collection of high-availability pairs of storage\ncontrollers, where a controller can access and take over its partner\u2019s storage\nif the partner fails. This architecture provides data protection and high\navailability if disk and controller failures occur, and if more redundancy is\nrequired, then data can be replicated synchronously or asynchronously on\nanother cluster.\n\n### 6.2 Internal Consistency Mechanisms\n\nFile systems differ in how they manage their internal consistency; that is,\nthe consistency of metadata and internal data structures. Examples are the\nconsistency between replicas of the same data or metadata and the atomicity\nand consistency of operations that involve multiple files or directories. For\nmany file systems, this mechanism is important to ensure consistency if\nfailures occur, and they also use the same mechanism to ensure atomicity and\nconsistency of concurrent operations by different clients. In Section 6.3, we\nsurvey approaches to multiclient coordination.\n\nOne of the most common methods for coordination is ensuring that a group of\nnodes always agrees on the same state, such as through the use of replicated\nstate machines, Byzantine agreement protocols (such as Paxos), two-phase\ncommit, three-phase commit, and their variants. The specific choice depends on\nthe desired performance and failure tolerance tradeoffs. For example, CalvinFS\n[68] uses Paxos to create an ordered list of pending transactions. Pond [54]\nachieves high failure tolerance by using a variant of a Byzantine fault-\ntolerant protocol that was originally developed by Castro and Liskov [10]. And\nOracle FSS [32] implements two-phase commit by using Paxos, which gives it\nbetter failure resilience properties than pure two-phase commit does. Two-\nphase commit by itself is a fast coordination protocol with just two network\nround-trips, but it is not resilient to coordinator failures.\n\nSeveral other noteworthy approaches include the following:\n\n  * No Coordination: In some cases, the overhead or the complexity of coordination is simply not worth the benefits, so the systems instead opt for just a best-effort approach that may not be entirely fault tolerant. For example, Flat Datacenter Storage [45] forgoes coordinating data in replicas and synchronizes only the metadata.\n\n  * Persistent Log: File systems can also ensure crash-consistency through the use of persistent logs that survive individual node failures and by replaying those logs when a node fails; examples of such systems are GPFS [58], FlexGroup volumes [30], and IndexFS [53]. Ivy [42] takes it a step further and also uses logs to undo the operations of rogue users.\n\n  * Distributed Locking: Another approach is to lock all the required resources before they are modified or even just accessed, depending on the desired level of consistency. Locks typically govern entire objects or blocks, but byte-range locking can also be implemented. A noteworthy example of such a system is GPFS [58].\n\n  * Optimistic Concurrency Control: In contrast to pessimistically acquiring locks, it is often possible to optimistically assume that most file system objects are not being concurrently accessed. The file system keeps track of the objects that it read and wrote during an operation (called the read sets and write sets) and validates that there are no conflicts with other concurrent operations before committing them. Although this approach is common in the database literature, it is underexplored in the file system literature. A noteworthy example of such a system is CalvinFS [68], which traces its lineage to the database community. Pond [54] supports application-specific consistency for data objects, which is flexible enough to implement optimistic concurrency control.\n\n  * Optimistic with Conflict Resolution: Similarly to optimistic concurrency control, the system can proceed without acquiring any locks, but the difference is that there is no strict validation step at the end. A node instead accepts all changes that are valid according to its view of the state and resolves conflicts later. The main disadvantage of this approach is that many kinds of conflicting updates require manual intervention, although they are usually rare. A prominent example of such a system is Coda [57].\n\nCombinations of some of these approaches are possible. In some file systems,\none of the nodes is responsible for ensuring coordination, but in others, the\nclient takes on this responsibility.\n\n### 6.3 Multiclient Coordination\n\nTable 6 summarizes five major mechanisms for coordinating concurrent file\nsystem operations and qualitatively examines the relevant tradeoffs: partition\ntolerance, write performance, and the location of the latest version of an\nobject. These mechanisms are orthogonal to locks and leases through which\nclients can coordinate explicitly.\n\nTable 6.\n\nWrite to / Coordination by| PartitionTolerance| WritePerf.| Latest VersionIs\non| Examples  \n---|---|---|---|---  \nSingle dedicated master| Consistent| Medium| The master| AFS [25], CephFS [73]  \nQuorum of \\\\( m \\\\) replicas| Consistent| Slow| \\\\( m \\\\) replicas| CalvinFS\n[68], Farsite [7]  \nAny replicas, resolve conflicts later| Available| Fast| A replica| Coda [57]  \nAny nodes, write to replicas later| Available| Fast| A replica, typically|\nWheelFS [63]  \nAny nodes, data has no home| Available| Fast| Any node| DeltaFS [80], IPFS [9]  \n  \nView Table\n\nTable 6. Coordination Taxonomy\n\n#### 6.3.1 Consistent Mechanisms.\n\nThe most common mechanisms that are found both in the literature and in\npractice favor file system consistency over availability during partitions.\nOut of these mechanisms, the most common approach designates one of the\nreplicas as the primary replica for a given file, directory, or block, which\nthen handles all writes and, if applicable, consistent reads. This solution is\nthe simplest, because it creates a single point of coordination between\nwriters in the common case of only one file system object being involved\n(e.g., writing to a file), but there is a risk that the given node could\nbecome a bottleneck. The master node can then update the remaining replicas\nsynchronously or asynchronously by using one of the internal coordination\nmechanisms, such as two-phase commit. Consistent reads are typically\nimplemented by contacting the master node.\n\nThe other mechanism that favors consistency over availability uses a quorum of\nreplicas to determine the global order of concurrent operations, typically by\nusing Paxos or a similar protocol. Performing a round of Paxos for every\nsingle operation would be expensive, so several systems, such as CalvinFS\n[68], first aggregate operations into batches and then get the quorum to agree\non the total order of batches. The result is a more scalable and fault-\ntolerant approach but at the cost of extra write latency. Consistent reads can\nbe performed by enqueuing them together with the writes to establish their\nglobal order [68].\n\nUsing \\\\( n \\\\), \\\\( r \\\\), and \\\\( w \\\\), and defining \\\\( m \\\\) to be the\nquorum size, with \\\\( m = 1 \\\\) in the case of single-master systems, we can\nexpress the updating of all replicas synchronously as \\\\( w = n \\\\) or\nasynchronously as \\\\( w = m \\\\). Single-master systems that synchronously\nupdate \\\\( 1 \\lt w \\lt n \\\\) replicas or quorum-based systems that update \\\\(\nm \\lt w \\lt n \\\\) replicas are uncommon. In the case of \\\\( w = m \\\\), a\nreader can perform a consistent read either by contacting \\\\( r \\gt n - w \\\\)\nnodes and getting the latest version or, much more commonly, by contacting the\nmaster node in single-master systems or by being enqueued with the write\noperations in quorum systems. Design points with \\\\( 1 \\lt r \\lt n \\\\) are\nuncommon despite being used in NoSQL. It is possible to achieve intermediate\nforms of consistency as well, such as causal consistency, by using vector\nclocks as in GlobalFS [46].\n\nIn the PACELC classification [6], a file system that always performs\nconsistent reads is PC/EC (consistent during partitions and during normal\noperation); a file system that performs fast, inconsistent reads is PC/EL\n(consistent during partitions but otherwise optimizing for latency). Some file\nsystems, such as CalvinFS [68] and Pond [54], allow clients to specify whether\nthey want to perform a consistent or an inconsistent read.\n\n#### 6.3.2 Available Mechanisms.\n\nSome file systems choose availability over consistency during partitions, and\nin some cases even during normal operation, at the expense of introducing\nconflicts, in which different clients can see different versions of the file\nsystem.\n\nThe most straightforward approach is to allow clients to write to any\navailable replicas, such as in Coda [57], and to resolve conflicts later. This\nmethod allows a client to write as long as it can contact at least one\nreplica, and it generally improves write performance, because the writes can\nbe performed by any replica without the need for coordination. It is\nparticularly useful in geo-distributed systems, where clients are allowed to\nwrite only to the closest replica or replicas.\n\nLocating the most recent data is slower, however, because typically all \\\\( n\n\\\\) nodes must be contacted. For example, Coda ensures consistent reads during\nnormal operation by contacting all available replicas and by determining which\none has the latest version. Inconsistencies can be detected proactively, or in\nthe case of Coda, lazily during reads, and they can be resolved automatically\nwith file-type-specific routines or with manual intervention. Simple\ninconsistencies in the directory structure can often be resolved\nautomatically, such as if clients on the different sides of the partition add\ndifferently named files to the same directory. Being able to automatically\nresolve more complicated kinds of inconsistencies would be a worthwhile area\nfor future research.\n\nA disadvantage of this approach is that a client cannot write to a file if all\nof the file\u2019s replicas are on the other side of a network partition. WheelFS\n[63] resolves this issue by allowing applications to create files on any node\nin the cluster and to later migrate them to the correct places. But in\ngeneral, all writes to an existing file must be processed by the file\u2019s\nprimary node, or if the user enables eventual consistency, either by the\nprimary node or by one of its replicas.\n\nTaking this approach to the extreme, in some file systems such as IPFS [9],\ndata has no authoritative home. Any node can accept any write, but it also\nmeans that the file system either must do extra work to determine the location\nof replicas, or that information must be communicated externally to the file\nsystem. An example of the former is the use of a DHT in IPFS [9]; an example\nof the latter is DeltaFS [80].\n\nThe use of \\\\( n \\\\), \\\\( r \\\\), and \\\\( w \\\\) to analyze these systems is\ntricky at best, because the number of nodes that these systems contact during\nreads and writes depends on how many of them are reachable. Also, sometimes\nthe system behavior changes if no replicas are available, such as in WheelFS.\nFor systems in which data has no authoritative home, the number of replicas,\n\\\\( n \\\\), may not even be defined. Many available systems choose to write\nonly \\\\( w_{\\mathrm{min}} = 1 \\\\) copy of the data before acknowledging the\nrequest, even if they would prefer to write more copies if more replica nodes\nwere available. Similarly, they would be satisfied to contact only \\\\(\nr_{\\mathrm{min}} = 1 \\\\) replica if only one were available, even if, for\nexample, Coda would during the normal operation prefer to check all \\\\(\nr_{\\mathrm{max}} = n \\\\) replicas.\n\nIn the PACELC classification [6], Coda is PA/EC (available during partitions,\nbut consistent during normal operation). File systems that read only one of\nthe replicas that are not guaranteed to always return the latest version are\nPA/EL (available during partitions and optimizing for latency over consistency\nduring normal operation).\n\n### 6.4 Leases\n\nMany file systems, such as AFS [25], CephFS [73], and Farsite [7], grant\nleases (sometimes called capabilities) to clients to allow them to cache reads\nand to buffer writes to improve latency if there are no other readers or\nwriters. Leases in many systems are always exclusive, but in other systems,\nthey can be shared. For example, Farsite [7] has an elaborate mechanism of\nleases and their compatibilities to emulate most of the semantics of NTFS.\n\nLeases in most systems are long-lived and revocable. For example, a client\nreleases a lease on a file only when the file is closed; sometimes clients\nhold on to leases even longer if they expect that the user may reopen the file\nsoon. The file system revokes the lease when another client opens the file. In\nsome systems, such as IndexFS [53], leases are short-lived, which greatly\nsimplifies recovery: If a node that manages leases fails, then the node that\nassumes its place simply waits for the longest possible allowed period for a\nlease before granting any new leases.\n\n### 6.5 POSIX Semantics\n\nSome file systems, such as CephFS [73], GPFS [58], and MooseFS [40], provide\nPOSIX or near-POSIX semantics, but many others, such as GFS [20], HDFS [60],\nand IPFS [9], do not. POSIX not only prescribes specific APIs, metadata (e.g.,\nthe standard file attributes), and features (e.g., files with more than one\nhard link), but it is also fairly specific about the semantics of various file\noperations. Many difficulties can arise from trying to support some of these\nsemantics, especially with respect to writes.\n\nFor example, one of the biggest challenges is the requirement for strong\nconsistency: The read that immediately follows a write must return the most\nrecently written data, even if the data were written by a different client\n[50]. It is generally not that hard to achieve if a file is opened by only one\nclient at a time. Some file systems simply get around this problem by relaxing\nthe POSIX semantics by providing a weaker consistency model, such as close-to-\nopen consistency, which is the standard for NFS and is sufficient for many\napplications.\n\nThe two most common approaches for achieving strong consistency with multiple\nclients are as follows [36]:\n\n  * Clients commit writes immediately to the distributed file system without any write-back caching, and the file system immediately replicates the data by using one of the aforementioned strongly consistent mechanisms. For example, CephFS [73] normally uses leases, but it falls back to this behavior whenever a file is opened by multiple writers (or multiple clients with at least one writer)\u2014unless they disable this behavior when they open the file by specifying the O_LAZY flag.\n\n  * Clients acquire byte-range locks on nonoverlapping parts of the file. For example, this approach is used in GPFS [58] and Lustre [37].\n\nA file system can use the same two approaches to address another difficult\npart of POSIX semantics: the requirement that overlapping writes from multiple\nclients be applied serially without interleaving.\n\nAnother challenging requirement is for the file size and the last modification\ntime to always be accurate. Updating these two values synchronously with the\nwrites limits the system\u2019s scalability, so file systems more commonly just\nensure that they return the correct values when they are read, such as by\nusing the stat call. For example, CephFS [73] suspends all writers, collects\nrelevant information from the writers, resolves the values, and then sends\nthem back to the caller. In GPFS [58], write operations acquire a shared write\nlock on the file\u2019s inode, which conflicts only with operations that require\nthe exact values, such as stat. Most file systems ignore\u2014or at least provide a\nconfiguration option to ignore\u2014updating the last access time (atime) on every\nread.\n\nYet another difficulty that stems from supporting POSIX semantics is the extra\nload that it places on the nodes due to tracking additional state per client\nor per open file. Other challenging parts of POSIX semantics include: atomic\nrenames across directories, the ability to read and to write to an unlinked\nfile until it is closed, and files with more than one hard link.\n\nSkip 7CASE STUDIES Section\n\n## 7 CASE STUDIES\n\nNow that we have provided a taxonomy of various design choices for the\ndifferent parts of the design stack, we will use it to briefly describe\nseveral noteworthy file systems. At the end of the section, we will use our\ntaxonomy for a quick file system design exercise.\n\nTable 7 summarizes the design options that have been selected by some of the\nfile systems that we used as examples in this article.\n\nTable 7.\n\nSystem| Path to Inode| SeparateData andMetadata| DataLocationPointer|\nDataLocationGranularity| MulticlientCoordination  \n---|---|---|---|---|---  \nAFS| Prefix table| No| N/A| Subtree| Single master  \nCalvinFS| Sharded database| Yes| Hybrid| Subfile| Quorum  \nCephFS| Remote links| Yes| Computed| Subfile| Single master  \nCoda| Prefix table| No| N/A| Subtree| Resolve conflicts later  \nFarsite| Remote links| Yes| Explicit| File| Quorum  \nFlexGroup volumes| Remote links| No| N/A| File| Single master  \nGFS| Single master| Yes| Explicit| Subfile| Single master  \nGPFS| Shared disk| No| N/A| Subfile| Single master  \nHDFS 1.x| Single master| Yes| Explicit| Subfile| Single master  \nHDFS 2.x and 3.x| Independent masters| Yes| Explicit| Subfile| Single master  \nIPFS| Overlay network| Yes| Explicit| File or subfile| Data has no home  \nLustre| Remote links| Yes| Explicit| Subfile| Single master  \nOceanStore/Pond| Overlay network| Yes| Computed| Subfile| Quorum  \nOracle FSS| Shared disk| No| N/A| Subfile| Quorum  \nOrion| Single master| Yes| Explicit| Subfile| Single master  \nTahoe-LAFS| Probabilistically computed| No| N/A| File| No coordination  \nWheelFS| Remote links| No| N/A| File| Configurable  \n  \nView Table\n\nTable 7. Key Design Decisions in Selected File Systems\n\n### 7.1 AFS and Coda\n\nTable 8 summarizes the key design choices in AFS [25] and in Coda [57], which\nis an evolution of AFS that adds replication and optimizes for availability\nand performance over consistency. We first discuss AFS and then Coda.\n\nTable 8.\n\nSystem| (a) AFS| (b) Coda  \n---|---|---  \nPath to Inode  \nType| Replicated prefix table| Replicated prefix table  \nInode to Data  \nSeparation of Data and Metadata?| No| No  \nData Location Pointer| N/A| N/A  \nData Placement Granularity| Subtree| Subtree  \nMutable Data?| Yes| Yes  \nClient Library?| Yes| Yes  \nRedundancy and Coordination  \nRedundancy| \\\\( n = 1, r = 1, w = 1 \\\\)| \\\\( n \\gt 1, r_\\mathrm{min} = 1,\nr_\\mathrm{max} = n, w_\\mathrm{min} = 1 \\\\)  \nInternal Consistency| N/A (no replication)| Optimistic with conflict\nresolution  \nMulticlient Coordination by| Single dedicated master| Any replicas, resolve\nconflicts later  \nClient-Server Coordination| Leases| Leases  \n  \nView Table\n\nTable 8. AFS and Coda\n\nThe path-to-inode translation in AFS uses a replicated prefix table, which\nmaps path prefixes to volumes, which are then mapped to nodes. The inode-to-\ndata translation is then very simple, because AFS colocates data and inodes on\nthe same node. Moreover, no replication occurs, which significantly simplifies\nthe system design.\n\nThe use of a replicated prefix table implies that metadata are partitioned by\nsubtrees, and it takes a single network round-trip to access any file or\ndirectory. Inodes and data are colocated, so all data can be accessed within\nthe one network round-trip, which results in low latency. However, moving\nfiles can result in data movement if the files are moved across volumes, and\nthere is no easy or built-in method to rebalance data if one volume becomes\ntoo hot or starts to run low on free space.\n\nCoda extends AFS by adding replication. It retains the replicated prefix\ntables and colocation of inodes and data from AFS, so it also inherits many of\nthe AFS strengths and weaknesses. Coda uses replication with an optimistic\ncoordination among nodes based on conflict resolution, where conflicts may\nsometimes need to be manually resolved. A consequence of this design decision\nis high partition tolerance, in which clients can continue to write even on\nthe minority side of a partition.\n\nThese decisions thus align well with the use cases for which AFS and Coda have\nbeen designed, including home directories. The systems offer low latency, good\ncolocation of data, and even disconnected operation. The limited degree of\nfile sharing aligns well with the decisions about replication and\ncoordination.\n\n### 7.2 CephFS\n\nTable 9, column (a), shows the design decisions of CephFS [73], which are\nquite different from AFS and focus on throughput and scalability over\npartition tolerance.\n\nTable 9.\n\nSystem| (a) CephFS| (b) IPFS  \n---|---|---  \nPath to Inode  \nType| Remote links| Overlay network  \nInode to Data  \nSeparation of Data and Metadata?| Yes| Yes  \nData Location Pointer| Computed| Explicit  \nData Placement Granularity| Subfile| File or subfile  \nMutable Data?| Yes| No  \nClient Library?| Yes| Yes  \nRedundancy and Coordination  \nRedundancy| \\\\( n \\gt 1, r = 1, w = n \\\\)| \\\\( n \\ge 1, r = 1, w = 1 \\\\)  \nInternal Consistency| Persistent log| No coordination  \nMulticlient Coordination by| Single dedicated master| Any nodes, data has no\nhome  \nClient-Server Coordination| Leases| None  \n  \nView Table\n\nTable 9. CephFS and IPFS\n\nIn CephFS, the path-to-inode translation is accomplished through an approach\nthat is reminiscent of remote links. This approach provides a very flexible\nmechanism for enabling its dynamic subtree partitioning, in which the\nnamespace is dynamically rebalanced and distributed among the metadata servers\naccording to the observed workload characteristics. Separation of data from\nmetadata allows the system to rebalance the metadata without having to move\nany data. Storing metadata in a persistent log outside of the metadata servers\nsimplifies failover and rebalancing.\n\nOne of the distinguishing characteristics of CephFS is that the location of\ndata is computed instead of being stored explicitly, which significantly\ndecreases the load on metadata servers. CephFS takes it further than most\nother systems by letting the computation determine the specific list of nodes\nfor any particular object; most other systems let the computation only\ndetermine a \u201cbucket\u201d and then use a small replicated database that maps\nbuckets to nodes.\n\nThe subfile data placement granularity enables high throughput, because a\nclient can fetch different parts of a file from different servers in parallel.\nSeparation of data and metadata costs CephFS some latency in the time to first\nbyte when opening a file, but after the file is open, the client can locally\ndetermine which nodes to contact to read or to write bytes from/to any\nparticular offset. For each underlying object, coordination is handled by a\nprimary node, which simplifies the design.\n\nOne disadvantage of computed data locations that negatively affects CephFS is\nthe high cost of adding and removing nodes due to rebalancing. CephFS is also\nnot designed to be partition tolerant, but this aspect is not important for\nmost of its intended use cases.\n\n### 7.3 IPFS\n\nTable 9, column (b), lists the design decisions in IPFS [9], which optimize\nfor low trust, high resilience, moderately high node churn, scalability, and\npartition tolerance. The goal of IPFS is not only to be resilient during\ntechnical failures, but also to significantly reduce the ability for any\nadministrative entity to significantly interfere with the file system\noperation. Those entities could be individuals, corporations, or even\ngovernments, who, for example, might have the power to add malicious nodes,\nshut down nodes, send malicious messages, and create network partitions (such\nas by interrupting a country\u2019s connection to the internet). IPFS also enables\nvery dynamic and fine-grained data movement, so that files are accessible\nclose to where they are used, and more popular files are replicated more\nwidely, which improves both resilience and performance.\n\nThe required high level of resilience and partition tolerance dictates the use\nof an overlay network for path-to-inode translations, because it can handle\nnode churn and many implementations are also highly partition tolerant. A\ndecentralized approach to computed path-to-inode translation (Section 4.6.1)\nmight be the runner-up option, because it can handle high node churn. However,\nthis method is not entirely appropriate, because it requires each node to know\nabout most of the other nodes in the system, which limits both scalability and\nresilience. Data in IPFS are referenced by hash and are immutable, which both\nsimplifies the system and makes it much harder for a rogue participant to\ncorrupt files.\n\nTwo big disadvantages of overlay networks are low performance and the lack of\ncontrol over where the data are stored. To overcome these issues and to\nprovide both good performance and resilience, such as to prevent important\nfiles from being stuck on the other side of a network partition, IPFS\naddresses this problem in two ways. First, it uses Coral [19] to improve the\nlocality of lookups in the overlay network, but this approach is mostly just\nan optimization (albeit an important one). The second way is perhaps more\nfundamental: IPFS stores file/subfile locations explicitly in the inodes,\nwhich gives it fine control over where data are stored, and it does not\nrequire data to have a home. Any file can be thus stored on any node, and in\nfact, an IPFS node by default creates a replica of all the files that it\naccesses. Given the assumption of geographical/network locality, this approach\nplaces files closer to where they may be needed next, creates more replicas of\nimportant files, and decreases the probability that a file could become\ninaccessible because of node failures and network partitions.\n\n### 7.4 A Design Exercise\n\nNow instead of analyzing existing file systems, let us pretend that we want to\ndesign a file system for home directory-like workloads for a medium-sized\ncorporate customer. The workload requires low latency (even on time to first\nbyte), good throughput (although not necessarily needing to optimize for high\nsingle-file throughput), low IT administration overhead, and easy expansion of\nthe cluster. For this exercise, we assume reliable networking without\npartitions, and we assume the need for strongly consistent rather than\navailable storage. Note that the following design may not be the only correct\nanswer.\n\n#### 7.4.1 Path-to-Inode and Inode-to-Data Translation.\n\nWe can easily eliminate several options for path-to-inode translation. Storing\nthe entire namespace on a single node or replicating it on several nodes would\nnot be scalable. The latter would be more scalable if we replicated only the\ndirectory structure, but it still would offer comparably low scalability over\nother options, especially if we wanted to keep low latency for directory\noperations. A shared-disk-like approach would require several network round-\ntrips to parse a path and for nodes to coordinate. It would also separate data\nfrom metadata, which would increase latency further.\n\nThe computed approach by itself makes scaling more difficult, and latency\nwould be higher, because descending into any directory would require\ncontacting a different node and would make prefix caches much less effective.\n\nReplicated prefix tables offer the best latency for path-to-inode translation.\nHowever, unless we separate data from metadata, there would be several\ndisadvantages, including: expensive movement of files between certain\nsubtrees, difficulty keeping free space balanced across nodes, and expensive\ncluster expansion. Separating data from metadata would solve such problems at\nthe expense of extra latency in time to first byte, however.\n\nRemote links are probably the best approach for path-to-inode translation. The\ntime to first byte might require several network round-trips the first time\nthat a file is accessed, but after the prefix cache has been populated, a file\nor a directory can be accessed with a single round-trip. Remote links make it\neasy to move files between any two directories, and they also make it easy to\nkeep free space balanced and to nondisruptively expand the cluster, even when\ndata and metadata are colocated.\n\nAnd the use of remote links with colocated data and metadata is probably the\nbest approach for inode-to-data translation. A runner-up might be replicated\nprefix tables with separation of data and metadata, which we can explore if we\ndiscover a problem with remote links later in the design exercise.\n\nThe next question is the type of pointers to use for remote links: explicit,\ncomputed, or hybrid. The \u201cexplicit, indirect\u201d option might be best to give the\nsystem the most control over data placement so that it can balance free space\nwell and so that cluster expansion does not result in any complications.\nReferring to nodes indirectly through an ID rather than through IP addresses\nis usually considered a good practice, because it is less fragile.\n\nBecause individual file throughput is not important for the intended use\ncases, and especially if we do not intend to host many very large files, file\ndata placement granularity should be sufficient.\n\n#### 7.4.2 Redundancy and Coordination.\n\nIf the distributed file system is built on top of highly reliable local\nstorage\u2014which might combine, for example, RAID, dual-socket drives, and a\nredundant cluster network\u2014then replication at the node level might not even be\nnecessary. This design is in fact reminiscent of FlexGroup volumes [30].\n\nHowever, if any node can fail together with its local storage, then we need to\nuse replication with \\\\( n \\gt 1 \\\\) and \\\\( w \\gt 1 \\\\). The choice of\nconsistency over availability dictates that we process writes through a single\ndedicated master or a quorum of nodes; the former usually results in lower\nlatency unless the node becomes overloaded. The master could coordinate with\nreplicas by using a protocol such as two-phase commit or one of its variants.\nWe could achieve \\\\( r = 1 \\\\) by requiring clients to always coordinate with\nthe master or by requiring \\\\( w = n \\\\) and weakening the consistency\nrequirements by a bit to allow clients to read from any replica. The former\ncould, however, result in an overloaded master node; the extent to which this\noverload would be an issue depends on the workload and on how masters are\nselected.\n\nRemote links work best if each remote link has one source and one destination,\nwhich obviously works well when there is no replication (\\\\( n = 1 \\\\)).\nHowever, it is possible to make it work with \\\\( n \\gt 1 \\\\) as demonstrated\nby Farsite [7]. Farsite gathers nodes that host metadata (Farsite separates\nmetadata from data) into small groups of 7 to 10 nodes; nodes in a group\nreplicate the same metadata and use Paxos to coordinate. A remote link then\nlinks from one group to another group, giving it effectively one virtual\nsource and one virtual destination.\n\nIf the replication factor in our file system is small, such as \\\\( n = 3 \\\\),\nit is conceivable to have groups of \\\\( n \\\\) nodes that are replicas of each\nother and still have colocated data and metadata.\n\nThis design is actually not too distant from Farsite [7], which additionally\nassumes that a small fraction of nodes can be actively malicious. Farsite thus\nuses groups of \\\\( n_\\mathrm{metadata} \\ge 7 \\\\) for replicated metadata that\nuse a Byzantine fault-tolerant protocol for coordination. But because this\nreplication factor is too high for data, Farsite separates data from metadata\nfor all but the smallest files and stores the files on data nodes with a\nsmaller replication factor.\n\nSkip 8CONCLUSION Section\n\n## 8 CONCLUSION\n\nDespite the vast research literature on distributed file systems, much of it\nexplores combinations of existing high-level designs, while improving upon\nthem and/or exploring other interesting problems, such as with novel storage\ndevices, new kinds of networks, and different security requirements. Although\nthis kind of work is very valuable and should be continued, there may be\nopportunities to develop new high-level designs or to drastically improve\nexisting ones, significantly contributing to the state of the art. Throughout\nthis article, we have mentioned several ideas for future research. As such,\nour hope is not only for this article to be a survey of existing design\noptions and their characteristics, but perhaps also to be an inspiration for\nfuture research.\n\nSkip ACKNOWLEDGMENTS Section\n\n## ACKNOWLEDGMENTS\n\nWe thank Keith Smith, David Slik, and the anonymous ACM TOS reviewers for\ntheir helpful comments and advice. We also thank Anna Giaconia for copyediting\nthis article.\n\n## REFERENCES\n\n  1. [1] 2000. Proceedings of the 4th USENIX Symposium on Operating System Design and Implementation (OSDI\u201900). USENIX Association.Google Scholar\n\n  2. [2] 2002. Proceedings of the 5th USENIX Symposium on Operating Systems Design and Implementation (OSDI\u201902). USENIX Association.Google Scholar\n\n  3. [3] 2015. Proceedings of the 13th USENIX Conference on File and Storage Technologies (FAST\u201915). USENIX Association.Google Scholar\n\n  4. [4] 2019. Proceedings of the 17th USENIX Conference on File and Storage Technologies (FAST\u201919). USENIX Association.Google Scholar\n\n  5. [5] 2019. Proceedings of the 2019 USENIX Annual Technical Conference (USENIX ATC\u201919). USENIX Association.Google Scholar\n\n  6. [6] Abadi Daniel. 2012. Consistency tradeoffs in modern distributed database system design: CAP is only part of the story. IEEE Comput. 45, 2 (2012), 37\u201342.Google ScholarDigital Library\n\nReference 1Reference 2\n\n  7. [7] Adya Atul, Bolosky William J., Castro Miguel, Cermak Gerald, Chaiken Ronnie, Douceur John R., Howell Jon, Lorch Jacob R., Theimer Marvin, and Wattenhofer Roger. 2002. FARSITE: Federated, available, and reliable storage for an incompletely trusted environment, see Reference OSD [2].Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n     * Reference 12\n     * Reference 13\n\n  8. [8] Anderson Darrell C., Chase Jeffrey S., and Vahdat Amin. 2000. Interposed request routing for scalable network storage, See Reference OSD [1], 259\u2013272.Google Scholar\n\nReference\n\n  9. [9] Benet Juan. 2014. IPFS\u2014Content Addressed, Versioned, P2P File System. arXiv preprint. arXiv:1407.356.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n     * Reference 12\n     * Reference 13\n     * Reference 14\n     * Reference 15\n     * Reference 16\n     * Reference 17\n     * Reference 18\n\n  10. [10] Castro Miguel and Liskov Barbara. 2000. Proactive recovery in a byzantine-fault-tolerant system, see Reference OSD [1], 273\u2013288.Google Scholar\n\nReference\n\n  11. [11] Chung Jae Yoon, Joe-Wong Carlee, Ha Sangtae, Hong James Won-Ki, and Chiang Mung. 2015. CYRUS: Towards client-defined cloud storage. In Proceedings of the 10th European Conference on Computer Systems (EuroSys\u201915). Association for Computing Machinery, 17:1\u201317:16.Google ScholarDigital Library\n\nReference\n\n  12. [12] Cowling James. 2016. Inside the Magic Pocket. Retrieved from https://blogs.dropbox.com/tech/2016/05/inside-the-magic-pocket/.Google Scholar\n\nReference 1Reference 2Reference 3\n\n  13. [13] Dabek Frank, Kaashoek M. Frans, Karger David R., Morris Robert Tappan, and Stoica Ion. 2001. Wide-area cooperative storage with CFS. In Proceedings of the 18th ACM Symposium on Operating Systems Principles (SOSP\u201901). Association for Computing Machinery, 202\u2013215.Google ScholarDigital Library\n\nReference\n\n  14. [14] DeCandia Giuseppe, Hastorun Deniz, Jampani Madan, Kakulapati Gunavardhan, Lakshman Avinash, Pilchin Alex, Sivasubramanian Swaminathan, Vosshall Peter, and Vogels Werner. 2007. Dynamo: Amazon\u2019s highly available key-value store. In Proceedings of the 21st ACM Symposium on Operating Systems Principles (SOSP\u201907). Association for Computing Machinery, 205\u2013220.Google ScholarDigital Library\n\nReference 1Reference 2Reference 3\n\n  15. [15] Di Wang. 2012. Distributed Namespace Status Phase I\u2014Remote Directories. Retrieved from https://wiki.lustre.org/images/4/41/LUG-2012-DNE_Phase_1-WangDi.pdf.Google Scholar\n\nReference\n\n  16. [16] Diaconu Cristian, Freedman Craig, Ismert Erik, Larson Per-\u00c5ke, Mittal Pravin, Stonecipher Ryan, Verma Nitin, and Zwilling Mike. 2013. Hekaton: SQL server\u2019s memory-optimized OLTP engine. In Proceedings of the ACM International Conference on Management of Data (SIGMOD\u201913). Association for Computing Machinery, 1243\u20131254.Google ScholarDigital Library\n\nReference\n\n  17. [17] Eisler Michael, Corbett Peter, Kazar Michael, Nydick Daniel S., and Wagner J. Christopher. 2007. Data ONTAP GX: A scalable storage cluster. In Proceedings of the 5th USENIX Conference on File and Storage Technologies (FAST\u201907). USENIX Association, 139\u2013152.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  18. [18] Fagin Ronald, Nievergelt Jurg, Pippenger Nicholas, and Strong H. Raymond. 1979. Extendible hashing\u2014a fast access method for dynamic files. ACM Trans. Datab. Syst. 4, 3 (1979), 315\u2013344.Google ScholarDigital Library\n\nReference\n\n  19. [19] Freedman Michael J., Freudenthal Eric, and Mazi\u00e8res David. 2004. Democratizing content publication with coral. In Proceedings of the 1st USENIX Symposium on Networked Systems Design and Implementation (NSDI\u201904). USENIX Association, 239\u2013252.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  20. [20] Ghemawat Sanjay, Gobioff Howard, and Leung Shun-Tak. 2003. The Google file system. In Proceedings of the 19th ACM Symposium on Operating Systems Principles (SOSP\u201903). Association for Computing Machinery, 29\u201343.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n\n  21. [21] Gluster. Storage for Your Cloud. Retrieved from https://www.gluster.org/.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n\n  22. [22] Hasan Ragib, Anwar Zahid, Yurcik William, Brumbaugh Larry, and Campbell Roy. 2005. A survey of peer-to-peer storage techniques for distributed file systems. In Proceedings of the International Conference on Information Technology: Coding and Computing (ITCC\u201905), Vol. 2. IEEE Computer Society, 205\u2013213.Google ScholarDigital Library\n\nReference\n\n  23. [23] Hildrum Kirsten, Kubiatowicz John, Rao Satish, and Zhao Ben Y.. 2002. Distributed object location in a dynamic network. In Proceedings of the 14th Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA\u201902). Association for Computing Machinery, 41\u201352.Google ScholarDigital Library\n\nReference\n\n  24. [24] Hitz Dave, Lau James, and Malcolm Michael A.. 1994. File system design for an NFS file server appliance. In Proceedings of the USENIX Winter 1994 Technical Conference (WTEC\u201994). USENIX Association, 235\u2013246.Google Scholar\n\nReference\n\n  25. [25] Howard John H., Kazar Michael L., Menees Sherri G., Nichols David A., Satyanarayanan Mahadev, Sidebotham Robert N., and West Michael J.. 1988. Scale and performance in a distributed file system. ACM Trans. Comput. Syst. 6, 1 (1988), 51\u201381.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n     * Reference 12\n     * Reference 13\n     * Reference 14\n     * Reference 15\n\n  26. [26] Huang Cheng, Simitci Huseyin, Xu Yikang, Ogus Aaron, Calder Brad, Gopalan Parikshit, Li Jin, and Yekhanin Sergey. 2012. Erasure coding in windows azure storage. In Proceedings of the 2012 USENIX Annual Technical Conference (USENIX ATC\u201912). USENIX Association, 15\u201326.Google Scholar\n\nReference\n\n  27. [27] Kadekodi Saurabh, Rashmi K. V., and Ganger Gregory R.. 2019. Cluster storage systems gotta have HeART: Improving storage efficiency by exploiting disk-reliability heterogeneity, see Reference FAS [4], 345\u2013358.Google Scholar\n\nReference\n\n  28. [28] Kang Junbin, Benlong, Wo Tianyu, Yu Weiren, Du Lian, Ma Shuai, and Huae Jinpeng. 2015. SpanFS: A scalable file system on fast storage devices. In Proceedings of the 2015 USENIX Annual Technical Conference (USENIX ATC\u201915). USENIX Association, 249\u2013261.Google Scholar\n\nReference\n\n  29. [29] Karger David, Lehman Eric, Leighton Tom, Panigrahy Rina, Levine Matthew, and Lewin Daniel. 1997. Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the world wide web. In Proceedings of the 29th Annual ACM Symposium on Theory of Computing (STOC\u201997). Association for Computing Machinery, 654\u2013663.Google ScholarDigital Library\n\nReference\n\n  30. [30] Kesavan Ram, Hennessey Jason, Jernigan Richard, Macko Peter, Smith Keith A., Tennant Daniel, and R. Bharadwaj V.2019. FlexGroup volumes: A distributed WAFL file system, see Reference ATC [5], 135\u2013148.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n     * Reference 12\n     * Reference 13\n     * Reference 14\n\n  31. [31] Kubiatowicz John, Bindel David, Chen Yan, Czerwinski Steven E., Eaton Patrick R., Geels Dennis, Gummadi Ramakrishna, Rhea Sean C., Weatherspoon Hakim, Weimer Westley, Wells Chris, and Zhao Ben Y.. 2000. OceanStore: An architecture for global-scale persistent storage. In Proceedings of the 9th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IX). Association for Computing Machinery, 190\u2013201.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n\n  32. [32] Kuszmaul Bradley C., Frigo Matteo, Paluska Justin Mazzola, and Sandler Alexander (Sasha). 2019. Everyone loves file: File storage service (FSS) in oracle cloud infrastructure, see Reference ATC [5], 15\u201332.Google Scholar\n\nReference 1Reference 2Reference 3\n\n  33. [33] Le Preslav. 2019. How we optimized magic pocket for cold storage. Retrieved from https://blogs.dropbox.com/tech/2019/05/how-we-optimized-magic-pocket-for-cold-storage/.Google Scholar\n\nReference\n\n  34. [34] libp2p. A Modular Network Stack. Retrieved from https://libp2p.io/.Google Scholar\n\nReference 1Reference 2\n\n  35. [35] LizardFS. LizardFS. Retrieved from https://lizardfs.com/.Google Scholar\n\nReference\n\n  36. [36] Lockwood Glenn. 2017. What\u2019s So Bad about POSIX I/O? Retrieved from https://www.nextplatform.com/2017/09/11/whats-bad-posix-io.Google Scholar\n\nReference\n\n  37. [37] Lustre 2017. Introduction to Lustre Architecture. Retrieved from https://wiki.lustre.org/images/6/64/LustreArchitecture-v4.pdf.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n\n  38. [38] Maymounkov Petar and Mazi\u00e8res David. 2002. Kademlia: A peer-to-peer information system based on the XOR metric. In Peer-To-Peer Systems: First International Workshop (IPTPS\u201902). Springer, 53\u201365.Google Scholar\n\nReference 1Reference 2\n\n  39. [39] Miltchev Stefan, Smith Jonathan M., Prevelakis Vassilis, Keromytis Angelos D., and Ioannidis Sotiris. 2008. Decentralized access control in distributed file systems. ACM Comput. Surv. 40, 3 (2008), 10:1\u201310:30.Google ScholarDigital Library\n\nReference\n\n  40. [40] MooseFS. MooseFS. Retrieved from https://moosefs.com/.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  41. [41] Muralidhar Subramanian, Lloyd Wyatt, Roy Sabyasachi, Hill Cory, Lin Ernest, Liu Weiwen, Pan Satadru, Shankar Shiva, Sivakumar Viswanath, Tang Linpeng, et al. 2014. f4: Facebook\u2019s warm BLOB storage system. In Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI\u201914). USENIX Association, 383\u2013398.Google Scholar\n\nReference 1Reference 2Reference 3\n\n  42. [42] Muthitacharoen Athicha, Morris Robert Tappan, Gil Thomer M., and Chen Benjie. 2002. Ivy: A read/write peer-to-peer file system, see Reference OSD [2].Google Scholar\n\nReference 1Reference 2\n\n  43. [43] MySQL. MySQL. Retrieved from https://www.mysql.com/.Google Scholar\n\nReference\n\n  44. [44] Niazi Salman, Ismail Mahmoud, Haridi Seif, Dowling Jim, Grohsschmiedt Steffen, and Ronstr\u00f6m Mikael. 2017. HopsFS: Scaling hierarchical file system metadata using NewSQL databases. In Proceedings of the 15th USENIX Conference on File and Storage Technologies (FAST\u201917). USENIX Association, 89\u2013104.Google Scholar\n\nReference 1Reference 2\n\n  45. [45] Nightingale Edmund B., Elson Jeremy, Fan Jinliang, Hofmann Owen S., Howell Jon, and Suzue Yutaka. 2012. Flat datacenter storage. In Proceedings of the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI\u201912). USENIX Association, 1\u201315.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n\n  46. [46] Pacheco Leandro, Halalai Raluca, Schiavoni Valerio, Pedone Fernando, Riviere Etienne, and Felber Pascal. 2016. GlobalFS: A strongly consistent multi-site file system. In Proceedings of the IEEE 35th Symposium on Reliable Distributed Systems (SRDS\u201916). IEEE Computer Society, 147\u2013156.Google ScholarCross Ref\n\nReference 1Reference 2\n\n  47. [47] Pan Satadru, Stavrinos Theano, Zhang Yunqiao, Sikaria Atul, Zakharov Pavel, Sharma Abhinav, P Shiva Shankar, Shuey Mike, Wareing Richard, Gangapuram Monika, Cao Guanglei, Preseau Christian, Singh Pratap, Patiejunas Kestutis, Tipton J. R., Katz-Bassett Ethan, and Lloyd Wyatt. 2021. Facebook\u2019s tectonic filesystem: Efficiency from exascale. In Proceedings of the 19th USENIX Conference on File and Storage Technologies (FAST\u201921). USENIX Association, 217\u2013231.Google Scholar\n\nReference\n\n  48. [48] Patil Swapnil, Gibson Garth A., Lang Samuel, and Polte Milo. 2007. GIGA+: Scalable directories for shared file systems. In Proceedings of the 2nd International Workshop on Petascale Data Storage (PDSW\u201907). Association for Computing Machinery, 26\u201329.Google ScholarDigital Library\n\nReference\n\n  49. [49] Plank James S.. 2013. Erasure codes for storage systems: A brief primer. ;login: USENIX Mag. 38, 6 (December 2013), 44\u201350.Google Scholar\n\nReference\n\n  50. [50] POSIX.1-2017. Portable Operating System Interface (POSIXTM) Base Specifications. IEEE Std 1003.1TM-2017 and the Open Group Technical Standard Base Specifications, Issue 7. Retrieved from https://pubs.opengroup.org/onlinepubs/9699919799/.Google Scholar\n\nReference\n\n  51. [51] Ramakrishnan Raghu, Sridharan Baskar, Douceur John R., Kasturi Pavan, Krishnamachari-Sampath Balaji, Krishnamoorthy Karthick, Li Peng, Manu Mitica, Michaylov Spiro, Ramos Rog\u00e9rio, Sharman Neil, Xu Zee, Barakat Youssef, Douglas Chris, Draves Richard, Naidu Shrikant S., Shastry Shankar, Sikaria Atul, Sun Simon, and Venkatesan Ramarathnam. 2017. Azure data lake store: A hyperscale distributed file service for big data analytics. In Proceedings of the 2017 ACM International Conference on Management of Data (SIGMOD\u201917). Association for Computing Machinery, 51\u201363.Google ScholarDigital Library\n\nReference 1Reference 2Reference 3\n\n  52. [52] Reed Irving S. and Solomon Gustave. 1960. Polynomial codes over certain finite fields. J. Soc. Industr. Appl. Math. 8, 2 (1960), 300\u2013304.Google ScholarCross Ref\n\nReference\n\n  53. [53] Ren Kai, Zheng Qing, Patil Swapnil, and Gibson Garth A.. 2014. IndexFS: Scaling file system metadata performance with stateless caching and bulk insertion. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC\u201914). IEEE Computer Society, 237\u2013248.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n\n  54. [54] Rhea Sean C., Eaton Patrick R., Geels Dennis, Weatherspoon Hakim, Zhao Ben Y., and Kubiatowicz John. 2003. Pond: The OceanStore prototype. In Proceedings of the 2nd USENIX Conference on File and Storage Technologies (FAST\u201903). USENIX Association.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n\n  55. [55] Sathiamoorthy Maheswaran, Asteris Megasthenis, Papailiopoulos Dimitris S., Dimakis Alexandros G., Vadali Ramkumar, Chen Scott, and Borthakur Dhruba. 2013. XORing elephants: Novel erasure codes for big data. Proc. VLDB Endow. 6, 5 (2013), 325\u2013336.Google ScholarDigital Library\n\nReference\n\n  56. [56] Satyanarayanan Mahadev. 1990. A survey of distributed file systems. Annu. Rev. Comput. Sci. 4, 1 (1990), 73\u2013104.Google ScholarCross Ref\n\nReference\n\n  57. [57] Satyanarayanan Mahadev, Kistler James J., Kumar Puneet, Okasaki Maria E., Siegel Ellen H., and Steere David C.. 1990. Coda: A highly available file system for a distributed workstation environment. IEEE Transactions on Computers 39, 4 (1990), 447\u2013459.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n\n  58. [58] Schmuck Frank B. and Haskin Roger L.. 2002. GPFS: A shared-disk file system for large computing clusters. In Proceedings of the 1st USENIX Conference on File and Storage Technologies (FAST\u201902). USENIX Association, 231\u2013244.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n\n  59. [59] Shepler Spencer, Eisler Mike, and Noveck Dave. 2010. Network File System (NFS) Version 4 Minor Version 1 Protocol. RFC 5661. Internet Engineering Task Force (IETF).Google ScholarCross Ref\n\nReference\n\n  60. [60] Shvachko Konstantin, Kuang Hairong, Radia Sanjay, and Chansler Robert. 2010. The hadoop distributed file system. In Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST\u201910). IEEE Computer Society.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n\n  61. [61] Stoica Ion, Morris Robert, Karger David, Kaashoek M. Frans, and Balakrishnan Hari. 2001. Chord: A scalable peer-to-peer lookup service for internet applications. In Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM\u201901). Association for Computing Machinery, 149\u2013160.Google ScholarDigital Library\n\nReference\n\n  62. [62] Stonebraker Michael. 1986. The case for shared nothing. IEEE Datab. Eng. Bull. 9, 1 (1986), 4\u20139.Google Scholar\n\nReference\n\n  63. [63] Stribling Jeremy, Sovran Yair, Zhang Irene, Pretzer Xavid, Li Jinyang, Kaashoek M. Frans, and Morris Robert Tappan. 2009. Flexible, wide-area storage for distributed systems with WheelFS. In Proceedings of the 6th USENIX Symposium on Networked Systems Design and Implementation (NSDI \u201909). USENIX Association, 43\u201358.Google ScholarDigital Library\n\nReference 1Reference 2\n\n  64. [64] Tahoe-LAFS. The Least-Authority File Store. Retrieved from https://www.tahoe-lafs.org/trac/tahoe-lafs.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  65. [65] Thaler David G. and Ravishankar Chinya V.. 1996. A Name-Based Mapping Scheme for Rendezvous. Technical Report CSE-TR-316-96. University of Michigan.Google Scholar\n\nReference\n\n  66. [66] Thaler David G. and Ravishankar Chinya V.. 1998. Using name-based mappings to increase hit rates. IEEE/ACM Trans. Netw. 6, 1 (1998), 1\u201314.Google ScholarDigital Library\n\nReference\n\n  67. [67] Thanh Tran Doan, Mohan Subaji, Choi Eunmi, Kim SangBum, and Kim Pilsung. 2008. A taxonomy and survey on distributed file systems. In Proceedings of the 4th International Conference on Networked Computing and Advanced Information Management (NCM\u201908), Vol. 1. IEEE Computer Society, 144\u2013149.Google ScholarDigital Library\n\nReference 1Reference 2\n\n  68. [68] Thomson Alexander and Abadi Daniel J.. 2015. CalvinFS: Consistent WAN replication and scalable metadata management for distributed file systems, see Reference FAS [3].Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n\n  69. [69] Thomson Alexander, Diamond Thaddeus, Weng Shu-Chun, Ren Kun, Shao Philip, and Abadi Daniel J.. 2012. Calvin: Fast distributed transactions for partitioned database systems. In Proceedings of the 2012 ACM International Conference on Management of Data (SIGMOD\u201912). Association for Computing Machinery.Google ScholarDigital Library\n\nReference\n\n  70. [70] Viotti Paolo and Vukolic Marko. 2016. Consistency in non-transactional distributed storage systems. ACM Comput. Surv. 49, 1 (2016), 19:1\u201319:34.Google ScholarDigital Library\n\nReference\n\n  71. [71] Vogels Werner. 2009. Eventually consistent. Commun. ACM 52, 1 (2009), 40\u201344.Google ScholarDigital Library\n\nReference\n\n  72. [72] Weil Sage A.. 2007. Ceph: Reliable, Scalable, and High-Performance Distributed Storage. Ph.D. Dissertation. University of California at Santa Cruz.Google Scholar\n\nReference\n\n  73. [73] Weil Sage A., Brandt Scott A., Miller Ethan L., Long Darrell D. E., and Maltzahn Carlos. 2006. Ceph: A scalable, high-performance distributed file system. In Proceedings of the 7th USENIX Symposium on Operating Systems Design and Implementation (OSDI\u201906). USENIX Association, 307\u2013320.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n     * Reference 12\n     * Reference 13\n     * Reference 14\n     * Reference 15\n     * Reference 16\n     * Reference 17\n\n  74. [74] Weil Sage A., Brandt Scott A., Miller Ethan L., and Maltzahn Carlos. 2006. CRUSH: Controlled, scalable, decentralized placement of replicated data. In Proceedings of the 2006 ACM/IEEE Conference on Supercomputing (SC\u201906). Association for Computing Machinery.Google ScholarDigital Library\n\nReference 1Reference 2Reference 3\n\n  75. [75] Welch Brent B. and Ousterhout John K.. 1986. Prefix tables: A simple mechanism for locating files in a distributed system. In Proceedings of the 6th International Conference on Distributed Computing Systems (ICDCS\u201986). IEEE Computer Society, 184\u2013189.Google Scholar\n\nReference 1Reference 2\n\n  76. [76] Xia Mingyuan, Saxena Mohit, Blaum Mario, and Pease David. 2015. A tale of two erasure codes in HDFS, see Reference FAS [3], 213\u2013226.Google Scholar\n\nReference\n\n  77. [77] Xu Jian and Swanson Steven. 2016. NOVA: A log-structured file system for hybrid volatile/non-volatile main memories. In Proceedings of the 14th USENIX Conference on File and Storage Technologies (FAST\u201916). USENIX Association, 323\u2013338.Google Scholar\n\nReference\n\n  78. [78] Yang Jian, Izraelevitz Joseph, and Swanson Steven. 2019. Orion: A distributed file system for non-volatile main memory and RDMA-capable networks, see Reference FAS [4], 221\u2013234.Google Scholar\n\nReference 1Reference 2Reference 3\n\n  79. [79] Zheng Qing, Ren Kai, and Gibson Garth A.. 2014. BatchFS: Scaling the file system control plane with client-funded metadata servers. In Proceedings of the 9th Parallel Data Storage Workshop (PDSW\u201914). IEEE Computer Society.Google ScholarDigital Library\n\nReference 1Reference 2\n\n  80. [80] Zheng Qing, Ren Kai, Gibson Garth A., Settlemyer Bradley W., and Grider Gary. 2015. DeltaFS: Exascale file systems scale better without dedicated servers. In Proceedings of the 10th Parallel Data Storage Workshop (PDSW\u201915). Association for Computing Machinery.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n\n## Cited By\n\nView all\n\n## Index Terms\n\n  1. ###### Survey of Distributed File System Design Choices\n\n    1. Computer systems organization\n\n      1. Architectures\n\n        1. Distributed architectures\n\n    2. Information systems\n\n      1. Information storage systems\n\n        1. Storage architectures\n\n          1. Distributed storage\n\n        2. Storage management\n\n    3. Software and its engineering\n\n      1. Software organization and properties\n\n        1. Contextual software domains\n\n          1. Operating systems\n\n            1. File systems management\n\n        2. Software system structures\n\n          1. Distributed systems organizing principles\n\n            1. Cloud computing\n\n## Recommendations\n\n  * ##### Optimizing Local File Accesses for FUSE-Based Distributed Storage\n\nSCC '12: Proceedings of the 2012 SC Companion: High Performance Computing,\nNetworking Storage and Analysis\n\nModern distributed file systems can store huge amounts of information while\nretaining the benefits of high reliability and performance. Many of these\nsystems are prototyped with FUSE, a popular framework for implementing user-\nlevel file systems. ...\n\nRead More\n\n  * ##### The Design and Evaluation of a Distributed Reliable File System\n\nPDCAT '09: Proceedings of the 2009 International Conference on Parallel and\nDistributed Computing, Applications and Technologies\n\nPeer-to-peer (P2P) systems are, in contrast to clientserver (C/S) systems,\nfault-tolerant, robust, and scalable. While C/S distributed file systems, such\nas NFS (Network File System) or SMB (Server Message Block), do not scale with\nrespect to the number ...\n\nRead More\n\n  * ##### State of the Art in Distributed File Systems: Increasing Performance\n\nECBS-EERC '11: Proceedings of the 2011 Second Eastern European Regional\nConference on the Engineering of Computer Based Systems\n\nNeed of storing huge amounts of data has grown over the past years. The data\nshould be stored for future reuse or for sharing among users. Data files can\nbe stored on a local file system or on a distributed file system. A\ndistributed file system ...\n\nRead More\n\n## Comments\n\n### Login options\n\nCheck if you have access through your login credentials or your institution to\nget full access on this article.\n\nSign in\n\n### Full Access\n\nGet this Article\n\n  * Information\n  * Contributors\n\n  * ### Published in\n\nACM Transactions on Storage Volume 18, Issue 1\n\nFebruary 2022\n\n245 pages\n\nISSN:1553-3077\n\nEISSN:1553-3093\n\nDOI:10.1145/3512348\n\n    * Editor:\n    * Sam H. Noh\n\nUlsan National Institute of Science and Technology, Ulsan, Republic of Korea\n\nIssue\u2019s Table of Contents\n\nCopyright \u00a9 2022 Copyright held by the owner/author(s).\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial\nInternational 4.0 License.\n\n### Sponsors\n\n### In-Cooperation\n\n### Publisher\n\nAssociation for Computing Machinery\n\nNew York, NY, United States\n\n### Publication History\n\n    * Published: 2 March 2022\n    * Accepted: 1 May 2021\n    * Revised: 1 November 2020\n    * Received: 1 May 2020\n\nPublished in tos Volume 18, Issue 1\n\n### Permissions\n\nRequest permissions about this article.\n\nRequest Permissions\n\n### Check for updates\n\n### Author Tags\n\n    * Distributed file systems\n    * distributed storage\n    * design options\n    * taxonomy\n    * survey\n\n### Qualifiers\n\n    * tutorial\n    * Refereed\n\n### Conference\n\n### Funding Sources\n\n  * ### Other Metrics\n\nView Article Metrics\n\n  * Bibliometrics\n  * Citations2\n\n  * ### Article Metrics\n\n    * 2\n\nTotal Citations\n\nView Citations\n\n    * 9,615\n\nTotal Downloads\n\n    * Downloads (Last 12 months)5,257\n    * Downloads (Last 6 weeks)2,697\n\n### Other Metrics\n\nView Author Metrics\n\n  * ### Cited By\n\nView all\n\n### PDF Format\n\nView or Download as a PDF file.\n\nPDF\n\n### eReader\n\nView online with eReader.\n\neReader\n\n### Digital Edition\n\nView this article in digital edition.\n\nView Digital Edition\n\n### HTML Format\n\nView this article in HTML Format .\n\nView HTML Format\n\n  1. [1] 2000. Proceedings of the 4th USENIX Symposium on Operating System Design and Implementation (OSDI\u201900). USENIX Association.Google Scholar\n\n  2. [2] 2002. Proceedings of the 5th USENIX Symposium on Operating Systems Design and Implementation (OSDI\u201902). USENIX Association.Google Scholar\n\n  3. [3] 2015. Proceedings of the 13th USENIX Conference on File and Storage Technologies (FAST\u201915). USENIX Association.Google Scholar\n\n  4. [4] 2019. Proceedings of the 17th USENIX Conference on File and Storage Technologies (FAST\u201919). USENIX Association.Google Scholar\n\n  5. [5] 2019. Proceedings of the 2019 USENIX Annual Technical Conference (USENIX ATC\u201919). USENIX Association.Google Scholar\n\n  6. [6] Abadi Daniel. 2012. Consistency tradeoffs in modern distributed database system design: CAP is only part of the story. IEEE Comput. 45, 2 (2012), 37\u201342.Google ScholarDigital Library\n\nReference 1Reference 2\n\n  7. [7] Adya Atul, Bolosky William J., Castro Miguel, Cermak Gerald, Chaiken Ronnie, Douceur John R., Howell Jon, Lorch Jacob R., Theimer Marvin, and Wattenhofer Roger. 2002. FARSITE: Federated, available, and reliable storage for an incompletely trusted environment, see Reference OSD [2].Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n     * Reference 12\n     * Reference 13\n\n  8. [8] Anderson Darrell C., Chase Jeffrey S., and Vahdat Amin. 2000. Interposed request routing for scalable network storage, See Reference OSD [1], 259\u2013272.Google Scholar\n\nReference\n\n  9. [9] Benet Juan. 2014. IPFS\u2014Content Addressed, Versioned, P2P File System. arXiv preprint. arXiv:1407.356.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n     * Reference 12\n     * Reference 13\n     * Reference 14\n     * Reference 15\n     * Reference 16\n     * Reference 17\n     * Reference 18\n\n  10. [10] Castro Miguel and Liskov Barbara. 2000. Proactive recovery in a byzantine-fault-tolerant system, see Reference OSD [1], 273\u2013288.Google Scholar\n\nReference\n\n  11. [11] Chung Jae Yoon, Joe-Wong Carlee, Ha Sangtae, Hong James Won-Ki, and Chiang Mung. 2015. CYRUS: Towards client-defined cloud storage. In Proceedings of the 10th European Conference on Computer Systems (EuroSys\u201915). Association for Computing Machinery, 17:1\u201317:16.Google ScholarDigital Library\n\nReference\n\n  12. [12] Cowling James. 2016. Inside the Magic Pocket. Retrieved from https://blogs.dropbox.com/tech/2016/05/inside-the-magic-pocket/.Google Scholar\n\nReference 1Reference 2Reference 3\n\n  13. [13] Dabek Frank, Kaashoek M. Frans, Karger David R., Morris Robert Tappan, and Stoica Ion. 2001. Wide-area cooperative storage with CFS. In Proceedings of the 18th ACM Symposium on Operating Systems Principles (SOSP\u201901). Association for Computing Machinery, 202\u2013215.Google ScholarDigital Library\n\nReference\n\n  14. [14] DeCandia Giuseppe, Hastorun Deniz, Jampani Madan, Kakulapati Gunavardhan, Lakshman Avinash, Pilchin Alex, Sivasubramanian Swaminathan, Vosshall Peter, and Vogels Werner. 2007. Dynamo: Amazon\u2019s highly available key-value store. In Proceedings of the 21st ACM Symposium on Operating Systems Principles (SOSP\u201907). Association for Computing Machinery, 205\u2013220.Google ScholarDigital Library\n\nReference 1Reference 2Reference 3\n\n  15. [15] Di Wang. 2012. Distributed Namespace Status Phase I\u2014Remote Directories. Retrieved from https://wiki.lustre.org/images/4/41/LUG-2012-DNE_Phase_1-WangDi.pdf.Google Scholar\n\nReference\n\n  16. [16] Diaconu Cristian, Freedman Craig, Ismert Erik, Larson Per-\u00c5ke, Mittal Pravin, Stonecipher Ryan, Verma Nitin, and Zwilling Mike. 2013. Hekaton: SQL server\u2019s memory-optimized OLTP engine. In Proceedings of the ACM International Conference on Management of Data (SIGMOD\u201913). Association for Computing Machinery, 1243\u20131254.Google ScholarDigital Library\n\nReference\n\n  17. [17] Eisler Michael, Corbett Peter, Kazar Michael, Nydick Daniel S., and Wagner J. Christopher. 2007. Data ONTAP GX: A scalable storage cluster. In Proceedings of the 5th USENIX Conference on File and Storage Technologies (FAST\u201907). USENIX Association, 139\u2013152.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  18. [18] Fagin Ronald, Nievergelt Jurg, Pippenger Nicholas, and Strong H. Raymond. 1979. Extendible hashing\u2014a fast access method for dynamic files. ACM Trans. Datab. Syst. 4, 3 (1979), 315\u2013344.Google ScholarDigital Library\n\nReference\n\n  19. [19] Freedman Michael J., Freudenthal Eric, and Mazi\u00e8res David. 2004. Democratizing content publication with coral. In Proceedings of the 1st USENIX Symposium on Networked Systems Design and Implementation (NSDI\u201904). USENIX Association, 239\u2013252.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  20. [20] Ghemawat Sanjay, Gobioff Howard, and Leung Shun-Tak. 2003. The Google file system. In Proceedings of the 19th ACM Symposium on Operating Systems Principles (SOSP\u201903). Association for Computing Machinery, 29\u201343.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n\n  21. [21] Gluster. Storage for Your Cloud. Retrieved from https://www.gluster.org/.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n\n  22. [22] Hasan Ragib, Anwar Zahid, Yurcik William, Brumbaugh Larry, and Campbell Roy. 2005. A survey of peer-to-peer storage techniques for distributed file systems. In Proceedings of the International Conference on Information Technology: Coding and Computing (ITCC\u201905), Vol. 2. IEEE Computer Society, 205\u2013213.Google ScholarDigital Library\n\nReference\n\n  23. [23] Hildrum Kirsten, Kubiatowicz John, Rao Satish, and Zhao Ben Y.. 2002. Distributed object location in a dynamic network. In Proceedings of the 14th Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA\u201902). Association for Computing Machinery, 41\u201352.Google ScholarDigital Library\n\nReference\n\n  24. [24] Hitz Dave, Lau James, and Malcolm Michael A.. 1994. File system design for an NFS file server appliance. In Proceedings of the USENIX Winter 1994 Technical Conference (WTEC\u201994). USENIX Association, 235\u2013246.Google Scholar\n\nReference\n\n  25. [25] Howard John H., Kazar Michael L., Menees Sherri G., Nichols David A., Satyanarayanan Mahadev, Sidebotham Robert N., and West Michael J.. 1988. Scale and performance in a distributed file system. ACM Trans. Comput. Syst. 6, 1 (1988), 51\u201381.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n     * Reference 12\n     * Reference 13\n     * Reference 14\n     * Reference 15\n\n  26. [26] Huang Cheng, Simitci Huseyin, Xu Yikang, Ogus Aaron, Calder Brad, Gopalan Parikshit, Li Jin, and Yekhanin Sergey. 2012. Erasure coding in windows azure storage. In Proceedings of the 2012 USENIX Annual Technical Conference (USENIX ATC\u201912). USENIX Association, 15\u201326.Google Scholar\n\nReference\n\n  27. [27] Kadekodi Saurabh, Rashmi K. V., and Ganger Gregory R.. 2019. Cluster storage systems gotta have HeART: Improving storage efficiency by exploiting disk-reliability heterogeneity, see Reference FAS [4], 345\u2013358.Google Scholar\n\nReference\n\n  28. [28] Kang Junbin, Benlong, Wo Tianyu, Yu Weiren, Du Lian, Ma Shuai, and Huae Jinpeng. 2015. SpanFS: A scalable file system on fast storage devices. In Proceedings of the 2015 USENIX Annual Technical Conference (USENIX ATC\u201915). USENIX Association, 249\u2013261.Google Scholar\n\nReference\n\n  29. [29] Karger David, Lehman Eric, Leighton Tom, Panigrahy Rina, Levine Matthew, and Lewin Daniel. 1997. Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the world wide web. In Proceedings of the 29th Annual ACM Symposium on Theory of Computing (STOC\u201997). Association for Computing Machinery, 654\u2013663.Google ScholarDigital Library\n\nReference\n\n  30. [30] Kesavan Ram, Hennessey Jason, Jernigan Richard, Macko Peter, Smith Keith A., Tennant Daniel, and R. Bharadwaj V.2019. FlexGroup volumes: A distributed WAFL file system, see Reference ATC [5], 135\u2013148.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n     * Reference 12\n     * Reference 13\n     * Reference 14\n\n  31. [31] Kubiatowicz John, Bindel David, Chen Yan, Czerwinski Steven E., Eaton Patrick R., Geels Dennis, Gummadi Ramakrishna, Rhea Sean C., Weatherspoon Hakim, Weimer Westley, Wells Chris, and Zhao Ben Y.. 2000. OceanStore: An architecture for global-scale persistent storage. In Proceedings of the 9th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IX). Association for Computing Machinery, 190\u2013201.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n\n  32. [32] Kuszmaul Bradley C., Frigo Matteo, Paluska Justin Mazzola, and Sandler Alexander (Sasha). 2019. Everyone loves file: File storage service (FSS) in oracle cloud infrastructure, see Reference ATC [5], 15\u201332.Google Scholar\n\nReference 1Reference 2Reference 3\n\n  33. [33] Le Preslav. 2019. How we optimized magic pocket for cold storage. Retrieved from https://blogs.dropbox.com/tech/2019/05/how-we-optimized-magic-pocket-for-cold-storage/.Google Scholar\n\nReference\n\n  34. [34] libp2p. A Modular Network Stack. Retrieved from https://libp2p.io/.Google Scholar\n\nReference 1Reference 2\n\n  35. [35] LizardFS. LizardFS. Retrieved from https://lizardfs.com/.Google Scholar\n\nReference\n\n  36. [36] Lockwood Glenn. 2017. What\u2019s So Bad about POSIX I/O? Retrieved from https://www.nextplatform.com/2017/09/11/whats-bad-posix-io.Google Scholar\n\nReference\n\n  37. [37] Lustre 2017. Introduction to Lustre Architecture. Retrieved from https://wiki.lustre.org/images/6/64/LustreArchitecture-v4.pdf.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n\n  38. [38] Maymounkov Petar and Mazi\u00e8res David. 2002. Kademlia: A peer-to-peer information system based on the XOR metric. In Peer-To-Peer Systems: First International Workshop (IPTPS\u201902). Springer, 53\u201365.Google Scholar\n\nReference 1Reference 2\n\n  39. [39] Miltchev Stefan, Smith Jonathan M., Prevelakis Vassilis, Keromytis Angelos D., and Ioannidis Sotiris. 2008. Decentralized access control in distributed file systems. ACM Comput. Surv. 40, 3 (2008), 10:1\u201310:30.Google ScholarDigital Library\n\nReference\n\n  40. [40] MooseFS. MooseFS. Retrieved from https://moosefs.com/.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  41. [41] Muralidhar Subramanian, Lloyd Wyatt, Roy Sabyasachi, Hill Cory, Lin Ernest, Liu Weiwen, Pan Satadru, Shankar Shiva, Sivakumar Viswanath, Tang Linpeng, et al. 2014. f4: Facebook\u2019s warm BLOB storage system. In Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI\u201914). USENIX Association, 383\u2013398.Google Scholar\n\nReference 1Reference 2Reference 3\n\n  42. [42] Muthitacharoen Athicha, Morris Robert Tappan, Gil Thomer M., and Chen Benjie. 2002. Ivy: A read/write peer-to-peer file system, see Reference OSD [2].Google Scholar\n\nReference 1Reference 2\n\n  43. [43] MySQL. MySQL. Retrieved from https://www.mysql.com/.Google Scholar\n\nReference\n\n  44. [44] Niazi Salman, Ismail Mahmoud, Haridi Seif, Dowling Jim, Grohsschmiedt Steffen, and Ronstr\u00f6m Mikael. 2017. HopsFS: Scaling hierarchical file system metadata using NewSQL databases. In Proceedings of the 15th USENIX Conference on File and Storage Technologies (FAST\u201917). USENIX Association, 89\u2013104.Google Scholar\n\nReference 1Reference 2\n\n  45. [45] Nightingale Edmund B., Elson Jeremy, Fan Jinliang, Hofmann Owen S., Howell Jon, and Suzue Yutaka. 2012. Flat datacenter storage. In Proceedings of the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI\u201912). USENIX Association, 1\u201315.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n\n  46. [46] Pacheco Leandro, Halalai Raluca, Schiavoni Valerio, Pedone Fernando, Riviere Etienne, and Felber Pascal. 2016. GlobalFS: A strongly consistent multi-site file system. In Proceedings of the IEEE 35th Symposium on Reliable Distributed Systems (SRDS\u201916). IEEE Computer Society, 147\u2013156.Google ScholarCross Ref\n\nReference 1Reference 2\n\n  47. [47] Pan Satadru, Stavrinos Theano, Zhang Yunqiao, Sikaria Atul, Zakharov Pavel, Sharma Abhinav, P Shiva Shankar, Shuey Mike, Wareing Richard, Gangapuram Monika, Cao Guanglei, Preseau Christian, Singh Pratap, Patiejunas Kestutis, Tipton J. R., Katz-Bassett Ethan, and Lloyd Wyatt. 2021. Facebook\u2019s tectonic filesystem: Efficiency from exascale. In Proceedings of the 19th USENIX Conference on File and Storage Technologies (FAST\u201921). USENIX Association, 217\u2013231.Google Scholar\n\nReference\n\n  48. [48] Patil Swapnil, Gibson Garth A., Lang Samuel, and Polte Milo. 2007. GIGA+: Scalable directories for shared file systems. In Proceedings of the 2nd International Workshop on Petascale Data Storage (PDSW\u201907). Association for Computing Machinery, 26\u201329.Google ScholarDigital Library\n\nReference\n\n  49. [49] Plank James S.. 2013. Erasure codes for storage systems: A brief primer. ;login: USENIX Mag. 38, 6 (December 2013), 44\u201350.Google Scholar\n\nReference\n\n  50. [50] POSIX.1-2017. Portable Operating System Interface (POSIXTM) Base Specifications. IEEE Std 1003.1TM-2017 and the Open Group Technical Standard Base Specifications, Issue 7. Retrieved from https://pubs.opengroup.org/onlinepubs/9699919799/.Google Scholar\n\nReference\n\n  51. [51] Ramakrishnan Raghu, Sridharan Baskar, Douceur John R., Kasturi Pavan, Krishnamachari-Sampath Balaji, Krishnamoorthy Karthick, Li Peng, Manu Mitica, Michaylov Spiro, Ramos Rog\u00e9rio, Sharman Neil, Xu Zee, Barakat Youssef, Douglas Chris, Draves Richard, Naidu Shrikant S., Shastry Shankar, Sikaria Atul, Sun Simon, and Venkatesan Ramarathnam. 2017. Azure data lake store: A hyperscale distributed file service for big data analytics. In Proceedings of the 2017 ACM International Conference on Management of Data (SIGMOD\u201917). Association for Computing Machinery, 51\u201363.Google ScholarDigital Library\n\nReference 1Reference 2Reference 3\n\n  52. [52] Reed Irving S. and Solomon Gustave. 1960. Polynomial codes over certain finite fields. J. Soc. Industr. Appl. Math. 8, 2 (1960), 300\u2013304.Google ScholarCross Ref\n\nReference\n\n  53. [53] Ren Kai, Zheng Qing, Patil Swapnil, and Gibson Garth A.. 2014. IndexFS: Scaling file system metadata performance with stateless caching and bulk insertion. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC\u201914). IEEE Computer Society, 237\u2013248.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n\n  54. [54] Rhea Sean C., Eaton Patrick R., Geels Dennis, Weatherspoon Hakim, Zhao Ben Y., and Kubiatowicz John. 2003. Pond: The OceanStore prototype. In Proceedings of the 2nd USENIX Conference on File and Storage Technologies (FAST\u201903). USENIX Association.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n\n  55. [55] Sathiamoorthy Maheswaran, Asteris Megasthenis, Papailiopoulos Dimitris S., Dimakis Alexandros G., Vadali Ramkumar, Chen Scott, and Borthakur Dhruba. 2013. XORing elephants: Novel erasure codes for big data. Proc. VLDB Endow. 6, 5 (2013), 325\u2013336.Google ScholarDigital Library\n\nReference\n\n  56. [56] Satyanarayanan Mahadev. 1990. A survey of distributed file systems. Annu. Rev. Comput. Sci. 4, 1 (1990), 73\u2013104.Google ScholarCross Ref\n\nReference\n\n  57. [57] Satyanarayanan Mahadev, Kistler James J., Kumar Puneet, Okasaki Maria E., Siegel Ellen H., and Steere David C.. 1990. Coda: A highly available file system for a distributed workstation environment. IEEE Transactions on Computers 39, 4 (1990), 447\u2013459.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n\n  58. [58] Schmuck Frank B. and Haskin Roger L.. 2002. GPFS: A shared-disk file system for large computing clusters. In Proceedings of the 1st USENIX Conference on File and Storage Technologies (FAST\u201902). USENIX Association, 231\u2013244.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n\n  59. [59] Shepler Spencer, Eisler Mike, and Noveck Dave. 2010. Network File System (NFS) Version 4 Minor Version 1 Protocol. RFC 5661. Internet Engineering Task Force (IETF).Google ScholarCross Ref\n\nReference\n\n  60. [60] Shvachko Konstantin, Kuang Hairong, Radia Sanjay, and Chansler Robert. 2010. The hadoop distributed file system. In Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST\u201910). IEEE Computer Society.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n\n  61. [61] Stoica Ion, Morris Robert, Karger David, Kaashoek M. Frans, and Balakrishnan Hari. 2001. Chord: A scalable peer-to-peer lookup service for internet applications. In Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM\u201901). Association for Computing Machinery, 149\u2013160.Google ScholarDigital Library\n\nReference\n\n  62. [62] Stonebraker Michael. 1986. The case for shared nothing. IEEE Datab. Eng. Bull. 9, 1 (1986), 4\u20139.Google Scholar\n\nReference\n\n  63. [63] Stribling Jeremy, Sovran Yair, Zhang Irene, Pretzer Xavid, Li Jinyang, Kaashoek M. Frans, and Morris Robert Tappan. 2009. Flexible, wide-area storage for distributed systems with WheelFS. In Proceedings of the 6th USENIX Symposium on Networked Systems Design and Implementation (NSDI \u201909). USENIX Association, 43\u201358.Google ScholarDigital Library\n\nReference 1Reference 2\n\n  64. [64] Tahoe-LAFS. The Least-Authority File Store. Retrieved from https://www.tahoe-lafs.org/trac/tahoe-lafs.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  65. [65] Thaler David G. and Ravishankar Chinya V.. 1996. A Name-Based Mapping Scheme for Rendezvous. Technical Report CSE-TR-316-96. University of Michigan.Google Scholar\n\nReference\n\n  66. [66] Thaler David G. and Ravishankar Chinya V.. 1998. Using name-based mappings to increase hit rates. IEEE/ACM Trans. Netw. 6, 1 (1998), 1\u201314.Google ScholarDigital Library\n\nReference\n\n  67. [67] Thanh Tran Doan, Mohan Subaji, Choi Eunmi, Kim SangBum, and Kim Pilsung. 2008. A taxonomy and survey on distributed file systems. In Proceedings of the 4th International Conference on Networked Computing and Advanced Information Management (NCM\u201908), Vol. 1. IEEE Computer Society, 144\u2013149.Google ScholarDigital Library\n\nReference 1Reference 2\n\n  68. [68] Thomson Alexander and Abadi Daniel J.. 2015. CalvinFS: Consistent WAN replication and scalable metadata management for distributed file systems, see Reference FAS [3].Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n\n  69. [69] Thomson Alexander, Diamond Thaddeus, Weng Shu-Chun, Ren Kun, Shao Philip, and Abadi Daniel J.. 2012. Calvin: Fast distributed transactions for partitioned database systems. In Proceedings of the 2012 ACM International Conference on Management of Data (SIGMOD\u201912). Association for Computing Machinery.Google ScholarDigital Library\n\nReference\n\n  70. [70] Viotti Paolo and Vukolic Marko. 2016. Consistency in non-transactional distributed storage systems. ACM Comput. Surv. 49, 1 (2016), 19:1\u201319:34.Google ScholarDigital Library\n\nReference\n\n  71. [71] Vogels Werner. 2009. Eventually consistent. Commun. ACM 52, 1 (2009), 40\u201344.Google ScholarDigital Library\n\nReference\n\n  72. [72] Weil Sage A.. 2007. Ceph: Reliable, Scalable, and High-Performance Distributed Storage. Ph.D. Dissertation. University of California at Santa Cruz.Google Scholar\n\nReference\n\n  73. [73] Weil Sage A., Brandt Scott A., Miller Ethan L., Long Darrell D. E., and Maltzahn Carlos. 2006. Ceph: A scalable, high-performance distributed file system. In Proceedings of the 7th USENIX Symposium on Operating Systems Design and Implementation (OSDI\u201906). USENIX Association, 307\u2013320.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n     * Reference 10\n     * Reference 11\n     * Reference 12\n     * Reference 13\n     * Reference 14\n     * Reference 15\n     * Reference 16\n     * Reference 17\n\n  74. [74] Weil Sage A., Brandt Scott A., Miller Ethan L., and Maltzahn Carlos. 2006. CRUSH: Controlled, scalable, decentralized placement of replicated data. In Proceedings of the 2006 ACM/IEEE Conference on Supercomputing (SC\u201906). Association for Computing Machinery.Google ScholarDigital Library\n\nReference 1Reference 2Reference 3\n\n  75. [75] Welch Brent B. and Ousterhout John K.. 1986. Prefix tables: A simple mechanism for locating files in a distributed system. In Proceedings of the 6th International Conference on Distributed Computing Systems (ICDCS\u201986). IEEE Computer Society, 184\u2013189.Google Scholar\n\nReference 1Reference 2\n\n  76. [76] Xia Mingyuan, Saxena Mohit, Blaum Mario, and Pease David. 2015. A tale of two erasure codes in HDFS, see Reference FAS [3], 213\u2013226.Google Scholar\n\nReference\n\n  77. [77] Xu Jian and Swanson Steven. 2016. NOVA: A log-structured file system for hybrid volatile/non-volatile main memories. In Proceedings of the 14th USENIX Conference on File and Storage Technologies (FAST\u201916). USENIX Association, 323\u2013338.Google Scholar\n\nReference\n\n  78. [78] Yang Jian, Izraelevitz Joseph, and Swanson Steven. 2019. Orion: A distributed file system for non-volatile main memory and RDMA-capable networks, see Reference FAS [4], 221\u2013234.Google Scholar\n\nReference 1Reference 2Reference 3\n\n  79. [79] Zheng Qing, Ren Kai, and Gibson Garth A.. 2014. BatchFS: Scaling the file system control plane with client-funded metadata servers. In Proceedings of the 9th Parallel Data Storage Workshop (PDSW\u201914). IEEE Computer Society.Google ScholarDigital Library\n\nReference 1Reference 2\n\n  80. [80] Zheng Qing, Ren Kai, Gibson Garth A., Settlemyer Bradley W., and Grider Gary. 2015. DeltaFS: Exascale file systems scale better without dedicated servers. In Proceedings of the 10th Parallel Data Storage Workshop (PDSW\u201915). Association for Computing Machinery.Google ScholarDigital Library\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n\n  * Figures\n  * Other\n\n### Share this Publication link\n\n### Share on Social Media\n\nShare on\n\nClose Figure Viewer\n\nBrowse AllReturn\n\n### Caption\n\nView Issue\u2019s Table of Contents\n\n## New Citation Alert added!\n\nThis alert has been successfully added and will be sent to:\n\nYou will be notified whenever a record that you have chosen has been cited.\n\nTo manage your alert preferences, click on the button below.\n\nManage my Alerts\n\n## New Citation Alert!\n\nPlease log in to your account\n\n## Export Citations\n\n## Footer\n\n### Categories\n\n  * Journals\n  * Magazines\n  * Books\n  * Proceedings\n  * SIGs\n  * Conferences\n  * Collections\n  * People\n\n### About\n\n  * About ACM Digital Library\n  * ACM Digital Library Board\n  * Subscription Information\n  * Author Guidelines\n  * Using ACM Digital Library\n  * All Holdings within the ACM Digital Library\n  * ACM Computing Classification System\n  * Digital Library Accessibility\n\n### Join\n\n  * Join ACM\n  * Join SIGs\n  * Subscribe to Publications\n  * Institutions and Libraries\n\n### Connect\n\n  * Contact\n  * Facebook\n  * Twitter\n  * Linkedin\n  * Feedback\n  * Bug Report\n\nThe ACM Digital Library is published by the Association for Computing\nMachinery. Copyright \u00a9 2024 ACM, Inc.\n\n  * Terms of Usage\n  * Privacy Policy\n  * Code of Ethics\n\nYour Search Results Download Request\n\nWe are preparing your search results for download ...\n\nWe will inform you here when the file is ready.\n\nDownload now!\n\nYour Search Results Download Request\n\nYour file of search results citations is now ready.\n\nDownload now!\n\nYour Search Results Download Request\n\nYour search export query has expired. Please try again.\n\n", "frontpage": false}
