{"aid": "40034271", "title": "Apple's Mysterious Fisheye Projection", "url": "https://blog.mikeswanson.com/post/747761863530528768/apples-mysterious-fisheye-projection", "domain": "mikeswanson.com", "votes": 10, "user": "beefman", "posted_at": "2024-04-14 20:38:38", "comments": 0, "source_title": "Apple\u2019s Mysterious Fisheye Projection", "source_text": "Mike Swanson's Blog \u2022 Apple\u2019s Mysterious Fisheye Projection\n\n# Mike Swanson's Blog\n\n  * Archive\n  * RSS\n\n## Apple\u2019s Mysterious Fisheye Projection\n\nIf you\u2019ve read my first post about Spatial Video, the second about Encoding\nSpatial Video, or if you\u2019ve used my command-line tool, you may recall a\nmention of Apple\u2019s mysterious \u201cfisheye\u201d projection format. Mysterious because\nthey\u2019ve documented a CMProjectionType.fisheye enumeration with no elaboration,\nthey stream their immersive Apple TV+ videos in this format, yet they\u2019ve\nprovided no method to produce or playback third-party content using this\nprojection type.\n\nAdditionally, the format is undocumented, they haven\u2019t responded to an open\nquestion on the Apple Discussion Forums asking for more detail, and they\ndidn\u2019t cover it in their WWDC23 sessions. As someone who has experience in\nthis area \u2013 and a relentless curiosity \u2013 I\u2019ve spent time digging-in to Apple\u2019s\nfisheye projection format, and this post shares what I\u2019ve learned.\n\nAs stated in my prior post, I am not an Apple employee, and everything I\u2019ve\nwritten here is based on my own history, experience (specifically my time at\nimmersive video startup, Pixvana, from 2016-2020), research, and\nexperimentation. I\u2019m sure that some of this is incorrect, and I hope we\u2019ll all\nlearn more at WWDC24.\n\n## Spherical Content\n\nImagine sitting in a swivel chair and looking straight ahead. If you tilt your\nhead to look straight up (at the zenith), that\u2019s 90 degrees. Likewise, if you\nwere looking straight ahead and tilted your head all the way down (at the\nnadir), that\u2019s also 90 degrees. So, your reality has a total vertical field-\nof-view of 90 + 90 = 180 degrees.\n\nSitting in that same chair, if you swivel 90 degrees to the left or 90 degrees\nto the right, you\u2019re able to view a full 90 + 90 = 180 degrees of horizontal\ncontent (your horizontal field-of-view). If you spun your chair all the way\naround to look at the \u201cback half\u201d of your environment, you would spin past a\nfull 360 degrees of content.\n\nWhen we talk about immersive video, it\u2019s common to only refer to the\nhorizontal field-of-view (like 180 or 360) with the assumption that the\nvertical field-of-view is always 180. Of course, this doesn\u2019t have to be true,\nbecause we can capture whatever we\u2019d like, edit whatever we\u2019d like, and\nplayback whatever we\u2019d like.\n\nBut when someone says something like VR180, they really mean immersive video\nthat has a 180-degree horizontal field-of-view and a 180-degree vertical\nfield-of-view. Similarly, 360 video is 360-degrees horizontally by 180-degrees\nvertically.\n\n## Projections\n\nWhen immersive video is played back in a device like the Apple Vision Pro, the\nMeta Quest, or others, the content is displayed as if a viewer\u2019s eyes are at\nthe center of a sphere watching video that is displayed on its inner surface.\nFor 180-degree content, this is a hemisphere. For 360-degree content, this is\na full sphere. But it can really be anything in between; at Pixvana, we\nsometimes referred to this as any-degree video.\n\nIt\u2019s here where we run into a small problem. How do we encode this immersive,\nspherical content? All the common video codecs (H.264, VP9, HEVC, MV-HEVC,\nAVC1, etc.) are designed to encode and decode data to and from a rectangular\nframe. So how do you take something like a spherical image of the Earth (i.e.\na globe) and store it in a rectangular shape? That sounds like a map to me.\nAnd indeed, that transformation is referred to as a map projection.\n\n## Equirectangular\n\nWhile there are many different projection types that each have useful\nproperties in specific situations, spherical video and images most commonly\nuse an equirectangular projection. This is a very simple transformation to\nperform (it looks more complicated than it is). Each x location on a\nrectangular image represents a longitude value on a sphere, and each y\nlocation represents a latitude. That\u2019s it. Because of these relationships,\nthis kind of projection can also be called a lat/long.\n\nImagine \u201cpeeling\u201d thin one-degree-tall strips from a globe, starting at the\nequator. We start there because it\u2019s the longest strip. To transform it to a\nrectangular shape, start by pasting that strip horizontally across the middle\nof a sheet of paper (in landscape orientation). Then, continue peeling and\npasting up or down in one-degree increments. Be sure to stretch each strip to\nbe as long as the first, meaning that the very short strips at the north and\nsouth poles are stretched a lot. Don\u2019t break them! When you\u2019re done, you\u2019ll\nhave a 360-degree equirectangular projection that looks like this.\n\nIf you did this exact same thing with half of the globe, you\u2019d end up with a\n180-degree equirectangular projection, sometimes called a half-equirect.\nPerformed digitally, it\u2019s common to allocate the same number of pixels to each\ndegree of image data. So, for a full 360-degree by 180-degree equirect, the\nrectangular video frame would have an aspect ratio of 2:1 (the horizontal\ndimension is twice the vertical dimension). For 180-degree by 180-degree\nvideo, it\u2019d be 1:1 (a square). Like many things, these aren\u2019t hard and fast\nrules, and for technical reasons, sometimes frames are stretched horizontally\nor vertically to fit within the capabilities of an encoder or playback device.\n\nThis is a 180-degree half equirectangular image overlaid with a grid to\nillustrate its distortions. It was created from the standard fisheye image\nfurther below. Watch an animated version of this transformation.\n\nWhat we\u2019ve described so far is equivalent to monoscopic (2D) video. For\nstereoscopic (3D) video, we need to pack two of these images into each\nframe...one for each eye. This is usually accomplished by arranging two images\nin a side-by-side or over/under layout. For full 360-degree stereoscopic video\nin an over/under layout, this makes the final video frame 1:1 (because we now\nhave 360 degrees of image data in both dimensions). As described in my prior\npost on Encoding Spatial Video, though, Apple has chosen to encode stereo\nvideo using MV-HEVC, so each eye\u2019s projection is stored in its own dedicated\nvideo layer, meaning that the reported video dimensions match that of a single\neye.\n\n## Standard Fisheye\n\nMost immersive video cameras feature one or more fisheye lenses. For\n180-degree stereo (the short way of saying stereoscopic) video, this is almost\nalways two lenses in a side-by-side configuration, separated by ~63-65mm, very\nmuch like human eyes (some 180 cameras).\n\nThe raw frames that are captured by these cameras are recorded as fisheye\nimages where each circular image area represents ~180 degrees (or more) of\nvisual content. In most workflows, these raw fisheye images are transformed\ninto an equirectangular or half-equirectangular projection for final delivery\nand playback.\n\nThis is a 180 degree standard fisheye image overlaid with a grid. This image\nis the source of the other images in this post.\n\n## Apple\u2019s Fisheye\n\nThis brings us to the topic of this post. As I stated in the introduction,\nApple has encoded the raw frames of their immersive videos in a \u201cfisheye\u201d\nprojection format. I know this, because I\u2019ve monitored the network traffic to\nmy Apple Vision Pro, and I\u2019ve seen the HLS streaming manifests that describe\neach of the network streams. This is how I originally discovered and reported\nthat these streams \u2013 in their highest quality representations \u2013 are ~50Mbps,\nHDR10, 4320x4320 per eye, at 90fps.\n\nWhile I can see the streaming manifests, I am unable to view the raw video\nframes, because all the immersive videos are protected by DRM. This makes\nperfect sense, and while I\u2019m a curious engineer who would love to see a raw\nfisheye frame, I am unwilling to go any further. So, in an earlier post, I\nasked anyone who knew more about the fisheye projection type to contact me\ndirectly. Otherwise, I figured I\u2019d just have to wait for WWDC24.\n\nLo and behold, not a week or two after my post, an acquaintance introduced me\nto Andrew Chang who said that he had also monitored his network traffic and\nnoticed that the Apple TV+ intro clip (an immersive version of this) is\nstreamed in-the-clear. And indeed, it is encoded in the same fisheye\nprojection. Bingo! Thank you, Andrew!\n\nNow, I can finally see a raw fisheye video frame. Unfortunately, the frame is\nmostly black and featureless, including only an Apple TV+ logo and some God\nrays. Not a lot to go on. Still, having a lot of experience with both\npractical and experimental projection types, I figured I\u2019d see what I could\nfigure out. And before you ask, no, I\u2019m not including the actual logo, raw\nframe, or video in this post, because it\u2019s not mine to distribute.\n\nImmediately, just based on logo distortions, it\u2019s clear that Apple\u2019s fisheye\nprojection format isn\u2019t the same as a standard fisheye recording. This isn\u2019t\ntoo surprising, given that it makes little sense to encode only a circular\nregion in the center of a square frame and leave the remainder black; you\ntypically want to use all the pixels in the frame to send as much data as\npossible (like the equirectangular format described earlier).\n\nAdditionally, instead of seeing the logo horizontally aligned, it\u2019s rotated 45\ndegrees clockwise, aligning it with the diagonal that runs from the upper-left\nto the lower-right of the frame. This makes sense, because the diagonal is the\nlongest dimension of the frame, and as a result, it can store more horizontal\n(post-rotation) pixels than if the frame wasn\u2019t rotated at all.\n\nThis is the same standard fisheye image from above transformed into a format\nthat seems very similar to Apple\u2019s fisheye format. Watch an animated version\nof this transformation.\n\nLikewise, the diagonal from the lower-left to the upper-right represents the\nvertical dimension of playback (again, post-rotation) providing a similar\nincrease in available pixels. This means that \u2013 during rotated playback \u2013 the\nnow-diagonal directions should contain the least amount of image data.\nCorrectly-tuned, this likely isn\u2019t visible, but it\u2019s interesting to note.\n\n## More Pixels\n\nYou might be asking, where do these \u201cextra\u201d pixels come from? I mean, if we\nstart with a traditional raw circular fisheye image captured from a camera and\njust stretch it out to cover a square frame, what have we gained? Those are\ngreat questions that have many possible answers.\n\nThis is why I liken video processing to turning knobs in a 747 cockpit: if you\nturn one of those knobs, you more-than-likely need to change something else to\nbalance it out. Which leads to turning more knobs, and so on. Video processing\nis frequently an optimization problem just like this. Some initial thoughts:\n\n  * It could be that the source video is captured at a higher resolution, and when transforming the video to a lower resolution, the \u201cextra\u201d image data is preserved by taking advantage of the square frame.\n  * Perhaps the camera optically transforms the circular fisheye image (using physical lenses) to fill more of the rectangular sensor during capture. This means that we have additional image data to start and storing it in this expanded fisheye format allows us to preserve more of it.\n  * Similarly, if we record the image using more than two lenses, there may be more data to preserve during the transformation. For what it\u2019s worth, it appears that Apple captures their immersive videos with a two-lens pair, and you can see them hiding in the speaker cabinets in the Alicia Keys video.\n\nThere are many other factors beyond the scope of this post that can influence\nthe design of Apple\u2019s fisheye format. Some of them include distortion\nhandling, the size of the area that\u2019s allocated to each pixel, where the \u201cmost\nimportant\u201d pixels are located in the frame, how high-frequency details affect\nencoder performance, how the distorted motion in the transformed frame\ninfluences motion estimation efficiency, how the pixels are sampled and\ndisplayed during playback, and much more.\n\n## Blender\n\nBut let\u2019s get back to that raw Apple fisheye frame. Knowing that the image\nrepresents ~180 degrees, I loaded up Blender and started to guess at a\npossible geometry for playback based on the visible distortions. At that\npoint, I wasn\u2019t sure if the frame encodes faces of the playback geometry or if\nthe distortions are related to another kind of mathematical mapping. Some of\nthe distortions are more severe than expected, though, and my mind couldn\u2019t\nimagine what kind of mesh corrected for those distortions (so tempted to blame\nmy aphantasia here, but my spatial senses are otherwise excellent).\n\nOne of the many meshes and UV maps that I\u2019ve experimented with in Blender.\n\n## Radial Stretching\n\nIf you\u2019ve ever worked with projection mappings, fisheye lenses,\nequirectangular images, camera calibration, cube mapping techniques, and so\nmuch more, Google has inevitably led you to one of Paul Bourke\u2019s many\nfantastic articles. I\u2019ve exchanged a few e-mails with Paul over the years, so\nI reached out to see if he had any insight.\n\nAfter some back-and-forth discussion over a couple of weeks, we both agreed\nthat Apple\u2019s fisheye projection is most similar to a technique called radial\nstretching (with that 45-degree clockwise rotation thrown in). You can read\nmore about this technique and others in Mappings between Sphere, Disc, and\nSquare and Marc B. Reynolds\u2019 interactive page on Square/Disc mappings.\n\nBasically, though, imagine a traditional centered, circular fisheye image that\ntouches each edge of a square frame. Now, similar to the equirectangular\nstrip-peeling exercise I described earlier with the globe, imagine peeling\none-degree wide strips radially from the center of the image and stretching\nthose along the same angle until they touch the edge of the square frame. As\nthe name implies, that\u2019s radial stretching. It\u2019s probably the technique you\u2019d\ninvent on your own if you had to come up with something.\n\nBy performing the reverse of this operation on a raw Apple fisheye frame, you\nend up with a pretty good looking version of the Apple TV+ logo. But, it\u2019s not\n100% correct. It appears that there is some additional logic being used along\nthe diagonals to reduce the amount of radial stretching and distortion (and\nperhaps to keep image data away from the encoded corners). I\u2019ve experimented\nwith many approaches, but I still can\u2019t achieve a 100% match. My best guess so\nfar uses simple beveled corners, and this is the same transformation I used\nfor the earlier image.\n\nIt\u2019s also possible that this last bit of distortion could be explained by a\nspecific projection geometry, and I\u2019ve iterated over many permutations that\nget close...but not all the way there. For what it\u2019s worth, I would be\nslightly surprised if Apple was encoding to a specific geometry because it\nadds unnecessary complexity to the toolchain and reduces overall flexibility.\n\nWhile I have been able to playback the Apple TV+ logo using the techniques\nI\u2019ve described, the frame lacks any real detail beyond its center. So, it\u2019s\nstill possible that the mapping I\u2019ve arrived at falls apart along the\nperiphery. Guess I\u2019ll continue to cross my fingers and hope that we learn more\nat WWDC24.\n\n## Conclusion\n\nThis post covered my experimentation with the technical aspects of Apple\u2019s\nfisheye projection format. Along the way, it\u2019s been fun to collaborate with\nAndrew, Paul, and others to work through the details. And while we were unable\nto arrive at a 100% solution, we\u2019re most definitely within range.\n\nThe remaining questions I have relate to why someone would choose this\nprojection format over half-equirectangular. Clearly Apple believes there are\nworthwhile benefits, or they wouldn\u2019t have bothered to build a toolchain to\ncapture, process, and stream video in this format. I can imagine many possible\nadvantages, and I\u2019ve enumerated some of them in this post. With time, I\u2019m sure\nwe\u2019ll learn more from Apple themselves and from experiments that all of us can\nrun when their fisheye format is supported by existing tools.\n\nIt\u2019s an exciting time to be revisiting immersive video, and we have Apple to\nthank for it.\n\nAs always, I love hearing from you. It keeps me motivated! Thank you for\nreading.\n\n  *     * #spatial\n    * #apple vision pro\n    * #video\n  * 2 hours ago\n  * 1\n  * Comments\n  * Permalink\n\nShare\n\nShort URL\n\nTwitterFacebookPinterestGoogle+\n\n## 1 Notes/ Hide\n\n  1. steranko liked this\n\n  2. michaeldswanson posted this\n\n> If you\u2019ve read my first post about Spatial Video, the second about Encoding\n> Spatial Video, or if you\u2019ve used...\n\n## Recent comments\n\n\u2190 Previous \u2022 Next \u2192\n\n## About\n\nHi. I'm Mike Swanson, technologist, owner of Juicy Bits, and former Microsoft\nemployee of 12 years. This is my blog. Have a comment or question? Contact me.\n\n## Pages\n\n  * Spatial Video Tool\n  * iOS Rounded Rect Script\n  * JBNSLayoutConstraint\n  * Ai->Canvas Plug-In\n  * Wallpaper Images\n\n## Me, Elsewhere\n\n  * @Anyware on Twitter\n  * Facebook Profile\n\n## Twitter\n\n  * RSS\n  * Random\n  * Archive\n  * Mobile\n\nEffector Theme \u2014 Tumblr themes by Pixel Union\n\n", "frontpage": true}
