{"aid": "40086358", "title": "Llama 3 8B Instruct quantized with GPTQ to fit in 10gb vRAM", "url": "https://huggingface.co/astronomer-io/Llama-3-8B-Instruct-GPTQ-8-Bit", "domain": "huggingface.co", "votes": 1, "user": "jlaneve", "posted_at": "2024-04-19 13:08:52", "comments": 2, "source_title": "astronomer-io/Llama-3-8B-Instruct-GPTQ-8-Bit \u00b7 Hugging Face", "source_text": "astronomer-io/Llama-3-8B-Instruct-GPTQ-8-Bit \u00b7 Hugging Face\n\nHugging Face\n\n#\n\nastronomer-io\n\n/\n\nLlama-3-8B-Instruct-GPTQ-8-Bit\n\nText Generation Transformers\n\nllama llama-3 facebook meta astronomer gptq pretrained quantized finetuned\nInference Endpoints conversational text-generation-inference 8-bit precision\n\nModel card Files Files and versions Community\n\nEdit model card\n\nThis model is generously created and made open source by Astronomer.\n\nAstronomer is the de facto company for Apache Airflow, the most trusted open-\nsource framework for data orchestration and MLOps.\n\n# Important Note Regarding a Known Bug in Llama 3\n\n  * Two files are modified to address a current issue regarding Llama 3 models keep on generating additional tokens non-stop until hitting max token limit.\n  * generation_config.json's eos_token_id have been modified to add the other EOS token that Llama-3 uses.\n  * tokenizer_config.json's chat_template has been modified to only add start generation token at the end of a prompt if add_generation_prompt is selected.\n  * For loading this model onto vLLM, make sure all requests have \"stop_token_ids\":[128001, 128009] to temporarily address the non-stop generation issue.\n\n    * vLLM does not yet respect generation_config.json.\n    * vLLM team is working on a a fix for this https://github.com/vllm-project/vllm/issues/4180\n\n# Llama-3-8B-Instruct-GPTQ-8-Bit\n\n  * Original Model creator: Meta Llama from Meta\n  * Original model: meta-llama/Meta-Llama-3-8B-Instruct\n  * Built with Meta Llama 3\n  * Quantized by Astronomer\n\n## Description\n\nThis repo contains 8 Bit quantized GPTQ model files for meta-llama/Meta-\nLlama-3-8B-Instruct.\n\nThis model can be loaded with just over 10GB of VRAM (compared to the original\n16.07GB model) and can be served lightning fast with the cheapest Nvidia GPUs\npossible (Nvidia T4, Nvidia K80, RTX 4070, etc).\n\nThe 8 bit GPTQ quant has minimum quality degradation from the original\nbfloat16 model due to its higher bitrate.\n\n## GPTQ Quantization Method\n\n  * This model is quantized by utilizing the AutoGPTQ library, following best practices noted by GPTQ paper\n  * Quantization is calibrated and aligned with random samples from the specified dataset (wikitext for now) for minimum accuracy loss.\n\nBranch| Bits| Group Size| Act Order| Damp %| GPTQ Dataset| Sequence Length|\nVRAM Size| ExLlama| Description  \n---|---|---|---|---|---|---|---|---|---  \nmain| 8| 32| Yes| 0.1| wikitext| 8192| 9.09 GB| No| 8-bit, with Act Order and\ngroup size 32g. Minimum accuracy loss with decent VRAM usage reduction.  \nMore variants to come| TBD| TBD| TBD| TBD| TBD| TBD| TBD| TBD| May upload\nadditional variants of GPTQ 8 bit models in the future using different\nparameters such as 128g group size and etc.  \n  \n## Serving this GPTQ model using vLLM\n\nTested serving this model via vLLM using an Nvidia T4 (16GB VRAM).\n\nTested with the below command\n\n    \n    \n    python -m vllm.entrypoints.openai.api_server --model astronomer-io/Llama-3-8B-Instruct-GPTQ-8-Bit --max-model-len 8192 --dtype float16\n\nFor the non-stop token generation bug, make sure to send requests with\nstop_token_ids\":[128001, 128009] to vLLM endpoint Example:\n\n    \n    \n    { \"model\": \"Llama-3-8B-Instruct-GPTQ-8-Bit\", \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Who created Llama 3?\"} ], \"max_tokens\": 2000, \"stop_token_ids\":[128001,128009] }\n\n### Contributors\n\n  * Quantized by David Xue, Machine Learning Engineer from Astronomer\n\nDownloads last month\n\n    0\n\n##\n\nQuantized from\n\n## Dataset used to train astronomer-io/Llama-3-8B-Instruct-GPTQ-8-Bit\n\n", "frontpage": false}
