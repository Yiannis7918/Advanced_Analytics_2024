{"aid": "40010045", "title": "Fun with GMMs", "url": "https://blog.quipu-strands.com/fun_with_GMMs", "domain": "quipu-strands.com", "votes": 1, "user": "abhgh", "posted_at": "2024-04-12 06:57:11", "comments": 0, "source_title": "Fun with GMMs", "source_text": "Fun with GMMs\n\nLoading [MathJax]/jax/output/HTML-CSS/fonts/TeX/fontdata.js\n\nA Not-So Primordial Soup\n\n# Fun with GMMs\n\nCreated: Apr 22, 2022. Last major update: Nov 16, 2023.\n\nGenerative Models have been all the rage in AI lately, be it image generators\nlike Stable Diffusion or text generators like ChatGPT. These are examples of\nfairly sophisticated generative systems. But whittled down to basics, they are\na means to:\n\n  * (a) concisely represent patterns in data, in a way that ...\n  * (b) they can generate later what they have \u201cseen\u201d.\n\nA bit like an artist who witnesses a scenery and later recreates it on canvas\nusing her memory; her memory acting as a generative model here.\n\nIn this post, I will try to illustrate this mechanism using a specific\ngenerative model: the Gaussian Mixture Model (GMM). We will use it to capture\npatterns in images. Pixels will be our data, and patterns are how they are\n\u201clumped\u201d together. Of course, this lumping is what humans perceive as the\nimage itself. Effectively then, much like our artist, we will use a generative\nmodel to \u201csee\u201d an image and then have it reproduce it later. Think of this as\na rudimentary, mostly visual, tutorial on GMMs, where we focus on their\nrepresentational capability. Or an article where I mostly ramble but touch\nupon GMMs, use of probabilities, all the while creating fancy art like the\nones below!\n\nRenderings obtained using a GMM.\n\nMy reason for picking GMMs is that they are an intuitive gateway to the world\nof generative modeling. They are also well-studied: the first paper that talks\nabout GMMs was published in 1894! GMMs also make some of the math convenient;\nwe\u2019ll see examples of this soon. Library-wise scikit has a nice implementation\nof GMMs, which I have used for this post. scipy has convenient functions to\nwork with the Gaussian distribution - which is also used here.\n\nI know I said this is going to be \u201cmostly\u201d visual. I won\u2019t be abnegating all\nmath here : we will discuss some necessary math, that is just about enough to\nunderstand how to model images. Here\u2019s the layout of the rest of this post.\n\n  * Crash Course in G and MM\n\n    * What are GMMs?\n    * Sampling from a GMM\n    * Multivariate Gaussians\n    * Contour Plots\n    * Some Properties\n  * Why Images?\n  * Black and White Images\n\n    * Sampling\n    * Grid-plotting\n    * Custom Mapping for Grid Plots\n  * Color Images\n\n    * Sampling\n    * Grid-plotting\n  * Notes\n\n## Crash Course in G and MM\n\nSome basics first, so that we can comfortably wield GMMs as tools. If you are\nfamiliar with the Gaussian distribution (also known as the Normal\ndistribution), GMMs, conditional and marginal probabilities, feel free to skip\nahead to the next section.\n\n### What are GMMs?\n\nThink of GMMs as a way to approximate functions of special kind: probability\ndensity functions (pdf). pdfs are a way to express probabilities over\nvariables that take continuous values. For ex., you might want to use a pdf to\ndescribe the percentage x of humidity in the air tomorrow, where x can be any\nreal number between 0 and 100, i.e., x\u2208[0,100]. The pdf value at x, denoted by\np(x) (often called the \u201cdensity\u201d), in some loose sense represents how likely\nthe value is, but is not a probability value in itself. However, if you sum\nthese values for a contiguous range or an \u201cinterval\u201d of xs, you end up with a\nvalid probability value. In the case of continuous values, such sums are\nintegrals; so, the probability of humidity being within 40%-60% is\n\u222b6040p(x)dx. Some well known pdfs are Gaussian (the star of our show, also\nknown as the Normal distribution), Poisson and Beta.\n\nLet\u2019s talk about the Gaussian for a bit. For a variable x, the density p(x)\ncan be calculated using this formula:\n\np(x)=1\u03c3\u221a2\u03c0e\u221212(x\u2212\u03bc\u03c3)2\n\nWe won\u2019t be using this formula. But note that it depends on the following two\nparameters:\n\n  * \u03bc: the mean or average value of the distribution.\n  * \u03c3: the standard deviation, e.g., how spread out is the distribution. Often we talk in terms of the square of this quantity, the variance \u03c32.\n\nSome common shorthands:\n\n  * A Gaussian distribution with parameters \u03bc and \u03c3 is N(\u03bc,\u03c32).\n  * The standard Normal is one with \u03bc=0 and \u03c3=1, and is denoted by N(0,1).\n  * Instead of saying p(x) is given by N(\u03bc,\u03c32), we may concisely write N(x;\u03bc,\u03c32) for the density of x.\n  * To say x was sampled from such a distribution, we write x\u223cN(\u03bc,\u03c32). This is also equivalent to saying that the density of such xs are given by N(x;\u03bc,\u03c32) (the expression in the previous point). What is used depends on the context: do we want to highlight sampling or density?\n\nNow, consider a practical scenario: you have some data, that doesn\u2019t look\nanything like one of the commonly known pdfs. How would you mathematically\nexpress its density? In a time-honoured tradition, we will use something that\nwe know to approximate what we don\u2019t know. We\u2019ll use a bunch or a mixture of\nsuperposed Gaussians (hence the name). To specify such a mixture, we need:\n\n  * Number of components, K.\n  * Parameters or shape of the components. We will denote component i by N(\u03bci,\u03c32i), where we need to figure out the values for \u03bci and \u03c3i.\n  * Weights wi of the components, or how much a component contributes to the overall density. We require two properties here (we\u2019ll see why soon): (a) 0\u2264wi\u22641, and (b) \u2211Kiwi=1.\n\nThe resultant density p(x) of a point due to this mixture is given by the sum\nof individual Gaussian densities multiplied by their weights:\n\np(x)=K\u2211i=1wiN(x;\u03bci,\u03c32i)\n\nThe physical interpretation is that x may have been generated by the ith\ncomponent with probability wi (which explains the properties needed of wi -\nthey\u2019re effectively probabilities), and then for this component, its density\nis given by N(x;\u03bci,\u03c32i). The net pdf value for a specific value for x due to\ncomponent i then is wiN(x;\u03bci,\u03c32i). Since x may be generated by any component,\nthe overall p(x) is given by the sum above.\n\nThis turns out to be a powerful device, since with the right number of\ncomponents, shapes and weights, one may approximate arbitrary distributions.\nThe plot below shows different 2-component GMMs, constructed out of the same\ntwo components (red dashed lines), but with different weights (mentioned in\nthe legend). You can see that this alone produces various different pdfs\n(solid lines).\n\nDifferent pdfs using a GMM\n\nLet\u2019s go back to the original question. In the figure below, we consider some\ndata points on the x-axis (shown with small vertical lines, known as a\nrugplot). The corresponding histogram is also shown. We try to approximate\nthis target distribution with a GMM with 3 components, shown with the red\ndashed lines. The black solid line shows the final pdf obtained using\nappropriate weights (not shown), which you\u2019d note, is quite close to the\nhistogram. This is a GMM in action.\n\nApproximating a data dsitribution with a GMM\n\nHere we won\u2019t discuss how we find the number of components, or their\nparameters, or their weights - the first one is a hyperparameter (so you\neither determine it based on some task specific metric or use a heuristic such\nas AIC), and the latter are typically determined using a popular algorithm\nknown as Expectation Maximization (EM), for which we use scikit\u2019s\nimplementation. Here\u2019s an interesting bit of trivia: the EM algorithm was\nproposed in 1977, which was much after the first use of GMMs (which, as\nmentioned above, was in 1894)!\n\nInterestingly, it is also possible to use an infinite number of components,\nbut that\u2019s again something we won\u2019t cover here.\n\n### Sampling from a GMM\n\nAs a quick stopover, let\u2019s ask ourselves the \u201cinverse\u201d question: how do we\ngenerate data from a GMM \u2211Ki=1N(x;\u03bci,\u03c32i)? Think about the physical\ninterpretation we discussed earlier; that\u2019s going to be helpful now. To\ngenerate one data instance x, we follow this two-step process:\n\n  * Use wi as probabilities to pick a particular component i.\n  * And then generate x\u223cN(\u03bci,\u03c32i).\n\nRepeat the above steps till you have the required number of samples.\n\nThis ends up producing instances from the components in proportion to their\nweights. The following video shows this for a 2-component GMM. For sampling a\npoint, first a component is chosen - this choice is temporarily highlighted in\nred. And then an instance is sampled - shown in blue. The black line shows the\npdf of the GMM, but if you sampled enough instances and sketched its empirical\ndistribution (using, say, a kdeplot), you\u2019d get something very close to this\npdf.\n\n### Multivariate Gaussians\n\nOf course, GMMs can be extended to an arbitrary number of d dimensions, i.e.,\nwhen x\u2208Rd. We augment the notation to denote such a pdf as p(x1,x2,..,xd),\nwhere xi denotes the ith dimension of a data instance x. The GMM now is\ncomposed of multivariate Gaussians N(\u03bc,\u03a3). The symbol for the mean is still \u03bc,\nbut now \u03bc\u2208Rd. Instead of the variance, we have a covariance matrix, where \u03a3i,j\n- the entry at the (i,j) index - denotes the covariance between values of xi\nand xj across the dataset. Since covariance doesn\u2019t depend on the order of\nvariables, \u03a3i,j=\u03a3j,i and thus \u03a3\u2208Rd\u00d7d is a symmetric matrix.\n\nAnd yes, unfortunately the symbol for summation \u2211, which is \\sum in LaTeX, and\nthe covariance matrix \u03a3 - \\Sigma - look very similar. I was tempted to use a\ndifferent symbol for the latter, but then I realized it might just be better\nto bite the bullet and get used to the common notation.\n\nThe GMM operates just the same, i.e., for x\u2208Rd, we have:\n\np(x)=K\u2211i=1wiN(x;\u03bci,\u03a3i)\n\n### Contour Plots\n\nWe want to be able to visualize these high-dimensional GMMs. In general,\nfaithful visualization of high-dimensional structures is hard, but,\nfortunately, there is a well known technique applicable to two-dimensions:\ncontour plots. In the figure below, (a) shows some data points in 2D. I have\nused a 2-component two-dimensional GMM to model this data. Instead of showing\nthe pdf on a z-axis in a 3D plot, we calculate the values z=p(x1,x2) values at\na whole lot of points in the input space, and then connect the points with\nidentical z values. We might also color the lines formed by these connections,\nto indicate how high or low z is. These lines are knowns as contours and (b)\nshows such a contour plot.\n\n(a) shows a 2-D dataset. (b) shows the contour plot of 2-component GMM fit to\nthe data.\n\nNote the legend in (b) - it has negative numbers. This is because (b) actually\nshows contours for logp(x1,x2) - which is a common practice (partly because it\nleads to better visualization, and partly because the quantity logp itself\nshows up in the math quite a bit).\n\n### Some Properties\n\nSo far the only thing we have used the GMM for is to sample data. This is\nuseful, but sometimes we want to use the model to answer specific questions\nabout the data, e.g., what is the mean value of the data? We\u2019ll briefly look\nat this aspect now.\n\nExpectation\n\nThe expectation or the expected value of variable is its average value\nweighted by the probabilites of taking those values:\n\nEp[x]=\u222b+\u221e\u2212\u221exp(x)dx\n\nThe subscript p in Ep[x] denotes the distribution wrt which we assume x\nvaries; we\u2019ll drop this since its often clear from context. While the integral\nranges from \u2212\u221e to +\u221e, it just indicates that we should account for all\npossible values x can take.\n\nFor a Normal distribution, E[x] is just its mean \u03bc. We can use this fact to\nconveniently compute the expectation for a GMM:\n\nE[x]=\u222b+\u221e\u2212\u221ex(K\u2211i=1wiN(x;\u03bci,\u03a3i))dx=\u222b+\u221e\u2212\u221e(K\u2211i=1wixN(x;\u03bci,\u03a3i))dx=K\u2211i=1(\u222b+\u221e\u2212\u221ewixN(x;\u03bci,\u03a3i)dx)=K\u2211i=1(wi\u222b+\u221e\u2212\u221exN(x;\u03bci,\u03a3i)dx)=K\u2211i=1wi\u03bci\n\nConditional Distributions\n\nIt\u2019s often useful to talk about the behavior of certain variables while\nholding other variables constant. For example, in a dataset comprised of Age\nand Income of people, you might want to know how Income varies for 25 year\nolds. You\u2019re technically looking for a conditional distribution: the\ndistribution of certain variables like Income, given a condition such as\nAge=25 over the other variables.\n\nThe density obtained after conditioning on a certain subset of dimensions, e.g., xd\u22122=a,xd\u22121=b,xd=c, is written as (note the | character):\n\np(x1,x2,...,xd\u22123|xd\u22122=a,xd\u22121=b,xd=c)\n\nWhat do such conditonal distrbutions look for the Gaussian? Consider a simple\ncase of two variables x1,x2, and conditioning on x1:\n\nConditional distribution for a Gaussian.\n\nLook at the various conditional distributions p(x2|x1), for different x1. In\nthe figure, this is equivalent to \u201cslicing\u201d through the Gaussian at specific\nvalues of x1, as shown by the three solid lines. Do these distributions/solid\nlines suspiciously look like Gaussians? They are! The conditional\ndistributions of a Gaussian are also Gaussian. We wouldn\u2019t go into proving\nthis interesting property, but its important to know it is true.\n\nNote that the above animation only serves to provide a good mental model and\ndoesn\u2019t present the whole picture. You can delve deeper into nuances this\nvisualization misses here, but if this is your first encounter with the\nnotion, you can skip it for now.\n\nBut what does this say about GMMs? Because of the above property, the\nconditional distribution for a GMM is also a GMM. This new GMM has different\nweights and components. You can find a derivation here. This insight will be\nhelpful when we deal with color images.\n\nOK, our short stop is over ... on to actual problem-solving!\n\n## Why Images?\n\nIf GMMs are about modeling data, how do images fit in? Let\u2019s take the example\nof black-and-white images: you can think of every single black pixel as a data\npoint existing at its location. Look at the image below - the pixels that make\nup the shape of the dog are the data we will use to fit our GMM.\n\nGMMs. Original image source: pixabay.com\n\nFor color images things get a bit complicated - but not that much, and we\u2019ll\nfind a way to deal with it later.\n\n## Black and White Images\n\nLet\u2019s start with black-and-white images since they are easier to model. Our\ntraining data for the GMM is essentally the collection of all coordinates\nwhere we have black pixels, as shown below:\n\nThe white regions in the above image DO NOT contribute to the dataset. The\nlearned GMM only knows where data is present. In this case the components are\n2-dimensional Gaussians since we are only modeling coordinates in a 2D plane.\n\n### Sampling\n\nWe now generate our first images <drum roll>! The recipe is simple:\n\n  * We create our training data as shown above, and fit a 2-dimensional GMM. We set the number of components K=500. No particular reason, I just picked an arbitrary high number.\n  * Create a contour plot for the GMM - which gives us our first fancy image. Art!\n\nGoing to back to our previous example, we have:\n\nContour plot.\n\nWe can also generate the image by sampling from the GMM. We have seen how to\nsample earlier - its pretty much the same now, except we have more components\nand each point we sample has 2 dimensions, and thus can be placed on a plane.\n\nWe\u2019ll sample multiple points (x1,x2), and at these locations, we place a blue\npixel. We show what these samples look like for different sample sizes in (a)\nand (b), for ~20k and ~40k points respectively.\n\nImages generated by sampling.\n\nOK, lets dial up the complexity a bit. We\u2019ll use an image of a butterfly this\ntime, that has more details. Plots (a) and (b) are the same as before, but we\nincrease the sample size in (c) and (d) to ~50k and ~100k respectively to\nhandle the increased image details.\n\nContour plot of a very psychedelic butterfly. Original image source:\npixabay.com.\n\nImages generated by sampling.\n\nNext, we will make the image even harder to learn - which effectively means\nthe image has a lot of detail, which makes it computationally challenging for\nthe model to learn. We\u2019ll use an image of the famous print Great Wave off\nKanagawa. Here, you can see the outputs have lost quite a bit of detail.\n\nContour plot. Original image source: wikipedia.com.\n\nImages generated by sampling.\n\nThese look cool - but there is another way to use our GMMs to generate these\nimages. Let\u2019s look at it next.\n\n### Grid-plotting\n\nInstead of sampling from the GMM, we can go over a grid of coordinates (x,y)\nin the input space, and color the pixel at the location based on the value for\np(x,y). The below image shows the process. The blank circles show locations\nyet to be colored.\n\nColor positions based on p(x,y) values.\n\nThe above image simplifies one detail: p(x,y) values are pdf values, so do not\nhave an upper-bound (but our grayscale ranges within [0,1]). We collect all\nthese values for our grid of points, scale them to lie in the range [0,1]\nbefore we plot our image.\n\nLet\u2019s start with the image of the dog. We see its almost perfectly recovered!\nWe probably expect this by now given its a simple sketch.\n\nGrid plot.\n\nHowever, notice the width of the lines. Do they seem thicker? That\u2019s because\npixels in the close neighborhood of the actual sketch lines also end up\ngetting a black-ish color - afterall, we are using a probabilistic model. As\ndetails in an image grow, you will see more of this effect. Below, we have the\nbutterfly image - you can actually see smudging near the borders, making it\nlook like a charcoal sketch.\n\nGrid plot.\n\nThe waves image is tricky because of the high amount of detail - and its not\nsurprising that we obtain a heavily smudged image, to the point it looks like\na blob!:\n\nGrid plot.\n\nBut fear not - we have another trick up our collective sleeves!\n\n### Custom Mapping for Grid Plots\n\nHow do we do better? A simple hack is to change the mapping of probabilities\nto color values, so that the numerous points with low probabilities are\nvisibly not dark. We can replace the implicit linear mapping we\u2019ve been using\nwith a bespoke non-linear mapping that achieves this outcome - in the figure\nbelow, (a) shows this with a piecewise linear function, where the dashed\norange line is the default linear mapping (shown for reference). The x-axis\nshows the scaled pdf values of pixels, and they y-axis shows the score they\nare mapped to on the gray colormap, which ranges between [0, 255]. Our version\nallows for very dark colors for only relatively high scaled pdf values.\n\nCustom mapping between probabilities and intensity of black color.\n\nBelow we compare the linear mapping in (a) to our custom mapping in (b). In\n(b) we do end up recovering some details; you can actually see Mt. Fuji now!\nWe can also explore playing with the number of components, point sizes, alpha\nvalues etc.\n\nWith custom mapping.\n\nThere are probably numerous ways to tweak the steps in this section to make\nthe images more aesthetic (and it can get quite addictive!), but lets stop for\nnow and instead direct our attention to modeling color images.\n\n## Color Images\n\nFinally, the pi\u00e8ce de r\u00e9sistance!\n\nThe challenge with color images is we can\u2019t just fit a model to the locations\nof certain pixels: ALL locations are important, because they all have colors.\nAdditionally, we also need to model the actual colors - in the previous case,\nthis is not something we explicitly modeled.\n\nLet\u2019s proceed by breaking down the problem. We can think of every location in\nthe image as having 3 colors: red (R), green (G), blue (B). These are also\nknown as channels. In the image below, the RGB channels for a color image are\nshown.\n\nThe original image is at the top left; the remaining images show the red,\ngreen and blue channels. Original image source: pixabay.com.\n\nWe need to model these RGB values at each location. For this we use a\n5-dimensional GMM, where the variables are the x,y coordinates followed by the\nr,g,b values present at that coordinate. The following image shows how the\ntraining data is created - a notable difference from the black-and-white case\nis that all locations are used.\n\nIn constrast to the black-and-white case, all pixels contribute to the data.\n\nThese GMMs are going to be larger because of both (a) a higher number of\ndimensions, and (b) a lot more points to model. I am also going to use a\nlarger number of components since there is more information to be learned.\n\n### Sampling\n\nSo we have our 5-dimensional GMM now (trained with K=1000 components this\ntime). We can generate our first set of images by sampling as before: sample a\npoint (x,y,r,g,b), and color the pixel at location (x,y) with the colors\n(r,g,b). A minor practical detail here is that you would need to clip the\nr,g,b sampled values to lie in the right range - because the GMM, not knowing\nthese are colors, might generate values beyond the valid range of [0, 255].\n\nBut it works. Behold!\n\nOriginal image source: pixabay.com\n\nOriginal image source: pixabay.com\n\nOriginal image source: pixabay.com\n\n### Grid-plotting\n\nRemember we were able to generate images earlier by plotting points on a grid?\nHow do we do that here? Well, the answer is a little complicated but doable.\n\nHere\u2019s the challenge: at a particular coordinate (x1,y1), you can ask your GMM\nfor a conditional distribution: p(r,g,b|x=x1,y=y1). Yes, you get a full-blown\ndifferent GMM over r,g,b values at every coordinate (because, like we\ndiscussed, the conditional of a GMM is also a GMM). How do we color a location\nwith a distribution? - well, here I just use its mean value. Effectively,\ngiven a specific coordinate (x1,y1), I calculate the mean values for the\n(r,g,b) tuple:\n\nE[r,g,b|x=x1,y=y1]\n\nThe below schematic shows this is what that looks like:\n\nGrid plot strategy.\n\nThis is computationally more expensive than the black-and-white case. When you\nuse the strategy on actual images, you end up with images with \u201csmudgy brush\nstrokes\u201d - something you may have noticed with other AI-generated art. But the\nresults are not bad at all!\n\nGrid plot.\n\nGrid plot.\n\nGrid plot.\n\n## Notes\n\nA list of miscellaneous points that I kept out of the main article to avoid\ncluttering.\n\n  * I fixed a lot of the GMM parameters to some sweet-spot between the training being not memory intensive and the outputs being aesthetically pleasing. Of course, lots of tweaking possible here.\n  * Can we use discriminative classifiers? Yes - as an example, for the black-and-white case, given a location (x,y) you could predict a label in {0,1} for white and black colored pixels respectively. Here\u2019s an example where I used LightGBM as the classifier:\n\nGrid plot using a discriminative classifier.\n\n  * Images were created using: matplotlib, seaborn, Inkscape, ezgif, gifsicle, Shutter.\n  * Thanks to pixabay for making their images available royalty-free. Specifically, I used these images:\n\n    * I extracted black-and-white images out of the original color images of the Dog and Butterfly.\n    * The images in the color section are from here: Kingfisher, Springbird, Boat in ocean.\n  * As mentioned earlier, the image of \u201cThe Great Wave off Kanagawa\u201d is from wikipedia.\n  * For various properties of the Gaussian, Bishop\u2019s book \u201cPattern Recognition and Machine Learning\u201d is a commonly cited reference (PDF). Properties of Gaussians can be found in Section 2.3 and GMMs are discussed in Section 9.2.\n\nThis was a fun article to write - so much fun that I kept playing around with\nvarious parameter settings and broader creative ideas (sadly, mostly\nunfinished) to the point that this has been languishing as a draft for more\nthan a year now! GMMs themselves are not used as much anymore, but I think of\nthem as a good place to start if you want to understand how to use\ndistributions for practical problems. GMMs aside, it is good to be familiar\nwith the properties of Gaussians since they keep cropping up in various places\nsuch as Probabilistic Circuits, Gaussian Processes, Bayesian Optimization and\nVariational Autoencoders.\n\n## A Not-So Primordial Soup\n\n  * A Not-So Primordial Soup\n\n  * LinkedIn\n  * Quora\n\nMy thought-recorder.\n\n", "frontpage": false}
