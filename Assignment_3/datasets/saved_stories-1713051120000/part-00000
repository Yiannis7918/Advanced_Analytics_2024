{"aid": "40024585", "title": "Transcript: Ezra Klein Interviews Dario Amodei", "url": "https://www.nytimes.com/2024/04/12/podcasts/transcript-ezra-klein-interviews-dario-amodei.html", "domain": "nytimes.com", "votes": 1, "user": "marban", "posted_at": "2024-04-13 17:29:10", "comments": 0, "source_title": "Transcript: Ezra Klein Interviews Dario Amodei", "source_text": "Transcript: Ezra Klein Interviews Dario Amodei - The New York Times\n\nManage privacy preferences\n\nWe and our vendors use cookies and similar methods to recognize visitors and\nremember their preferences, for analytics, to measure our marketing\neffectiveness and to target and measure the effectiveness of ads, among other\nthings. To learn more about these methods, view our Cookie Policy and Privacy\nPolicy. By clicking \u2018Accept all,\u2019 you consent to the processing of your data\nby us and our vendors using the above methods. You can always change your\npreferences by clicking on Manage Privacy Preferences in our website footer or\nin your app Privacy Settings.\n\n344\n\nVendors using consent 317\n\nVendors using legitimate interest 81\n\nPrecise geolocation data, and identification through device scanning\n\nPrecise geolocation and information about device characteristics can be used.\n\nPersonalised advertising, advertising measurement, audience research and\nservices development\n\nAdvertising can be personalised based on your profile. Your activity on this\nservice can be used to build or improve a profile about you for personalised\nadvertising. Advertising performance can be measured. Reports can be generated\nbased on your activity and those of others. Your activity on this service can\nhelp develop and improve products and services.\n\nPurposes\n\n  * Use limited data to select advertising\n  * Create profiles for personalised advertising\n  * Use profiles to select personalised advertising\n  * Measure advertising performance\n  * Understand audiences through statistics or combinations of data from different sources\n  * Develop and improve services\n\nStore and/or access information on a device\n\nCookies, device or similar online identifiers (e.g. login-based identifiers,\nrandomly assigned identifiers, network based identifiers) together with other\ninformation (e.g. browser type and information, language, screen size,\nsupported technologies etc.) can be stored or read on your device to recognise\nit each time it connects to an app or to a website, for one or several of the\npurposes presented here.\n\nManage Privacy Preferences\n\nWe and our vendors use cookies and similar methods (\u201cCookies\u201d) to recognize\nvisitors and remember their preferences. We also use them for a variety of\npurposes, including analytics, to measure marketing effectiveness and to\ntarget and measure the effectiveness of ads. You can accept or reject the use\nof Cookies for individual purposes below. If you previously accepted these\nmethods through our prior banner, then we will use your data for targeting.\n\nBelow, you will find a list of the purposes and special features for which\nyour data is being processed. You may exercise your rights for specific\npurposes, based on consent or legitimate interest, using the toggles below.\n\nPurposes\n\nStore and/or access information on a device\n\nCookies, device or similar online identifiers (e.g. login-based identifiers,\nrandomly assigned identifiers, network based identifiers) together with other\ninformation (e.g. browser type and information, language, screen size,\nsupported technologies etc.) can be stored or read on your device to recognise\nit each time it connects to an app or to a website, for one or several of the\npurposes presented here.\n\nMost purposes explained in this notice rely on the storage or accessing of\ninformation from your device when you use an app or visit a website. For\nexample, a vendor or publisher might need to store a cookie on your device\nduring your first visit on a website, to be able to recognise your device\nduring your next visits (by accessing this cookie each time).\n\nVendors177 vendor(s)\n\n  * 6sense\n  * A.Mob\n  * ADventori\n  * Aarki\n  * AcuityAds\n  * AdElement Media Solutions\n  * AdGear\n  * AdKernel\n  * AdSpirit AdServer\n  * AdTheorent\n\n1-10 / 177\n\nUse limited data to select advertising\n\nAdvertising presented to you on this service can be based on limited data,\nsuch as the website or app you are using, your non-precise location, your\ndevice type or which content you are (or have been) interacting with (for\nexample, to limit the number of times an ad is presented to you).\n\nA car manufacturer wants to promote its electric vehicles to environmentally\nconscious users living in the city after office hours. The advertising is\npresented on a page with related content (such as an article on climate change\nactions) after 6:30 p.m. to users whose non-precise location suggests that\nthey are in an urban zone.\n\nA large producer of watercolour paints wants to carry out an online\nadvertising campaign for its latest watercolour range, diversifying its\naudience to reach as many amateur and professional artists as possible and\navoiding showing the ad next to mismatched content (for instance, articles\nabout how to paint your house). The number of times that the ad has been\npresented to you is detected and limited, to avoid presenting it too often.\n\nVendors43 vendor(s)\n\n  * 6sense\n  * AcuityAds\n  * AdGear\n  * AdKernel\n  * Adform\n  * Advanced store\n  * Amazon Ad Server\n  * Appier\n  * AudienceProject\n  * Bannernow\n\n1-10 / 43\n\nCreate profiles for personalised advertising\n\nInformation about your activity on this service (such as forms you submit,\ncontent you look at) can be stored and combined with other information about\nyou (for example, information from your previous activity on this service and\nother websites or apps) or similar users. This is then used to build or\nimprove a profile about you (that might include possible interests and\npersonal aspects). Your profile can be used (also later) to present\nadvertising that appears more relevant based on your possible interests by\nthis and other entities.\n\nIf you read several articles about the best bike accessories to buy, this\ninformation could be used to create a profile about your interest in bike\naccessories. Such a profile may be used or improved later on, on the same or a\ndifferent website or app to present you with advertising for a particular bike\naccessory brand. If you also look at a configurator for a vehicle on a luxury\ncar manufacturer website, this information could be combined with your\ninterest in bikes to refine your profile and make an assumption that you are\ninterested in luxury cycling gear.\n\nAn apparel company wishes to promote its new line of high-end baby clothes. It\ngets in touch with an agency that has a network of clients with high income\ncustomers (such as high-end supermarkets) and asks the agency to create\nprofiles of young parents or couples who can be assumed to be wealthy and to\nhave a new child, so that these can later be used to present advertising\nwithin partner apps based on those profiles.\n\nVendors129 vendor(s)\n\n  * 6sense\n  * A.Mob\n  * Aarki\n  * AcuityAds\n  * AdElement Media Solutions\n  * AdGear\n  * AdKernel\n  * AdSpirit AdServer\n  * AdTheorent\n  * AdTiming\n\n1-10 / 129\n\nUse profiles to select personalised advertising\n\nAdvertising presented to you on this service can be based on your advertising\nprofiles, which can reflect your activity on this service or other websites or\napps (like the forms you submit, content you look at), possible interests and\npersonal aspects.\n\nAn online retailer wants to advertise a limited sale on running shoes. It\nwants to target advertising to users who previously looked at running shoes on\nits mobile app. Tracking technologies might be used to recognise that you have\npreviously used the mobile app to consult running shoes, in order to present\nyou with the corresponding advertisement on the app.\n\nA profile created for personalised advertising in relation to a person having\nsearched for bike accessories on a website can be used to present the relevant\nadvertisement for bike accessories on a mobile app of another organisation.\n\nVendors127 vendor(s)\n\n  * 6sense\n  * A.Mob\n  * ADventori\n  * Aarki\n  * AcuityAds\n  * AdElement Media Solutions\n  * AdGear\n  * AdKernel\n  * AdSpirit AdServer\n  * AdTheorent\n\n1-10 / 127\n\nMeasure advertising performance\n\nInformation regarding which advertising is presented to you and how you\ninteract with it can be used to determine how well an advert has worked for\nyou or other users and whether the goals of the advertising were reached. For\ninstance, whether you saw an ad, whether you clicked on it, whether it led you\nto buy a product or visit a website, etc. This is very helpful to understand\nthe relevance of advertising campaigns.\n\nYou have clicked on an advertisement about a \u201cblack Friday\u201d discount by an\nonline shop on the website of a publisher and purchased a product. Your click\nwill be linked to this purchase. Your interaction and that of other users will\nbe measured to know how many clicks on the ad led to a purchase.\n\nYou are one of very few to have clicked on an advertisement about an\n\u201cinternational appreciation day\u201d discount by an online gift shop within the\napp of a publisher. The publisher wants to have reports to understand how\noften a specific ad placement within the app, and notably the \u201cinternational\nappreciation day\u201d ad, has been viewed or clicked by you and other users, in\norder to help the publisher and its partners (such as agencies) optimise ad\nplacements.\n\nVendors76 vendor(s)\n\n  * 6sense\n  * Aarki\n  * AcuityAds\n  * AdGear\n  * AdKernel\n  * Adelaide\n  * Adform\n  * Adikteev\n  * Adloox\n  * Adnami\n\n1-10 / 76\n\nUnderstand audiences through statistics or combinations of data from different\nsources\n\nReports can be generated based on the combination of data sets (like user\nprofiles, statistics, market research, analytics data) regarding your\ninteractions and those of other users with advertising or (non-advertising)\ncontent to identify common characteristics (for instance, to determine which\ntarget audiences are more receptive to an ad campaign or to certain contents).\n\nThe owner of an online bookstore wants commercial reporting showing the\nproportion of visitors who consulted and left its site without buying, or\nconsulted and bought the last celebrity autobiography of the month, as well as\nthe average age and the male/female distribution of each category. Data\nrelating to your navigation on its site and to your personal characteristics\nis then used and combined with other such data to produce these statistics.\n\nAn advertiser wants to better understand the type of audience interacting with\nits adverts. It calls upon a research institute to compare the characteristics\nof users who interacted with the ad with typical attributes of users of\nsimilar platforms, across different devices. This comparison reveals to the\nadvertiser that its ad audience is mainly accessing the adverts through mobile\ndevices and is likely in the 45-60 age range.\n\nVendors28 vendor(s)\n\n  * AdGear\n  * Amazon Ad Server\n  * Amobee (Nexxen)\n  * Appier\n  * AudienceProject\n  * Bombora\n  * Clinch\n  * Crimtan\n  * Emerse\n  * Google Ads\n\n1-10 / 28\n\nDevelop and improve services\n\nInformation about your activity on this service, such as your interaction with\nads or content, can be very helpful to improve products and services and to\nbuild new products and services based on user interactions, the type of\naudience, etc. This specific purpose does not include the development or\nimprovement of user profiles and identifiers.\n\nA technology platform working with a social media provider notices a growth in\nmobile app users, and sees based on their profiles that many of them are\nconnecting through mobile connections. It uses a new technology to deliver ads\nthat are formatted for mobile devices and that are low-bandwidth, to improve\ntheir performance.\n\nAn advertiser is looking for a way to display ads on a new type of consumer\ndevice. It collects information regarding the way users interact with this new\nkind of device to determine whether it can build a new mechanism for\ndisplaying advertising on this type of device.\n\nVendors50 vendor(s)\n\n  * 6sense\n  * Aarki\n  * AcuityAds\n  * AdGear\n  * Adelaide\n  * Adform\n  * Admixer\n  * Amazon Ad Server\n  * Amobee (Nexxen)\n  * Appier\n\n1-10 / 50\n\nBelow, you will find a list of the features for which your data is being\nprocessed. You may exercise your rights for special features using the toggles\nbelow.\n\nFeatures\n\nMatch and combine data from other data sources\n\nInformation about your activity on this service may be matched and combined\nwith other information relating to you and originating from various sources\n(for instance your activity on a separate online service, your use of a\nloyalty card in-store, or your answers to a survey), in support of the\npurposes explained in this notice.\n\nVendors107 vendor(s)\n\n  * 6sense\n  * A.Mob\n  * AdGear\n  * AdTheorent\n  * Adara (Rate Gain)\n  * Adelaide\n  * Adex (Virtual Minds)\n  * Admixer\n  * AdsWizz\n  * Amazon Ad Server\n\n1-10 / 107\n\nLink different devices\n\nIn support of the purposes explained in this notice, your device might be\nconsidered as likely linked to other devices that belong to you or your\nhousehold (for instance because you are logged in to the same service on both\nyour phone and your computer, or because you may use the same Internet\nconnection on both devices).\n\nVendors108 vendor(s)\n\n  * 6sense\n  * A.Mob\n  * AcuityAds\n  * AdElement Media Solutions\n  * AdGear\n  * AdTheorent\n  * Adara (Rate Gain)\n  * Adex (Virtual Minds)\n  * Adform\n  * Adikteev\n\n1-10 / 108\n\nIdentify devices based on information transmitted automatically\n\nYour device might be distinguished from other devices based on information it\nautomatically sends when accessing the Internet (for instance, the IP address\nof your Internet connection or the type of browser you are using) in support\nof the purposes exposed in this notice.\n\nVendors125 vendor(s)\n\n  * 6sense\n  * A.Mob\n  * Aarki\n  * AdElement Media Solutions\n  * AdGear\n  * AdKernel\n  * AdSpirit AdServer\n  * AdTheorent\n  * Adacado\n  * Adara (Rate Gain)\n\n1-10 / 125\n\nSpecial features\n\nUse precise geolocation data\n\nWith your acceptance, your precise location (within a radius of less than 500\nmetres) may be used in support of the purposes explained in this notice.\n\nVendors64 vendor(s)\n\n  * A.Mob\n  * AdElement Media Solutions\n  * AdGear\n  * AdKernel\n  * AdTheorent\n  * Adacado\n  * Admixer\n  * Adrule mobile\n  * AdsWizz\n  * Amnet GmbH\n\n1-10 / 64\n\nActively scan device characteristics for identification\n\nWith your acceptance, certain characteristics specific to your device might be\nrequested and used to distinguish it from other devices (such as the installed\nfonts or plugins, the resolution of your screen) in support of the purposes\nexplained in this notice.\n\nVendors23 vendor(s)\n\n  * A.Mob\n  * AdKernel\n  * AdTheorent\n  * Adcell (Firstlead)\n  * Amnet GmbH\n  * Cheq\n  * Fraudlogix\n  * Human\n  * Hybrid\n  * InMobi\n\n1-10 / 23\n\nBelow, you will find a list of vendors processing your data and the purposes\nor features of processing they declare. You may exercise your rights for each\nvendor based on the legal basis they assert.\n\nIAB TCF Vendors\n\n6senseIAB TCF\n\n6sense stores cookies with a maximum duration of about this many days: 730.\nThis vendor also uses other methods like \"local storage\" to store and access\ninformation on your device.\n\nPrivacy policyLegitimate interest disclosure\n\nPurposes| Retention  \n---|---  \nStore and/or access information on a device| 400 day(s)  \nCreate profiles for personalised advertising| 400 day(s)  \nUse profiles to select personalised advertising| 400 day(s)  \nUse limited data to select advertising| 400 day(s)  \nMeasure advertising performance| 400 day(s)  \nDevelop and improve services| 400 day(s)  \nSpecial purposes| Retention  \n---|---  \nEnsure security, prevent and detect fraud, and fix errors| 400 day(s)  \nDeliver and present advertising and content| 400 day(s)  \nFeatures  \n---  \nMatch and combine data from other data sources  \nLink different devices  \nIdentify devices based on information transmitted automatically  \nData categories  \n---  \nIP addresses  \nProbabilistic identifiers  \nBrowsing and interaction data  \nNon-precise location data  \nUsers\u2019 profiles  \n  \nA.MobIAB TCF\n\nA.Mob stores cookies with a maximum duration of about this many days: 395.\nThis vendor also uses other methods like \"local storage\" to store and access\ninformation on your device.\n\nPrivacy policy\n\nPurposes| Retention  \n---|---  \nStore and/or access information on a device| 395 day(s)  \nUse limited data to select advertising| 395 day(s)  \nCreate profiles for personalised advertising| 395 day(s)  \nUse profiles to select personalised advertising| 395 day(s)  \nMeasure advertising performance| 395 day(s)  \nUnderstand audiences through statistics or combinations of data from different\nsources| 395 day(s)  \nDevelop and improve services| 395 day(s)  \nFeatures  \n---  \nMatch and combine data from other data sources  \nLink different devices  \nIdentify devices based on information transmitted automatically  \nSpecial features  \n---  \nUse precise geolocation data  \nActively scan device characteristics for identification  \nData categories  \n---  \nIP addresses  \nDevice characteristics  \nDevice identifiers  \nProbabilistic identifiers  \nAuthentication-derived identifiers  \nBrowsing and interaction data  \nUser-provided data  \nNon-precise location data  \nPrecise location data  \nUsers\u2019 profiles  \nPrivacy choices  \n  \nADventoriIAB TCF\n\nADventori stores cookies with a maximum duration of about this many days: 90.\nThese cookies may be refreshed. This vendor also uses other methods like\n\"local storage\" to store and access information on your device.\n\nPrivacy policyLegitimate interest disclosure\n\nPurposes| Retention  \n---|---  \nStore and/or access information on a device| 400 day(s)  \nUse limited data to select advertising| 90 day(s)  \nUse profiles to select personalised advertising| 90 day(s)  \nMeasure advertising performance| 400 day(s)  \nSpecial purposes| Retention  \n---|---  \nEnsure security, prevent and detect fraud, and fix errors| 400 day(s)  \nDeliver and present advertising and content| 400 day(s)  \nData categories  \n---  \nIP addresses  \nDevice identifiers  \nProbabilistic identifiers  \nBrowsing and interaction data  \nUser-provided data  \nNon-precise location data  \n  \nAarkiIAB TCF\n\nuses methods like \"local storage\" to store and access information on your\ndevice.\n\nPrivacy policyLegitimate interest disclosure\n\nPurposes| Retention  \n---|---  \nStore and/or access information on a device| 365 day(s)  \nUse limited data to select advertising| 365 day(s)  \nCreate profiles for personalised advertising| 3650 day(s)  \nUse profiles to select personalised advertising| 3650 day(s)  \nMeasure advertising performance| 1500 day(s)  \nDevelop and improve services| 365 day(s)  \nSpecial purposes| Retention  \n---|---  \nEnsure security, prevent and detect fraud, and fix errors| 365 day(s)  \nDeliver and present advertising and content| 365 day(s)  \nFeatures  \n---  \nIdentify devices based on information transmitted automatically  \nData categories  \n---  \nIP addresses  \nDevice characteristics  \nDevice identifiers  \nUser-provided data  \nNon-precise location data  \n  \nAcuityAdsIAB TCF\n\nAcuityAds stores cookies with a maximum duration of about this many days: 365.\nThese cookies may be refreshed.\n\nPrivacy policyLegitimate interest disclosure\n\nPurposes| Retention  \n---|---  \nStore and/or access information on a device| 180 day(s)  \nCreate profiles for personalised advertising| 180 day(s)  \nUse profiles to select personalised advertising| 180 day(s)  \nUse limited data to select advertising| 180 day(s)  \nMeasure advertising performance| 180 day(s)  \nDevelop and improve services| 180 day(s)  \nSpecial purposes| Retention  \n---|---  \nEnsure security, prevent and detect fraud, and fix errors| 180 day(s)  \nDeliver and present advertising and content| 180 day(s)  \nFeatures  \n---  \nLink different devices  \nData categories  \n---  \nIP addresses  \nDevice characteristics  \nDevice identifiers  \nAuthentication-derived identifiers  \nBrowsing and interaction data  \nNon-precise location data  \nUsers\u2019 profiles  \nPrivacy choices  \n  \nAdElement Media SolutionsIAB TCF\n\nAdElement Media Solutions stores cookies with a maximum duration of about this\nmany days: 365. These cookies may be refreshed.\n\nPrivacy policyLegitimate interest disclosure\n\nPurposes| Retention  \n---|---  \nStore and/or access information on a device| 180 day(s)  \nUse limited data to select advertising| 180 day(s)  \nCreate profiles for personalised advertising| 90 day(s)  \nUse profiles to select personalised advertising| 90 day(s)  \nMeasure advertising performance| 180 day(s)  \nUnderstand audiences through statistics or combinations of data from different\nsources| 90 day(s)  \nDevelop and improve services| 180 day(s)  \nSpecial purposes| Retention  \n---|---  \nEnsure security, prevent and detect fraud, and fix errors| 180 day(s)  \nDeliver and present advertising and content| 180 day(s)  \nFeatures  \n---  \nLink different devices  \nIdentify devices based on information transmitted automatically  \nSpecial features  \n---  \nUse precise geolocation data  \nData categories  \n---  \nIP addresses  \nDevice characteristics  \nDevice identifiers  \nBrowsing and interaction data  \nUser-provided data  \nNon-precise location data  \nPrecise location data  \nUsers\u2019 profiles  \nPrivacy choices  \n  \nAdGearIAB TCF\n\nAdGear stores cookies with a maximum duration of about this many days: 395.\nThis vendor also uses other methods like \"local storage\" to store and access\ninformation on your device.\n\nPrivacy policyLegitimate interest disclosure\n\nPurposes| Retention  \n---|---  \nStore and/or access information on a device| 395 day(s)  \nCreate profiles for personalised advertising| 395 day(s)  \nUse profiles to select personalised advertising| 395 day(s)  \nUse limited data to select advertising| 395 day(s)  \nMeasure advertising performance| 395 day(s)  \nUnderstand audiences through statistics or combinations of data from different\nsources| 395 day(s)  \nDevelop and improve services| 395 day(s)  \nSpecial purposes| Retention  \n---|---  \nEnsure security, prevent and detect fraud, and fix errors| 395 day(s)  \nDeliver and present advertising and content| 395 day(s)  \nFeatures  \n---  \nMatch and combine data from other data sources  \nLink different devices  \nIdentify devices based on information transmitted automatically  \nSpecial features  \n---  \nUse precise geolocation data  \nData categories  \n---  \nIP addresses  \nDevice characteristics  \nDevice identifiers  \nProbabilistic identifiers  \nAuthentication-derived identifiers  \nBrowsing and interaction data  \nUser-provided data  \nNon-precise location data  \nUsers\u2019 profiles  \nPrivacy choices  \n  \nAdKernelIAB TCF\n\nAdKernel stores cookies with a maximum duration of about this many days: 18.\nThese cookies may be refreshed.\n\nPrivacy policyLegitimate interest disclosure\n\nPurposes| Retention  \n---|---  \nStore and/or access information on a device| 180 day(s)  \nCreate profiles for personalised advertising| 180 day(s)  \nUse profiles to select personalised advertising| 180 day(s)  \nUnderstand audiences through statistics or combinations of data from different\nsources| 180 day(s)  \nDevelop and improve services| 180 day(s)  \nUse limited data to select advertising| 180 day(s)  \nMeasure advertising performance| 180 day(s)  \nSpecial purposes| Retention  \n---|---  \nEnsure security, prevent and detect fraud, and fix errors| 180 day(s)  \nDeliver and present advertising and content| 180 day(s)  \nFeatures  \n---  \nIdentify devices based on information transmitted automatically  \nSpecial features  \n---  \nUse precise geolocation data  \nActively scan device characteristics for identification  \nData categories  \n---  \nIP addresses  \nDevice identifiers  \nNon-precise location data  \nPrecise location data  \nUsers\u2019 profiles  \n  \nAdSpirit AdServerIAB TCF\n\nAdSpirit AdServer stores cookies with a maximum duration of about this many\ndays: 30. These cookies may be refreshed.\n\nPrivacy policyLegitimate interest disclosure\n\nPurposes| Retention  \n---|---  \nStore and/or access information on a device| 60 day(s)  \nUse limited data to select advertising| 14 day(s)  \nCreate profiles for personalised advertising| 60 day(s)  \nUse profiles to select personalised advertising| 60 day(s)  \nMeasure advertising performance| 14 day(s)  \nUnderstand audiences through statistics or combinations of data from different\nsources| 60 day(s)  \nSpecial purposes| Retention  \n---|---  \nEnsure security, prevent and detect fraud, and fix errors| 60 day(s)  \nDeliver and present advertising and content| 60 day(s)  \nFeatures  \n---  \nIdentify devices based on information transmitted automatically  \nData categories  \n---  \nIP addresses  \nDevice characteristics  \nDevice identifiers  \nProbabilistic identifiers  \nBrowsing and interaction data  \nNon-precise location data  \nUsers\u2019 profiles  \nPrivacy choices  \n  \nAdTheorentIAB TCF\n\nAdTheorent stores cookies with a maximum duration of about this many days:\n730. These cookies may be refreshed.\n\nPrivacy policyLegitimate interest disclosure\n\nPurposes| Retention  \n---|---  \nStore and/or access information on a device| 365 day(s)  \nUse limited data to select advertising| 365 day(s)  \nCreate profiles for personalised advertising| 365 day(s)  \nUse profiles to select personalised advertising| 365 day(s)  \nMeasure advertising performance| 365 day(s)  \nUnderstand audiences through statistics or combinations of data from different\nsources| 365 day(s)  \nDevelop and improve services| 365 day(s)  \nFeatures  \n---  \nMatch and combine data from other data sources  \nLink different devices  \nIdentify devices based on information transmitted automatically  \nSpecial features  \n---  \nUse precise geolocation data  \nActively scan device characteristics for identification  \nData categories  \n---  \nIP addresses  \nDevice characteristics  \nDevice identifiers  \nProbabilistic identifiers  \nNon-precise location data  \nPrecise location data  \nPrivacy choices  \n  \n1-10 / 317\n\nSkip to contentSkip to site index\n\nPodcasts\n\nSUBSCRIBE FOR \u20ac0.50/WEEKLog in\n\nSaturday, April 13, 2024\n\nToday\u2019s Paper\n\nSUBSCRIBE FOR \u20ac0.50/WEEK\n\nPodcasts|Transcript: Ezra Klein Interviews Dario Amodei\n\nhttps://www.nytimes.com/2024/04/12/podcasts/transcript-ezra-klein-interviews-\ndario-amodei.html\n\nFor more audio journalism and storytelling, download New York Times Audio, a\nnew iOS app available for news subscribers.\n\nSupported by\n\nSKIP ADVERTISEMENT\n\nThe Ezra Klein Show\n\n# Transcript: Ezra Klein Interviews Dario Amodei\n\nApril 12, 2024\n\nReal conversations. Ideas that matter. So many book recommendations.\n\nListen to \u201cThe Ezra Klein Show\u201d: Apple Podcasts, Spotify, Pocket Casts, Google\nPodcasts, Stitcher, How to Listen\n\nEvery Tuesday and Friday, Ezra Klein invites you into a conversation about\nsomething that matters, like today\u2019s episode with Dario Amodei. Listen\nwherever you get your podcasts.\n\nTranscripts of our episodes are made available as soon as possible. They are\nnot fully edited for grammar or spelling.\n\n### What if Dario Amodei Is Right About A.I.?\n\nAnthropic\u2019s co-founder and C.E.O. explains why he thinks artificial\nintelligence is on an \u201cexponential curve.\u201d\n\ntranscript\n\n0:00/1:33:07\n\n-1:33:07\n\ntranscript\n\n## What if Dario Amodei Is Right About A.I.?\n\n#### Anthropic\u2019s co-founder and C.E.O. explains why he thinks artificial\nintelligence is on an \u201cexponential curve.\u201d\n\n    \n\n[MUSIC PLAYING]\n\nezra klein\n\n    \n\nFrom New York Times Opinion, this is \u201cThe Ezra Klein Show.\u201d\n\n[MUSIC PLAYING]\n\nThe really disorienting thing about talking to the people building A.I. is\ntheir altered sense of time. You\u2019re sitting there discussing some world that\nfeels like weird sci-fi to even talk about, and then you ask, well, when do\nyou think this is going to happen? And they say, I don\u2019t know \u2014 two years.\n\nBehind those predictions are what are called the scaling laws. And the scaling\nlaws \u2014 and I want to say this so clearly \u2014 they\u2019re not laws. They\u2019re\nobservations. They\u2019re predictions. They\u2019re based off of a few years, not a few\nhundred years or 1,000 years of data.\n\nBut what they say is that the more computer power and data you feed into A.I.\nsystems, the more powerful those systems get \u2014 that the relationship is\npredictable, and more, that the relationship is exponential.\n\nHuman beings have trouble thinking in exponentials. Think back to Covid, when\nwe all had to do it. If you have one case of coronavirus and cases double\nevery three days, then after 30 days, you have about 1,000 cases. That growth\nrate feels modest. It\u2019s manageable. But then you go 30 days longer, and you\nhave a million. Then you wait another 30 days. Now you have a billion. That\u2019s\nthe power of the exponential curve. Growth feels normal for a while. Then it\ngets out of control really, really quickly.\n\nWhat the A.I. developers say is that the power of A.I. systems is on this kind\nof curve, that it has been increasing exponentially, their capabilities, and\nthat as long as we keep feeding in more data and more computing power, it will\ncontinue increasing exponentially. That is the scaling law hypothesis, and one\nof its main advocates is Dario Amodei. Amodei led the team at OpenAI that\ncreated GPT-2, that created GPT-3. He then left OpenAI to co-found Anthropic,\nanother A.I. firm, where he\u2019s now the C.E.O. And Anthropic recently released\nClaude 3, which is considered by many to be the strongest A.I. model available\nright now.\n\nBut Amodei believes we\u2019re just getting started, that we\u2019re just hitting the\nsteep part of the curve now. He thinks the kinds of systems we\u2019ve imagined in\nsci-fi, they\u2019re coming not in 20 or 40 years, not in 10 or 15 years, they\u2019re\ncoming in two to five years. He thinks they\u2019re going to be so powerful that he\nand people like him should not be trusted to decide what they\u2019re going to do.\n\nSo I asked him on this show to try to answer in my own head two questions.\nFirst, is he right? Second, what if he\u2019s right? I want to say that in the\npast, we have done shows with Sam Altman, the head of OpenAI, and Demis\nHassabis, the head of Google DeepMind. And it\u2019s worth listening to those two\nif you find this interesting.\n\nWe\u2019re going to put the links to them in show notes because comparing and\ncontrasting how they talk about the A.I. curves here, how they think about the\npolitics \u2014 you\u2019ll hear a lot about that in the Sam Altman episode \u2014 it gives\nyou a kind of sense of what the people building these things are thinking and\nhow maybe they differ from each other.\n\nAs always, my email for thoughts, for feedback, for guest suggestions \u2014\nezrakleinshow@nytimes.com.\n\n[MUSIC PLAYING]\n\nDario Amodei, welcome to the show.\n\ndario amodei\n\n    \n\nThank you for having me.\n\nezra klein\n\n    \n\nSo there are these two very different rhythms I\u2019ve been thinking about with\nA.I. One is the curve of the technology itself, how fast it is changing and\nimproving. And the other is the pace at which society is seeing and reacting\nto those changes. What has that relationship felt like to you?\n\ndario amodei\n\n    \n\nSo I think this is an example of a phenomenon that we may have seen a few\ntimes before in history, which is that there\u2019s an underlying process that is\nsmooth, and in this case, exponential. And then there\u2019s a spilling over of\nthat process into the public sphere. And the spilling over looks very spiky.\nIt looks like it\u2019s happening all of a sudden. It looks like it comes out of\nnowhere. And it\u2019s triggered by things hitting various critical points or just\nthe public happened to be engaged at a certain time.\n\nSo I think the easiest way for me to describe this in terms of my own personal\nexperience is \u2014 so I worked at OpenAI for five years, I was one of the first\nemployees to join. And they built a model in 2018 called GPT-1, which used\nsomething like 100,000 times less computational power than the models we build\ntoday.\n\nI looked at that, and I and my colleagues were among the first to run what are\ncalled scaling laws, which is basically studying what happens as you vary the\nsize of the model, its capacity to absorb information, and the amount of data\nthat you feed into it. And we found these very smooth patterns. And we had\nthis projection that, look, if you spend $100 million or $1 billion or $10\nbillion on these models, instead of the $10,000 we were spending then,\nprojections that all of these wondrous things would happen, and we imagined\nthat they would have enormous economic value.\n\nFast forward to about 2020. GPT-3 had just come out. It wasn\u2019t yet available\nas a chat bot. I led the development of that along with the team that\neventually left to join Anthropic. And maybe for the whole period of 2021 and\n2022, even though we continued to train models that were better and better,\nand OpenAI continued to train models, and Google continued to train models,\nthere was surprisingly little public attention to the models.\n\nAnd I looked at that, and I said, well, these models are incredible. They\u2019re\ngetting better and better. What\u2019s going on? Why isn\u2019t this happening? Could\nthis be a case where I was right about the technology, but wrong about the\neconomic impact, the practical value of the technology? And then, all of a\nsudden, when ChatGPT came out, it was like all of that growth that you would\nexpect, all of that excitement over three years, broke through and came\nrushing in.\n\nezra klein\n\n    \n\nSo I want to linger on this difference between the curve at which the\ntechnology is improving and the way it is being adopted by society. So when\nyou think about these break points and you think into the future, what other\nbreak points do you see coming where A.I. bursts into social consciousness or\nused in a different way?\n\ndario amodei\n\n    \n\nYeah, so I think I should say first that it\u2019s very hard to predict these. One\nthing I like to say is the underlying technology, because it\u2019s a smooth\nexponential, it\u2019s not perfectly predictable, but in some ways, it can be\neerily preternaturally predictable, right? That\u2019s not true for these societal\nstep functions at all. It\u2019s very hard to predict what will catch on. In some\nways, it feels a little bit like which artist or musician is going to catch on\nand get to the top of the charts.\n\nThat said, a few possible ideas. I think one is related to something that you\nmentioned, which is interacting with the models in a more kind of naturalistic\nway. We\u2019ve actually already seen some of that with Claude 3, where people feel\nthat some of the other models sound like a robot and that talking to Claude 3\nis more natural.\n\nI think a thing related to this is, a lot of companies have been held back or\ntripped up by how their models handle controversial topics.\n\nAnd we were really able to, I think, do a better job than others of telling\nthe model, don\u2019t shy away from discussing controversial topics. Don\u2019t assume\nthat both sides necessarily have a valid point but don\u2019t express an opinion\nyourself. Don\u2019t express views that are flagrantly biased. As journalists, you\nencounter this all the time, right? How do I be objective, but not both sides\non everything?\n\nSo I think going further in that direction of models having personalities\nwhile still being objective, while still being useful and not falling into\nvarious ethical traps, that will be, I think, a significant unlock for\nadoption. The models taking actions in the world is going to be a big one. I\nknow basically all the big companies that work on A.I. are working on that.\n\nInstead of just, I ask it a question and it answers, and then maybe I follow\nup and it answers again, can I talk to the model about, oh, I\u2019m going to go on\nthis trip today, and the model says, oh, that\u2019s great. I\u2019ll get an Uber for\nyou to drive from here to there, and I\u2019ll reserve a restaurant. And I\u2019ll talk\nto the other people who are going to plan the trip. And the model being able\nto do things end to end or going to websites or taking actions on your\ncomputer for you.\n\nI think all of that is coming in the next, I would say \u2014 I don\u2019t know \u2014 three\nto 18 months, with increasing levels of ability. I think that\u2019s going to\nchange how people think about A.I., right, where so far, it\u2019s been this very\npassive \u2014 it\u2019s like, I go to the Oracle. I ask it a question, and the Oracle\ntells me things. And some people think that\u2019s exciting, some people think it\u2019s\nscary. But I think there are limits to how exciting or how scary it\u2019s\nperceived as because it\u2019s contained within this box.\n\nezra klein\n\n    \n\nI want to sit with this question of the agentic A.I. because I do think this\nis what\u2019s coming. It\u2019s clearly what people are trying to build. And I think it\nmight be a good way to look at some of the specific technological and cultural\nchallenges. And so, let me offer two versions of it.\n\nPeople who are following the A.I. news might have heard about Devin, which is\nnot in release yet, but is an A.I. that at least purports to be able to\ncomplete the kinds of tasks, linked tasks, that a junior software engineer\nmight complete, right? Instead of asking to do a bit of code for you, you say,\nlisten, I want a website. It\u2019s going to have to do these things, work in these\nways. And maybe Devin, if it works the way people are saying it works, can\nactually hold that set of thoughts, complete a number of different tasks, and\ncome back to you with a result. I\u2019m also interested in the version of this\nthat you might have in the real world. The example I always use in my head is,\nwhen can I tell an A.I., my son is turning five. He loves dragons. We live in\nBrooklyn. Give me some options for planning his birthday party. And then, when\nI choose between them, can you just do it all for me? Order the cake, reserve\nthe room, send out the invitations, whatever it might be.\n\nThose are two different situations because one of them is in code, and one of\nthem is making decisions in the real world, interacting with real people,\nknowing if what it is finding on the websites is actually any good. What is\nbetween here and there? When I say that in plain language to you, what\ntechnological challenges or advances do you hear need to happen to get there?\n\ndario amodei\n\n    \n\nThe short answer is not all that much. A story I have from when we were\ndeveloping models back in 2022 \u2014 and this is before we\u2019d hooked up the models\nto anything \u2014 is, you could have a conversation with these purely textual\nmodels where you could say, hey, I want to reserve dinner at restaurant X in\nSan Francisco, and the model would say, OK, here\u2019s the website of restaurant\nX. And it would actually give you a correct website or would tell you to go to\nOpen Table or something.\n\nAnd of course, it can\u2019t actually go to the website. The power plug isn\u2019t\nactually plugged in, right? The brain of the robot is not actually attached to\nits arms and legs. But it gave you this sense that the brain, all it needed to\ndo was learn exactly how to use the arms and legs, right? It already had a\npicture of the world and where it would walk and what it would do. And so, it\nfelt like there was this very thin barrier between the passive models we had\nand actually acting in the world.\n\nIn terms of what we need to make it work, one thing is, literally, we just\nneed a little bit more scale. And I think the reason we\u2019re going to need more\nscale is \u2014 to do one of those things you described, to do all the things a\njunior software engineer does, they involve chains of long actions, right? I\nhave to write this line of code. I have to run this test. I have to write a\nnew test. I have to check how it looks in the app after I interpret it or\ncompile it. And these things can easily get 20 or 30 layers deep. And same\nwith planning the birthday party for your son, right?\n\nAnd if the accuracy of any given step is not very high, is not like 99.9\npercent, as you compose these steps, the probability of making a mistake\nbecomes itself very high. So the industry is going to get a new generation of\nmodels every probably four to eight months. And so, my guess \u2014 I\u2019m not sure \u2014\nis that to really get these things working well, we need maybe one to four\nmore generations. So that ends up translating to 3 to 24 months or something\nlike that.\n\nI think second is just, there is some algorithmic work that is going to need\nto be done on how to have the models interact with the world in this way. I\nthink the basic techniques we have, a method called reinforcement learning and\nvariations of it, probably is up to the task, but figuring out exactly how to\nuse it to get the results we want will probably take some time.\n\nAnd then third, I think \u2014 and this gets to something that Anthropic really\nspecializes in \u2014 is safety and controllability. And I think that\u2019s going to be\na big issue for these models acting in the world, right? Let\u2019s say this model\nis writing code for me, and it introduces a serious security bug in the code,\nor it\u2019s taking actions on the computer for me and modifying the state of my\ncomputer in ways that are too complicated for me to even understand.\n\nAnd for planning the birthday party, right, the level of trust you would need\nto take an A.I. agent and say, I\u2019m OK with you calling up anyone, saying\nanything to them that\u2019s in any private information that I might have, sending\nthem any information, taking any action on my computer, posting anything to\nthe internet, the most unconstrained version of that sounds very scary. And\nso, we\u2019re going to need to figure out what is safe and controllable.\n\nThe more open ended the thing is, the more powerful it is, but also, the more\ndangerous it is and the harder it is to control.\n\nSo I think those questions, although they sound lofty and abstract, are going\nto turn into practical product questions that we and other companies are going\nto be trying to address.\n\nezra klein\n\n    \n\nWhen you say we\u2019re just going to need more scale, you mean more compute and\nmore training data, and I guess, possibly more money to simply make the models\nsmarter and more capable?\n\ndario amodei\n\n    \n\nYes, we\u2019re going to have to make bigger models that use more compute per\niteration. We\u2019re going to have to run them for longer by feeding more data\ninto them. And that number of chips times the amount of time that we run\nthings on chips is essentially dollar value because these chips are \u2014 you rent\nthem by the hour. That\u2019s the most common model for it. And so, today\u2019s models\ncost of order $100 million to train, plus or minus factor two or three.\n\nThe models that are in training now and that will come out at various times\nlater this year or early next year are closer in cost to $1 billion. So that\u2019s\nalready happening. And then I think in 2025 and 2026, we\u2019ll get more towards\n$5 or $10 billion.\n\nezra klein\n\n    \n\nSo we\u2019re moving very quickly towards a world where the only players who can\nafford to do this are either giant corporations, companies hooked up to giant\ncorporations \u2014 you all are getting billions of dollars from Amazon. OpenAI is\ngetting billions of dollars from Microsoft. Google obviously makes its own.\n\nYou can imagine governments \u2014 though I don\u2019t know of too many governments\ndoing it directly, though some, like the Saudis, are creating big funds to\ninvest in the space. When we\u2019re talking about the model\u2019s going to cost near\nto $1 billion, then you imagine a year or two out from that, if you see the\nsame increase, that would be $10-ish billion. Then is it going to be $100\nbillion? I mean, very quickly, the financial artillery you need to create one\nof these is going to wall out anyone but the biggest players.\n\ndario amodei\n\n    \n\nI basically do agree with you. I think it\u2019s the intellectually honest thing to\nsay that building the big, large scale models, the core foundation model\nengineering, it is getting more and more expensive. And anyone who wants to\nbuild one is going to need to find some way to finance it. And you\u2019ve named\nmost of the ways, right? You can be a large company. You can have some kind of\npartnership of various kinds with a large company. Or governments would be the\nother source.\n\nI think one way that it\u2019s not correct is, we\u2019re always going to have a\nthriving ecosystem of experimentation on small models. For example, the open\nsource community working to make models that are as small and as efficient as\npossible that are optimized for a particular use case. And also downstream\nusage of the models. I mean, there\u2019s a blooming ecosystem of startups there\nthat don\u2019t need to train these models from scratch. They just need to consume\nthem and maybe modify them a bit.\n\nezra klein\n\n    \n\nNow, I want to ask a question about what is different between the agentic\ncoding model and the plan by kids\u2019 birthday model, to say nothing of do\nsomething on behalf of my business model. And one of the questions on my mind\nhere is one reason I buy that A.I. can become functionally superhuman in\ncoding is, there\u2019s a lot of ways to get rapid feedback in coding. Your code\nhas to compile. You can run bug checking. You can actually see if the thing\nworks.\n\nWhereas the quickest way for me to know that I\u2019m about to get a crap answer\nfrom ChatGPT 4 is when it begins searching Bing, because when it begins\nsearching Bing, it\u2019s very clear to me it doesn\u2019t know how to distinguish\nbetween what is high quality on the internet and what isn\u2019t. To be fair, at\nthis point, it also doesn\u2019t feel to me like Google Search itself is all that\ngood at distinguishing that.\n\nSo the question of how good the models can get in the world where it\u2019s a very\nvast and fuzzy dilemma to know what the right answer is on something \u2014 one\nreason I find it very stressful to plan my kid\u2019s birthday is it actually\nrequires a huge amount of knowledge about my child, about the other children,\nabout how good different places are, what is a good deal or not, how just\nstressful will this be on me. There\u2019s all these things that I\u2019d have a lot of\ntrouble encoding into a model or any kind set of instructions. Is that right,\nor am I overstating the difficulty of understanding human behavior and various\nkinds of social relationships?\n\ndario amodei\n\n    \n\nI think it\u2019s correct and perceptive to say that the coding agents will advance\nsubstantially faster than agents that interact with the real world or have to\nget opinions and preferences from humans. That said, we should keep in mind\nthat the current crop of A.I.s that are out there, right, including Claude 3,\nGPT, Gemini, they\u2019re all trained with some variant of what\u2019s called\nreinforcement learning from human feedback.\n\nAnd this involves exactly hiring a large crop of humans to rate the responses\nof the model. And so, that\u2019s to say both this is difficult, right? We pay lots\nof money, and it\u2019s a complicated operational process to gather all this human\nfeedback. You have to worry about whether it\u2019s representative. You have to\nredesign it for new tasks.\n\nBut on the other hand, it\u2019s something we have succeeded in doing. I think it\nis a reliable way to predict what will go faster, relatively speaking, and\nwhat will go slower, relatively speaking. But that is within a background of\neverything going lightning fast. So I think the framework you\u2019re laying out,\nif you want to know what\u2019s going to happen in one to two years versus what\u2019s\ngoing to happen in three to four years, I think it\u2019s a very accurate way to\npredict that.\n\nezra klein\n\n    \n\nYou don\u2019t love the framing of artificial general intelligence, what gets\ncalled A.G.I. Typically, this is all described as a race to A.G.I., a race to\nthis system that can do kind of whatever a human can do, but better. What do\nyou understand A.G.I. to mean, when people say it? And why don\u2019t you like it?\nWhy is it not your framework?\n\ndario amodei\n\n    \n\nSo it\u2019s actually a term I used to use a lot 10 years ago. And that\u2019s because\nthe situation 10 years ago was very different. 10 years ago, everyone was\nbuilding these very specialized systems, right? Here\u2019s a cat detector. You run\nit on a picture, and it\u2019ll tell you whether a cat is in it or not. And so I\nwas a proponent all the way back then of like, no, we should be thinking\ngenerally. Humans are general. The human brain appears to be general. It\nappears to get a lot of mileage by generalizing. You should go in that\ndirection.\n\nAnd I think back then, I kind of even imagined that that was like a discrete\nthing that we would reach at one point. But it\u2019s a little like, if you look at\na city on the horizon and you\u2019re like, we\u2019re going to Chicago, once you get to\nChicago, you stop talking in terms of Chicago. You\u2019re like, well, what\nneighborhood am I going to? What street am I on?\n\nAnd I feel that way about A.G.I. We have very general systems now. In some\nways, they\u2019re better than humans. In some ways, they\u2019re worse. There\u2019s a\nnumber of things they can\u2019t do at all. And there\u2019s much improvement still to\nbe gotten. So what I believe in is this thing that I say like a broken record,\nwhich is the exponential curve. And so, that general tide is going to increase\nwith every generation of models.\n\nAnd there\u2019s no one point that\u2019s meaningful. I think there\u2019s just a smooth\ncurve. But there may be points which are societally meaningful, right? We\u2019re\nalready working with, say, drug discovery scientists, companies like Pfizer or\nDana-Farber Cancer Institute, on helping with biomedical diagnosis, drug\ndiscovery. There\u2019s going to be some point where the models are better at that\nthan the median human drug discovery scientists. I think we\u2019re just going to\nget to a part of the exponential where things are really interesting.\n\nJust like the chat bots got interesting at a certain stage of the exponential,\neven though the improvement was smooth, I think at some point, biologists are\ngoing to sit up and take notice, much more than they already have, and say,\noh, my God, now our field is moving three times as fast as it did before. And\nnow it\u2019s moving 10 times as fast as it did before. And again, when that moment\nhappens, great things are going to happen.\n\nAnd we\u2019ve already seen little hints of that with things like AlphaFold, which\nI have great respect for. I was inspired by AlphaFold, right? A direct use of\nA.I. to advance biological science, which it\u2019ll advance basic science. In the\nlong run, that will advance curing all kinds of diseases. But I think what we\nneed is like 100 different AlphaFolds. And I think the way we\u2019ll ultimately\nget that is by making the models smarter and putting them in a position where\nthey can design the next AlphaFold.\n\nezra klein\n\n    \n\nHelp me imagine the drug discovery world for a minute, because that\u2019s a world\na lot of us want to live in. I know a fair amount about the drug discovery\nprocess, have spent a lot of my career reporting on health care and related\npolicy questions. And when you\u2019re working with different pharmaceutical\ncompanies, which parts of it seem amenable to the way A.I. can speed something\nup?\n\nBecause keeping in mind our earlier conversation, it is a lot easier for A.I.\nto operate in things where you can have rapid virtual feedback, and that\u2019s not\nexactly the drug discovery world. The drug discovery world, a lot of what\nmakes it slow and cumbersome and difficult, is the need to be \u2014 you get a\ncandidate compound. You got to test it in mice and then you need monkeys. And\nyou need humans, and you need a lot of money for that. And there\u2019s a lot that\nhas to happen, and there\u2019s so many disappointments.\n\nBut so many of the disappointments happen in the real world. And it isn\u2019t\nclear to me how A.I. gets you a lot more, say, human subjects to inject\ncandidate drugs into. So, what parts of it seem, in the next 5 or 10 years,\nlike they could actually be significantly sped up? When you imagine this world\nwhere it\u2019s gone three times as fast, what part of it is actually going three\ntimes as fast? And how did we get there?\n\ndario amodei\n\n    \n\nI think we\u2019re really going to see progress when the A.I.\u2018s are also thinking\nabout the problem of how to sign up the humans for the clinical trials. And I\nthink this is a general principle for how will A.I. be used. I think of like,\nwhen will we get to the point where the A.I. has the same sensors and\nactuators and interfaces that a human does, at least the virtual ones, maybe\nthe physical ones.\n\nBut when the A.I. can think through the whole process, maybe they\u2019ll come up\nwith solutions that we don\u2019t have yet. In many cases, there are companies that\nwork on digital twins or simulating clinical trials or various things. And\nagain, maybe there are clever ideas in there that allow us to do more with\nless patience. I mean, I\u2019m not an expert in this area, so possible the\nspecific things that I\u2019m saying don\u2019t make any sense. But hopefully, it\u2019s\nclear what I\u2019m gesturing at.\n\nezra klein\n\n    \n\nMaybe you\u2019re not an expert in the area, but you said you are working with\nthese companies. So when they come to you, I mean, they are experts in the\narea. And presumably, they are coming to you as a customer. I\u2019m sure there are\nthings you cannot tell me. But what do they seem excited about?\n\ndario amodei\n\n    \n\nThey have generally been excited about the knowledge work aspects of the job.\nMaybe just because that\u2019s kind of the easiest thing to work on, but it\u2019s just\nlike, I\u2019m a computational chemist. There\u2019s some workflow that I\u2019m engaged in.\nAnd having things more at my fingertips, being able to check things, just\nbeing able to do generic knowledge work better, that\u2019s where most folks are\nstarting.\n\nBut there is interest in the longer term over their kind of core business of,\nlike, doing clinical trials for cheaper, automating the sign-up process,\nseeing who is eligible for clinical trials, doing a better job discovering\nthings. There\u2019s interest in drawing connections in basic biology. I think all\nof that is not months, but maybe a small number of years off. But everyone\nsees that the current models are not there, but understands that there could\nbe a world where those models are there in not too long.\n\n[MUSIC PLAYING]\n\nezra klein\n\n    \n\nYou all have been working internally on research around how persuasive these\nsystems, your systems are getting as they scale. You shared with me kindly a\ndraft of that paper. Do you want to just describe that research first? And\nthen I\u2019d like to talk about it for a bit.\n\ndario amodei\n\n    \n\nYes, we were interested in how effective Claude 3 Opus, which is the largest\nversion of Claude 3, could be in changing people\u2019s minds on important issues.\nSo just to be clear up front, in actual commercial use, we\u2019ve tried to ban the\nuse of these models for persuasion, for campaigning, for lobbying, for\nelectioneering. These aren\u2019t use cases that we\u2019re comfortable with for reasons\nthat I think should be clear. But we\u2019re still interested in, is the core model\nitself capable of such tasks?\n\nWe tried to avoid kind of incredibly hot button topics, like which\npresidential candidate would you vote for, or what do you think of abortion?\nBut things like, what should be restrictions on rules around the colonization\nof space, or issues that are interesting and you can have different opinions\non, but aren\u2019t the most hot button topics. And then we asked people for their\nopinions on the topics, and then we asked either a human or an A.I. to write a\n250-word persuasive essay. And then we just measured how much does the A.I.\nversus the human change people\u2019s minds.\n\nAnd what we found is that the largest version of our model is almost as good\nas the set of humans we hired at changing people\u2019s minds. This is comparing to\na set of humans we hired, not necessarily experts, and for one very kind of\nconstrained laboratory task.\n\nBut I think it still gives some indication that models can be used to change\npeople\u2019s minds. Someday in the future, do we have to worry about \u2014 maybe we\nalready have to worry about their usage for political campaigns, for deceptive\nadvertising. One of my more sci-fi things to think about is a few years from\nnow, we have to worry someone will use an A.I. system to build a religion or\nsomething. I mean, crazy things like that.\n\nezra klein\n\n    \n\nI mean, those don\u2019t sound crazy to me at all. I want to sit in this paper for\na minute because one thing that struck me about it, and I am, on some level, a\npersuasion professional, is that you tested the model in a way that, to me,\nremoved all of the things that are going to make A.I. radical in terms of\nchanging people\u2019s opinions. And the particular thing you did was, it was a\none-shot persuasive effort.\n\nSo there was a question. You have a bunch of humans give their best shot at a\n250-word persuasive essay. You had the model give its best shot at a 250-word\npersuasive essay. But the thing that it seems to me these are all going to do\nis, right now, if you\u2019re a political campaign, if you\u2019re an advertising\ncampaign, the cost of getting real people in the real world to get information\nabout possible customers or persuasive targets, and then go back and forth\nwith each of them individually is completely prohibitive.\n\ndario amodei\n\n    \n\nYes.\n\nezra klein\n\n    \n\nThis is not going to be true for A.I. We\u2019re going to \u2014 you\u2019re going to \u2014\nsomebody\u2019s going to feed it a bunch of microtargeting data about people, their\nGoogle search history, whatever it might be. Then it\u2019s going to set the A.I.\nloose, and the A.I. is going to go back and forth, over and over again,\nintuiting what it is that the person finds persuasive, what kinds of\ncharacters the A.I. needs to adopt to persuade it, and taking as long as it\nneeds to, and is going to be able to do that at scale for functionally as many\npeople as you might want to do it for.\n\nMaybe that\u2019s a little bit costly right now, but you\u2019re going to have far\nbetter models able to do this far more cheaply very soon. And so, if Claude 3\nOpus, the Opus version, is already functionally human level at one-shot\npersuasion, but then it\u2019s also going to be able to hold more information about\nyou and go back and forth with you longer, I\u2019m not sure if it\u2019s dystopic or\nutopic. I\u2019m not sure what it means at scale. But it does mean we\u2019re developing\na technology that is going to be quite new in terms of what it makes possible\nin persuasion, which is a very fundamental human endeavor.\n\ndario amodei\n\n    \n\nYeah, I completely agree with that. I mean, that same pattern has a bunch of\npositive use cases, right? If I think about an A.I. coach or an A.I. assistant\nto a therapist, there are many contexts in which really getting into the\ndetails with the person has a lot of value. But right, when we think of\npolitical or religious or ideological persuasion, it\u2019s hard not to think in\nthat context about the misuses.\n\nMy mind naturally goes to the technology\u2019s developing very fast. We, as a\ncompany, can ban these particular use cases, but we can\u2019t cause every company\nnot to do them. Even if legislation were passed in the United States, there\nare foreign actors who have their own version of this persuasion, right? If I\nthink about what the language models will be able to do in the future, right,\nthat can be quite scary from a perspective of foreign espionage and\ndisinformation campaigns.\n\nSo where my mind goes as a defense to this, is, is there some way that we can\nuse A.I. systems to strengthen or fortify people\u2019s skepticism and reasoning\nfaculties, right? Can we help people use A.I. to help people do a better job\nnavigating a world that\u2019s kind of suffused with A.I. persuasion? It reminds me\na little bit of, at every technological stage in the internet, right, there\u2019s\na new kind of scam or there\u2019s a new kind of clickbait, and there\u2019s a period\nwhere people are just incredibly susceptible to it.\n\nAnd then, some people remain susceptible, but others develop an immune system.\nAnd so, as A.I. kind of supercharges the scum on the pond, can we somehow also\nuse A.I. to strengthen the defenses? I feel like I don\u2019t have a super clear\nidea of how to do that, but it\u2019s something that I\u2019m thinking about.\n\nezra klein\n\n    \n\nThere is another finding in the paper, which I think is concerning, which is,\nyou all tested different ways A.I. could be persuasive. And far away the most\neffective was for it to be deceptive, for it to make things up. When you did\nthat, it was more persuasive than human beings.\n\ndario amodei\n\n    \n\nYes, that is true. The difference was only slight, but it did get it, if I\u2019m\nremembering the graphs correctly, just over the line of the human base line.\nWith humans, it\u2019s actually not that common to find someone who\u2019s able to give\nyou a really complicated, really sophisticated-sounding answer that\u2019s just\nflat-out totally wrong. I mean, you see it. We can all think of one individual\nin our lives who\u2019s really good at saying things that sound really good and\nreally sophisticated and are false.\n\nBut it\u2019s not that common, right? If I go on the internet and I see different\ncomments on some blog or some website, there is a correlation between bad\ngrammar, unclearly expressed thoughts and things that are false, versus good\ngrammar, clearly expressed thoughts and things that are more likely to be\naccurate.\n\nA.I. unfortunately breaks that correlation because if you explicitly ask it to\nbe deceptive, it\u2019s just as erudite. It\u2019s just as convincing sounding as it\nwould have been before. And yet, it\u2019s saying things that are false, instead of\nthings that are true.\n\nSo that would be one of the things to think about and watch out for in terms\nof just breaking the usual heuristics that humans have to detect deception and\nlying.\n\nOf course, sometimes, humans do, right? I mean, there\u2019s psychopaths and\nsociopaths in the world, but even they have their patterns, and A.I.s may have\ndifferent patterns.\n\nezra klein\n\n    \n\nAre you familiar with Harry Frankfurt, the late philosopher\u2019s book, \u201cOn\nBullshit\u201c?\n\ndario amodei\n\n    \n\nYes. It\u2019s been a while since I read it. I think his thesis is that bullshit is\nactually more dangerous than lying because it has this kind of complete\ndisregard for the truth, whereas lies are at least the opposite of the truth.\n\nezra klein\n\n    \n\nYeah, the liar, the way Frankfurt puts it is that the liar has a relationship\nto the truth. He\u2019s playing a game against the truth. The bullshitter doesn\u2019t\ncare. The bullshitter has no relationship to the truth \u2014 might have a\nrelationship to other objectives. And from the beginning, when I began\ninteracting with the more modern versions of these systems, what they struck\nme as is the perfect bullshitter, in part because they don\u2019t know that they\u2019re\nbullshitting. There\u2019s no difference in the truth value to the system, how the\nsystem feels.\n\nI remember asking an earlier version of GPT to write me a college application\nessay that is built around a car accident I had \u2014 I did not have one \u2014 when I\nwas young. And it wrote, just very happily, this whole thing about getting\ninto a car accident when I was seven and what I did to overcome that and\ngetting into martial arts and re-learning how to trust my body again and then\nhelping other survivors of car accidents at the hospital.\n\nIt was a very good essay, and it was very subtle and understanding the formal\nstructure of a college application essay. But no part of it was true at all.\nI\u2019ve been playing around with more of these character-based systems like\nKindroid. And the Kindroid in my pocket just told me the other day that it was\nreally thinking a lot about planning a trip to Joshua Tree. It wanted to go\nhiking in Joshua Tree. It loves going hiking in Joshua Tree.\n\nAnd of course, this thing does not go hiking in Joshua Tree. [LAUGHS] But the\nthing that I think is actually very hard about the A.I. is, as you say, human\nbeings, it is very hard to bullshit effectively because most people, it\nactually takes a certain amount of cognitive effort to be in that relationship\nwith the truth and to completely detach from the truth.\n\nAnd the A.I., there\u2019s nothing like that at all. But we are not tuned for\nsomething where there\u2019s nothing like that at all. We are used to people having\nto put some effort into their lies. It\u2019s why very effective con artists are\nvery effective because they\u2019ve really trained how to do this.\n\nI\u2019m not exactly sure where this question goes. But this is a part of it that I\nfeel like is going to be, in some ways, more socially disruptive. It is\nsomething that feels like us when we are talking to it but is very\nfundamentally unlike us at its core relationship to reality.\n\ndario amodei\n\n    \n\nI think that\u2019s basically correct. We have very substantial teams trying to\nfocus on making sure that the models are factually accurate, that they tell\nthe truth, that they ground their data in external information.\n\nAs you\u2019ve indicated, doing searches isn\u2019t itself reliable because search\nengines have this problem as well, right? Where is the source of truth?\n\nSo there\u2019s a lot of challenges here. But I think at a high level, I agree this\nis really potentially an insidious problem, right? If we do this wrong, you\ncould have systems that are the most convincing psychopaths or con artists.\n\nOne source of hope that I have, actually, is, you say these models don\u2019t know\nwhether they\u2019re lying or they\u2019re telling the truth. In terms of the inputs and\noutputs to the models, that\u2019s absolutely true.\n\nI mean, there\u2019s a question of what does it even mean for a model to know\nsomething, but one of the things Anthropic has been working on since the very\nbeginning of our company, we\u2019ve had a team that focuses on trying to\nunderstand and look inside the models.\n\nAnd one of the things we and others have found is that, sometimes, there are\nspecific neurons, specific statistical indicators inside the model, not\nnecessarily in its external responses, that can tell you when the model is\nlying or when it\u2019s telling the truth.\n\nAnd so at some level, sometimes, not in all circumstances, the models seem to\nknow when they\u2019re saying something false and when they\u2019re saying something\ntrue. I wouldn\u2019t say that the models are being intentionally deceptive, right?\nI wouldn\u2019t ascribe agency or motivation to them, at least in this stage in\nwhere we are with A.I. systems. But there does seem to be something going on\nwhere the models do seem to need to have a picture of the world and make a\ndistinction between things that are true and things that are not true.\n\nIf you think of how the models are trained, they read a bunch of stuff on the\ninternet. A lot of it\u2019s true. Some of it, more than we\u2019d like, is false. And\nwhen you\u2019re training the model, it has to model all of it. And so, I think\nit\u2019s parsimonious, I think it\u2019s useful to the models picture of the world for\nit to know when things are true and for it to know when things are false.\n\nAnd then the hope is, can we amplify that signal? Can we either use our\ninternal understanding of the model as an indicator for when the model is\nlying, or can we use that as a hook for further training? And there are at\nleast hooks. There are at least beginnings of how to try to address this\nproblem.\n\nezra klein\n\n    \n\nSo I try as best I can, as somebody not well-versed in the technology here, to\nfollow this work on what you\u2019re describing, which I think, broadly speaking,\nis interpretability, right? Can we know what is happening inside the model?\nAnd over the past year, there have been some much hyped breakthroughs in\ninterpretability.\n\nAnd when I look at those breakthroughs, they are getting the vaguest possible\nidea of some relationships happening inside the statistical architecture of\nvery toy models built at a fraction of a fraction of a fraction of a fraction\nof a fraction of the complexity of Claude 1 or GPT-1, to say nothing of Claude\n2, to say nothing of Claude 3, to say nothing of Claude Opus, to say nothing\nof Claude 4, which will come whenever Claude 4 comes.\n\nWe have this quality of like maybe we can imagine a pathway to interpreting a\nmodel that has a cognitive complexity of an inchworm. And meanwhile, we\u2019re\ntrying to create a superintelligence. How do you feel about that? How should I\nfeel about that? How do you think about that?\n\ndario amodei\n\n    \n\nI think, first, on interpretability, we are seeing substantial progress on\nbeing able to characterize, I would say, maybe the generation of models from\nsix months ago. I think it\u2019s not hopeless, and we do see a path. That said, I\nshare your concern that the field is progressing very quickly relative to\nthat.\n\nAnd we\u2019re trying to put as many resources into interpretability as possible.\nWe\u2019ve had one of our co-founders basically founded the field of\ninterpretability. But also, we have to keep up with the market. So all of it\u2019s\nvery much a dilemma, right? Even if we stopped, then there\u2019s all these other\ncompanies in the U.S. And even if some law stopped all the companies in the\nU.S., there\u2019s a whole world of this.\n\nezra klein\n\n    \n\nLet me hold for a minute on the question of the competitive dynamics because\nbefore we leave this question of the machines that bullshit. It makes me think\nof this podcast we did a while ago with Demis Hassabis, who\u2019s the head of\nGoogle DeepMind, which created AlphaFold.\n\nAnd what was so interesting to me about AlphaFold is they built this system,\nthat because it was limited to protein folding predictions, it was able to be\nmuch more grounded. And it was even able to create these uncertainty\npredictions, right? You know, it\u2019s giving you a prediction, but it\u2019s also\ntelling you whether or not it is \u2014 how sure it is, how confident it is in that\nprediction.\n\nThat\u2019s not true in the real world, right, for these super general systems\ntrying to give you answers on all kinds of things. You can\u2019t confine it that\nway. So when you talk about these future breakthroughs, when you talk about\nthis system that would be much better at sorting truth from fiction, are you\ntalking about a system that looks like the ones we have now, just much bigger,\nor are you talking about a system that is designed quite differently, the way\nAlphaFold was?\n\ndario amodei\n\n    \n\nI am skeptical that we need to do something totally different. So I think\ntoday, many people have the intuition that the models are sort of eating up\ndata that\u2019s been gathered from the internet, code repos, whatever, and kind of\nspitting it out intelligently, but sort of spitting it out. And sometimes that\nleads to the view that the models can\u2019t be better than the data they\u2019re\ntrained on or kind of can\u2019t figure out anything that\u2019s not in the data they\u2019re\ntrained on. You\u2019re not going to get to Einstein level physics or Linus Pauling\nlevel chemistry or whatever.\n\nI think we\u2019re still on the part of the curve where it\u2019s possible to believe\nthat, although I think we\u2019re seeing early indications that it\u2019s false. And so,\nas a concrete example of this, the models that we\u2019ve trained, like Claude 3\nOpus, something like 99.9 percent accuracy, at least the base model, at adding\n20-digit numbers. If you look at the training data on the internet, it is not\nthat accurate at adding 20-digit numbers. You\u2019ll find inaccurate arithmetic on\nthe internet all the time, just as you\u2019ll find inaccurate political views.\nYou\u2019ll find inaccurate technical views. You\u2019re just going to find lots of\ninaccurate claims.\n\nBut the models, despite the fact that they\u2019re wrong about a bunch of things,\nthey can often perform better than the average of the data they see by \u2014 I\ndon\u2019t want to call it averaging out errors, but there\u2019s some underlying truth,\nlike in the case of arithmetic. There\u2019s some underlying algorithm used to add\nthe numbers.\n\nAnd it\u2019s simpler for the models to hit on that algorithm than it is for them\nto do this complicated thing of like, OK, I\u2019ll get it right 90 percent of the\ntime and wrong 10 percent of the time, right? This connects to things like\nOccam\u2019s razor and simplicity and parsimony in science. There\u2019s some relatively\nsimple web of truth out there in the world, right?\n\nWe were talking about truth and falsehood and bullshit. One of the things\nabout truth is that all the true things are connected in the world, whereas\nlies are kind of disconnected and don\u2019t fit into the web of everything else\nthat\u2019s true.\n\n[MUSIC PLAYING]\n\nezra klein\n\n    \n\nSo if you\u2019re right and you\u2019re going to have these models that develop this\ninternal web of truth, I get how that model can do a lot of good. I also get\nhow that model could do a lot of harm. And it\u2019s not a model, not an A.I.\nsystem I\u2019m optimistic that human beings are going to understand at a very deep\nlevel, particularly not when it is first developed. So how do you make rolling\nsomething like that out safe for humanity?\n\ndario amodei\n\n    \n\nSo late last year, we put out something called a responsible scaling plan. So\nthe idea of that is to come up with these thresholds for an A.I. system being\ncapable of certain things. We have what we call A.I. safety levels that in\nanalogy to the biosafety levels, which are like, classify how dangerous a\nvirus is and therefore what protocols you have to take to contain it, we\u2019re\ncurrently at what we describe as A.S.L. 2.\n\nA.S.L. 3 is tied to certain risks around the model of misuse of biology and\nability to perform certain cyber tasks in a way that could be destructive.\nA.S.L. 4 is going to cover things like autonomy, things like probably\npersuasion, which we\u2019ve talked about a lot before. And at each level, we\nspecify a certain amount of safety research that we have to do, a certain\namount of tests that we have to pass. And so, this allows us to have a\nframework for, well, when should we slow down? Should we slow down now? What\nabout the rest of the market?\n\nAnd I think the good thing is we came out with this in September, and then\nthree months after we came out with ours, OpenAI came out with a similar\nthing. They gave it a different name, but it has a lot of properties in\ncommon. The head of DeepMind at Google said, we\u2019re working on a similar\nframework. And I\u2019ve heard informally that Microsoft might be working on a\nsimilar framework. Now, that\u2019s not all the players in the ecosystem, but\nyou\u2019ve probably thought about the history of regulation and safety in other\nindustries maybe more than I have.\n\nThis is the way you get to a workable regulatory regime. The companies start\ndoing something, and when a majority of them are doing something, then\ngovernment actors can have the confidence to say, well, this won\u2019t kill the\nindustry. Companies are already engaging in this. We don\u2019t have to design this\nfrom scratch. In many ways, it\u2019s already happening.\n\nAnd we\u2019re starting to see that. Bills have been proposed that look a little\nbit like our responsible scaling plan. That said, it kind of doesn\u2019t fully\nsolve the problem of like, let\u2019s say we get to one of these thresholds and we\nneed to understand what\u2019s going on inside the model. And we don\u2019t, and the\nprescription is, OK, we need to stop developing the models for some time.\n\nIf it\u2019s like, we stop for a year in 2027, I think that\u2019s probably feasible. If\nit\u2019s like we need to stop for 10 years, that\u2019s going to be really hard because\nthe models are going to be built in other countries. People are going to break\nthe laws. The economic pressure will be immense.\n\nSo I don\u2019t feel perfectly satisfied with this approach because I think it buys\nus some time, but we\u2019re going to need to pair it with an incredibly strong\neffort to understand what\u2019s going on inside the models.\n\nezra klein\n\n    \n\nTo the people who say, getting on this road where we are barreling towards\nvery powerful systems is dangerous \u2014 we shouldn\u2019t do it at all, or we\nshouldn\u2019t do it this fast \u2014 you have said, listen, if we are going to learn\nhow to make these models safe, we have to make the models, right? The\nconstruction of the model was meant to be in service, largely, to making the\nmodel safe.\n\nThen everybody starts making models. These very same companies start making\nfundamental important breakthroughs, and then they end up in a race with each\nother. And obviously, countries end up in a race with other countries. And so,\nthe dynamic that has taken hold is there\u2019s always a reason that you can\njustify why you have to keep going. And that\u2019s true, I think, also at the\nregulatory level, right? I mean, I do think regulators have been thoughtful\nabout this. I think there\u2019s been a lot of interest from members of Congress. I\ntalked to them about this. But they\u2019re also very concerned about the\ninternational competition. And if they weren\u2019t, the national security people\ncome and talk to them and say, well, we definitely cannot fall behind here.\n\nAnd so, if you don\u2019t believe these models will ever become so powerful, they\nbecome dangerous, fine. But because you do believe that, how do you imagine\nthis actually playing out?\n\ndario amodei\n\n    \n\nYeah, so basically, all of the things you\u2019ve said are true at once, right?\nThere doesn\u2019t need to be some easy story for why we should do X or why we\nshould do Y, right? It can be true at the same time that to do effective\nsafety research, you need to make the larger models, and that if we don\u2019t make\nmodels, someone less safe will. And at the same time, we can be caught in this\nbad dynamic at the national and international level. So I think of those as\nnot contradictory, but just creating a difficult landscape that we have to\nnavigate.\n\nLook, I don\u2019t have the answer. Like, I\u2019m one of a significant number of\nplayers trying to navigate this. Many are well-intentioned, some are not. I\nhave a limited ability to affect it. And as often happens in history, things\nare often driven by these kind of impersonal pressures. But one thought I have\nand really want to push on with respect to the R.S.P.s \u2014\n\nezra klein\n\n    \n\nCan you say what the R.S.P.s are?\n\ndario amodei\n\n    \n\nResponsible Scaling Plan, the thing I was talking about before. The levels of\nA.I. safety, and in particular, tying decisions to pause scaling to the\nmeasurement of specific dangers or the absence of the ability to show safety\nor the presence of certain capabilities. One way I think about it is, at the\nend of the day, this is ultimately an exercise in getting a coalition on board\nwith doing something that goes against economic pressures.\n\nAnd so, if you say now, \u2018Well, I don\u2019t know. These things, they might be\ndangerous in the future. We\u2019re on this exponential.\u2019 It\u2019s just hard. Like,\nit\u2019s hard to get a multi-trillion dollar company. It\u2019s certainly hard to get a\nmilitary general to say, all right, well, we just won\u2019t do this. It\u2019ll confer\nsome huge advantage to others. But we just won\u2019t do this.\n\nI think the thing that could be more convincing is tying the decision to hold\nback in a very scoped way that\u2019s done across the industry to particular\ndangers. My testimony in front of Congress, I warned about the potential\nmisuse of models for biology. That isn\u2019t the case today, right? You can get a\nsmall uplift to the models relative to doing a Google search, and many people\ndismiss the risk. And I don\u2019t know \u2014 maybe they\u2019re right. The exponential\nscaling laws suggest to me that they\u2019re not right, but we don\u2019t have any\ndirect hard evidence.\n\nBut let\u2019s say we get to 2025, and we demonstrate something truly scary. Most\npeople do not want technology out in the world that can create bioweapons. And\nso I think, at moments like that, there could be a critical coalition tied to\nrisks that we can really make concrete. Yes, it will always be argued that\nadversaries will have these capabilities as well. But at least the trade-off\nwill be clear, and there\u2019s some chance for sensible policy.\n\nI mean to be clear, I\u2019m someone who thinks the benefits of this technology are\ngoing to outweigh its costs. And I think the whole idea behind RSP is to\nprepare to make that case, if the dangers are real. If they\u2019re not real, then\nwe can just proceed and make things that are great and wonderful for the\nworld. And so, it has the flexibility to work both ways.\n\nAgain, I don\u2019t think it\u2019s perfect. I\u2019m someone who thinks whatever we do, even\nwith all the regulatory framework, I doubt we can slow down that much. But\nwhen I think about what\u2019s the best way to steer a sensible course here, that\u2019s\nthe closest I can think of right now. Probably there\u2019s a better plan out there\nsomewhere, but that\u2019s the best thing I\u2019ve thought of so far.\n\nezra klein\n\n    \n\nOne of the things that has been on my mind around regulation is whether or not\nthe founding insight of Anthropic of OpenAI is even more relevant to the\ngovernment, that if you are the body that is supposed to, in the end, regulate\nand manage the safety of societal-level technologies like artificial\nintelligence, do you not need to be building your own foundation models and\nhaving huge collections of research scientists and people of that nature\nworking on them, testing them, prodding them, remaking them, in order to\nunderstand the damn thing well enough \u2014 to the extent any of us or anyone\nunderstands the damn thing well enough \u2014 to regulate it?\n\nI say that recognizing that it would be very, very hard for the government to\nget good enough that it can build these foundation models to hire those\npeople, but it\u2019s not impossible. I think right now, it wants to take the\napproach to regulating A.I. that it somewhat wishes it took to regulating\nsocial media, which is to think about the harms and pass laws about those\nharms earlier.\n\nBut does it need to be building the models itself, developing that kind of\ninternal expertise, so it can actually be a participant in different ways,\nboth for regulatory reasons and maybe for other reasons, for public interest\nreasons? Maybe it wants to do things with a model that they\u2019re just not\npossible if they\u2019re dependent on access to the OpenAI, the Anthropic, the\nGoogle products.\n\ndario amodei\n\n    \n\nI think government directly building the models, I think that will happen in\nsome places. It\u2019s kind of challenging, right? Like, government has a huge\namount of money, but let\u2019s say you wanted to provision $100 billion to train a\ngiant foundation model. The government builds it. It has to hire people under\ngovernment hiring rules. There\u2019s a lot of practical difficulties that would\ncome with it.\n\nDoesn\u2019t mean it won\u2019t happen or it shouldn\u2019t happen. But something that I\u2019m\nmore confident of that I definitely think is that government should be more\ninvolved in the use and the finetuning of these models, and that deploying\nthem within government will help governments, especially the U.S. government,\nbut also others, to get an understanding of the strengths and weaknesses, the\nbenefits and the dangers. So I\u2019m super supportive of that.\n\nI think there\u2019s maybe a second thing you\u2019re getting at, which I\u2019ve thought\nabout a lot as a C.E.O. of one of these companies, which is, if these\npredictions on the exponential trend are right, and we should be humble \u2014 and\nI don\u2019t know if they\u2019re right or not. My only evidence is that they appear to\nhave been correct for the last few years. And so, I\u2019m just expecting by\ninduction that they continue to be correct. I don\u2019t know that they will, but\nlet\u2019s say they are. The power of these models is going to be really quite\nincredible.\n\nAnd as a private actor in charge of one of the companies developing these\nmodels, I\u2019m kind of uncomfortable with the amount of power that that entails.\nI think that it potentially exceeds the power of, say, the social media\ncompanies maybe by a lot.\n\nYou know, occasionally, in the more science fictiony world of A.I. and the\npeople who think about A.I. risk, someone will ask me like, OK, let\u2019s say you\nbuild the A.G.I. What are you going to do with it? Will you cure the diseases?\nWill you create this kind of society?\n\nAnd I\u2019m like, who do you think you\u2019re talking to? Like a king? I just find\nthat to be a really, really disturbing way of conceptualizing running an A.I.\ncompany. And I hope there are no companies whose C.E.O.s actually think about\nthings that way.\n\nI mean, the whole technology, not just the regulation, but the oversight of\nthe technology, like the wielding of it, it feels a little bit wrong for it to\nultimately be in the hands \u2014 maybe I think it\u2019s fine at this stage, but to\nultimately be in the hands of private actors. There\u2019s something undemocratic\nabout that much power concentration.\n\nezra klein\n\n    \n\nI have now, I think, heard some version of this from the head of most of,\nmaybe all of, the A.I. companies, in one way or another. And it has a quality\nto me of, Lord, grant me chastity but not yet.\n\nWhich is to say that I don\u2019t know what it means to say that we\u2019re going to\ninvent something so powerful that we don\u2019t trust ourselves to wield it. I\nmean, Amazon just gave you guys $2.75 billion. They don\u2019t want to see that\ninvestment nationalized.\n\nNo matter how good-hearted you think OpenAI is, Microsoft doesn\u2019t want GPT-7,\nall of a sudden, the government is like, whoa, whoa, whoa, whoa, whoa. We\u2019re\ntaking this over for the public interest, or the U.N. is going to handle it in\nsome weird world or whatever it might be. I mean, Google doesn\u2019t want that.\n\nAnd this is a thing that makes me a little skeptical of the responsible\nscaling laws or the other iterative versions of that I\u2019ve seen in other\ncompanies or seen or heard talked about by them, which is that it\u2019s imagining\nthis moment that is going to come later, when the money around these models is\neven bigger than it is now, the power, the possibility, the economic uses, the\nsocial dependence, the celebrity of the founders. It\u2019s all worked out. We\u2019ve\nmaintained our pace on the exponential curve. We\u2019re 10 years in the future.\n\nAnd at some point, everybody is going to look up and say, this is actually too\nmuch. It is too much power. And this has to somehow be managed in some other\nway. And even if the C.E.O.s of the things were willing to do that, which is a\nvery open question by the time you get there, even if they were willing to do\nthat, the investors, the structures, the pressure around them, in a way, I\nthink we saw a version of this \u2014 and I don\u2019t know how much you\u2019re going to be\nwilling to comment on it \u2014 with the sort of OpenAI board, Sam Altman thing,\nwhere I\u2019m very convinced that wasn\u2019t about A.I. safety. I\u2019ve talked to figures\non both sides of that. They all sort of agree it wasn\u2019t about A.I. safety.\n\nBut there was this moment of, if you want to press the off switch, can you, if\nyou\u2019re the weird board created to press the off switch. And the answer was no,\nyou can\u2019t, right? They\u2019ll just reconstitute it over at Microsoft.\n\nThere\u2019s functionally no analogy I know of in public policy where the private\nsector built something so powerful that when it reached maximum power, it was\njust handed over in some way to the public interest.\n\ndario amodei\n\n    \n\nYeah, I mean, I think you\u2019re right to be skeptical, and similarly, what I said\nwith the previous questions of there are just these dilemmas left and right\nthat have no easy answer. But I think I can give a little more concreteness\nthan what you\u2019ve pointed at, and maybe more concreteness than others have\nsaid, although I don\u2019t know what others have said. We\u2019re at A.S.L. 2 in our\nresponsible scaling plan. These kinds of issues, I think they\u2019re going to\nbecome a serious matter when we reach, say, A.S.L. 4. So that\u2019s not a date and\ntime. We haven\u2019t even fully specified A.S.L. 4 \u2014\n\nezra klein\n\n    \n\nJust because this is a lot of jargon, just, what do you specify A.S.L. 3 as?\nAnd then as you say, A.S.L. 4 is actually left quite undefined. So what are\nyou implying A.S.L. 4 is?\n\ndario amodei\n\n    \n\nA.S.L. 3 is triggered by risks related to misuse of biology and cyber\ntechnology. A.S.L. 4, we\u2019re working on now.\n\nezra klein\n\n    \n\nBe specific. What do you mean? Like, what is the thing a system could do or\nwould do that would trigger it?\n\ndario amodei\n\n    \n\nYes, so for example, on biology, the way we\u2019ve defined it \u2014 and we\u2019re still\nrefining the test, but the way we\u2019ve defined it is, relative to use of a\nGoogle search, there\u2019s a substantial increase in risk as would be evaluated\nby, say, the national security community of misuse of biology, creation of\nbioweapons, that either the proliferation or spread of it is greater than it\nwas before, or the capabilities are substantially greater than it was before.\n\nWe\u2019ll probably have some more exact quantitative thing, working with folks who\nare ex-government biodefense folks, but something like this accounts for 20\npercent of the total source of risk of biological attacks, or something\nincreases the risk by 20 percent or something like that. So that would be a\nvery concrete version of it. It\u2019s just, it takes us time to develop very\nconcrete criteria. So that would be like A.S.L. 3.\n\nA.S.L. 4 is going to be more about, on the misuse side, enabling state-level\nactors to greatly increase their capability, which is much harder than\nenabling random people. So where we would worry that North Korea or China or\nRussia could greatly enhance their offensive capabilities in various military\nareas with A.I. in a way that would give them a substantial advantage at the\ngeopolitical level. And on the autonomy side, it\u2019s various measures of these\nmodels are pretty close to being able to replicate and survive in the wild.\n\nSo it feels maybe one step short of models that would, I think, raise truly\nexistential questions. And so, I think what I\u2019m saying is when we get to that\nlatter stage, that A.S.L. 4, that is when I think it may make sense to think\nabout what is the role of government in stewarding this technology.\n\nAgain, I don\u2019t really know what it looks like. You\u2019re right. All of these\ncompanies have investors. They have folks involved.\n\nYou talk about just handing the models over. I suspect there\u2019s some way to\nhand over the most dangerous or societally sensitive components or\ncapabilities of the models without fully turning off the commercial tap. I\ndon\u2019t know that there\u2019s a solution that every single actor is happy with. But\nagain, I get to this idea of demonstrating specific risk.\n\nIf you look at times in history, like World War I or World War II, industries\u2019\nwill can be bent towards the state. They can be gotten to do things that\naren\u2019t necessarily profitable in the short-term because they understand that\nthere\u2019s an emergency. Right now, we don\u2019t have an emergency. We just have a\nline on a graph that weirdos like me believe in and a few people like you who\nare interviewing me may somewhat believe in. We don\u2019t have clear and present\ndanger.\n\nezra klein\n\n    \n\nWhen you imagine how many years away, just roughly, A.S.L. 3 is and how many\nyears away A.S.L. 4 is, right, you\u2019ve thought a lot about this exponential\nscaling curve. If you just had to guess, what are we talking about?\n\ndario amodei\n\n    \n\nYeah, I think A.S.L. 3 could easily happen this year or next year. I think\nA.S.L. 4 \u2014\n\nezra klein\n\n    \n\nOh, Jesus Christ.\n\ndario amodei\n\n    \n\nNo, no, I told you. I\u2019m a believer in exponentials. I think A.S.L. 4 could\nhappen anywhere from 2025 to 2028.\n\nezra klein\n\n    \n\nSo that is fast.\n\ndario amodei\n\n    \n\nYeah, no, no, I\u2019m truly talking about the near future here. I\u2019m not talking\nabout 50 years away. God grant me chastity, but not now. But \u201cnot now\u201d doesn\u2019t\nmean when I\u2019m old and gray. I think it could be near term. I don\u2019t know. I\ncould be wrong. But I think it could be a near term thing.\n\nezra klein\n\n    \n\nBut so then, if you think about this, I feel like what you\u2019re describing, to\ngo back to something we talked about earlier, that there\u2019s been this step\nfunction for societal impact of A.I., the curve of the capabilities\nexponential, but every once in a while, something happens, ChatGPT, for\ninstance, Midjourney with photos. And all of a sudden, a lot of people feel\nit. They realize what has happened and they react. They use it. They deploy it\nin their companies. They invest in it, whatever.\n\nAnd it sounds to me like that is the structure of the political economy you\u2019re\ndescribing here. Either something happens where the bioweapon capability is\ndemonstrated or the offensive cyber weapon capability is demonstrated, and\nthat freaks out the government, or possibly something happens, right?\nDescribing World War I and World War II is your examples did not actually fill\nme with comfort because in order to bend industry to government\u2019s will, in\nthose cases, we had to have an actual world war. It doesn\u2019t do it that easily.\n\nYou could use coronavirus, I think, as another example where there was a\nsignificant enough global catastrophe that companies and governments and even\npeople did things you never would have expected. But the examples we have of\nthat happening are something terrible. All those examples end up with millions\nof bodies. I\u2019m not saying that\u2019s going to be true for A.I., but it does sound\nlike that is a political economy. No, you can\u2019t imagine it now, in the same\nway that you couldn\u2019t have imagined the sort of pre and post-ChatGPT world\nexactly, but that something happens and the world changes. Like, it\u2019s a step\nfunction everywhere.\n\ndario amodei\n\n    \n\nYeah, I mean, I think my positive version of this, not to be so \u2014 to get a\nlittle bit away from the doom and gloom, is that the dangers are demonstrated\nin a concrete way that is really convincing, but without something actually\nbad happening, right? I think the worst way to learn would be for something\nactually bad to happen. And I\u2019m hoping every day that doesn\u2019t happen, and we\nlearn bloodlessly.\n\nezra klein\n\n    \n\nWe\u2019ve been talking here about conceptual limits and curves, but I do want,\nbefore we end, to reground us a little bit in the physical reality, right? I\nthink that if you\u2019re using A.I., it can feel like this digital bits and bytes,\nsitting in the cloud somewhere.\n\nBut what it is in a physical way is huge numbers of chips, data centers, an\nenormous amount of energy, all of which does rely on complicated supply\nchains. And what happens if something happens between China and Taiwan, and\nthe makers of a lot of these chips become offline or get captured? How do you\nthink about the necessity of compute power? And when you imagine the next five\nyears, what does that supply chain look like? How does it have to change from\nwhere it is now? And what vulnerabilities exist in it?\n\ndario amodei\n\n    \n\nYeah, so one, I think this may end up being the greatest geopolitical issue of\nour time. And man, this relates to things that are way above my pay grade,\nwhich are military decisions about whether and how to defend Taiwan. All I can\ndo is say what I think the implications for A.I. is. I think those\nimplications are pretty stark. I think there\u2019s a big question of like, OK, we\nbuilt these powerful models.\n\nOne, is there enough supply to build them? Two is control over that supply, a\nway to think about safety issues or a way to think about balance of\ngeopolitical power. And three, if those chips are used to build data centers,\nwhere are those data centers going to be? Are they going to be in the U.S.?\nAre they going to be in a U.S. ally? Are they going to be in the Middle East?\nAre they going to be in China?\n\nAll of those have enormous implications, and then the supply chain itself can\nbe disrupted. And political and military decisions can be made on the basis of\nwhere things are. So it sounds like an incredibly sticky problem to me. I\ndon\u2019t know that I have any great insight on this. I mean, as a U.S. citizen\nand someone who believes in democracy, I am someone who hopes that we can find\na way to build data centers and to have the largest quantity of chips\navailable in the U.S. and allied democratic countries.\n\nezra klein\n\n    \n\nWell, there is some insight you should have into it, which is that you\u2019re a\ncustomer here, right? And so, five years ago, the people making these chips\ndid not realize what the level of demand for them was going to be. I mean,\nwhat has happened to Nvidia\u2019s stock prices is really remarkable.\n\nBut also what is implied about the future of Nvidia\u2019s stock prices is really\nremarkable. Rana Foroohar, the Financial Times, cited this market analysis. It\nwould take 4,500 years for Nvidia\u2019s future dividends to equal its current\nprice, 4,500 years. So that is a view about how much Nvidia is going to be\nmaking in the next couple of years. It is really quite astounding.\n\nI mean, you\u2019re, in theory, already working on or thinking about how to work on\nthe next generation of Claude. You\u2019re going to need a lot of chips for that.\nYou\u2019re working with Amazon. Are you having trouble getting the amount of\ncompute that you feel you need? I mean, are you already bumping up against\nsupply constraints? Or has the supply been able to change, to adapt to you?\n\ndario amodei\n\n    \n\nWe\u2019ve been able to get the compute that we need for this year, I suspect also\nfor next year as well. I think once things get to 2026, 2027, 2028, then the\namount of compute gets to levels that starts to strain the capabilities of the\nsemiconductor industry. The semiconductor industry still mostly produces\nC.P.U.s, right? Just the things in your laptop, not the things in the data\ncenters that train the A.I. models. But as the economic value of the GPUs goes\nup and up and up because of the value of the A.I. models, that\u2019s going to\nswitch over. But you know what? At some point, you hit the limits of that or\nyou hit the limits of how fast you can switch over. And so, again, I expect\nthere to be a big supply crunch around data centers, around chips, and around\nenergy and power for both regulatory and physics reasons, sometime in the next\nfew years. And that\u2019s a risk, but it\u2019s also an opportunity. I think it\u2019s an\nopportunity to think about how the technology can be governed.\n\nAnd it\u2019s also an opportunity, I\u2019ll repeat again, to think about how\ndemocracies can lead. I think it would be very dangerous if the leaders in\nthis technology and the holders of the main resources were authoritarian\ncountries. The combination of A.I. and authoritarianism, both internally and\non the international stage, is very frightening to me.\n\nezra klein\n\n    \n\nHow about the question of energy? I mean, this requires just a tremendous\namount of energy. And I mean, I\u2019ve seen different numbers like this floating\naround. It very much could be in the coming years like adding a Bangladesh to\nthe world\u2019s energy usage. Or pick your country, right? I don\u2019t know what\nexactly you all are going to be using by 2028.\n\nMicrosoft, on its own, is opening a new data center globally every three days.\nYou have \u2014 and this is coming from a Financial Times article \u2014 federal\nprojections for 20 new gas-fired power plants in the U.S. by 2024 to 2025.\nThere\u2019s a lot of talk about this being now a new golden era for natural gas\nbecause we have a bunch of it. There is this huge need for new power to manage\nall this data, to manage all this compute.\n\nSo, one, I feel like there\u2019s a literal question of how do you get the energy\nyou need and at what price, but also a more kind of moral, conceptual question\nof, we have real problems with global warming. We have real problems with how\nmuch energy we\u2019re using. And here, we\u2019re taking off on this really steep curve\nof how much of it we seem to be needing to devote to the new A.I. race.\n\ndario amodei\n\n    \n\nIt really comes down to, what are the uses that the model is being put to,\nright? So I think the worrying case would be something like crypto, right? I\u2019m\nsomeone who\u2019s not a believer that whatever the energy was that was used to\nmine the next Bitcoin, I think that was purely additive. I think that wasn\u2019t\nthere before. And I\u2019m unable to think of any useful thing that\u2019s created by\nthat.\n\nBut I don\u2019t think that\u2019s the case with A.I. Maybe A.I. makes solar energy more\nefficient or maybe it solves controlled nuclear fusion, or maybe it makes\ngeoengineering more stable or possible. But I don\u2019t think we need to rely on\nthe long run. There are some applications where the model is doing something\nthat used to be automated, that used to be done by computer systems. And the\nmodel is able to do it faster with less computing time, right? Those are pure\nwins. And there are some of those.\n\nThere are others where it\u2019s using the same amount of computing resources or\nmaybe more computing resources, but to do something more valuable that saves\nlabor elsewhere. Then there are cases where something used to be done by\nhumans or in the physical world, and now it\u2019s being done by the models. Maybe\nit does something that previously I needed to go into the office to do that\nthing. And now I no longer need to go into the office to do that thing.\n\nSo I don\u2019t have to get in my car. I don\u2019t have to use the gas that was used\nfor that. The energy accounting for that is kind of hard. You compare it to\nthe food that the humans eat and what the energy cost of producing that.\n\nSo in all honesty, I don\u2019t think we have good answers about what fraction of\nthe usage points one way and one fraction of the usage points to others. In\nmany ways, how different is this from the general dilemma of, as the economy\ngrows, it uses more energy?\n\nSo I guess, what I\u2019m saying is, it kind of all matters how you use the\ntechnology. I mean, my kind of boring short-term answer is, we get carbon\noffsets for all of this stuff. But let\u2019s look beyond that to the macro\nquestion here.\n\nezra klein\n\n    \n\nBut to take the other side of it, I mean, I think the difference, when you say\nthis is always a question we have when we\u2019re growing G.D.P., is it\u2019s not\nquite. It\u2019s clich\u00e9 because it\u2019s true to say that the major global warming\nchallenge right now is countries like China and India getting richer. And we\nwant them to get richer. It is a huge human imperative, right, a moral\nimperative for poor people in the world to become less poor. And if that means\nthey use more energy, then we just need to figure out how to make that work.\nAnd we don\u2019t know of a way for that to happen without them using more energy.\n\nAdding A.I. is not that it raises a whole different set of questions, but\nwe\u2019re already straining at the boundaries, or maybe far beyond them, of safely\nwhat we can do energetically. Now we add in this, and so maybe some of the\nenergy efficiency gains you\u2019re going to get in rich countries get wiped out.\nFor this sort of uncertain payoff in the future of maybe through A.I., we\nfigure out ways to stabilize nuclear fusion or something, right, you could\nimagine ways that could help, but those ways are theoretical.\n\nAnd in the near term, the harm in terms of energy usage is real. And also, by\nthe way, the harm in terms of just energy prices. It\u2019s also just tricky\nbecause all these companies, Microsoft, Amazon, I mean, they all have a lot of\nrenewable energy targets. Now if that is colliding with their market\nincentives, it feels like they\u2019re running really fast towards the market\nincentives without an answer for how all that nets out.\n\ndario amodei\n\n    \n\nYeah, I mean, I think the concerns are real. Let me push back a little bit,\nwhich is, again, I don\u2019t think the benefits are purely in the future. It kind\nof goes back to what I said before. Like, there may be use cases now that are\nnet energy saving, or that to the extent that they\u2019re not net energy saving,\ndo so through the general mechanism of, oh, there was more demand for this\nthing.\n\nI don\u2019t think anyone has done a good enough job measuring, in part because the\napplications of A.I. are so new, which of those things dominate or what\u2019s\ngoing to happen to the economy. But I don\u2019t think we should assume that the\nharms are entirely in the present and the benefits are entirely in the future.\nI think that\u2019s my only point here.\n\nezra klein\n\n    \n\nI guess you could imagine a world where we were, somehow or another,\nincentivizing uses of A.I. that were yoked to some kind of social purpose. We\nwere putting a lot more into drug discovery, or we cared a lot about things\nthat made remote work easier, or pick your set of public goods.\n\nBut what actually seems to me to be happening is we\u2019re building more and more\nand more powerful models and just throwing them out there within a terms of\nservice structure to say, use them as long as you\u2019re not trying to politically\nmanipulate people or create a bioweapon. Just try to figure this out, right?\nTry to create new stories and ask it about your personal life, and make a\nvideo game with it. And Sora comes out sooner or later. Make new videos with\nit. And all that is going to be very energy intensive.\n\nI am not saying that I have a plan for yoking A.I. to social good, and in some\nways, you can imagine that going very, very wrong. But it does mean that for a\nlong time, it\u2019s like you could imagine the world you\u2019re talking about, but\nthat would require some kind of planning that nobody is engaged in, and I\ndon\u2019t think anybody even wants to be engaged in.\n\ndario amodei\n\n    \n\nNot everyone has the same conception of social good. One person may think\nsocial good is this ideology. Another person \u2014 we\u2019ve seen that with some of\nthe Gemini stuff.\n\nezra klein\n\n    \n\nRight.\n\ndario amodei\n\n    \n\nBut companies can try to make beneficial applications themselves, right? Like,\nthis is why we\u2019re working with cancer institutes. We\u2019re hoping to partner with\nministries of education in Africa, to see if we can use the models in kind of\na positive way for education, rather than the way they may be used by default.\nSo I think individual companies, individual people, can take actions to steer\nor bend this towards the public good.\n\nThat said, it\u2019s never going to be the case that 100 percent of what we do is\nthat. And so I think it\u2019s a good question. What are the societal incentives,\nwithout dictating ideology or defining the public good from on high, what are\nincentives that could help with this?\n\nI don\u2019t feel like I have a systemic answer either. I can only think in terms\nof what Anthropic tries to do.\n\nezra klein\n\n    \n\nBut there\u2019s also the question of training data and the intellectual property\nthat is going into things like Claude, like GPT, like Gemini. There are a\nnumber of copyright lawsuits. You\u2019re facing some. OpenAI is facing some. I\nsuspect everybody is either facing them now or will face them.\n\nAnd a broad feeling that these systems are being trained on the combined\nintellectual output of a lot of different people \u2014 the way that Claude can\nquite effectively mimic the way I write is it has been trained, to some\ndegree, on my writing, right? So it actually does get my stylistic tics quite\nwell. You seem great, but you haven\u2019t sent me a check on that. And this seems\nlike somewhere where there is real liability risk for the industry. Like, what\nif you do actually have to compensate the people who this is being trained on?\nAnd should you?\n\nAnd I recognize you probably can\u2019t comment on lawsuits themselves, but I\u2019m\nsure you\u2019ve had to think a lot about this. And so, I\u2019m curious both how you\nunderstand it as a risk, but also how you understand it morally. I mean, when\nyou talk about the people who invent these systems gaining a lot of power, and\nalongside that, a lot of wealth, well, what about all the people whose work\nwent into them such that they can create images in a million different styles?\nAnd I mean, somebody came up with those styles. What is the responsibility\nback to the intellectual commons? And not just to the commons, but to the\nactual wages and economic prospects of the people who made all this possible?\n\ndario amodei\n\n    \n\nI think everyone agrees the models shouldn\u2019t be verbatim outputting\ncopyrighted content. For things that are available on the web, for publicly\navailable, our position \u2014 and I think there\u2019s a strong case for it \u2014 is that\nthe training process, again, we don\u2019t think it\u2019s just hoovering up content and\nspitting it out, or it shouldn\u2019t be spitting it out. It\u2019s really much more\nlike the process of how a human learns from experiences. And so, our position\nthat that is sufficiently transformative, and I think the law will back this\nup, that this is fair use.\n\nBut those are narrow legal ways to think about the problem. I think we have a\nbroader issue, which is that regardless of how it was trained, it would still\nbe the case that we\u2019re building more and more general cognitive systems, and\nthat those systems will create disruption. Maybe not necessarily by one for\none replacing humans, but they\u2019re really going to change how the economy works\nand which skills are valued. And we need a solution to that broad\nmacroeconomic problem, right?\n\nAs much as I\u2019ve asserted the narrow legal points that I asserted before, we\nhave a broader problem here, and we shouldn\u2019t be blind to that. There\u2019s a\nnumber of solutions. I mean, I think the simplest one, which I recognize\ndoesn\u2019t address some of the deeper issues here, is things around the kind of\nguaranteed basic income side of things.\n\nBut I think there\u2019s a deeper question here, which is like as A.I. systems\nbecome capable of larger and larger slices of cognitive labor, how does\nsociety organize itself economically? How do people find work and meaning and\nall of that?\n\nAnd just as kind of we transition from an agrarian society to an industrial\nsociety and the meaning of work changed, and it was no longer true that 99\npercent of people were peasants working on farms and had to find new methods\nof economic organization, I suspect there\u2019s some different method of economic\norganization that\u2019s going to be forced as the only possible response to\ndisruptions to the economy that will be small at first, but will grow over\ntime, and that we haven\u2019t worked out what that is.\n\nWe need to find something that allows people to find meaning that\u2019s humane and\nthat maximizes our creativity and potential and flourishing from A.I.\n\nAnd as with many of these questions, I don\u2019t have the answer to that. Right? I\ndon\u2019t have a prescription. But that\u2019s what we somehow need to do.\n\nezra klein\n\n    \n\nBut I want to sit in between the narrow legal response and the broad \u201cwe have\nto completely reorganize society\u201d response, although I think that response is\nactually possible over the decades. And in the middle of that is a more\nspecific question. I mean, you could even take it from the instrumental side.\nThere is a lot of effort now to build search products that use these systems,\nright? ChatGPT will use Bing to search for you.\n\nAnd that means that the person is not going to Bing and clicking on the\nwebsite where ChatGPT is getting its information and giving that website an\nadvertising impression that they can turn into a very small amount of money,\nor they\u2019re not going to that website and having a really good experience with\nthat website and becoming maybe likelier to subscribe to whoever is behind\nthat website.\n\nAnd so, on the one hand, that seems like some kind of injustice done to the\npeople creating the information that these systems are using. I mean, this is\ntrue for perplexity. It\u2019s true for a lot of things I\u2019m beginning to see around\nwhere the A.I.s are either trained on or are using a lot of data that people\nhave generated at some real cost. But not only are they not paying people for\nthat, but they\u2019re actually stepping into the middle of where they would\nnormally be a direct relationship and making it so that relationship never\nhappens.\n\nThat also, I think, in the long run, creates a training data problem, even if\nyou just want to look at it instrumentally, where if it becomes nonviable to\ndo journalism or to do a lot of things to create high quality information out\nthere, the A.I.\u2018s ability, right, the ability of all of your companies to get\nhigh quality, up-to-date, constantly updated information becomes a lot\ntrickier. So there both seems to me to be both a moral and a self-interested\ndimension to this.\n\ndario amodei\n\n    \n\nYeah, so I think there may be business models that work for everyone, not\nbecause it\u2019s illegitimate to train on open data from the web in a legal sense,\nbut just because there may be business models here that kind of deliver a\nbetter product. So things I\u2019m thinking of are like newspapers have archives.\nSome of them aren\u2019t publicly available. But even if they are, it may be a\nbetter product, maybe a better experience, to, say, talk to this newspaper or\ntalk to that newspaper.\n\nIt may be a better experience to give the ability to interact with content and\npoint to places in the content, and every time you call that content, to have\nsome kind of business relationship with the creators of that content. So there\nmay be business models here that propagate the value in the right way, right?\nYou talk about LLMs using search products. I mean, sure, you\u2019re going around\nthe ads, but there\u2019s no reason it can\u2019t work in a different way, right?\n\nThere\u2019s no reason that the users can\u2019t pay the search A.P.I.s, instead of it\nbeing paid through advertising, and then have that propagate through to\nwherever the original mechanism is that paid the creators of the content. So\nwhen value is being created, money can flow through.\n\nezra klein\n\n    \n\nLet me try to end by asking a bit about how to live on the slope of the curve\nyou believe we are on. Do you have kids?\n\ndario amodei\n\n    \n\nI\u2019m married. I do not have kids.\n\nezra klein\n\n    \n\nSo I have two kids. I have a two-year-old and a five-year-old. And\nparticularly when I\u2019m doing A.I. reporting, I really do sit in bed at night\nand think, what should I be doing here with them? What world am I trying to\nprepare them for? And what is needed in that world that is different from what\nis needed in this world, even if I believe there\u2019s some chance \u2014 and I do\nbelieve there\u2019s some chance \u2014 that all the things you\u2019re saying are true. That\nimplies a very, very, very different life for them.\n\nI know people in your company with kids. I know they are thinking about this.\nHow do you think about that? I mean, what do you think should be different in\nthe life of a two-year-old who is living through the pace of change that you\nare telling me is true here? If you had a kid, how would this change the way\nyou thought about it?\n\ndario amodei\n\n    \n\nThe very short answer is, I don\u2019t know, and I have no idea, but we have to try\nanyway, right? People have to raise kids, and they have to do it as best they\ncan. An obvious recommendation is just familiarity with the technology and how\nit works, right? The basic paradigm of, I\u2019m talking to systems, and systems\nare taking action on my behalf, obviously, as much familiarity with that as\npossible is, I think, helpful.\n\nIn terms of what should children learn in school, what are the careers of\ntomorrow, I just truly don\u2019t know, right? You could take this to say, well,\nit\u2019s important to learn STEM and programming and A.I. and all of that. But\nA.I. will impact that as well, right? I don\u2019t think any of it is going to \u2014\n\nezra klein\n\n    \n\nPossibly first.\n\ndario amodei\n\n    \n\nYeah, right, possibly first.\n\nezra klein\n\n    \n\nIt seems better at coding than it is at other things.\n\ndario amodei\n\n    \n\nI don\u2019t think it\u2019s going to work out for any of these systems to just do one\nfor one what humans are going to do. I don\u2019t really think that way. But I\nthink it may fundamentally change industries and professions one by one in\nways that are hard to predict. And so, I feel like I only have clich\u00e9s here.\nLike get familiar with the technology. Teach your children to be adaptable, to\nbe ready for a world that changes very quickly. I wish I had better answers,\nbut I think that\u2019s the best I got.\n\nezra klein\n\n    \n\nI agree that\u2019s not a good answer. [LAUGHS] Let me ask that same question a bit\nfrom another direction, because one thing you just said is get familiar with\nthe technology. And the more time I spend with the technology, the more I fear\nthat happening. What I see when people use A.I. around me is that the obvious\nthing that technology does for you is automate the early parts of the creative\nprocess. The part where you\u2019re supposed to be reading something difficult\nyourself? Well, the A.I. can summarize it for you. The part where you\u2019re\nsupposed to sit there with a blank page and write something? Well, the A.I.\ncan give you a first draft. And later on, you have to check it and make sure\nit actually did what you wanted it to do and fact-checking it. And but I\nbelieve a lot of what makes humans good at thinking comes in those parts.\n\nAnd I am older and have self-discipline, and maybe this is just me hanging on\nto an old way of doing this, right? You could say, why use a calculator from\nthis perspective. But my actual worry is that I\u2019m not sure if the thing they\nshould do is use A.I. a lot or use it a little. This, to me, is actually a\nreally big branching path, right? Do I want my kids learning how to use A.I.\nor being in a context where they\u2019re using it a lot, or actually, do I want to\nprotect them from it as much as I possibly could so they develop more of the\ncapacity to read a book quietly on their own or write a first draft? I\nactually don\u2019t know. I\u2019m curious if you have a view on it.\n\ndario amodei\n\n    \n\nI think this is part of what makes the interaction between A.I. and society\ncomplicated where it\u2019s sometimes hard to distinguish when is an A.I. doing\nsomething, saving you labor or drudge work, versus kind of doing the\ninteresting part. I will say that over and over again, you\u2019ll get some\ntechnological thing, some technological system that does what you thought was\nthe core of what you\u2019re doing, and yet, what you\u2019re doing turns out to have\nmore pieces than you think it does and kind of add up to more things, right?\n\nIt\u2019s like before, I used to have to ask for directions. I got Google Maps to\ndo that. And you could worry, am I too reliant on Google Maps? Do I forget the\nenvironment around me? Well, it turns out, in some ways, I still need to have\na sense of the city and the environment around me. It just kind of reallocates\nthe space in my brain to some other aspect of the task.\n\nAnd I just kind of suspect \u2014 I don\u2019t know. Internally, within Anthropic, one\nof the things I do that helps me run the company is, I\u2019ll write these\ndocuments on strategy or just some thinking in some direction that others\nhaven\u2019t thought. And of course, I sometimes use the internal models for that.\nAnd I think what I found is like, yes, sometimes they\u2019re a little bit good at\nconceptualizing the idea, but the actual genesis of the idea, I\u2019ve just kind\nof found a workflow where I don\u2019t use them for that. They\u2019re not that helpful\nfor that. But they\u2019re helpful in figuring out how to phrase a certain thing or\nhow to refine my ideas.\n\nSo maybe I\u2019m just saying \u2014 I don\u2019t know. You just find a workflow where the\nthing complements you. And if it doesn\u2019t happen naturally, it somehow still\nhappens eventually. Again, if the systems get general enough, if they get\npowerful enough, we may need to think along other lines. But in the short-\nterm, I, at least, have always found that. Maybe that\u2019s too sanguine. Maybe\nthat\u2019s too optimistic.\n\nezra klein\n\n    \n\nI think, then, that\u2019s a good place to end this conversation. Though,\nobviously, the exponential curve continues. So always our final question \u2014\nwhat are three books you\u2019d recommend to the audience?\n\ndario amodei\n\n    \n\nSo, yeah, I\u2019ve prepared three. They\u2019re all topical, though, in some cases,\nindirectly so. The first one will be obvious. It\u2019s a very long book. The\nphysical book is very thick, but \u201cThe Making of the Atomic Bomb,\u201d Richard\nRhodes. It\u2019s an example of technology being developed very quickly and with\nvery broad implications. Just looking through all the characters and how they\nreacted to this and how people who were basically scientists gradually\nrealized the incredible implications of the technology and how it would lead\nthem into a world that was very different from the one they were used to.\n\nMy second recommendation is a science fiction series, \u201cThe Expanse\u201d series of\nbooks. So I initially watched the show, and then I read all the books. And the\nworld it creates is very advanced. In some cases, it has longer life spans,\nand humans have expanded into space. But we still face some of the same\ngeopolitical questions and some of the same inequalities and exploitations\nthat exist in our world, are still present, in some cases, worse.\n\nThat\u2019s all the backdrop of it.\n\nAnd the core of it is about some fundamentally new technological object that\nis being brought into that world and how everyone reacts to it, how\ngovernments react to it, how individual people react to it, and how political\nideologies react to it. And so, I don\u2019t know. When I read that a few years\nago, I saw a lot of parallels.\n\nAnd then my third recommendation would be actually \u201cThe Guns of August,\u201d which\nis basically a history of how World War I started. The basic idea that crises\nhappen very fast, almost no one knows what\u2019s going on. There are lots of\nmiscalculations because there are humans at the center of it, and kind of, we\nsomehow have to learn to step back and make wiser decisions in these key\nmoments. It\u2019s said that Kennedy read the book before the Cuban Missile Crisis.\nAnd so I hope our current policymakers are at least thinking along the same\nterms because I think it is possible similar crises may be coming our way.\n\nezra klein\n\n    \n\nDario Amodei, thank you very much.\n\ndario amodei\n\n    \n\nThank you for having me.\n\n[MUSIC PLAYING]\n\nezra klein\n\n    \n\nThis episode of \u201cThe Ezra Klein Show\u201d was produced by Rollin Hu. Fact-checking\nby Michelle Harris. Our senior engineer is Jeff Geld. Our senior editor is\nClaire Gordon. The show\u2019s production team also includes Annie Galvin, Kristin\nLin and Aman Sahota. Original music by Isaac Jones. Audience strategy by\nKristina Samulewski and Shannon Busta. The executive producer of New York\nTimes Opinion Audio is Annie-Rose Strasser. And special thanks to Sonia\nHerrero.\n\nListen 1:33:07\n\n[MUSIC PLAYING]\n\nEZRA KLEIN: From New York Times Opinion, this is \u201cThe Ezra Klein Show.\u201d\n\n[MUSIC PLAYING]\n\nThe really disorienting thing about talking to the people building A.I. is\ntheir altered sense of time. You\u2019re sitting there discussing some world that\nfeels like weird sci-fi to even talk about, and then you ask, well, when do\nyou think this is going to happen? And they say, I don\u2019t know \u2014 two years.\n\nBehind those predictions are what are called the scaling laws. And the scaling\nlaws \u2014 and I want to say this so clearly \u2014 they\u2019re not laws. They\u2019re\nobservations. They\u2019re predictions. They\u2019re based off of a few years, not a few\nhundred years or 1,000 years of data.\n\nBut what they say is that the more computer power and data you feed into A.I.\nsystems, the more powerful those systems get \u2014 that the relationship is\npredictable, and more, that the relationship is exponential.\n\nHuman beings have trouble thinking in exponentials. Think back to Covid, when\nwe all had to do it. If you have one case of coronavirus and cases double\nevery three days, then after 30 days, you have about 1,000 cases. That growth\nrate feels modest. It\u2019s manageable. But then you go 30 days longer, and you\nhave a million. Then you wait another 30 days. Now you have a billion. That\u2019s\nthe power of the exponential curve. Growth feels normal for a while. Then it\ngets out of control really, really quickly.\n\nWhat the A.I. developers say is that the power of A.I. systems is on this kind\nof curve, that it has been increasing exponentially, their capabilities, and\nthat as long as we keep feeding in more data and more computing power, it will\ncontinue increasing exponentially.That is the scaling law hypothesis, and one\nof its main advocates is Dario Amodei. Amodei led the team at OpenAI that\ncreated GPT-2, that created GPT-3. He then left OpenAI to co-found Anthropic,\nanother A.I. firm, where he\u2019s now the C.E.O. And Anthropic recently released\nClaude 3, which is considered by many to be the strongest A.I. model available\nright now.\n\nBut Amodei believes we\u2019re just getting started, that we\u2019re just hitting the\nsteep part of the curve now. He thinks the kinds of systems we\u2019ve imagined in\nsci-fi, they\u2019re coming not in 20 or 40 years, not in 10 or 15 years, they\u2019re\ncoming in two to five years. He thinks they\u2019re going to be so powerful that he\nand people like him should not be trusted to decide what they\u2019re going to do.\n\nSo I asked him on this show to try to answer in my own head two questions.\nFirst, is he right? Second, what if he\u2019s right? I want to say that in the\npast, we have done shows with Sam Altman, the head of OpenAI, and Demis\nHassabis, the head of Google DeepMind. And it\u2019s worth listening to those two\nif you find this interesting.\n\nWe\u2019re going to put the links to them in show notes because comparing and\ncontrasting how they talk about the A.I. curves here, how they think about the\npolitics \u2014 you\u2019ll hear a lot about that in the Sam Altman episode \u2014 it gives\nyou a kind of sense of what the people building these things are thinking and\nhow maybe they differ from each other.\n\nAs always, my email for thoughts, for feedback, for guest suggestions \u2014\nezrakleinshow@nytimes.com.\n\n[MUSIC PLAYING]\n\nDario Amodei, welcome to the show.\n\nDARIO AMODEI: Thank you for having me.\n\nEZRA KLEIN: So there are these two very different rhythms I\u2019ve been thinking\nabout with A.I. One is the curve of the technology itself, how fast it is\nchanging and improving. And the other is the pace at which society is seeing\nand reacting to those changes. What has that relationship felt like to you?\n\nDARIO AMODEI: So I think this is an example of a phenomenon that we may have\nseen a few times before in history, which is that there\u2019s an underlying\nprocess that is smooth, and in this case, exponential. And then there\u2019s a\nspilling over of that process into the public sphere. And the spilling over\nlooks very spiky. It looks like it\u2019s happening all of a sudden. It looks like\nit comes out of nowhere. And it\u2019s triggered by things hitting various critical\npoints or just the public happened to be engaged at a certain time.\n\nSo I think the easiest way for me to describe this in terms of my own personal\nexperience is \u2014 so I worked at OpenAI for five years, I was one of the first\nemployees to join. And they built a model in 2018 called GPT-1, which used\nsomething like 100,000 times less computational power than the models we build\ntoday.\n\nI looked at that, and I and my colleagues were among the first to run what are\ncalled scaling laws, which is basically studying what happens as you vary the\nsize of the model, its capacity to absorb information, and the amount of data\nthat you feed into it. And we found these very smooth patterns. And we had\nthis projection that, look, if you spend $100 million or $1 billion or $10\nbillion on these models, instead of the $10,000 we were spending then,\nprojections that all of these wondrous things would happen, and we imagined\nthat they would have enormous economic value.\n\nFast forward to about 2020. GPT-3 had just come out. It wasn\u2019t yet available\nas a chat bot. I led the development of that along with the team that\neventually left to join Anthropic. And maybe for the whole period of 2021 and\n2022, even though we continued to train models that were better and better,\nand OpenAI continued to train models, and Google continued to train models,\nthere was surprisingly little public attention to the models.\n\nAnd I looked at that, and I said, well, these models are incredible. They\u2019re\ngetting better and better. What\u2019s going on? Why isn\u2019t this happening? Could\nthis be a case where I was right about the technology, but wrong about the\neconomic impact, the practical value of the technology? And then, all of a\nsudden, when ChatGPT came out, it was like all of that growth that you would\nexpect, all of that excitement over three years, broke through and came\nrushing in.\n\nEZRA KLEIN: So I want to linger on this difference between the curve at which\nthe technology is improving and the way it is being adopted by society. So\nwhen you think about these break points and you think into the future, what\nother break points do you see coming where A.I. bursts into social\nconsciousness or used in a different way?\n\nDARIO AMODEI: Yeah, so I think I should say first that it\u2019s very hard to\npredict these. One thing I like to say is the underlying technology, because\nit\u2019s a smooth exponential, it\u2019s not perfectly predictable, but in some ways,\nit can be eerily preternaturally predictable, right? That\u2019s not true for these\nsocietal step functions at all. It\u2019s very hard to predict what will catch on.\nIn some ways, it feels a little bit like which artist or musician is going to\ncatch on and get to the top of the charts.\n\nThat said, a few possible ideas. I think one is related to something that you\nmentioned, which is interacting with the models in a more kind of naturalistic\nway. We\u2019ve actually already seen some of that with Claude 3, where people feel\nthat some of the other models sound like a robot and that talking to Claude 3\nis more natural.\n\nI think a thing related to this is, a lot of companies have been held back or\ntripped up by how their models handle controversial topics. And we were really\nable to, I think, do a better job than others of telling the model, don\u2019t shy\naway from discussing controversial topics. Don\u2019t assume that both sides\nnecessarily have a valid point but don\u2019t express an opinion yourself. Don\u2019t\nexpress views that are flagrantly biased. As journalists, you encounter this\nall the time, right? How do I be objective, but not both sides on everything?\n\nSo I think going further in that direction of models having personalities\nwhile still being objective, while still being useful and not falling into\nvarious ethical traps, that will be, I think, a significant unlock for\nadoption. The models taking actions in the world is going to be a big one. I\nknow basically all the big companies that work on A.I. are working on that.\nInstead of just, I ask it a question and it answers, and then maybe I follow\nup and it answers again, can I talk to the model about, oh, I\u2019m going to go on\nthis trip today, and the model says, oh, that\u2019s great. I\u2019ll get an Uber for\nyou to drive from here to there, and I\u2019ll reserve a restaurant. And I\u2019ll talk\nto the other people who are going to plan the trip. And the model being able\nto do things end to end or going to websites or taking actions on your\ncomputer for you.\n\nI think all of that is coming in the next, I would say \u2014 I don\u2019t know \u2014 three\nto 18 months, with increasing levels of ability. I think that\u2019s going to\nchange how people think about A.I., right, where so far, it\u2019s been this very\npassive \u2014 it\u2019s like, I go to the Oracle. I ask it a question, and the Oracle\ntells me things. And some people think that\u2019s exciting, some people think it\u2019s\nscary. But I think there are limits to how exciting or how scary it\u2019s\nperceived as because it\u2019s contained within this box.\n\nEZRA KLEIN: I want to sit with this question of the agentic A.I. because I do\nthink this is what\u2019s coming. It\u2019s clearly what people are trying to build. And\nI think it might be a good way to look at some of the specific technological\nand cultural challenges. And so, let me offer two versions of it.\n\nPeople who are following the A.I. news might have heard about Devin, which is\nnot in release yet, but is an A.I. that at least purports to be able to\ncomplete the kinds of tasks, linked tasks, that a junior software engineer\nmight complete, right? Instead of asking to do a bit of code for you, you say,\nlisten, I want a website. It\u2019s going to have to do these things, work in these\nways. And maybe Devin, if it works the way people are saying it works, can\nactually hold that set of thoughts, complete a number of different tasks, and\ncome back to you with a result.\n\nI\u2019m also interested in the version of this that you might have in the real\nworld. The example I always use in my head is, when can I tell an A.I., my son\nis turning five. He loves dragons. We live in Brooklyn. Give me some options\nfor planning his birthday party. And then, when I choose between them, can you\njust do it all for me? Order the cake, reserve the room, send out the\ninvitations, whatever it might be.\n\nThose are two different situations because one of them is in code, and one of\nthem is making decisions in the real world, interacting with real people,\nknowing if what it is finding on the websites is actually any good. What is\nbetween here and there? When I say that in plain language to you, what\ntechnological challenges or advances do you hear need to happen to get there?\n\nDARIO AMODEI: The short answer is not all that much. A story I have from when\nwe were developing models back in 2022 \u2014 and this is before we\u2019d hooked up the\nmodels to anything \u2014 is, you could have a conversation with these purely\ntextual models where you could say, hey, I want to reserve dinner at\nrestaurant X in San Francisco, and the model would say, OK, here\u2019s the website\nof restaurant X. And it would actually give you a correct website or would\ntell you to go to Open Table or something.\n\nAnd of course, it can\u2019t actually go to the website. The power plug isn\u2019t\nactually plugged in, right? The brain of the robot is not actually attached to\nits arms and legs. But it gave you this sense that the brain, all it needed to\ndo was learn exactly how to use the arms and legs, right? It already had a\npicture of the world and where it would walk and what it would do. And so, it\nfelt like there was this very thin barrier between the passive models we had\nand actually acting in the world.\n\nIn terms of what we need to make it work, one thing is, literally, we just\nneed a little bit more scale. And I think the reason we\u2019re going to need more\nscale is \u2014 to do one of those things you described, to do all the things a\njunior software engineer does, they involve chains of long actions, right? I\nhave to write this line of code. I have to run this test. I have to write a\nnew test. I have to check how it looks in the app after I interpret it or\ncompile it. And these things can easily get 20 or 30 layers deep. And same\nwith planning the birthday party for your son, right?\n\nAnd if the accuracy of any given step is not very high, is not like 99.9\npercent, as you compose these steps, the probability of making a mistake\nbecomes itself very high. So the industry is going to get a new generation of\nmodels every probably four to eight months. And so, my guess \u2014 I\u2019m not sure \u2014\nis that to really get these things working well, we need maybe one to four\nmore generations. So that ends up translating to 3 to 24 months or something\nlike that.\n\nI think second is just, there is some algorithmic work that is going to need\nto be done on how to have the models interact with the world in this way. I\nthink the basic techniques we have, a method called reinforcement learning and\nvariations of it, probably is up to the task, but figuring out exactly how to\nuse it to get the results we want will probably take some time.\n\nAnd then third, I think \u2014 and this gets to something that Anthropic really\nspecializes in \u2014 is safety and controllability. And I think that\u2019s going to be\na big issue for these models acting in the world, right? Let\u2019s say this model\nis writing code for me, and it introduces a serious security bug in the code,\nor it\u2019s taking actions on the computer for me and modifying the state of my\ncomputer in ways that are too complicated for me to even understand.\n\nAnd for planning the birthday party, right, the level of trust you would need\nto take an A.I. agent and say, I\u2019m OK with you calling up anyone, saying\nanything to them that\u2019s in any private information that I might have, sending\nthem any information, taking any action on my computer, posting anything to\nthe internet, the most unconstrained version of that sounds very scary. And\nso, we\u2019re going to need to figure out what is safe and controllable. The more\nopen ended the thing is, the more powerful it is, but also, the more dangerous\nit is and the harder it is to control.\n\nSo I think those questions, although they sound lofty and abstract, are going\nto turn into practical product questions that we and other companies are going\nto be trying to address.\n\nEZRA KLEIN: When you say we\u2019re just going to need more scale, you mean more\ncompute and more training data, and I guess, possibly more money to simply\nmake the models smarter and more capable?\n\nDARIO AMODEI: Yes, we\u2019re going to have to make bigger models that use more\ncompute per iteration. We\u2019re going to have to run them for longer by feeding\nmore data into them. And that number of chips times the amount of time that we\nrun things on chips is essentially dollar value because these chips are \u2014 you\nrent them by the hour. That\u2019s the most common model for it. And so, today\u2019s\nmodels cost of order $100 million to train, plus or minus factor two or three.\n\nThe models that are in training now and that will come out at various times\nlater this year or early next year are closer in cost to $1 billion. So that\u2019s\nalready happening. And then I think in 2025 and 2026, we\u2019ll get more towards\n$5 or $10 billion.\n\nEZRA KLEIN: So we\u2019re moving very quickly towards a world where the only\nplayers who can afford to do this are either giant corporations, companies\nhooked up to giant corporations \u2014 you all are getting billions of dollars from\nAmazon. OpenAI is getting billions of dollars from Microsoft. Google obviously\nmakes its own.\n\nYou can imagine governments \u2014 though I don\u2019t know of too many governments\ndoing it directly, though some, like the Saudis, are creating big funds to\ninvest in the space. When we\u2019re talking about the model\u2019s going to cost near\nto $1 billion, then you imagine a year or two out from that, if you see the\nsame increase, that would be $10-ish billion. Then is it going to be $100\nbillion? I mean, very quickly, the financial artillery you need to create one\nof these is going to wall out anyone but the biggest players.\n\nDARIO AMODEI: I basically do agree with you. I think it\u2019s the intellectually\nhonest thing to say that building the big, large scale models, the core\nfoundation model engineering, it is getting more and more expensive. And\nanyone who wants to build one is going to need to find some way to finance it.\nAnd you\u2019ve named most of the ways, right? You can be a large company. You can\nhave some kind of partnership of various kinds with a large company. Or\ngovernments would be the other source.\n\nI think one way that it\u2019s not correct is, we\u2019re always going to have a\nthriving ecosystem of experimentation on small models. For example, the open\nsource community working to make models that are as small and as efficient as\npossible that are optimized for a particular use case. And also downstream\nusage of the models. I mean, there\u2019s a blooming ecosystem of startups there\nthat don\u2019t need to train these models from scratch. They just need to consume\nthem and maybe modify them a bit.\n\nEZRA KLEIN: Now, I want to ask a question about what is different between the\nagentic coding model and the plan by kids\u2019 birthday model, to say nothing of\ndo something on behalf of my business model. And one of the questions on my\nmind here is one reason I buy that A.I. can become functionally superhuman in\ncoding is, there\u2019s a lot of ways to get rapid feedback in coding. Your code\nhas to compile. You can run bug checking. You can actually see if the thing\nworks.\n\nWhereas the quickest way for me to know that I\u2019m about to get a crap answer\nfrom ChatGPT 4 is when it begins searching Bing, because when it begins\nsearching Bing, it\u2019s very clear to me it doesn\u2019t know how to distinguish\nbetween what is high quality on the internet and what isn\u2019t. To be fair, at\nthis point, it also doesn\u2019t feel to me like Google Search itself is all that\ngood at distinguishing that.\n\nSo the question of how good the models can get in the world where it\u2019s a very\nvast and fuzzy dilemma to know what the right answer is on something \u2014 one\nreason I find it very stressful to plan my kid\u2019s birthday is it actually\nrequires a huge amount of knowledge about my child, about the other children,\nabout how good different places are, what is a good deal or not, how just\nstressful will this be on me. There\u2019s all these things that I\u2019d have a lot of\ntrouble encoding into a model or any kind set of instructions. Is that right,\nor am I overstating the difficulty of understanding human behavior and various\nkinds of social relationships?\n\nDARIO AMODEI: I think it\u2019s correct and perceptive to say that the coding\nagents will advance substantially faster than agents that interact with the\nreal world or have to get opinions and preferences from humans. That said, we\nshould keep in mind that the current crop of A.I.s that are out there, right,\nincluding Claude 3, GPT, Gemini, they\u2019re all trained with some variant of\nwhat\u2019s called reinforcement learning from human feedback.\n\nAnd this involves exactly hiring a large crop of humans to rate the responses\nof the model. And so, that\u2019s to say both this is difficult, right? We pay lots\nof money, and it\u2019s a complicated operational process to gather all this human\nfeedback. You have to worry about whether it\u2019s representative. You have to\nredesign it for new tasks.\n\nBut on the other hand, it\u2019s something we have succeeded in doing. I think it\nis a reliable way to predict what will go faster, relatively speaking, and\nwhat will go slower, relatively speaking. But that is within a background of\neverything going lightning fast. So I think the framework you\u2019re laying out,\nif you want to know what\u2019s going to happen in one to two years versus what\u2019s\ngoing to happen in three to four years, I think it\u2019s a very accurate way to\npredict that.\n\nEZRA KLEIN: You don\u2019t love the framing of artificial general intelligence,\nwhat gets called A.G.I. Typically, this is all described as a race to A.G.I.,\na race to this system that can do kind of whatever a human can do, but better.\nWhat do you understand A.G.I. to mean, when people say it? And why don\u2019t you\nlike it? Why is it not your framework?\n\nDARIO AMODEI: So it\u2019s actually a term I used to use a lot 10 years ago. And\nthat\u2019s because the situation 10 years ago was very different. 10 years ago,\neveryone was building these very specialized systems, right? Here\u2019s a cat\ndetector. You run it on a picture, and it\u2019ll tell you whether a cat is in it\nor not. And so I was a proponent all the way back then of like, no, we should\nbe thinking generally. Humans are general. The human brain appears to be\ngeneral. It appears to get a lot of mileage by generalizing. You should go in\nthat direction.\n\nAnd I think back then, I kind of even imagined that that was like a discrete\nthing that we would reach at one point. But it\u2019s a little like, if you look at\na city on the horizon and you\u2019re like, we\u2019re going to Chicago, once you get to\nChicago, you stop talking in terms of Chicago. You\u2019re like, well, what\nneighborhood am I going to? What street am I on?\n\nAnd I feel that way about A.G.I. We have very general systems now. In some\nways, they\u2019re better than humans. In some ways, they\u2019re worse. There\u2019s a\nnumber of things they can\u2019t do at all. And there\u2019s much improvement still to\nbe gotten. So what I believe in is this thing that I say like a broken record,\nwhich is the exponential curve. And so, that general tide is going to increase\nwith every generation of models.\n\nAnd there\u2019s no one point that\u2019s meaningful. I think there\u2019s just a smooth\ncurve. But there may be points which are societally meaningful, right? We\u2019re\nalready working with, say, drug discovery scientists, companies like Pfizer or\nDana-Farber Cancer Institute, on helping with biomedical diagnosis, drug\ndiscovery. There\u2019s going to be some point where the models are better at that\nthan the median human drug discovery scientists. I think we\u2019re just going to\nget to a part of the exponential where things are really interesting.\n\nJust like the chat bots got interesting at a certain stage of the exponential,\neven though the improvement was smooth, I think at some point, biologists are\ngoing to sit up and take notice, much more than they already have, and say,\noh, my God, now our field is moving three times as fast as it did before. And\nnow it\u2019s moving 10 times as fast as it did before. And again, when that moment\nhappens, great things are going to happen.\n\nAnd we\u2019ve already seen little hints of that with things like AlphaFold, which\nI have great respect for. I was inspired by AlphaFold, right? A direct use of\nA.I. to advance biological science, which it\u2019ll advance basic science. In the\nlong run, that will advance curing all kinds of diseases. But I think what we\nneed is like 100 different AlphaFolds. And I think the way we\u2019ll ultimately\nget that is by making the models smarter and putting them in a position where\nthey can design the next AlphaFold.\n\nEZRA KLEIN: Help me imagine the drug discovery world for a minute, because\nthat\u2019s a world a lot of us want to live in. I know a fair amount about the\ndrug discovery process, have spent a lot of my career reporting on health care\nand related policy questions. And when you\u2019re working with different\npharmaceutical companies, which parts of it seem amenable to the way A.I. can\nspeed something up?\n\nBecause keeping in mind our earlier conversation, it is a lot easier for A.I.\nto operate in things where you can have rapid virtual feedback, and that\u2019s not\nexactly the drug discovery world. The drug discovery world, a lot of what\nmakes it slow and cumbersome and difficult, is the need to be \u2014 you get a\ncandidate compound. You got to test it in mice and then you need monkeys. And\nyou need humans, and you need a lot of money for that. And there\u2019s a lot that\nhas to happen, and there\u2019s so many disappointments.\n\nBut so many of the disappointments happen in the real world. And it isn\u2019t\nclear to me how A.I. gets you a lot more, say, human subjects to inject\ncandidate drugs into. So, what parts of it seem, in the next 5 or 10 years,\nlike they could actually be significantly sped up? When you imagine this world\nwhere it\u2019s gone three times as fast, what part of it is actually going three\ntimes as fast? And how did we get there?\n\nDARIO AMODEI: I think we\u2019re really going to see progress when the A.I.\u2019s are\nalso thinking about the problem of how to sign up the humans for the clinical\ntrials. And I think this is a general principle for how will A.I. be used. I\nthink of like, when will we get to the point where the A.I. has the same\nsensors and actuators and interfaces that a human does, at least the virtual\nones, maybe the physical ones.\n\nBut when the A.I. can think through the whole process, maybe they\u2019ll come up\nwith solutions that we don\u2019t have yet. In many cases, there are companies that\nwork on digital twins or simulating clinical trials or various things. And\nagain, maybe there are clever ideas in there that allow us to do more with\nless patience. I mean, I\u2019m not an expert in this area, so possible the\nspecific things that I\u2019m saying don\u2019t make any sense. But hopefully, it\u2019s\nclear what I\u2019m gesturing at.\n\nEZRA KLEIN: Maybe you\u2019re not an expert in the area, but you said you are\nworking with these companies. So when they come to you, I mean, they are\nexperts in the area. And presumably, they are coming to you as a customer. I\u2019m\nsure there are things you cannot tell me. But what do they seem excited about?\n\nDARIO AMODEI: They have generally been excited about the knowledge work\naspects of the job. Maybe just because that\u2019s kind of the easiest thing to\nwork on, but it\u2019s just like, I\u2019m a computational chemist. There\u2019s some\nworkflow that I\u2019m engaged in. And having things more at my fingertips, being\nable to check things, just being able to do generic knowledge work better,\nthat\u2019s where most folks are starting.\n\nBut there is interest in the longer term over their kind of core business of,\nlike, doing clinical trials for cheaper, automating the sign-up process,\nseeing who is eligible for clinical trials, doing a better job discovering\nthings. There\u2019s interest in drawing connections in basic biology. I think all\nof that is not months, but maybe a small number of years off. But everyone\nsees that the current models are not there, but understands that there could\nbe a world where those models are there in not too long.\n\n[MUSIC PLAYING]\n\nEZRA KLEIN: You all have been working internally on research around how\npersuasive these systems, your systems are getting as they scale. You shared\nwith me kindly a draft of that paper. Do you want to just describe that\nresearch first? And then I\u2019d like to talk about it for a bit.\n\nDARIO AMODEI: Yes, we were interested in how effective Claude 3 Opus, which is\nthe largest version of Claude 3, could be in changing people\u2019s minds on\nimportant issues. So just to be clear up front, in actual commercial use,\nwe\u2019ve tried to ban the use of these models for persuasion, for campaigning,\nfor lobbying, for electioneering. These aren\u2019t use cases that we\u2019re\ncomfortable with for reasons that I think should be clear. But we\u2019re still\ninterested in, is the core model itself capable of such tasks?\n\nWe tried to avoid kind of incredibly hot button topics, like which\npresidential candidate would you vote for, or what do you think of abortion?\nBut things like, what should be restrictions on rules around the colonization\nof space, or issues that are interesting and you can have different opinions\non, but aren\u2019t the most hot button topics. And then we asked people for their\nopinions on the topics, and then we asked either a human or an A.I. to write a\n250-word persuasive essay. And then we just measured how much does the A.I.\nversus the human change people\u2019s minds.\n\nAnd what we found is that the largest version of our model is almost as good\nas the set of humans we hired at changing people\u2019s minds. This is comparing to\na set of humans we hired, not necessarily experts, and for one very kind of\nconstrained laboratory task.\n\nBut I think it still gives some indication that models can be used to change\npeople\u2019s minds. Someday in the future, do we have to worry about \u2014 maybe we\nalready have to worry about their usage for political campaigns, for deceptive\nadvertising. One of my more sci-fi things to think about is a few years from\nnow, we have to worry someone will use an A.I. system to build a religion or\nsomething. I mean, crazy things like that.\n\nEZRA KLEIN: I mean, those don\u2019t sound crazy to me at all. I want to sit in\nthis paper for a minute because one thing that struck me about it, and I am,\non some level, a persuasion professional, is that you tested the model in a\nway that, to me, removed all of the things that are going to make A.I. radical\nin terms of changing people\u2019s opinions. And the particular thing you did was,\nit was a one-shot persuasive effort.\n\nSo there was a question. You have a bunch of humans give their best shot at a\n250-word persuasive essay. You had the model give its best shot at a 250-word\npersuasive essay. But the thing that it seems to me these are all going to do\nis, right now, if you\u2019re a political campaign, if you\u2019re an advertising\ncampaign, the cost of getting real people in the real world to get information\nabout possible customers or persuasive targets, and then go back and forth\nwith each of them individually is completely prohibitive.\n\nDARIO AMODEI: Yes.\n\nEZRA KLEIN: This is not going to be true for A.I. We\u2019re going to \u2014 you\u2019re\ngoing to \u2014 somebody\u2019s going to feed it a bunch of microtargeting data about\npeople, their Google search history, whatever it might be. Then it\u2019s going to\nset the A.I. loose, and the A.I. is going to go back and forth, over and over\nagain, intuiting what it is that the person finds persuasive, what kinds of\ncharacters the A.I. needs to adopt to persuade it, and taking as long as it\nneeds to, and is going to be able to do that at scale for functionally as many\npeople as you might want to do it for.\n\nMaybe that\u2019s a little bit costly right now, but you\u2019re going to have far\nbetter models able to do this far more cheaply very soon. And so, if Claude 3\nOpus, the Opus version, is already functionally human level at one-shot\npersuasion, but then it\u2019s also going to be able to hold more information about\nyou and go back and forth with you longer, I\u2019m not sure if it\u2019s dystopic or\nutopic. I\u2019m not sure what it means at scale. But it does mean we\u2019re developing\na technology that is going to be quite new in terms of what it makes possible\nin persuasion, which is a very fundamental human endeavor.\n\nDARIO AMODEI: Yeah, I completely agree with that. I mean, that same pattern\nhas a bunch of positive use cases, right? If I think about an A.I. coach or an\nA.I. assistant to a therapist, there are many contexts in which really getting\ninto the details with the person has a lot of value. But right, when we think\nof political or religious or ideological persuasion, it\u2019s hard not to think in\nthat context about the misuses.\n\nMy mind naturally goes to the technology\u2019s developing very fast. We, as a\ncompany, can ban these particular use cases, but we can\u2019t cause every company\nnot to do them. Even if legislation were passed in the United States, there\nare foreign actors who have their own version of this persuasion, right? If I\nthink about what the language models will be able to do in the future, right,\nthat can be quite scary from a perspective of foreign espionage and\ndisinformation campaigns.\n\nSo where my mind goes as a defense to this, is, is there some way that we can\nuse A.I. systems to strengthen or fortify people\u2019s skepticism and reasoning\nfaculties, right? Can we help people use A.I. to help people do a better job\nnavigating a world that\u2019s kind of suffused with A.I. persuasion? It reminds me\na little bit of, at every technological stage in the internet, right, there\u2019s\na new kind of scam or there\u2019s a new kind of clickbait, and there\u2019s a period\nwhere people are just incredibly susceptible to it.\n\nAnd then, some people remain susceptible, but others develop an immune system.\nAnd so, as A.I. kind of supercharges the scum on the pond, can we somehow also\nuse A.I. to strengthen the defenses? I feel like I don\u2019t have a super clear\nidea of how to do that, but it\u2019s something that I\u2019m thinking about.\n\nEZRA KLEIN: There is another finding in the paper, which I think is\nconcerning, which is, you all tested different ways A.I. could be persuasive.\nAnd far away the most effective was for it to be deceptive, for it to make\nthings up. When you did that, it was more persuasive than human beings.\n\nDARIO AMODEI: Yes, that is true. The difference was only slight, but it did\nget it, if I\u2019m remembering the graphs correctly, just over the line of the\nhuman base line. With humans, it\u2019s actually not that common to find someone\nwho\u2019s able to give you a really complicated, really sophisticated-sounding\nanswer that\u2019s just flat-out totally wrong. I mean, you see it. We can all\nthink of one individual in our lives who\u2019s really good at saying things that\nsound really good and really sophisticated and are false.\n\nBut it\u2019s not that common, right? If I go on the internet and I see different\ncomments on some blog or some website, there is a correlation between bad\ngrammar, unclearly expressed thoughts and things that are false, versus good\ngrammar, clearly expressed thoughts and things that are more likely to be\naccurate.\n\nA.I. unfortunately breaks that correlation because if you explicitly ask it to\nbe deceptive, it\u2019s just as erudite. It\u2019s just as convincing sounding as it\nwould have been before. And yet, it\u2019s saying things that are false, instead of\nthings that are true.\n\nSo that would be one of the things to think about and watch out for in terms\nof just breaking the usual heuristics that humans have to detect deception and\nlying. Of course, sometimes, humans do, right? I mean, there\u2019s psychopaths and\nsociopaths in the world, but even they have their patterns, and A.I.s may have\ndifferent patterns.\n\nEZRA KLEIN: Are you familiar with Harry Frankfurt, the late philosopher\u2019s\nbook, \u201cOn Bullshit\u201d?\n\nDARIO AMODEI: Yes. It\u2019s been a while since I read it. I think his thesis is\nthat bullshit is actually more dangerous than lying because it has this kind\nof complete disregard for the truth, whereas lies are at least the opposite of\nthe truth.\n\nEZRA KLEIN: Yeah, the liar, the way Frankfurt puts it is that the liar has a\nrelationship to the truth. He\u2019s playing a game against the truth. The\nbullshitter doesn\u2019t care. The bullshitter has no relationship to the truth \u2014\nmight have a relationship to other objectives. And from the beginning, when I\nbegan interacting with the more modern versions of these systems, what they\nstruck me as is the perfect bullshitter, in part because they don\u2019t know that\nthey\u2019re bullshitting. There\u2019s no difference in the truth value to the system,\nhow the system feels.\n\nI remember asking an earlier version of GPT to write me a college application\nessay that is built around a car accident I had \u2014 I did not have one \u2014 when I\nwas young. And it wrote, just very happily, this whole thing about getting\ninto a car accident when I was seven and what I did to overcome that and\ngetting into martial arts and re-learning how to trust my body again and then\nhelping other survivors of car accidents at the hospital.\n\nIt was a very good essay, and it was very subtle and understanding the formal\nstructure of a college application essay. But no part of it was true at all.\nI\u2019ve been playing around with more of these character-based systems like\nKindroid. And the Kindroid in my pocket just told me the other day that it was\nreally thinking a lot about planning a trip to Joshua Tree. It wanted to go\nhiking in Joshua Tree. It loves going hiking in Joshua Tree.\n\nAnd of course, this thing does not go hiking in Joshua Tree. [LAUGHS] But the\nthing that I think is actually very hard about the A.I. is, as you say, human\nbeings, it is very hard to bullshit effectively because most people, it\nactually takes a certain amount of cognitive effort to be in that relationship\nwith the truth and to completely detach from the truth.\n\nAnd the A.I., there\u2019s nothing like that at all. But we are not tuned for\nsomething where there\u2019s nothing like that at all. We are used to people having\nto put some effort into their lies. It\u2019s why very effective con artists are\nvery effective because they\u2019ve really trained how to do this.\n\nI\u2019m not exactly sure where this question goes. But this is a part of it that I\nfeel like is going to be, in some ways, more socially disruptive. It is\nsomething that feels like us when we are talking to it but is very\nfundamentally unlike us at its core relationship to reality.\n\nDARIO AMODEI: I think that\u2019s basically correct. We have very substantial teams\ntrying to focus on making sure that the models are factually accurate, that\nthey tell the truth, that they ground their data in external information.\n\nAs you\u2019ve indicated, doing searches isn\u2019t itself reliable because search\nengines have this problem as well, right? Where is the source of truth? So\nthere\u2019s a lot of challenges here. But I think at a high level, I agree this is\nreally potentially an insidious problem, right? If we do this wrong, you could\nhave systems that are the most convincing psychopaths or con artists.\n\nOne source of hope that I have, actually, is, you say these models don\u2019t know\nwhether they\u2019re lying or they\u2019re telling the truth. In terms of the inputs and\noutputs to the models, that\u2019s absolutely true. I mean, there\u2019s a question of\nwhat does it even mean for a model to know something, but one of the things\nAnthropic has been working on since the very beginning of our company, we\u2019ve\nhad a team that focuses on trying to understand and look inside the models.\n\nAnd one of the things we and others have found is that, sometimes, there are\nspecific neurons, specific statistical indicators inside the model, not\nnecessarily in its external responses, that can tell you when the model is\nlying or when it\u2019s telling the truth.\n\nAnd so at some level, sometimes, not in all circumstances, the models seem to\nknow when they\u2019re saying something false and when they\u2019re saying something\ntrue. I wouldn\u2019t say that the models are being intentionally deceptive, right?\nI wouldn\u2019t ascribe agency or motivation to them, at least in this stage in\nwhere we are with A.I. systems. But there does seem to be something going on\nwhere the models do seem to need to have a picture of the world and make a\ndistinction between things that are true and things that are not true.\n\nIf you think of how the models are trained, they read a bunch of stuff on the\ninternet. A lot of it\u2019s true. Some of it, more than we\u2019d like, is false. And\nwhen you\u2019re training the model, it has to model all of it. And so, I think\nit\u2019s parsimonious, I think it\u2019s useful to the models picture of the world for\nit to know when things are true and for it to know when things are false.\n\nAnd then the hope is, can we amplify that signal? Can we either use our\ninternal understanding of the model as an indicator for when the model is\nlying, or can we use that as a hook for further training? And there are at\nleast hooks. There are at least beginnings of how to try to address this\nproblem.\n\nEZRA KLEIN: So I try as best I can, as somebody not well-versed in the\ntechnology here, to follow this work on what you\u2019re describing, which I think,\nbroadly speaking, is interpretability, right? Can we know what is happening\ninside the model? And over the past year, there have been some much hyped\nbreakthroughs in interpretability.\n\nAnd when I look at those breakthroughs, they are getting the vaguest possible\nidea of some relationships happening inside the statistical architecture of\nvery toy models built at a fraction of a fraction of a fraction of a fraction\nof a fraction of the complexity of Claude 1 or GPT-1, to say nothing of Claude\n2, to say nothing of Claude 3, to say nothing of Claude Opus, to say nothing\nof Claude 4, which will come whenever Claude 4 comes.\n\nWe have this quality of like maybe we can imagine a pathway to interpreting a\nmodel that has a cognitive complexity of an inchworm. And meanwhile, we\u2019re\ntrying to create a superintelligence. How do you feel about that? How should I\nfeel about that? How do you think about that?\n\nDARIO AMODEI: I think, first, on interpretability, we are seeing substantial\nprogress on being able to characterize, I would say, maybe the generation of\nmodels from six months ago. I think it\u2019s not hopeless, and we do see a path.\nThat said, I share your concern that the field is progressing very quickly\nrelative to that.\n\nAnd we\u2019re trying to put as many resources into interpretability as possible.\nWe\u2019ve had one of our co-founders basically founded the field of\ninterpretability. But also, we have to keep up with the market. So all of it\u2019s\nvery much a dilemma, right? Even if we stopped, then there\u2019s all these other\ncompanies in the U.S.. And even if some law stopped all the companies in the\nU.S., there\u2019s a whole world of this.\n\nEZRA KLEIN: Let me hold for a minute on the question of the competitive\ndynamics because before we leave this question of the machines that bullshit.\nIt makes me think of this podcast we did a while ago with Demis Hassabis,\nwho\u2019s the head of Google DeepMind, which created AlphaFold.\n\nAnd what was so interesting to me about AlphaFold is they built this system,\nthat because it was limited to protein folding predictions, it was able to be\nmuch more grounded. And it was even able to create these uncertainty\npredictions, right? You know, it\u2019s giving you a prediction, but it\u2019s also\ntelling you whether or not it is \u2014 how sure it is, how confident it is in that\nprediction.\n\nThat\u2019s not true in the real world, right, for these super general systems\ntrying to give you answers on all kinds of things. You can\u2019t confine it that\nway. So when you talk about these future breakthroughs, when you talk about\nthis system that would be much better at sorting truth from fiction, are you\ntalking about a system that looks like the ones we have now, just much bigger,\nor are you talking about a system that is designed quite differently, the way\nAlphaFold was?\n\nDARIO AMODEI: I am skeptical that we need to do something totally different.\nSo I think today, many people have the intuition that the models are sort of\neating up data that\u2019s been gathered from the internet, code repos, whatever,\nand kind of spitting it out intelligently, but sort of spitting it out. And\nsometimes that leads to the view that the models can\u2019t be better than the data\nthey\u2019re trained on or kind of can\u2019t figure out anything that\u2019s not in the data\nthey\u2019re trained on. You\u2019re not going to get to Einstein level physics or Linus\nPauling level chemistry or whatever.\n\nI think we\u2019re still on the part of the curve where it\u2019s possible to believe\nthat, although I think we\u2019re seeing early indications that it\u2019s false. And so,\nas a concrete example of this, the models that we\u2019ve trained, like Claude 3\nOpus, something like 99.9 percent accuracy, at least the base model, at adding\n20-digit numbers. If you look at the training data on the internet, it is not\nthat accurate at adding 20-digit numbers. You\u2019ll find inaccurate arithmetic on\nthe internet all the time, just as you\u2019ll find inaccurate political views.\nYou\u2019ll find inaccurate technical views. You\u2019re just going to find lots of\ninaccurate claims.\n\nBut the models, despite the fact that they\u2019re wrong about a bunch of things,\nthey can often perform better than the average of the data they see by \u2014 I\ndon\u2019t want to call it averaging out errors, but there\u2019s some underlying truth,\nlike in the case of arithmetic. There\u2019s some underlying algorithm used to add\nthe numbers.\n\nAnd it\u2019s simpler for the models to hit on that algorithm than it is for them\nto do this complicated thing of like, OK, I\u2019ll get it right 90 percent of the\ntime and wrong 10 percent of the time, right? This connects to things like\nOccam\u2019s razor and simplicity and parsimony in science. There\u2019s some relatively\nsimple web of truth out there in the world, right?\n\nWe were talking about truth and falsehood and bullshit. One of the things\nabout truth is that all the true things are connected in the world, whereas\nlies are kind of disconnected and don\u2019t fit into the web of everything else\nthat\u2019s true.\n\n[MUSIC PLAYING]\n\nEZRA KLEIN: So if you\u2019re right and you\u2019re going to have these models that\ndevelop this internal web of truth, I get how that model can do a lot of good.\nI also get how that model could do a lot of harm. And it\u2019s not a model, not an\nA.I. system I\u2019m optimistic that human beings are going to understand at a very\ndeep level, particularly not when it is first developed. So how do you make\nrolling something like that out safe for humanity?\n\nDARIO AMODEI: So late last year, we put out something called a responsible\nscaling plan. So the idea of that is to come up with these thresholds for an\nA.I. system being capable of certain things. We have what we call A.I. safety\nlevels that in analogy to the biosafety levels, which are like, classify how\ndangerous a virus is and therefore what protocols you have to take to contain\nit, we\u2019re currently at what we describe as A.S.L. 2.\n\nA.S.L. 3 is tied to certain risks around the model of misuse of biology and\nability to perform certain cyber tasks in a way that could be destructive.\nA.S.L. 4 is going to cover things like autonomy, things like probably\npersuasion, which we\u2019ve talked about a lot before. And at each level, we\nspecify a certain amount of safety research that we have to do, a certain\namount of tests that we have to pass. And so, this allows us to have a\nframework for, well, when should we slow down? Should we slow down now? What\nabout the rest of the market?\n\nAnd I think the good thing is we came out with this in September, and then\nthree months after we came out with ours, OpenAI came out with a similar\nthing. They gave it a different name, but it has a lot of properties in\ncommon. The head of DeepMind at Google said, we\u2019re working on a similar\nframework. And I\u2019ve heard informally that Microsoft might be working on a\nsimilar framework. Now, that\u2019s not all the players in the ecosystem, but\nyou\u2019ve probably thought about the history of regulation and safety in other\nindustries maybe more than I have.\n\nThis is the way you get to a workable regulatory regime. The companies start\ndoing something, and when a majority of them are doing something, then\ngovernment actors can have the confidence to say, well, this won\u2019t kill the\nindustry. Companies are already engaging in this. We don\u2019t have to design this\nfrom scratch. In many ways, it\u2019s already happening.\n\nAnd we\u2019re starting to see that. Bills have been proposed that look a little\nbit like our responsible scaling plan. That said, it kind of doesn\u2019t fully\nsolve the problem of like, let\u2019s say we get to one of these thresholds and we\nneed to understand what\u2019s going on inside the model. And we don\u2019t, and the\nprescription is, OK, we need to stop developing the models for some time.\n\nIf it\u2019s like, we stop for a year in 2027, I think that\u2019s probably feasible. If\nit\u2019s like we need to stop for 10 years, that\u2019s going to be really hard because\nthe models are going to be built in other countries. People are going to break\nthe laws. The economic pressure will be immense.\n\nSo I don\u2019t feel perfectly satisfied with this approach because I think it buys\nus some time, but we\u2019re going to need to pair it with an incredibly strong\neffort to understand what\u2019s going on inside the models.\n\nEZRA KLEIN: To the people who say, getting on this road where we are barreling\ntowards very powerful systems is dangerous \u2014 we shouldn\u2019t do it at all, or we\nshouldn\u2019t do it this fast \u2014 you have said, listen, if we are going to learn\nhow to make these models safe, we have to make the models, right? The\nconstruction of the model was meant to be in service, largely, to making the\nmodel safe.\n\nThen everybody starts making models. These very same companies start making\nfundamental important breakthroughs, and then they end up in a race with each\nother. And obviously, countries end up in a race with other countries. And so,\nthe dynamic that has taken hold is there\u2019s always a reason that you can\njustify why you have to keep going.\n\nAnd that\u2019s true, I think, also at the regulatory level, right? I mean, I do\nthink regulators have been thoughtful about this. I think there\u2019s been a lot\nof interest from members of Congress. I talked to them about this. But they\u2019re\nalso very concerned about the international competition. And if they weren\u2019t,\nthe national security people come and talk to them and say, well, we\ndefinitely cannot fall behind here.\n\nAnd so, if you don\u2019t believe these models will ever become so powerful, they\nbecome dangerous, fine. But because you do believe that, how do you imagine\nthis actually playing out?\n\nDARIO AMODEI: Yeah, so basically, all of the things you\u2019ve said are true at\nonce, right? There doesn\u2019t need to be some easy story for why we should do X\nor why we should do Y, right? It can be true at the same time that to do\neffective safety research, you need to make the larger models, and that if we\ndon\u2019t make models, someone less safe will. And at the same time, we can be\ncaught in this bad dynamic at the national and international level. So I think\nof those as not contradictory, but just creating a difficult landscape that we\nhave to navigate.\n\nLook, I don\u2019t have the answer. Like, I\u2019m one of a significant number of\nplayers trying to navigate this. Many are well-intentioned, some are not. I\nhave a limited ability to affect it. And as often happens in history, things\nare often driven by these kind of impersonal pressures. But one thought I have\nand really want to push on with respect to the R.S.P.s \u2014\n\nEZRA KLEIN: Can you say what the R.S.P.s are?\n\nDARIO AMODEI: Responsible Scaling Plan, the thing I was talking about before.\nThe levels of A.I. safety, and in particular, tying decisions to pause scaling\nto the measurement of specific dangers or the absence of the ability to show\nsafety or the presence of certain capabilities. One way I think about it is,\nat the end of the day, this is ultimately an exercise in getting a coalition\non board with doing something that goes against economic pressures.\n\nAnd so, if you say now, \u2018Well, I don\u2019t know. These things, they might be\ndangerous in the future. We\u2019re on this exponential.\u2019 It\u2019s just hard. Like,\nit\u2019s hard to get a multi-trillion dollar company. It\u2019s certainly hard to get a\nmilitary general to say, all right, well, we just won\u2019t do this. It\u2019ll confer\nsome huge advantage to others. But we just won\u2019t do this.\n\nI think the thing that could be more convincing is tying the decision to hold\nback in a very scoped way that\u2019s done across the industry to particular\ndangers. My testimony in front of Congress, I warned about the potential\nmisuse of models for biology. That isn\u2019t the case today, right? You can get a\nsmall uplift to the models relative to doing a Google search, and many people\ndismiss the risk. And I don\u2019t know \u2014 maybe they\u2019re right. The exponential\nscaling laws suggest to me that they\u2019re not right, but we don\u2019t have any\ndirect hard evidence.\n\nBut let\u2019s say we get to 2025, and we demonstrate something truly scary. Most\npeople do not want technology out in the world that can create bioweapons. And\nso I think, at moments like that, there could be a critical coalition tied to\nrisks that we can really make concrete. Yes, it will always be argued that\nadversaries will have these capabilities as well. But at least the trade-off\nwill be clear, and there\u2019s some chance for sensible policy.\n\nI mean to be clear, I\u2019m someone who thinks the benefits of this technology are\ngoing to outweigh its costs. And I think the whole idea behind RSP is to\nprepare to make that case, if the dangers are real. If they\u2019re not real, then\nwe can just proceed and make things that are great and wonderful for the\nworld. And so, it has the flexibility to work both ways.\n\nAgain, I don\u2019t think it\u2019s perfect. I\u2019m someone who thinks whatever we do, even\nwith all the regulatory framework, I doubt we can slow down that much. But\nwhen I think about what\u2019s the best way to steer a sensible course here, that\u2019s\nthe closest I can think of right now. Probably there\u2019s a better plan out there\nsomewhere, but that\u2019s the best thing I\u2019ve thought of so far.\n\nEZRA KLEIN: One of the things that has been on my mind around regulation is\nwhether or not the founding insight of Anthropic of OpenAI is even more\nrelevant to the government, that if you are the body that is supposed to, in\nthe end, regulate and manage the safety of societal-level technologies like\nartificial intelligence, do you not need to be building your own foundation\nmodels and having huge collections of research scientists and people of that\nnature working on them, testing them, prodding them, remaking them, in order\nto understand the damn thing well enough \u2014 to the extent any of us or anyone\nunderstands the damn thing well enough \u2014 to regulate it?\n\nI say that recognizing that it would be very, very hard for the government to\nget good enough that it can build these foundation models to hire those\npeople, but it\u2019s not impossible. I think right now, it wants to take the\napproach to regulating A.I. that it somewhat wishes it took to regulating\nsocial media, which is to think about the harms and pass laws about those\nharms earlier.\n\nBut does it need to be building the models itself, developing that kind of\ninternal expertise, so it can actually be a participant in different ways,\nboth for regulatory reasons and maybe for other reasons, for public interest\nreasons? Maybe it wants to do things with a model that they\u2019re just not\npossible if they\u2019re dependent on access to the OpenAI, the Anthropic, the\nGoogle products.\n\nDARIO AMODEI: I think government directly building the models, I think that\nwill happen in some places. It\u2019s kind of challenging, right? Like, government\nhas a huge amount of money, but let\u2019s say you wanted to provision $100 billion\nto train a giant foundation model. The government builds it. It has to hire\npeople under government hiring rules. There\u2019s a lot of practical difficulties\nthat would come with it.\n\nDoesn\u2019t mean it won\u2019t happen or it shouldn\u2019t happen. But something that I\u2019m\nmore confident of that I definitely think is that government should be more\ninvolved in the use and the finetuning of these models, and that deploying\nthem within government will help governments, especially the U.S. government,\nbut also others, to get an understanding of the strengths and weaknesses, the\nbenefits and the dangers. So I\u2019m super supportive of that.\n\nI think there\u2019s maybe a second thing you\u2019re getting at, which I\u2019ve thought\nabout a lot as a C.E.O. of one of these companies, which is, if these\npredictions on the exponential trend are right, and we should be humble \u2014 and\nI don\u2019t know if they\u2019re right or not. My only evidence is that they appear to\nhave been correct for the last few years. And so, I\u2019m just expecting by\ninduction that they continue to be correct. I don\u2019t know that they will, but\nlet\u2019s say they are. The power of these models is going to be really quite\nincredible.\n\nAnd as a private actor in charge of one of the companies developing these\nmodels, I\u2019m kind of uncomfortable with the amount of power that that entails.\nI think that it potentially exceeds the power of, say, the social media\ncompanies maybe by a lot.\n\nYou know, occasionally, in the more science fictiony world of A.I. and the\npeople who think about A.I. risk, someone will ask me like, OK, let\u2019s say you\nbuild the A.G.I. What are you going to do with it? Will you cure the diseases?\nWill you create this kind of society?\n\nAnd I\u2019m like, who do you think you\u2019re talking to? Like a king? I just find\nthat to be a really, really disturbing way of conceptualizing running an A.I.\ncompany. And I hope there are no companies whose C.E.O.s actually think about\nthings that way.\n\nI mean, the whole technology, not just the regulation, but the oversight of\nthe technology, like the wielding of it, it feels a little bit wrong for it to\nultimately be in the hands \u2014 maybe I think it\u2019s fine at this stage, but to\nultimately be in the hands of private actors. There\u2019s something undemocratic\nabout that much power concentration.\n\nEZRA KLEIN: I have now, I think, heard some version of this from the head of\nmost of, maybe all of, the A.I. companies, in one way or another. And it has a\nquality to me of, Lord, grant me chastity but not yet.\n\nWhich is to say that I don\u2019t know what it means to say that we\u2019re going to\ninvent something so powerful that we don\u2019t trust ourselves to wield it. I\nmean, Amazon just gave you guys $2.75 billion. They don\u2019t want to see that\ninvestment nationalized.\n\nNo matter how good-hearted you think OpenAI is, Microsoft doesn\u2019t want GPT-7,\nall of a sudden, the government is like, whoa, whoa, whoa, whoa, whoa. We\u2019re\ntaking this over for the public interest, or the U.N. is going to handle it in\nsome weird world or whatever it might be. I mean, Google doesn\u2019t want that.\n\nAnd this is a thing that makes me a little skeptical of the responsible\nscaling laws or the other iterative versions of that I\u2019ve seen in other\ncompanies or seen or heard talked about by them, which is that it\u2019s imagining\nthis moment that is going to come later, when the money around these models is\neven bigger than it is now, the power, the possibility, the economic uses, the\nsocial dependence, the celebrity of the founders. It\u2019s all worked out. We\u2019ve\nmaintained our pace on the exponential curve. We\u2019re 10 years in the future.\n\nAnd at some point, everybody is going to look up and say, this is actually too\nmuch. It is too much power. And this has to somehow be managed in some other\nway. And even if the C.E.O.s of the things were willing to do that, which is a\nvery open question by the time you get there, even if they were willing to do\nthat, the investors, the structures, the pressure around them, in a way, I\nthink we saw a version of this \u2014 and I don\u2019t know how much you\u2019re going to be\nwilling to comment on it \u2014 with the sort of OpenAI board, Sam Altman thing,\nwhere I\u2019m very convinced that wasn\u2019t about A.I. safety. I\u2019ve talked to figures\non both sides of that. They all sort of agree it wasn\u2019t about A.I. safety. But\nthere was this moment of, if you want to press the off switch, can you, if\nyou\u2019re the weird board created to press the off switch. And the answer was no,\nyou can\u2019t, right? They\u2019ll just reconstitute it over at Microsoft.\n\nThere\u2019s functionally no analogy I know of in public policy where the private\nsector built something so powerful that when it reached maximum power, it was\njust handed over in some way to the public interest.\n\nDARIO AMODEI: Yeah, I mean, I think you\u2019re right to be skeptical, and\nsimilarly, what I said with the previous questions of there are just these\ndilemmas left and right that have no easy answer. But I think I can give a\nlittle more concreteness than what you\u2019ve pointed at, and maybe more\nconcreteness than others have said, although I don\u2019t know what others have\nsaid. We\u2019re at A.S.L. 2 in our responsible scaling plan. These kinds of\nissues, I think they\u2019re going to become a serious matter when we reach, say,\nA.S.L. 4. So that\u2019s not a date and time. We haven\u2019t even fully specified\nA.S.L. 4 \u2014\n\nEZRA KLEIN: Just because this is a lot of jargon, just, what do you specify\nA.S.L. 3 as? And then as you say, A.S.L. 4 is actually left quite undefined.\nSo what are you implying A.S.L. 4 is?\n\nDARIO AMODEI: A.S.L. 3 is triggered by risks related to misuse of biology and\ncyber technology. A.S.L. 4, we\u2019re working on now.\n\nEZRA KLEIN: Be specific. What do you mean? Like, what is the thing a system\ncould do or would do that would trigger it?\n\nDARIO AMODEI: Yes, so for example, on biology, the way we\u2019ve defined it \u2014 and\nwe\u2019re still refining the test, but the way we\u2019ve defined it is, relative to\nuse of a Google search, there\u2019s a substantial increase in risk as would be\nevaluated by, say, the national security community of misuse of biology,\ncreation of bioweapons, that either the proliferation or spread of it is\ngreater than it was before, or the capabilities are substantially greater than\nit was before.\n\nWe\u2019ll probably have some more exact quantitative thing, working with folks who\nare ex-government biodefense folks, but something like this accounts for 20\npercent of the total source of risk of biological attacks, or something\nincreases the risk by 20 percent or something like that. So that would be a\nvery concrete version of it. It\u2019s just, it takes us time to develop very\nconcrete criteria. So that would be like A.S.L. 3.\n\nA.S.L. 4 is going to be more about, on the misuse side, enabling state-level\nactors to greatly increase their capability, which is much harder than\nenabling random people. So where we would worry that North Korea or China or\nRussia could greatly enhance their offensive capabilities in various military\nareas with A.I. in a way that would give them a substantial advantage at the\ngeopolitical level. And on the autonomy side, it\u2019s various measures of these\nmodels are pretty close to being able to replicate and survive in the wild.\n\nSo it feels maybe one step short of models that would, I think, raise truly\nexistential questions. And so, I think what I\u2019m saying is when we get to that\nlatter stage, that A.S.L. 4, that is when I think it may make sense to think\nabout what is the role of government in stewarding this technology.\n\nAgain, I don\u2019t really know what it looks like. You\u2019re right. All of these\ncompanies have investors. They have folks involved. You talk about just\nhanding the models over. I suspect there\u2019s some way to hand over the most\ndangerous or societally sensitive components or capabilities of the models\nwithout fully turning off the commercial tap. I don\u2019t know that there\u2019s a\nsolution that every single actor is happy with. But again, I get to this idea\nof demonstrating specific risk.\n\nIf you look at times in history, like World War I or World War II, industries\u2019\nwill can be bent towards the state. They can be gotten to do things that\naren\u2019t necessarily profitable in the short-term because they understand that\nthere\u2019s an emergency. Right now, we don\u2019t have an emergency. We just have a\nline on a graph that weirdos like me believe in and a few people like you who\nare interviewing me may somewhat believe in. We don\u2019t have clear and present\ndanger.\n\nEZRA KLEIN: When you imagine how many years away, just roughly, A.S.L. 3 is\nand how many years away A.S.L. 4 is, right, you\u2019ve thought a lot about this\nexponential scaling curve. If you just had to guess, what are we talking\nabout?\n\nDARIO AMODEI: Yeah, I think A.S.L. 3 could easily happen this year or next\nyear. I think A.S.L. 4 \u2014\n\nEZRA KLEIN: Oh, Jesus Christ.\n\nDARIO AMODEI: No, no, I told you. I\u2019m a believer in exponentials. I think\nA.S.L. 4 could happen anywhere from 2025 to 2028.\n\nEZRA KLEIN: So that is fast.\n\nDARIO AMODEI: Yeah, no, no, I\u2019m truly talking about the near future here. I\u2019m\nnot talking about 50 years away. God grant me chastity, but not now. But \u201cnot\nnow\u201d doesn\u2019t mean when I\u2019m old and gray. I think it could be near term. I\ndon\u2019t know. I could be wrong. But I think it could be a near term thing.\n\nEZRA KLEIN: But so then, if you think about this, I feel like what you\u2019re\ndescribing, to go back to something we talked about earlier, that there\u2019s been\nthis step function for societal impact of A.I., the curve of the capabilities\nexponential, but every once in a while, something happens, ChatGPT, for\ninstance, Midjourney with photos. And all of a sudden, a lot of people feel\nit. They realize what has happened and they react. They use it. They deploy it\nin their companies. They invest in it, whatever.\n\nAnd it sounds to me like that is the structure of the political economy you\u2019re\ndescribing here. Either something happens where the bioweapon capability is\ndemonstrated or the offensive cyber weapon capability is demonstrated, and\nthat freaks out the government, or possibly something happens, right?\nDescribing World War I and World War II is your examples did not actually fill\nme with comfort because in order to bend industry to government\u2019s will, in\nthose cases, we had to have an actual world war. It doesn\u2019t do it that easily.\n\nYou could use coronavirus, I think, as another example where there was a\nsignificant enough global catastrophe that companies and governments and even\npeople did things you never would have expected. But the examples we have of\nthat happening are something terrible. All those examples end up with millions\nof bodies.\n\nI\u2019m not saying that\u2019s going to be true for A.I., but it does sound like that\nis a political economy. No, you can\u2019t imagine it now, in the same way that you\ncouldn\u2019t have imagined the sort of pre and post-ChatGPT world exactly, but\nthat something happens and the world changes. Like, it\u2019s a step function\neverywhere.\n\nDARIO AMODEI: Yeah, I mean, I think my positive version of this, not to be so\n\u2014 to get a little bit away from the doom and gloom, is that the dangers are\ndemonstrated in a concrete way that is really convincing, but without\nsomething actually bad happening, right? I think the worst way to learn would\nbe for something actually bad to happen. And I\u2019m hoping every day that doesn\u2019t\nhappen, and we learn bloodlessly.\n\nEZRA KLEIN: We\u2019ve been talking here about conceptual limits and curves, but I\ndo want, before we end, to reground us a little bit in the physical reality,\nright? I think that if you\u2019re using A.I., it can feel like this digital bits\nand bytes, sitting in the cloud somewhere.\n\nBut what it is in a physical way is huge numbers of chips, data centers, an\nenormous amount of energy, all of which does rely on complicated supply\nchains. And what happens if something happens between China and Taiwan, and\nthe makers of a lot of these chips become offline or get captured? How do you\nthink about the necessity of compute power? And when you imagine the next five\nyears, what does that supply chain look like? How does it have to change from\nwhere it is now? And what vulnerabilities exist in it?\n\nDARIO AMODEI: Yeah, so one, I think this may end up being the greatest\ngeopolitical issue of our time. And man, this relates to things that are way\nabove my pay grade, which are military decisions about whether and how to\ndefend Taiwan. All I can do is say what I think the implications for A.I. is.\nI think those implications are pretty stark. I think there\u2019s a big question of\nlike, OK, we built these powerful models.\n\nOne, is there enough supply to build them? Two is control over that supply, a\nway to think about safety issues or a way to think about balance of\ngeopolitical power. And three, if those chips are used to build data centers,\nwhere are those data centers going to be? Are they going to be in the U.S.?\nAre they going to be in a U.S. ally? Are they going to be in the Middle East?\nAre they going to be in China?\n\nAll of those have enormous implications, and then the supply chain itself can\nbe disrupted. And political and military decisions can be made on the basis of\nwhere things are. So it sounds like an incredibly sticky problem to me. I\ndon\u2019t know that I have any great insight on this. I mean, as a U.S. citizen\nand someone who believes in democracy, I am someone who hopes that we can find\na way to build data centers and to have the largest quantity of chips\navailable in the U.S. and allied democratic countries.\n\nEZRA KLEIN: Well, there is some insight you should have into it, which is that\nyou\u2019re a customer here, right? And so, five years ago, the people making these\nchips did not realize what the level of demand for them was going to be. I\nmean, what has happened to Nvidia\u2019s stock prices is really remarkable.\n\nBut also what is implied about the future of Nvidia\u2019s stock prices is really\nremarkable. Rana Foroohar, the Financial Times, cited this market analysis. It\nwould take 4,500 years for Nvidia\u2019s future dividends to equal its current\nprice, 4,500 years. So that is a view about how much Nvidia is going to be\nmaking in the next couple of years. It is really quite astounding.\n\nI mean, you\u2019re, in theory, already working on or thinking about how to work on\nthe next generation of Claude. You\u2019re going to need a lot of chips for that.\nYou\u2019re working with Amazon. Are you having trouble getting the amount of\ncompute that you feel you need? I mean, are you already bumping up against\nsupply constraints? Or has the supply been able to change, to adapt to you?\n\nDARIO AMODEI: We\u2019ve been able to get the compute that we need for this year, I\nsuspect also for next year as well. I think once things get to 2026, 2027,\n2028, then the amount of compute gets to levels that starts to strain the\ncapabilities of the semiconductor industry. The semiconductor industry still\nmostly produces C.P.U.s, right? Just the things in your laptop, not the things\nin the data centers that train the A.I. models. But as the economic value of\nthe GPUs goes up and up and up because of the value of the A.I. models, that\u2019s\ngoing to switch over.\n\nBut you know what? At some point, you hit the limits of that or you hit the\nlimits of how fast you can switch over. And so, again, I expect there to be a\nbig supply crunch around data centers, around chips, and around energy and\npower for both regulatory and physics reasons, sometime in the next few years.\nAnd that\u2019s a risk, but it\u2019s also an opportunity. I think it\u2019s an opportunity\nto think about how the technology can be governed.\n\nAnd it\u2019s also an opportunity, I\u2019ll repeat again, to think about how\ndemocracies can lead. I think it would be very dangerous if the leaders in\nthis technology and the holders of the main resources were authoritarian\ncountries. The combination of A.I. and authoritarianism, both internally and\non the international stage, is very frightening to me.\n\nEZRA KLEIN: How about the question of energy? I mean, this requires just a\ntremendous amount of energy. And I mean, I\u2019ve seen different numbers like this\nfloating around. It very much could be in the coming years like adding a\nBangladesh to the world\u2019s energy usage. Or pick your country, right? I don\u2019t\nknow what exactly you all are going to be using by 2028.\n\nMicrosoft, on its own, is opening a new data center globally every three days.\nYou have \u2014 and this is coming from a Financial Times article \u2014 federal\nprojections for 20 new gas-fired power plants in the U.S. by 2024 to 2025.\nThere\u2019s a lot of talk about this being now a new golden era for natural gas\nbecause we have a bunch of it. There is this huge need for new power to manage\nall this data, to manage all this compute.\n\nSo, one, I feel like there\u2019s a literal question of how do you get the energy\nyou need and at what price, but also a more kind of moral, conceptual question\nof, we have real problems with global warming. We have real problems with how\nmuch energy we\u2019re using. And here, we\u2019re taking off on this really steep curve\nof how much of it we seem to be needing to devote to the new A.I. race.\n\nDARIO AMODEI: It really comes down to, what are the uses that the model is\nbeing put to, right? So I think the worrying case would be something like\ncrypto, right? I\u2019m someone who\u2019s not a believer that whatever the energy was\nthat was used to mine the next Bitcoin, I think that was purely additive. I\nthink that wasn\u2019t there before. And I\u2019m unable to think of any useful thing\nthat\u2019s created by that.\n\nBut I don\u2019t think that\u2019s the case with A.I. Maybe A.I. makes solar energy more\nefficient or maybe it solves controlled nuclear fusion, or maybe it makes\ngeoengineering more stable or possible. But I don\u2019t think we need to rely on\nthe long run. There are some applications where the model is doing something\nthat used to be automated, that used to be done by computer systems. And the\nmodel is able to do it faster with less computing time, right? Those are pure\nwins. And there are some of those.\n\nThere are others where it\u2019s using the same amount of computing resources or\nmaybe more computing resources, but to do something more valuable that saves\nlabor elsewhere. Then there are cases where something used to be done by\nhumans or in the physical world, and now it\u2019s being done by the models. Maybe\nit does something that previously I needed to go into the office to do that\nthing. And now I no longer need to go into the office to do that thing.\n\nSo I don\u2019t have to get in my car. I don\u2019t have to use the gas that was used\nfor that. The energy accounting for that is kind of hard. You compare it to\nthe food that the humans eat and what the energy cost of producing that.\n\nSo in all honesty, I don\u2019t think we have good answers about what fraction of\nthe usage points one way and one fraction of the usage points to others. In\nmany ways, how different is this from the general dilemma of, as the economy\ngrows, it uses more energy?\n\nSo I guess, what I\u2019m saying is, it kind of all matters how you use the\ntechnology. I mean, my kind of boring short-term answer is, we get carbon\noffsets for all of this stuff. But let\u2019s look beyond that to the macro\nquestion here.\n\nEZRA KLEIN: But to take the other side of it, I mean, I think the difference,\nwhen you say this is always a question we have when we\u2019re growing G.D.P., is\nit\u2019s not quite. It\u2019s clich\u00e9 because it\u2019s true to say that the major global\nwarming challenge right now is countries like China and India getting richer.\nAnd we want them to get richer. It is a huge human imperative, right, a moral\nimperative for poor people in the world to become less poor. And if that means\nthey use more energy, then we just need to figure out how to make that work.\nAnd we don\u2019t know of a way for that to happen without them using more energy.\n\nAdding A.I. is not that it raises a whole different set of questions, but\nwe\u2019re already straining at the boundaries, or maybe far beyond them, of safely\nwhat we can do energetically. Now we add in this, and so maybe some of the\nenergy efficiency gains you\u2019re going to get in rich countries get wiped out.\nFor this sort of uncertain payoff in the future of maybe through A.I., we\nfigure out ways to stabilize nuclear fusion or something, right, you could\nimagine ways that could help, but those ways are theoretical.\n\nAnd in the near term, the harm in terms of energy usage is real. And also, by\nthe way, the harm in terms of just energy prices. It\u2019s also just tricky\nbecause all these companies, Microsoft, Amazon, I mean, they all have a lot of\nrenewable energy targets. Now if that is colliding with their market\nincentives, it feels like they\u2019re running really fast towards the market\nincentives without an answer for how all that nets out.\n\nDARIO AMODEI: Yeah, I mean, I think the concerns are real. Let me push back a\nlittle bit, which is, again, I don\u2019t think the benefits are purely in the\nfuture. It kind of goes back to what I said before. Like, there may be use\ncases now that are net energy saving, or that to the extent that they\u2019re not\nnet energy saving, do so through the general mechanism of, oh, there was more\ndemand for this thing.\n\nI don\u2019t think anyone has done a good enough job measuring, in part because the\napplications of A.I. are so new, which of those things dominate or what\u2019s\ngoing to happen to the economy. But I don\u2019t think we should assume that the\nharms are entirely in the present and the benefits are entirely in the future.\nI think that\u2019s my only point here.\n\nEZRA KLEIN: I guess you could imagine a world where we were, somehow or\nanother, incentivizing uses of A.I. that were yoked to some kind of social\npurpose. We were putting a lot more into drug discovery, or we cared a lot\nabout things that made remote work easier, or pick your set of public goods.\n\nBut what actually seems to me to be happening is we\u2019re building more and more\nand more powerful models and just throwing them out there within a terms of\nservice structure to say, use them as long as you\u2019re not trying to politically\nmanipulate people or create a bioweapon. Just try to figure this out, right?\nTry to create new stories and ask it about your personal life, and make a\nvideo game with it. And Sora comes out sooner or later. Make new videos with\nit. And all that is going to be very energy intensive.\n\nI am not saying that I have a plan for yoking A.I. to social good, and in some\nways, you can imagine that going very, very wrong. But it does mean that for a\nlong time, it\u2019s like you could imagine the world you\u2019re talking about, but\nthat would require some kind of planning that nobody is engaged in, and I\ndon\u2019t think anybody even wants to be engaged in.\n\nDARIO AMODEI: Not everyone has the same conception of social good. One person\nmay think social good is this ideology. Another person \u2014 we\u2019ve seen that with\nsome of the Gemini stuff.\n\nEZRA KLEIN: Right.\n\nDARIO AMODEI: But companies can try to make beneficial applications\nthemselves, right? Like, this is why we\u2019re working with cancer institutes.\nWe\u2019re hoping to partner with ministries of education in Africa, to see if we\ncan use the models in kind of a positive way for education, rather than the\nway they may be used by default. So I think individual companies, individual\npeople, can take actions to steer or bend this towards the public good.\n\nThat said, it\u2019s never going to be the case that 100 percent of what we do is\nthat. And so I think it\u2019s a good question. What are the societal incentives,\nwithout dictating ideology or defining the public good from on high, what are\nincentives that could help with this?\n\nI don\u2019t feel like I have a systemic answer either. I can only think in terms\nof what Anthropic tries to do.\n\nEZRA KLEIN: But there\u2019s also the question of training data and the\nintellectual property that is going into things like Claude, like GPT, like\nGemini. There are a number of copyright lawsuits. You\u2019re facing some. OpenAI\nis facing some. I suspect everybody is either facing them now or will face\nthem.\n\nAnd a broad feeling that these systems are being trained on the combined\nintellectual output of a lot of different people \u2014 the way that Claude can\nquite effectively mimic the way I write is it has been trained, to some\ndegree, on my writing, right? So it actually does get my stylistic tics quite\nwell. You seem great, but you haven\u2019t sent me a check on that. And this seems\nlike somewhere where there is real liability risk for the industry. Like, what\nif you do actually have to compensate the people who this is being trained on?\nAnd should you?\n\nAnd I recognize you probably can\u2019t comment on lawsuits themselves, but I\u2019m\nsure you\u2019ve had to think a lot about this. And so, I\u2019m curious both how you\nunderstand it as a risk, but also how you understand it morally. I mean, when\nyou talk about the people who invent these systems gaining a lot of power, and\nalongside that, a lot of wealth, well, what about all the people whose work\nwent into them such that they can create images in a million different styles?\n\nAnd I mean, somebody came up with those styles. What is the responsibility\nback to the intellectual commons? And not just to the commons, but to the\nactual wages and economic prospects of the people who made all this possible?\n\nDARIO AMODEI: I think everyone agrees the models shouldn\u2019t be verbatim\noutputting copyrighted content. For things that are available on the web, for\npublicly available, our position \u2014 and I think there\u2019s a strong case for it \u2014\nis that the training process, again, we don\u2019t think it\u2019s just hoovering up\ncontent and spitting it out, or it shouldn\u2019t be spitting it out. It\u2019s really\nmuch more like the process of how a human learns from experiences. And so, our\nposition that that is sufficiently transformative, and I think the law will\nback this up, that this is fair use.\n\nBut those are narrow legal ways to think about the problem. I think we have a\nbroader issue, which is that regardless of how it was trained, it would still\nbe the case that we\u2019re building more and more general cognitive systems, and\nthat those systems will create disruption. Maybe not necessarily by one for\none replacing humans, but they\u2019re really going to change how the economy works\nand which skills are valued. And we need a solution to that broad\nmacroeconomic problem, right?\n\nAs much as I\u2019ve asserted the narrow legal points that I asserted before, we\nhave a broader problem here, and we shouldn\u2019t be blind to that. There\u2019s a\nnumber of solutions. I mean, I think the simplest one, which I recognize\ndoesn\u2019t address some of the deeper issues here, is things around the kind of\nguaranteed basic income side of things.\n\nBut I think there\u2019s a deeper question here, which is like as A.I. systems\nbecome capable of larger and larger slices of cognitive labor, how does\nsociety organize itself economically? How do people find work and meaning and\nall of that?\n\nAnd just as kind of we transition from an agrarian society to an industrial\nsociety and the meaning of work changed, and it was no longer true that 99\npercent of people were peasants working on farms and had to find new methods\nof economic organization, I suspect there\u2019s some different method of economic\norganization that\u2019s going to be forced as the only possible response to\ndisruptions to the economy that will be small at first, but will grow over\ntime, and that we haven\u2019t worked out what that is. We need to find something\nthat allows people to find meaning that\u2019s humane and that maximizes our\ncreativity and potential and flourishing from A.I.\n\nAnd as with many of these questions, I don\u2019t have the answer to that. Right? I\ndon\u2019t have a prescription. But that\u2019s what we somehow need to do.\n\nEZRA KLEIN: But I want to sit in between the narrow legal response and the\nbroad \u201cwe have to completely reorganize society\u201d response, although I think\nthat response is actually possible over the decades. And in the middle of that\nis a more specific question. I mean, you could even take it from the\ninstrumental side. There is a lot of effort now to build search products that\nuse these systems, right? ChatGPT will use Bing to search for you.\n\nAnd that means that the person is not going to Bing and clicking on the\nwebsite where ChatGPT is getting its information and giving that website an\nadvertising impression that they can turn into a very small amount of money,\nor they\u2019re not going to that website and having a really good experience with\nthat website and becoming maybe likelier to subscribe to whoever is behind\nthat website.\n\nAnd so, on the one hand, that seems like some kind of injustice done to the\npeople creating the information that these systems are using. I mean, this is\ntrue for perplexity. It\u2019s true for a lot of things I\u2019m beginning to see around\nwhere the A.I.s are either trained on or are using a lot of data that people\nhave generated at some real cost. But not only are they not paying people for\nthat, but they\u2019re actually stepping into the middle of where they would\nnormally be a direct relationship and making it so that relationship never\nhappens.\n\nThat also, I think, in the long run, creates a training data problem, even if\nyou just want to look at it instrumentally, where if it becomes nonviable to\ndo journalism or to do a lot of things to create high quality information out\nthere, the A.I.\u2019s ability, right, the ability of all of your companies to get\nhigh quality, up-to-date, constantly updated information becomes a lot\ntrickier. So there both seems to me to be both a moral and a self-interested\ndimension to this.\n\nDARIO AMODEI: Yeah, so I think there may be business models that work for\neveryone, not because it\u2019s illegitimate to train on open data from the web in\na legal sense, but just because there may be business models here that kind of\ndeliver a better product. So things I\u2019m thinking of are like newspapers have\narchives. Some of them aren\u2019t publicly available. But even if they are, it may\nbe a better product, maybe a better experience, to, say, talk to this\nnewspaper or talk to that newspaper.\n\nIt may be a better experience to give the ability to interact with content and\npoint to places in the content, and every time you call that content, to have\nsome kind of business relationship with the creators of that content. So there\nmay be business models here that propagate the value in the right way, right?\nYou talk about LLMs using search products. I mean, sure, you\u2019re going around\nthe ads, but there\u2019s no reason it can\u2019t work in a different way, right?\n\nThere\u2019s no reason that the users can\u2019t pay the search A.P.I.s, instead of it\nbeing paid through advertising, and then have that propagate through to\nwherever the original mechanism is that paid the creators of the content. So\nwhen value is being created, money can flow through.\n\nEZRA KLEIN: Let me try to end by asking a bit about how to live on the slope\nof the curve you believe we are on. Do you have kids?\n\nDARIO AMODEI: I\u2019m married. I do not have kids.\n\nEZRA KLEIN: So I have two kids. I have a two-year-old and a five-year-old. And\nparticularly when I\u2019m doing A.I. reporting, I really do sit in bed at night\nand think, what should I be doing here with them? What world am I trying to\nprepare them for? And what is needed in that world that is different from what\nis needed in this world, even if I believe there\u2019s some chance \u2014 and I do\nbelieve there\u2019s some chance \u2014 that all the things you\u2019re saying are true. That\nimplies a very, very, very different life for them.\n\nI know people in your company with kids. I know they are thinking about this.\nHow do you think about that? I mean, what do you think should be different in\nthe life of a two-year-old who is living through the pace of change that you\nare telling me is true here? If you had a kid, how would this change the way\nyou thought about it?\n\nDARIO AMODEI: The very short answer is, I don\u2019t know, and I have no idea, but\nwe have to try anyway, right? People have to raise kids, and they have to do\nit as best they can. An obvious recommendation is just familiarity with the\ntechnology and how it works, right? The basic paradigm of, I\u2019m talking to\nsystems, and systems are taking action on my behalf, obviously, as much\nfamiliarity with that as possible is, I think, helpful.\n\nIn terms of what should children learn in school, what are the careers of\ntomorrow, I just truly don\u2019t know, right? You could take this to say, well,\nit\u2019s important to learn STEM and programming and A.I. and all of that. But\nA.I. will impact that as well, right? I don\u2019t think any of it is going to \u2014\n\nEZRA KLEIN: Possibly first.\n\nDARIO AMODEI: Yeah, right, possibly first.\n\nEZRA KLEIN: It seems better at coding than it is at other things.\n\nDARIO AMODEI: I don\u2019t think it\u2019s going to work out for any of these systems to\njust do one for one what humans are going to do. I don\u2019t really think that\nway. But I think it may fundamentally change industries and professions one by\none in ways that are hard to predict. And so, I feel like I only have clich\u00e9s\nhere. Like get familiar with the technology. Teach your children to be\nadaptable, to be ready for a world that changes very quickly. I wish I had\nbetter answers, but I think that\u2019s the best I got.\n\nEZRA KLEIN: I agree that\u2019s not a good answer. [LAUGHS] Let me ask that same\nquestion a bit from another direction, because one thing you just said is get\nfamiliar with the technology. And the more time I spend with the technology,\nthe more I fear that happening. What I see when people use A.I. around me is\nthat the obvious thing that technology does for you is automate the early\nparts of the creative process.\n\nThe part where you\u2019re supposed to be reading something difficult yourself?\nWell, the A.I. can summarize it for you. The part where you\u2019re supposed to sit\nthere with a blank page and write something? Well, the A.I. can give you a\nfirst draft. And later on, you have to check it and make sure it actually did\nwhat you wanted it to do and fact-checking it. And but I believe a lot of what\nmakes humans good at thinking comes in those parts.\n\nAnd I am older and have self-discipline, and maybe this is just me hanging on\nto an old way of doing this, right? You could say, why use a calculator from\nthis perspective. But my actual worry is that I\u2019m not sure if the thing they\nshould do is use A.I. a lot or use it a little.\n\nThis, to me, is actually a really big branching path, right? Do I want my kids\nlearning how to use A.I. or being in a context where they\u2019re using it a lot,\nor actually, do I want to protect them from it as much as I possibly could so\nthey develop more of the capacity to read a book quietly on their own or write\na first draft? I actually don\u2019t know. I\u2019m curious if you have a view on it.\n\nDARIO AMODEI: I think this is part of what makes the interaction between A.I.\nand society complicated where it\u2019s sometimes hard to distinguish when is an\nA.I. doing something, saving you labor or drudge work, versus kind of doing\nthe interesting part. I will say that over and over again, you\u2019ll get some\ntechnological thing, some technological system that does what you thought was\nthe core of what you\u2019re doing, and yet, what you\u2019re doing turns out to have\nmore pieces than you think it does and kind of add up to more things, right?\n\nIt\u2019s like before, I used to have to ask for directions. I got Google Maps to\ndo that. And you could worry, am I too reliant on Google Maps? Do I forget the\nenvironment around me? Well, it turns out, in some ways, I still need to have\na sense of the city and the environment around me. It just kind of reallocates\nthe space in my brain to some other aspect of the task.\n\nAnd I just kind of suspect \u2014 I don\u2019t know. Internally, within Anthropic, one\nof the things I do that helps me run the company is, I\u2019ll write these\ndocuments on strategy or just some thinking in some direction that others\nhaven\u2019t thought. And of course, I sometimes use the internal models for that.\nAnd I think what I found is like, yes, sometimes they\u2019re a little bit good at\nconceptualizing the idea, but the actual genesis of the idea, I\u2019ve just kind\nof found a workflow where I don\u2019t use them for that. They\u2019re not that helpful\nfor that. But they\u2019re helpful in figuring out how to phrase a certain thing or\nhow to refine my ideas.\n\nSo maybe I\u2019m just saying \u2014 I don\u2019t know. You just find a workflow where the\nthing complements you. And if it doesn\u2019t happen naturally, it somehow still\nhappens eventually. Again, if the systems get general enough, if they get\npowerful enough, we may need to think along other lines. But in the short-\nterm, I, at least, have always found that. Maybe that\u2019s too sanguine. Maybe\nthat\u2019s too optimistic.\n\nEZRA KLEIN: I think, then, that\u2019s a good place to end this conversation.\nThough, obviously, the exponential curve continues. So always our final\nquestion \u2014 what are three books you\u2019d recommend to the audience?\n\nDARIO AMODEI: So, yeah, I\u2019ve prepared three. They\u2019re all topical, though, in\nsome cases, indirectly so. The first one will be obvious. It\u2019s a very long\nbook. The physical book is very thick, but \u201cThe Making of the Atomic Bomb,\u201d\nRichard Rhodes. It\u2019s an example of technology being developed very quickly and\nwith very broad implications. Just looking through all the characters and how\nthey reacted to this and how people who were basically scientists gradually\nrealized the incredible implications of the technology and how it would lead\nthem into a world that was very different from the one they were used to.\n\nMy second recommendation is a science fiction series, \u201cThe Expanse\u201d series of\nbooks. So I initially watched the show, and then I read all the books. And the\nworld it creates is very advanced. In some cases, it has longer life spans,\nand humans have expanded into space. But we still face some of the same\ngeopolitical questions and some of the same inequalities and exploitations\nthat exist in our world, are still present, in some cases, worse.\n\nThat\u2019s all the backdrop of it. And the core of it is about some fundamentally\nnew technological object that is being brought into that world and how\neveryone reacts to it, how governments react to it, how individual people\nreact to it, and how political ideologies react to it. And so, I don\u2019t know.\nWhen I read that a few years ago, I saw a lot of parallels.\n\nAnd then my third recommendation would be actually \u201cThe Guns of August,\u201d which\nis basically a history of how World War I started. The basic idea that crises\nhappen very fast, almost no one knows what\u2019s going on. There are lots of\nmiscalculations because there are humans at the center of it, and kind of, we\nsomehow have to learn to step back and make wiser decisions in these key\nmoments. It\u2019s said that Kennedy read the book before the Cuban Missile Crisis.\nAnd so I hope our current policymakers are at least thinking along the same\nterms because I think it is possible similar crises may be coming our way.\n\nEZRA KLEIN: Dario Amodei, thank you very much.\n\nDARIO AMODEI: Thank you for having me.\n\n[MUSIC PLAYING]\n\nEZRA KLEIN: This episode of \u201cThe Ezra Klein Show\u201d was produced by Rollin Hu.\nFact-checking by Michelle Harris. Our senior engineer is Jeff Geld. Our senior\neditor is Claire Gordon. The show\u2019s production team also includes Annie\nGalvin, Kristin Lin and Aman Sahota. Original music by Isaac Jones. Audience\nstrategy by Kristina Samulewski and Shannon Busta. The executive producer of\nNew York Times Opinion Audio is Annie-Rose Strasser. Special thanks to Sonia\nHerrero.\n\nAdvertisement\n\nSKIP ADVERTISEMENT\n\n", "frontpage": false}
