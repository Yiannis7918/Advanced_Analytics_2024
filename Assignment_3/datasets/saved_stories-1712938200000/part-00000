{"aid": "40010990", "title": "32x reduced memory usage with binary quantization", "url": "https://weaviate.io/blog/binary-quantization", "domain": "weaviate.io", "votes": 2, "user": "riadsila", "posted_at": "2024-04-12 10:08:19", "comments": 0, "source_title": "32x Reduced Memory Usage With Binary Quantization", "source_text": "32x Reduced Memory Usage With Binary Quantization | Weaviate - Vector Database\n\nSkip to main content\n\nWe've updated the Python Client - introduced typing, faster imports, intuitive\ncode, and more. Read Shape the Future - Try Our New Python Client API to learn\nmore.\n\n# 32x Reduced Memory Usage With Binary Quantization\n\nApril 2, 2024 \u00b7 19 min read\n\nAbdel Rodriguez\n\nVector Index Researcher\n\nZain Hasan\n\nDeveloper Advocate\n\nOverview\n\n  * What is Binary Quantization? - How BQ reduces the memory requirements of running Weaviate.\n  * Details of Working with Binarized Vectors - What to consider when using binary quantization.\n  * Weaviate Performance Improvements with BQ - QPS vs. recall, indexing times, and memory savings.\n  * Comparing Product and Binary Quantization - Experiments comparing PQ and BQ.\n  * Benchmarking BQ - Scripts for benchmarking BQ with your own data.\n\n## \ud83e\uddeeWhat is Binary Quantization?\n\nTo perform search current vector databases build massive vector indices and\nsearch over them in memory. This allows for real-time responses to queries\nhowever, compute costs can add up. Binary quantization(BQ) is a vector\ncompression algorithm that allows you to trade-off between retrieval accuracy\nand memory requirements.\n\nLets understand how this technique works by way of an analogy. Imagine every\nvector you want to store is a home address. The details of this home address\nallow you to precisely locate where someone lives including the country,\nstate, city, street number, and even down to the house number. The price you\npay for this pin-point accuracy is that each address takes up more memory to\nstore, search and read. Similarly, to locate a vector in space you can think\nof each dimension as instructions on exactly how far to move along an axis.\nThe sign of the number at that dimension tells you which direction to move and\nthe magnitude of the number tells you how much to move in that direction.\n\nThe process of binary quantization(BQ) takes all the numbers stored per\ndimension of a vector and only retains a 1(for positive values) or a 0(for\nnegative values) capturing the sign of the corresponding number. That seems a\nbit radical, how can you ever locate a vector without information on how far\nalong to move in the direction of an axis? Though BQ might sound like a crazy\nproposition, it does work particularly well for high-dimensional vectors.\nLet's see why!\n\nFig: Elementwise binary quantization of a vector from float32 to 1 bit.\n\nEnabling Binary Quantization in Weaviate\n\nTo learn how to enable and configure Weaviate to use BQ refer to the docs\nhere. You can read more about HNSW+BQ in the documentation here.\n\nBinarizing is not exclusive to embedding vectors, we can also understand it\nfrom the other domains in which it is used - for example in computer vision.\nImagine you have an image. The process of binarizing that image entails\nlooking at every pixel. If that pixel is larger than a certain threshold\nvalue, we will replace it with 1; otherwise, we will replace it with 0. The\nresulting image is a black-and-white binary image. This process loses image\ndetails but significantly compresses the image size.\n\nOriginal Image| Binarized Image  \n---|---  \n  \nFig: An image when binary quantized\n\nNow, let's consider what the process of binarizing a vector embedding for a\nsentence looks like. In the figure below we have vectorized the sentence: \u201cAll\nyour vector embeddings belong to you!\u201d into a 384-dimensional vector. The\nfirst image shows all 384 numbers of the vector, each one being a 32-bit\nfloating point, as color shades on a heatmap. Each vector dimension dictates\nthe colour gradient on the heatmap. The image below shows the same vector, but\nwe threshold the vector such that all positive valued dimensions are turned\ninto 1(white) and negative valued dimensions are turned to 0(black). As a\nresult, we get a black-and-white heatmap that looks kind of like a bar code.\nThis is what the process of binary quantization looks like for vector\nembeddings! The resulting vectors are much smaller but also less detailed.\n\nSentence: \"All your vector embeddings belong to you!\"  \n---  \n  \nFig: Visualization of a sentence vector embedding when binary quantized\n\nThe concept of binary quantization simplifies the encoding of vectors by\nretaining only their directionality. Each vector dimension is encoded with a\nsingle bit, indicating whether it's positive or negative. For instance, a\nvector like [12, 1, -100, 0.003, -0.001, 128, -1000, 0.0001] would be\ncondensed into a single byte, resulting in the binary sequence\n[1,1,0,1,0,1,0,1]. This massively reduces the amount of space every vector\ntakes by a factor of 32x by converting the number stored at every dimension\nfrom a float32 down to 1-bit. However, reversing this process is impossible -\nmaking this a lossy compression technique.\n\n## \ud83d\udcd0Details of Working with Binarized Vectors\n\n### Distance Between Binary Vectors\n\nFirst, let\u2019s consider how we compute the distance between two BQ compressed\nvectors. The solution is straightforward: since we're solely concerned with\ntheir directional aspects, we only need to assess the agreement between their\ndimensions. This requires you to count the differing bits in both compressed\nrepresentations. Here you can leverage bitwise operations and is significantly\nfaster than computing the distance between non-binarized vectors.\n\nFor example, considering the vector [12, 1, -100, 0.03, -0.01, 128, -100,\n0.01] encoded as 11010101, and a second vector [11, 4, -99, -0.01, 0.02, 130,\n-150, 0.02] encoded as 11001101, the distance between them is determined by\nthe count of differing bits, resulting in a distance of 2. This is more\npopularly known as the hamming distance between the two binary vectors.\n\n### The importance of data distribution for BQ\n\nUnlike product quantization, which we cover in a previous blog here, binary\nquantization is not universally applicable to all types of data - we will see\nillustration of why this is this case in a second. However, assuming we are\noperating with normalized data, particularly when utilizing the cosine metric,\nthere's no need for concern as Weaviate handles data normalization for you\nseamlessly. Now, let's discuss the implications of increasing the dimension\ncount.\n\nBQ with 1 Dimensional Vectors:\n\nIn the picture below, we plot the only two possible points, in red, for a\nnormalized one-dimensional space (0, 1). The quantizer would assign the code\nwith a single bit set to one to a positive vector and the code with a single\nbit set to zero to a negative vector.\n\nFig: BQ with 1 dimensional vectors\n\nLet's broaden our perspective to encompass two dimensions. Here, we anticipate\nall vectors lying within the circle centered at (0,0) with a radius of 1\n(normalized vectors). Our focus shifts to understanding how the quantizer\ndivides the data into four distinct regions, leveraging two codes and two\ndimensions to achieve the power of two.\n\nOriginal Vectors| BQ Vectors  \n---|---  \n  \nFig: BQ with 2 dimensional vectors\n\nIn this context, the green region (coded as 11) aggregates points where both\ndimensions are positive, while the blue region (coded as 00) encompasses\npoints where both dimensions are negative. The red region (coded as 10)\nrepresents scenarios where the first dimension is positive and the second is\nnegative, while the yellow region (coded as 01) signifies instances where the\nfirst dimension is negative while the second is positive.\n\nImportantly, within each region, any point shares a zero distance with any\nother point within the same region. Conversely, points in adjacent regions\nmaintain a distance of 1 from one another. When traversing to the opposite\nregion, the maximal distance extends to 2.\n\nThis delineation underscores the critical role of data distribution. While\nwe've emphasized normalized data, it's worth noting that while normalization\nisn't mandatory, it significantly aligns with the scenario described.\nContrastingly, let's analyze another scenario.\n\nOriginal Vectors| BQ Vectors  \n---|---  \n  \nAll of our data falls within the quadrant where both dimensions are positive.\nConsequently, all vectors are encoded as '11', making it challenging to\ndiscern between most vectors. This scenario exemplifies how the distribution\nof data can render binary quantization impractical. As previously noted, while\nnormalization isn't obligatory, opting for normalized data provides\nreassurance in terms of data distribution, facilitating the use of binary\nquantization.\n\nHowever, if your data isn't normalized, ensuring balance and logical division\nof regions becomes imperative. Consider the following example.\n\nFig: Displays some counter-intuitive facts about hamming distances between 2d\nbinary vectors\n\nIn this scenario, counterintuitively, the quantizer would indicate that yellow\npoints are further from red points and closer to both blue and green points.\nWhile this holds in angular-based metrics like cosine, it contradicts the\ninterpretation under the L2 metric where we can see that the yellow and red\npoints are closer in proximity.\n\nBQ with N-Dimensional Vectors\n\nLet's talk about the density of your data and the ability to uniquely\nrepresent vectors after applying binary quantization. By comparing the number\nof dimensions to the number of data points, you can anticipate the level of\ncollisions, we define a collision occurring between two vectors as the vectors\nhaving the same representation once binarized. As seen in previous examples,\nwith binary quantization in two dimensions, you're limited to constructing\nfour regions. Consequently, when the number of vectors exceeds four,\ncollisions occur, rendering distinction impossible.\n\nHowever, the upside lies in the exponential growth of regions with additional\ndimensions. With each dimension, the number of regions doubles (2d), offering\ngreater potential for differentiation between vector representations. For\ninstance, with 756 dimensions, you already have astonishing 2756 regions at\nyour disposal - this makes collisions between vectors highly improbable even\nwhen you have billions or trillions of vectors. With 1.5K dimensions, the\nnumber of regions can easily accommodate any practical amount of vectors\nwithout a single collision.\n\nFig: Displays the exponentially decreasing probability for collisions in\nvector representations. In essence, this means that the higher the\ndimensionality of your data, the more effectively you can expect binary\nquantization to perform.\n\n## \ud83d\ude80 Performance Improvements with BQ\n\nLet's revisit the advantages of binary quantization. Typically, we employ\nquantization to conserve memory, encoding each dimension as a single bit. In\nWeaviate, floating-point vectors are represented as float32 arrays, yielding a\ncompression rate of 1:32, which is already commendable.\n\nHowever, there's a significant secondary benefit: bitwise operations now\nsuffice for distance calculations between compressed binary vectors. This\ncalculation involves a straightforward exclusive OR (XOR) operation between\ntwo binary arrays, tallying the resulting ones. Moreover, Go offers SIMD-\noptimized operations for these binary functions, resulting in notably faster\ncomputation than with original vectors. But how much faster exactly?\n\nTo address this inquiry, we present brute-force search results using our\nquantizer and the original vectors. We conduct searches across 10,000 vectors\nwith 100 queries, across dimensions ranging from 768, 1536, to 4608.\n\nDimensions| 768d| 1536d| 4608d  \n---|---|---|---  \nLatency uncompressed (microseconds)| 1771.85| 3703.68| 16724.41  \nLatency compressed (microseconds)| 230.72 (13%)| 353.3 (9%)| 896.37 (5%)  \nRecall| 0.745| 0.744| 0.757  \n  \nTable 1: Vector search time improvements between uncompressed and BQ.\n\nWhile the recall isn't exceptionally high, we tackle this concern later\nthrough over fetching candidate neighbors and rescoring results. It's worth\nnoting that as the vector dimension gets larger, we observe more significant\nspeedups. For instance, when brute forcing 768-dimensional compressed vectors,\nwe require only 13% of the time compared to using uncompressed vectors.\nSimilarly, for 1536-dimensional compressed vectors, we need just 9% of the\ntime, and for 4608-dimensional ones, only 5% of the time compared to\nuncompressed vectors.\n\nTraditionally, we rely on building a graph to facilitate approximate search,\nas manually searching through millions of vectors is impractical. However,\nwith such significant time reductions, brute forcing your data becomes a\nviable option. For instance, in the case of 768 dimensions, brute forcing 1\nmillion vectors should only take 23 milliseconds. Even in the worst-case\nscenario (4608 dimensions), it's now feasible, requiring approximately 90\nmilliseconds.\n\nSo, what's the bottom line? Can Weaviate deliver lightning-fast brute-force\nsearches for your data? The answer depends on the size of your data and your\nsearch speed expectations.\n\nThere are several advantages to brute force searching your data. Firstly, you\ncan bypass the need for data indexing, saving the time required to build the\nindex. While indexing in Weaviate isn't overly sluggish, brute forcing allows\nyou to skip this step entirely. Secondly, you no longer need to store a\nproximity graph, resulting in further memory savings. In fact, if you opt to\nbrute force search your data directly from disk, memory usage becomes\nnegligible \u2013 a mere 100MB is sufficient to host your application.\n\nWeaviate recently introduced the flat index, offering the option to brute\nforce data either from disk (the default behavior) or by retaining only\ncompressed data in memory and fetching a small selection of full vectors from\ndisk for final candidate rescoring. Both approaches expedite data ingestion\ncompared to the traditional HNSW index while also reducing memory consumption.\nHowever, if your requirements demand high performance, HNSW remains the\npreferred choice. Nevertheless, the flat index presents a cost-effective,\nhigh-performing alternative. Furthermore, Weaviate now supports binary\nquantization (BQ) for use with both the flat and HNSW indexes.\n\n### Indexing Time Improvements\n\nNow, let's discuss the performance metrics. All experiments were conducted\nusing the Go benchmark. Towards the end of this blog, we'll provide\ninformation for those interested in replicating these experiments with their\ndata. To kick things off, we'll start with a modest dataset of 100,000 vectors\nfrom DBPedia, utilizing ADA002 (1536 dimensions) and Cohere v2 (4096\ndimensions) embeddings, beginning with looking at indexing time.\n\nDimensions| 1536| 4096  \n---|---|---  \nFlat index| 5s| 7s  \nHnsw index| 47s| 1m36s  \nHnsw index+BQ| 21s| 25s  \n  \nTable 2: Indexing time improvements between flat, HNSW uncompressed and BQ\nindices.\n\nAs mentioned previously, the flat index eliminates the need for data indexing.\nInstead, we simply send the data to the server and store it in the stores.\nConversely, the HSNW index necessitates data indexing. It's worth noting that\nthe HNSW index can also benefit significantly from this compression in terms\nof indexing time.\n\n### Memory Footprint Improvements\n\nNow, let's discuss the memory footprint. We'll differentiate between the\navailable options for the flat index as they exhibit varying footprints. When\nusing the flat index with or without BQ, all data is retrieved from disk,\nindependent of data size. If we opt to cache the compressed data, it's stored\nin memory. As observed, the footprint remains lower than that produced by\nHNSW+BQ, as no proximity graph resulting from the indexing process is\nrequired. Additionally, we'll showcase the footprint for the HNSW cases. In\nboth cases, you can anticipate the memory footprint to grow more or less\nlinearly with both dimensions and vectors.\n\nDimensions| 1536| 4096  \n---|---|---  \nFlat index| 77MB| 77MB  \nFlat index + BQ + Cache| 141MB| 183MB  \nHnsw index| 1.02GB| 1.79GB  \nHnsw index+BQ| 214MB| 297MB  \n  \nTable 3: Memory footprint improvements for the various indices.\n\n### Latency Analysis\n\nFinally, let us take a look at the QPS(queries per second) vs. Recall curves\nto get an idea of the solutions' performance. To produce such curves, we\nmodified the ef parameter for the case of HNSW and the rescoringLimit for the\ncase of the flat index. We also used a total of 10 concurrent cores to measure\nthe QPS.\n\nFig: QPS vs Recall for 100k DBPedia vectors\n\nNote the low QPS in the scenario of the pure flat index (depicted as the green\ndot at the bottom right). Here, we're compelled to retrieve all full vectors\nfrom the disk and conduct brute-force operations over the uncompressed\nvectors. While this approach is notably slow, it requires zero memory\nallocation.\n\nMoving on, we employ the same process but integrate binary quantization (BQ).\nIn this iteration, we retrieve less information from disk, as we only need to\naccess the compressed vectors (32x smaller than their uncompressed\ncounterparts). Moreover, brute force operations are expedited, as we operate\nsolely with bits. We generate a candidates list and subsequently rescore them.\nDuring the rescoring process, we retrieve only a small subset of the full\nvectors to construct the final solution. This intermediary solution still\nmaintains a memory-free operation while delivering significantly enhanced\nperformance. It's important to note that this approach hinges on BQ\ncompatibility; otherwise, optimal recall may not be achieved. Additionally,\nensuring a sufficiently high rescoringLimit is important to guarantee good\nrecall.\n\nLastly, we introduce the flat index with cached compressed vectors\n(illustrated by the blue curve). Depending on your recall expectations, you\ncan anticipate QPS ranging from 600 to 1000. However, the memory footprint\nslightly increases in this scenario, as the list of compressed vectors is\nretained in memory, with only a small subset of full vector candidates fetched\nfrom disk.\n\nNext, we'll present the corresponding results for cases involving larger\ndimensions.\n\nFig: QPS vs Recall for 100k DBPedia vectors comparing various indices.\n\nGiven these results it's worth considering the following: for a relatively\nsmall set of vectors (~100,000), if you're aiming for exceptionally high\nrecall, the performance disparity between the flat-compressed-cached curve and\nany HNSW solution isn't notably significant. Some may argue that ~100,000\nvectors isn't a substantial amount, and that's a valid point. However, let's\ndiscuss the implications of combining this feature with Multi-tenancy.\n\nWeaviate ensures total information isolation for each tenant. Imagine having\n~1000 tenants, each with ~100,000 vectors. Surprisingly, the expected\nperformance remains more or less consistent. Suddenly, 100 million vectors\nconstitute a substantial amount of data. Moreover, Weaviate facilitates rapid\ntenant deactivation/lazy reactivation, enabling the creation of an\nexceptionally performant solution with an incredibly low memory footprint,\nprovided you've designed a robust architecture.\n\nNow, let's scale the numbers up further. What should we anticipate with a\nlarger dataset? Unfortunately, brute force scales linearly with the data size.\nIf we were to increase the dataset to 1,000,000 vectors, the QPS would be\nroughly 10 times slower than those exhibited here. However, even with this\nincreased latency, brute force remains a viable option for certain\napplications.\n\nFig: QPS vs Recall for 1000k DBPedia vectors comparing various indices.\n\n## \u2696\ufe0fComparing Product and Binary Quantization\n\nNow that you have multiple quantization techniques available in Weaviate the\nquestion arises which one is better and where to use PQ vs. BQ. This decision\nwill come down to your specific data and will require you to run your own\nbenchmarks. We make code and instructions on how to do this available in the\nnext section. The memory and performance experiments below are meant to make\nthe PQ vs. BQ choice a little easier for you.\n\nFig: QPS vs Recall for 1000k DBPedia vectors comparing HNSW with and without\nPQ and BQ compression.\n\nNotice that the main contribution of BQ is not just that it compresses the\nvectors. The more efficient bitwise distance calculations also play a\nsignificant role. This is why the flat+bq option we discussed above is such a\ngood choice. Not only do we need to read less from disk, but also, the faster\ndistance calculations make brute force faster in Weaviate.\n\nIndex| Indexing Time| Memory Usage  \n---|---|---  \nHNSW| 8m42s| 6437.05MB  \nHNSW+PQ| 21m25s| 930.36MB  \nHNSW+BQ| 3m43s| 711.38MB  \nFLAT+BQ| 54s| 260.16MB  \nFLAT+BQ+CACHE| 53s| 352.99MB  \n  \nTable 4: Comparing the various indices and compression techniques.\n\nNotice how BQ shortens the indexing time with HNSW enormously.\n\n## \ud83e\uddd1\ud83d\udcbbBenchmarking BQ with your own data\n\nHere we provide code and instructions that will help you to balance recall,\nlatency, and memory footprint optimally by allowing you to replicate these\nabove experiments yourself on your data.\n\nWe have included some very useful tools in this repository. To easily run\nthese tests (or any test using your data) you would need the data to be in\nhdf5 format with the same specifications as described in the ANN Benchmarks.\nYou could index your data using the Go benchmarker. This benchmarker could\ngive you a better idea of QPS using concurrent queries. It takes several\nparameters that you could explore but in our runs, we use the following\ncommands:\n\n    \n    \n    go run main.go ann-benchmark -v ~/Documents/datasets/dbpedia-100k-openai-ada002.hdf5 -d cosine --indexType flat\n\nNotice the -d parameter for the distance and \u2013indexType for switching between\nhnsw and flat.\n\nTo run compressed (BQ enabled):\n\n    \n    \n    go run main.go ann-benchmark -v ~/Documents/datasets/dbpedia-100k-openai-ada002.hdf5 -d cosine --indexType flat --bq enabled\n\nNotice the -bq parameter is used to activate compression.\n\nOnce you run the script, you will see the different metrics in your terminal\nat the end of the run. Pay special attention to the QPS and recall. The\nresults will be saved in JSON format under a repository named results in the\nsame path as the script. Next, you can also run the visualize.py script to\nproduce the same graphs we show in this post. Your graphs will be available in\nthe same path as the script as output.png.\n\nHappy compressing!\ud83d\ude80\n\n## Ready to start building?\n\nCheck out the Quickstart tutorial, and begin building amazing apps with the\nfree trial of Weaviate Cloud Services (WCS).\n\nGitHub\n\nForum\n\nSlack\n\nX (Twitter)\n\n## Don't want to miss another blog post?\n\nSign up for our bi-weekly newsletter to stay updated!\n\nBy submitting, I agree to the Terms of Service and Privacy Policy.\n\nTags:\n\n  * research\n  * engineering\n\nEdit this page\n\n  * \ud83e\uddeeWhat is Binary Quantization?\n  * \ud83d\udcd0Details of Working with Binarized Vectors\n\n    * Distance Between Binary Vectors\n    * The importance of data distribution for BQ\n  * \ud83d\ude80 Performance Improvements with BQ\n\n    * Indexing Time Improvements\n    * Memory Footprint Improvements\n    * Latency Analysis\n  * \u2696\ufe0fComparing Product and Binary Quantization\n  * \ud83e\uddd1\ud83d\udcbbBenchmarking BQ with your own data\n\nWeaviate Cloud Services\n\n  * Pricing\n  * Console\n  * Partners\n  * Security\n  * Terms & Policies\n\nCommunity\n\n  * Slack\n  * Instagram\n  * Twitter\n  * GitHub\n  * Linkedin\n  * Forum\n\nMeetups\n\n  * Amsterdam\n  * Boston\n  * New York\n  * San Francisco\n  * Toronto\n\nMore\n\n  * Blog\n  * Podcast\n  * Playbook\n  * Newsletter\n\nCopyright \u00a9 2024 Weaviate, B.V. Built with Docusaurus.\n\n", "frontpage": false}
