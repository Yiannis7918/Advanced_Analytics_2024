{"aid": "39960642", "title": "Testing Frontend \u2013 Lessons from a decade, leading Palantir's main frontend", "url": "https://www.meticulous.ai/blog/lessons-from-a-decade", "domain": "meticulous.ai", "votes": 10, "user": "Gabriel_h", "posted_at": "2024-04-07 13:31:54", "comments": 2, "source_title": "Testing Frontend \u2014 Lessons from over a million lines of TypeScript at Palantir", "source_text": "Testing Frontend \u2014 Lessons from over a million lines of TypeScript at Palantir\n\nSign In\n\nBook a Demo\n\nBack\n\nDecember 30, 2023\n\n# Testing Frontend \u2014 Lessons from over a million lines of TypeScript at\nPalantir\n\n#\n\nTesting\n\n#\n\nTypeScript\n\n#\n\nJavaScript\n\n#\n\nFeatured\n\nQuentin Spencer-Harper\n\nContents\n\nLessons from a decade: the most critical three\n\nGetting testing right is the only investment that has the potential to double\nyour engineering velocity\n\nMaintenance cost really matters\n\n#1: Unit tests: test the right sized unit, at the minimal cut\n\n#2: (Probably) don\u2019t write Enzyme or component tests\n\n#3: Scalable integration tests: look for stable APIs in your app where the\nvariation in the values passed through that API is the root of a lot of the\ncomplexity in your app\n\nWhat tests are worth writing?\n\nTake the denominator to zero\n\n## Lessons from a decade: the most critical three\n\nPrior to Meticulous.ai I spent ten years at Palantir helping lead engineering\nfor Palantir\u2019s main frontend. Through this we learnt how to test frontends\neffectively, the hard way.\n\nWith dozens of apps, 100s of developers, and millions of lines of TypeScript\nwe were able to try different approaches, study the root causes behind each\nbug that made it to production, iterate and learn. We combined our own\nexperiences with talking with other tech leaders - and reading the best\nmaterials we could find out there. Our learnings over those years are\nsomething I\u2019ve never been able to find somewhere else. This post distills the\nmost critical three.\n\nThere\u2019s a lot to FE stability beyond just testing (monitoring & alerting,\nincremental metric-dependent rollout, fast or auto rollback etc.). Here we\nconcentrate just on testing. Here\u2019s why:\n\n## Getting testing right is the only investment that has the potential to\ndouble your engineering velocity\n\nThe main determinant of the impact of an engineering org is building the right\nproduct^\u2020: prioritising the investments & experiments that give you maximal\ninformation on product/market fit, and iterating on a daily or weekly cycle\nwith your users.\n\nBut if you look at pure engineering changes \u2014 developer education, developer\ntooling, developer practices etc. \u2014 most only make incremental improvements to\nyour total velocity. Few have the potential to double your impact. Testing is\nthe single exception.\n\nIf you look carefully at how your team spends their hours, only a fraction of\nit is likely hammering out code \u2014 in most teams more time is sunk into:\n\n  * Manually testing the code as you write it\n  * Testing at PR stage, testing again after addressing PR comments, and sometimes testing again at release stage\n  * Triaging, repro\u2019ing, root causing and fixing issues that occur in the field\n  * ... and all the context switching in-between\n\nIn contrast if you can make any change with confidence then you can not only\niterate dramatically faster (shipping code at the speed you can write it), you\ncan also refactor continuously with complete ease, and ensure you can maintain\nthat initial velocity over time. When developers don\u2019t have the confidence to\nmake sweeping changes they tend to just avoid large refactors to protect their\nindividual velocity. The result can be a self-reinforcing downward spiral: as\nthe codebase quality degrades, refactors become even more risky, and we become\neven more cautious to do them.\n\nDespite this most automated frontend tests we\u2019ve seen teams add (whether at\nthe unit level or the end-to-end level) actually make the team slower rather\nthan faster. The time taken to author them and, much more significantly, to\nmaintain them, outweighs the value they provide.\n\nHowever... if you think carefully about a testing strategy for your projects\nyou can make new tests trivial to write, and often almost entirely eliminate\nany maintenance. If you can crack this then you can move extremely fast. But\nbefore we move onto our three most important learnings in how to make that\nhappen, it\u2019s worth understanding the most critical determinant of success in a\ntesting strategy: maintenance cost.\n\n\u2020 The second largest determinant of impact for mature engineering\norganisations may well be careful strategy (and extreme caution!) around\nmigrations and rebuilds where you have to match existing functionality or\ncompatibility. Tip: break it down into incrementally deliverable steps that\nallow you to iterate as you learn the full costs and tradeoffs involved.\nDeliver incremental value and ensure you can successfully abandon at any\npoint.\n\n## Maintenance cost really matters\n\nRoughly speaking, given a fixed budget, the number of tests you can afford in\nthe long run is inversely proportional to the maintenance cost of each test:\n\nThe more tests you can afford the more coverage you have, and the more value\nyour tests provide. This means the maintenance cost really matters. A fixed\ndecrease in maintenance cost per test can give an exponential increase in the\ncollective value of your tests.\n\nThere are two strategies to achieve this: the first is to assume the tests\nbreak all the time, but make it lightning fast to understand whether the\nchange is expected or not, and to update the tests or snapshots. In this case\nevery second matters: shaving off the time to review and update from 5 seconds\nto 4 seconds can be transformative.\n\nThe second strategy is to avoid having to update tests at all \u2014 and the most\nimportant principle here is to test at the minimal cut:\n\n## #1: Unit tests: test the right sized unit, at the minimal cut\n\nWhen you write a test, whether a unit test or an integration test, you have to\ndecide the scope of your test \u2014 what application code do you run as part of\nyour test, and what do you mock out.\n\nOften it\u2019s tempting to fall into the trap of having a unit test test a single\nclass, function or component \u2014 but more often than not this doesn\u2019t make\nsense.\n\nImagine you have an app that calculates an optimal route between two map\npoints and renders it to a canvas:\n\nYour test can test the route calculation and the route rendering separately,\nor test it together. It may be easier to cover all the route-calculation edge\ncases (different types of route tradeoffs) and all the rendering edge cases\n(different types of curves) by testing each component separately. If the way\nyou want to render routes changes frequently then you won\u2019t need to touch the\ntests for the route-calculation edge cases; and when something breaks it\u2019ll be\neasier to narrow it down to the part of code broken. Each of these modules has\nenough complexity to be worth testing separately.\n\nHowever now imagine that your optimal route calculation module uses a sub-\nfunction to calculate the time taken for each potential sub-part of the route,\nand then utilises that to calculate the optimal overall route. In this case\nit\u2019s possible that it makes more sense to test the whole route calculation\nmodule as a single unit: you only care that it continues to return the optimal\nroute over time, not how the internals are wired together. You may want to\nchange the internal implementation (say, improving performance) over time\nwithout breaking the tests.\n\nIn general you can imagine your app as a network of collaborating modules\n(functions/classes/components etc.):\n\nWhen you write a unit test, you decide what application code is inside the\nscope of the unit test:\n\nAny API that has to be mocked out adds fragility to the test. Try to minimise\nthe API surface that your test boundary bisects, and bisect APIs that are as\nstable as possible: APIs that naturally reflect your domain, rather than APIs\nthat are internal implementation details that may change over time.\n\nAny communication lines crossing this boundary have to be mocked out. And if\nany of those communication lines change - say due to refactoring how the\nmodules communicate, or by changing the APIs of those communication lines \u2014\nthen your test breaks and you have to update it. You therefore want to choose\na minimum cut, that gives maximal coverage: choose the boundary to intersect a\nminimum API surface, that is as stable as possible, while containing inside of\nthe boundary code of a sufficient complexity to be worth testing as a single\nunit.\n\nIf you choose those boundaries poorly then your tests will frequently break as\nthe codebase evolves, and you may find their cost exceeds their value:\n\nThe art of writing productive tests requires working out how to carve up your\napplication such that each test exposes itself to a minimal set of APIs that\nare as stable as possible.\n\nGetting the scope of the tests matters more than pretty much anything else.\nWhen deciding the scope of your tests, consider:\n\n  * What behaviour do you care about preserving over time? If your test implicitly tests implementation details you don\u2019t care about then you may be testing at the wrong level.\n  * Choose natural boundaries for your units to test, that run across stable API lines in your codebase that are easy to mock.\n  * Have the scope of the test sufficiently large such that the unit is complex enough to be worth testing, and the test does not assert too many implementation details. But not so large that it becomes hard to test the many combinations of edge cases, and hard to keep track of what you have vs haven\u2019t tested.\n\n## #2: (Probably) don\u2019t write Enzyme or component tests\n\nComponent tests are unit tests that render a React component in a virtual\nenvironment and test interacting with it. We\u2019ve found these tests are rarely\nworthwhile writing, except for maybe a minority of the more complex\ncollections of components in your application. If you do write them, it often\nmakes sense to test a collection of collaborating utilities and components\nthat together form a single much more complex component.\n\nEven in the most extreme case where you\u2019d expect such tests to be a clear win\n\u2014 Palantir\u2019s open source component framework BlueprintJS, which has complex\ncomponents, used by 10s of 1000s of projects, where the slightest bug can have\nhuge consequences \u2014 it wasn\u2019t always clear that the Enzyme tests actually paid\noff.\n\nThe reason is:\n\n(1) these tests are slow to write and fragile to maintain - you\u2019re often\ntweaking your UX and components, and as you do so the tests need updating\n\n(2) most of the complexity can be more reliably tested by pulling the complex\nlogic out of your component, into utility functions, separate from the UI\nitself, and testing those. These tests are less fragile since they break only\nwhen you change your abstractions or logic: not when you change the structure\nof your DOM. They interact with a much smaller API surface.\n\nIf you have over 50% test coverage from manual unit tests in a frontend\ncodebase then you\u2019ve over-invested in testing: you\u2019re almost certainly slowing\nyourself down rather than speeding yourself up.\n\n## #3: Scalable integration tests: look for stable APIs in your app where the\nvariation in the values passed through that API is the root of a lot of the\ncomplexity in your app\n\n\\- and build a structure that makes each next integration test trivial to add,\nand costs zero to maintain\n\nThink carefully about your application \u2014 where does the majority of the\ncomplexity you want to test come from? What APIs (internal and external)\nchange frequently vs rarely? And can you setup structures that make each\nindividual test trivial to add, and align the testing boundary with the APIs\nthat change rarely, such that these tests rarely need updating?\n\nThis won\u2019t work with all applications, but it does for many. MapboxGL, an open\nsource library for rendering maps, is a great example. All the information to\nrender a map is defined by a JSON document called a style spec. Their main\ntest suite maps style specs to screenshots of the resultant rendered map:\nadding a test is just a case of dropping a style spec JSON file into a folder.\nIf the code changes it\u2019ll show you the difference between the before and after\nscreenshot, and you can merge the updated snapshot. The style spec is public\nAPI for MapboxGL, so it rarely if ever breaks \u2014 and since the style spec never\nbreaks the tests never break.\n\nSimilar techniques can be used with any application that has a stable API\nformat, where variation in the values provided to that API account for the\nmajority of the complexity that needs to be tested. This includes any app\nwhere users author things \u2014 like Figma (designs), Google Docs (docs), and\nSlack (messages). But it can also cover anywhere there\u2019s a stable API between\nservices, or inside an app.\n\nFor example, one of Palantir\u2019s backing services is responsible for returning\nsets of objects from the database systems given a certain query. The tests\nconsist of a library of triples of files: the first file defines the objects\nto populate into the test database, the second a query, and the third a\ngenerated snapshot of the expected results. Adding a test is just a case of\ndropping in a couple of extra files. Since the query format should never break\nthe tests are write-once-change-never: your test coverage grows over time but\nyou never need to update an existing test.\n\n## What tests are worth writing?\n\nOk, so:\n\n  * Don\u2019t fall into the trap of every unit test testing a single function/class.\n  * Choose the size and the scope of the code covered by each test judiciously: not too big to be complex to cover all edge cases; but large enough to cover sufficiently complex functionality to make the test pay off, with a minimal surface to mock out \u2014 and pick a boundary that runs along stable API lines in your codebase that are easy to mock.\n  * If you\u2019re writing tests manually, don\u2019t aim for 100% coverage for FE apps (or anywhere near) \u2014 instead think whether the test will provide more value than it costs\n  * Skip the component tests\n  * Look for opportunities specific to your application, and build a structure that makes each next integration test trivial to add, and costs zero to maintain.\n\nBut what tests are worth writing? If you\u2019re still writing tests manually,\nrather than automatically generating tests, then we\u2019d suggest:\n\n  * Write unit tests for sufficiently complex logic (a single test may test may a small network of functions/classes collaborating together).\n  * Write a smaller number of integration tests for key parts of the system, that test multiple of those modules collaborating together.\n  * And a couple of end to end tests to smoke test key flows that everything works together as a whole.\n\nYou don\u2019t want too many end-to-end tests, or to use these to cover all edge\ncases, since they\u2019ll be extremely expensive to maintain as your UX evolves.\nThis is true whether they\u2019re written in code or recorded through a point-and-\nclick tool.\n\nThese techniques will accelerate your team. But they won\u2019t let you double your\nengineering velocity, not even close. To get there you\u2019ll need to move beyond\nmanually writing and maintaining tests, and:\n\n## Take the denominator to zero\n\nA large web-app has 10s of 1000s of edge cases (logical branches, feature\nflags, configurations, screen sizes, locales, variations in user data &\ncontent and flows initiated from different starting states and in different\norders). As long as you\u2019re manually writing and maintaining tests it\u2019ll be\ninfeasible (and unadvisable) to get anywhere close to 100% test coverage: the\nmaintenance cost will kill you.\n\nA different approach is needed. For that we first need to step back:\n\nAt most engineering organisations every feature is used at least once before\nit\u2019s deployed to production: developers click around on localhost while they\niterate on their branch, previewing their changes. The issue is that every\nexisting user flow can not feasibly be retested on every future change to the\ncodebase (or even on follow up commits on the original pull request). You have\nan N^2 problem.\n\nHowever those daily interactions on localhost, staging stacks, and preview\nURLs by your development team are the key. This data stream describes the\ncomplex flows that unlock the path to test every feature of your app -- and\ncan be used to automatically test for regressions.\n\nMeticulous AI is the category leader here - and can even be used to sample\nproduction flows too. Every interaction is recorded, and by understanding\nevery line of code executed by each flow it maintains a visual snapshot test\nsuite that covers close to 100% of your code. This suite tests your apps UI,\nuser flows and logic at all layers, across all edge cases, all branches, all\nfeature flags. As your app evolves, your test suite evolves. The tests are\nexecuted in an environment that preserves determinism from the browsers\nscheduling layer up \u2014 so no flakes.\n\nYou\u2019ve taken the denominator to zero, and the maximum maintainable test count\nbecomes unbounded.\n\nIf you\u2019ve ever had the rare chance of working on a project with testing at\nthis level you\u2019ll know that programming feels completely different. Every dev\ncan refactor with complete confidence and speed. And so they do, keeping the\ncode fresh & nimble. You\u2019ll be able to move faster than you\u2019ve likely ever\nexperienced before in your career.\n\nNever write, debug or fix a test again. Book a call here to learn more.\n\n## Why Meticulous?\n\nTry it now\n\nMeticulous is a tool for software engineers to catch visual regressions in web\napplications without writing or maintaining UI tests. Meticulous isolates the\nfrontend code by mocking out all network calls, using the previously recorded\nnetwork responses. This means Meticulous never causes side effects and you\ndon\u2019t need a staging environment.\n\nQuentin Spencer-Harper\n\nLinkedInTwitter\n\nShare this\n\n### Related\n\nSee all\n\nMarch 26, 2024\n\nPower <> Meticulous case study\n\nAn overview of Power's feedback regarding their experience with Meticulous.\n\nGabriel Spencer-Harper\n\n#\n\nResources\n\nJanuary 16, 2024\n\n5 Reflect Alternatives to Consider in 2024\n\nExplore in-depth analyses of Reflect and five compelling alternatives,\nassessing their strengths, limitations, and pricing, providing a comprehensive\nguide for choosing the right no-code testing tool in 2024.\n\nKasper Siig\n\n#\n\nTesting\n\nJanuary 16, 2024\n\nVisual Regression Testing in a React App | Step-by-Step Tutorial\n\nA guide on how to do visual regression testing in a React app - a step-by-step\ntutorial. This includes an introduction to visual regression testing, spinning\nup an example app and a step-by-step guide on creating some visual regression\ntests.\n\nGeshan Manandhar\n\n#\n\nTesting\n\n#\n\nReact\n\n#\n\nTutorial\n\n## Ready to start?\n\nSet up in minutes and generate tests to cover your whole application.\n\nGet Started\n\nWhat\n\nHow it WorksSecurityDocs\n\nwho\n\nAbout UsBlog\n\nConnect\n\nLinkedinTwitter\n\nSubscribe to Updates\n\nThank you! Your submission has been received!\n\nOops! Something went wrong while submitting the form.\n\n\u00a9 2023 Meticulous. All rights reserved.\n\nTermsPrivacy\n\n", "frontpage": false}
