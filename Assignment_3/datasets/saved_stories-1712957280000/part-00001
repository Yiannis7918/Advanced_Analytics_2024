{"aid": "40014016", "title": "How Do Machines 'Grok' Data?", "url": "https://www.quantamagazine.org/how-do-machines-grok-data-20240412/", "domain": "quantamagazine.org", "votes": 2, "user": "milleramp", "posted_at": "2024-04-12 15:27:33", "comments": 0, "source_title": "How Do Machines \u2018Grok\u2019 Data? | Quanta Magazine", "source_text": "How Do Machines \u2018Grok\u2019 Data? | Quanta Magazine\n\nWe care about your data, and we'd like to use cookies to give you a smooth\nbrowsing experience. Please agree and read more about our privacy policy.\n\nHow Do Machines \u2018Grok\u2019 Data?\n\nRead Later\n\n###### Share\n\nCopied!\n\n  * Comments\n\n  * Read Later\n\nartificial intelligence\n\n# How Do Machines \u2018Grok\u2019 Data?\n\nBy Anil Ananthaswamy\n\nApril 12, 2024\n\nBy apparently overtraining them, researchers have seen neural networks\ndiscover novel solutions to problems.\n\nRead Later\n\nIrene P\u00e9rez for Quanta Magazine\n\nBy Anil Ananthaswamy\n\nContributing Writer\n\nApril 12, 2024\n\nView PDF/Print Mode\n\nartificial intelligencecomputer scienceexplainersmachine learningneural\nnetworksAll topics\n\n## Introduction\n\nFor all their brilliance, artificial neural networks remain as inscrutable as\never. As these networks get bigger, their abilities explode, but deciphering\ntheir inner workings has always been near impossible. Researchers are\nconstantly looking for any insights they can find into these models.\n\nA few years ago, they discovered a new one.\n\nIn January 2022, researchers at OpenAI, the company behind ChatGPT, reported\nthat these systems, when accidentally allowed to munch on data for much longer\nthan usual, developed unique ways of solving problems. Typically, when\nengineers build machine learning models out of neural networks \u2014 composed of\nunits of computation called artificial neurons \u2014 they tend to stop the\ntraining at a certain point, called the overfitting regime. This is when the\nnetwork basically begins memorizing its training data and often won\u2019t\ngeneralize to new, unseen information. But when the OpenAI team accidentally\ntrained a small network way beyond this point, it seemed to develop an\nunderstanding of the problem that went beyond simply memorizing \u2014 it could\nsuddenly ace any test data.\n\nThe researchers named the phenomenon \u201cgrokking,\u201d a term coined by science-\nfiction author Robert A. Heinlein to mean understanding something \u201cso\nthoroughly that the observer becomes a part of the process being observed.\u201d\nThe overtrained neural network, designed to perform certain mathematical\noperations, had learned the general structure of the numbers and internalized\nthe result. It had grokked and become the solution.\n\n\u201cThis [was] very exciting and thought provoking,\u201d said Mikhail Belkin of the\nUniversity of California, San Diego, who studies the theoretical and empirical\nproperties of neural networks. \u201cIt spurred a lot of follow-up work.\u201d\n\nIndeed, others have replicated the results and even reverse-engineered them.\nThe most recent papers not only clarified what these neural networks are doing\nwhen they grok but also provided a new lens through which to examine their\ninnards. \u201cThe grokking setup is like a good model organism for understanding\nlots of different aspects of deep learning,\u201d said Eric Michaud of the\nMassachusetts Institute of Technology.\n\nPeering inside this organism is at times quite revealing. \u201cNot only can you\nfind beautiful structure, but that beautiful structure is important for\nunderstanding what\u2019s going on internally,\u201d said Neel Nanda, now at Google\nDeepMind in London.\n\n## Beyond Limits\n\nFundamentally, the job of a machine learning model seems simple: Transform a\ngiven input into a desired output. It\u2019s the learning algorithm\u2019s job to look\nfor the best possible function that can do that. Any given model can only\naccess a limited set of functions, and that set is often dictated by the\nnumber of the parameters in the model, which in the case of neural networks is\nroughly equivalent to the number of connections between artificial neurons.\n\nShare this article\n\nCopied!\n\nNewsletter\n\nGet Quanta Magazine delivered to your inbox\n\nRecent newsletters\n\nNeel Nanda helped show how neural networks that had grokked modular arithmetic\ntransformed the numbers using complicated mathematics.\n\nCourtesy of Neel Nanda\n\n## Introduction\n\nAs a network trains, it tends to learn more complex functions, and the\ndiscrepancy between the expected output and the actual one starts falling for\ntraining data. Even better, this discrepancy, known as loss, also starts going\ndown for test data, which is new data not used in training. But at some point,\nthe model starts to overfit, and while the loss on training data keeps\nfalling, the test data\u2019s loss starts to rise. So, typically, that\u2019s when\nresearchers stop training the network.\n\nThat was the prevailing wisdom when the team at OpenAI began exploring how a\nneural network could do math. They were using a small transformer \u2014 a network\narchitecture that\u2019s recently revolutionized large language models \u2014 to do\ndifferent kinds of modular arithmetic, in which you work with a limited set\nnumbers that loop back on themselves. Modulo 12, for example, can be done on a\nclock face: 11 + 2 = 1. The team showed the network examples of adding two\nnumbers, a and b, to produce an output, c, in modulo 97 (equivalent to a clock\nface with 97 numbers). They then tested the transformer on unseen combinations\nof a and b to see if it could correctly predict c.\n\nAs expected, when the network entered the overfitting regime, the loss on the\ntraining data came close to zero (it had begun memorizing what it had seen),\nand the loss on the test data began climbing. It wasn\u2019t generalizing. \u201cAnd\nthen one day, we got lucky,\u201d said team leader Alethea Power, speaking in\nSeptember 2022 at a conference in San Francisco. \u201cAnd by lucky, I mean\nforgetful.\u201d\n\nartificial intelligence\n\n### New Theory Suggests Chatbots Can Understand Text\n\nJanuary 22, 2024\n\nRead Later\n\nThe team member who was training the network went on vacation and forgot to\nstop the training. As this version of the network continued to train, it\nsuddenly became accurate on unseen data. Automatic testing revealed this\nunexpected accuracy to the rest of the team, and they soon realized that the\nnetwork had found clever ways of arranging the numbers a and b. Internally,\nthe network represents the numbers in some high-dimensional space, but when\nthe researchers projected these numbers down to 2D space and mapped them, the\nnumbers formed a circle.\n\nThis was astonishing. The team never told the model it was doing modulo 97\nmath, or even what modulo meant \u2014 they just showed it examples of arithmetic.\nThe model seemed to have stumbled upon some deeper, analytical solution \u2014 an\nequation that generalized to all combinations of a and b, even beyond the\ntraining data. The network had grokked, and the accuracy on test data shot up\nto 100%. \u201cThis is weird,\u201d Power told her audience.\n\nThe team verified the results using different tasks and different networks.\nThe discovery held up.\n\n## Of Clocks and Pizzas\n\nBut what was the equation the network had found? The OpenAI paper didn\u2019t say,\nbut the result caught Nanda\u2019s attention. \u201cOne of the core mysteries and\nannoying things about neural networks is that they\u2019re very good at what they\ndo, but that by default, we have no idea how they work,\u201d said Nanda, whose\nwork focuses on reverse-engineering a trained network to figure out what\nalgorithms it learned.\n\nNanda was fascinated by the OpenAI discovery, and he decided to pick apart a\nneural network that had grokked. He designed an even simpler version of the\nOpenAI neural network so that he could closely examine the model\u2019s parameters\nas it learned to do modular arithmetic. He saw the same behavior: overfitting\nthat gave way to generalization and an abrupt improvement in test accuracy.\nHis network was also arranging numbers in a circle. It took some effort, but\nNanda eventually figured out why.\n\nWhile it was representing the numbers on a circle, the network wasn\u2019t simply\ncounting off digits like a kindergartner watching a clock: It was doing some\nsophisticated mathematical manipulations. By studying the values of the\nnetwork\u2019s parameters, Nanda and colleagues revealed that it was adding the\nclock numbers by performing \u201cdiscrete Fourier transforms\u201d on them \u2014\ntransforming the numbers using trigonometric functions such as sines and\ncosines and then manipulating these values using trigonometric identities to\narrive at the solution. At least, this was what his particular network was\ndoing.\n\nWhen a team at MIT followed up on Nanda\u2019s work, they showed that the grokking\nneural networks don\u2019t always discover this \u201cclock\u201d algorithm. Sometimes, the\nnetworks instead find what the researchers call the \u201cpizza\u201d algorithm. This\napproach imagines a pizza divided into slices and numbered in order. To add\ntwo numbers, imagine drawing arrows from the center of the pizza to the\nnumbers in question, then calculating the line that bisects the angle formed\nby the first two arrows. This line passes through the middle of some slice of\nthe pizza: The number of the slice is the sum of the two numbers. These\noperations can also be written down in terms of trigonometric and algebraic\nmanipulations of the sines and cosines of a and b, and they\u2019re theoretically\njust as accurate as the clock approach.\n\nZiming Liu followed up on Nanda\u2019s work and discovered a second way that\ngrokking networks were processing data, along with several other approaches\nthat didn\u2019t seem to have interpretations.\n\nWenting Gong\n\n## Introduction\n\n\u201cBoth [the] clock and pizza algorithms have this circular representation,\u201d\nsaid Ziming Liu, a member of the MIT team. \u201cBut ... how they leverage these\nsines and cosines are different. That\u2019s why we call them different\nalgorithms.\u201d\n\nAnd that still wasn\u2019t all. After training numerous networks to do modulo math,\nLiu and colleagues discovered that about 40% of algorithms discovered by these\nnetworks were varieties of the pizza or clock algorithms. The team hasn\u2019t been\nable to decipher what the networks are doing the rest of the time. For the\npizza and clock algorithms, \u201cit just happens that it finds something we humans\ncan interpret,\u201d Liu said.\n\nAnd whatever the algorithm a network learns when it groks a problem, it\u2019s even\nmore powerful at generalization than researchers suspected. When a team at the\nUniversity of Maryland fed a simple neural network training data with random\nerrors, the network at first behaved as expected: Overfit the training data,\nerrors and all, and perform poorly on uncorrupted test data. However, once the\nnetwork grokked and began answering the test questions correctly, it could\nproduce correct answers even for the wrong entries, forgetting the memorized\nincorrect answers and generalizing even to its training data. \u201cThe grokking\ntask is actually quite robust to these kinds of corruptions,\u201d said Darshil\nDoshi, one of the paper\u2019s authors.\n\n## Battle for Control\n\nAs a result, researchers are now beginning to understand the process that\nleads up to a network grokking its data. Nanda sees the apparent outward\nsuddenness of grokking as the outcome of a gradual internal transition from\nmemorization to generalization, which use two different algorithms inside the\nneural network. When a network begins learning, he said, it first figures out\nthe easier memorizing algorithm; however, even though the algorithm is\nsimpler, it requires considerable resources, as the network needs to memorize\neach instance of the training data. But even as it is memorizing, parts of the\nneural network start forming circuits that implement the general solution. The\ntwo algorithms compete for resources during training, but generalization\neventually wins out if the network is trained with an additional ingredient\ncalled regularization.\n\n\u201cRegularization slowly drifts the solution toward the generalization\nsolution,\u201d said Liu. This is a process that reduces the model\u2019s functional\ncapacity \u2014 the complexity of the function that the model can learn. As\nregularization prunes the model\u2019s complexity, the generalizing algorithm,\nwhich is less complex, eventually triumphs. \u201cGeneralization is simpler for the\nsame [level of] performance,\u201d said Nanda. Finally, the neural network discards\nthe memorizing algorithm.\n\nSo, while the delayed ability to generalize seems to emerge suddenly,\ninternally the network\u2019s parameters are steadily learning the generalizing\nalgorithm. It\u2019s only when the network has both learned the generalizing\nalgorithm and completely removed the memorizing algorithm that you get\ngrokking. \u201cIt\u2019s possible for things that seem sudden to actually be gradual\nunder the surface,\u201d Nanda said \u2014 an issue that has also come up in other\nmachine learning research.\n\n## Related:\n\n  1. ### How Quickly Do Large Language Models Learn Unexpected Skills?\n\n  2. ### How Chain-of-Thought Reasoning Helps Neural Networks Compute\n\n  3. ### Will Transformers Take Over Artificial Intelligence?\n\nDespite these breakthroughs, it\u2019s important to remember that grokking research\nis still in its infancy. So far, researchers have studied only extremely small\nnetworks, and it\u2019s not clear if these findings will hold with bigger, more\npowerful networks. Belkin also cautions that modular arithmetic is \u201ca drop in\nthe ocean\u201d compared with all the different tasks being done by today\u2019s neural\nnetworks. Reverse-engineering a neural network\u2019s solution for such math might\nnot be enough to understand the general principles that drive these networks\ntoward generalization. \u201cIt\u2019s great to study the trees,\u201d Belkin said. \u201cBut we\nalso have to study the forest.\u201d\n\nNonetheless, the ability to peer inside these networks and understand them\nanalytically has huge implications. For most of us, Fourier transforms and\nbisecting arcs of circles are a very weird way to do modulo addition \u2014 human\nneurons just don\u2019t think like that. \u201cBut if you\u2019re built out of linear\nalgebra, it actually makes a lot of sense to do it like this,\u201d said Nanda.\n\n\u201cThese weird [artificial] brains work differently from our own,\u201d he said.\n\u201c[They] have their own rules and structure. We need to learn to think how a\nneural network thinks.\u201d\n\nBy Anil Ananthaswamy\n\nContributing Writer\n\nApril 12, 2024\n\nView PDF/Print Mode\n\nartificial intelligencecomputer scienceexplainersmachine learningneural\nnetworksAll topics\n\nShare this article\n\nCopied!\n\nNewsletter\n\nGet Quanta Magazine delivered to your inbox\n\nRecent newsletters\n\nThe Quanta Newsletter\n\nGet highlights of the most important news delivered to your email inbox\n\nRecent newsletters\n\n## Also in Computer Science\n\nTuring Award\n\n### Avi Wigderson, Complexity Theory Pioneer, Wins Turing Award\n\nBy Stephen Ornes\n\nApril 10, 2024\n\n3\n\nRead Later\n\nQ&A\n\n### The Researcher Who Explores Computation by Conjuring New Worlds\n\nBy Ben Brubaker\n\nMarch 27, 2024\n\n1\n\nRead Later\n\nquantum computing\n\n### The Best Qubits for Quantum Computing Might Just Be Atoms\n\nBy Philip Ball\n\nMarch 25, 2024\n\n5\n\nRead Later\n\n## Comment on this article\n\nQuanta Magazine moderates comments to facilitate an informed, substantive,\ncivil conversation. Abusive, profane, self-promotional, misleading, incoherent\nor off-topic comments will be rejected. Moderators are staffed during regular\nbusiness hours (New York time) and can only accept comments written in\nEnglish.\n\n## Next article\n\nMy Fantastic Voyage at Quanta Magazine\n\nAll Rights Reserved \u00a9 2024\n\nAn editorially independent publication supported by the Simons Foundation.\n\nLog in to Quanta\n\n## Use your social network\n\nor\n\nDon't have an account yet? Sign up\n\nSign Up\n\nCreating an account means you accept Quanta Magazine's Terms & Conditions and\nPrivacy Policy\n\nForgot your password?\n\nWe\u2019ll email you instructions to reset your password\n\nChange your password\n\nEnter your new password\n\n", "frontpage": false}
