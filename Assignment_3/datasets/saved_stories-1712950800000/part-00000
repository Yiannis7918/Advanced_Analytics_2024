{"aid": "40012688", "title": "RAG Vector Database Traps and How to Avoid Them", "url": "https://vectorize.io/rag-vector-database-traps/", "domain": "vectorize.io", "votes": 1, "user": "bytearray", "posted_at": "2024-04-12 13:37:48", "comments": 0, "source_title": "5 RAG Vector Database Traps and How to Avoid Them - Vectorize", "source_text": "5 RAG Vector Database Traps and How to Avoid Them - Vectorize\n\nWe use essential cookies to make our site work. With your consent, we may also\nuse non-essential cookies to improve user experience, personalize content, and\nanalyze website traffic. For these reasons, we may share your site usage data\nwith our analytics partners. By clicking \u201cAccept,\u201d you agree to our website's\ncookie use as described in our Cookie Policy. You can change your cookie\nsettings at any time by clicking \u201cPreferences.\u201d\n\n  * Use Cases\n\n    * Question Answering Systems\n    * AI Copilots\n    * Call Center Automation\n    * Content Automation\n    * Hyper-personalization\n  * Blog\n  * About\n  * Learn\n\n    * Prompt Engineering\n    * Retrieval Augmented Generation (RAG)\n    * Vector Database Guide\n  * Contact\n\nContact Us\n\n#### Be on of the first to try Vectorize!\n\nEdit Content\n\nRAG, Vector Database\n\n# 5 RAG Vector Database Traps and How to Avoid Them\n\nApril 10, 2024 Chris Latimer No comments yet\n\n## Retrieval Augmented Generation (RAG) and Vector Databases\n\nFor developers that are building generative AI features, retrieval augmented\ngeneration is becoming the standard way to connect external knowledge bases to\nthe large language models (LLMs) that are powering their applications.\n\nIncreasingly, developers are discovering that the best way to handle the\nretrieval part of RAG is with a vector database. On paper, this process is\nsimple. You populate your vector database with your most relevant documents.\nYou query that database using a similarity search. The database returns back\nrelevant documents or parts of those documents. You then give that context to\nyour LLM, and get back better, more accurate responses from your LLM.\n\nEasy right? Well... Often that\u2019s not the case. And one of the most frustrating\nexperiences for an AI engineer is to build a RAG application, wire everything\nup, and then discover things aren\u2019t working as you\u2019d like. The LLM produces\nhallucinations, users are frustrated by their poor results, and you\u2019re left to\ntroubleshoot the problem, which is no small feat given the non-deterministic\nnature of these systems!\n\n## How Do RAG Models Work? Architecture Overview\n\nTo understand where things can go wrong, we need to start by examining the\nvarious steps we need to go through to create a fully working retrieval\naugmented generation (RAG) solution. The end state of a RAG application will\nusually look something like this:\n\nIf you aren\u2019t familiar with the basics of RAG architecture, you should read\nour very thorough introduction to retrieval augmented generation, for a more\ncomplete overview.\n\nIn order to build out this RAG application architecture, there are several key\nsteps we need to complete before our application is fully functional. It is\nvery easy to make a mistake in any of these steps that can torpedo the overall\neffectiveness of your RAG architecture. To start, though, let\u2019s explain what\nthe steps are and how to go about building out your LLM tech stack.\n\n### Extracting relevant documents from source systems\n\nDepending on the application you\u2019re building, your large language model may\nneed context that lives in file systems, SaaS platforms, traditional\ndatabases, or in a knowledge base. This extraction process can be accomplished\nin a number of ways such as via one-off scripts to parse PDF files in python\nusing a library like PyPDF2 or PDFMiner.\n\nYou may also need to integrate with knowledge bases or SaaS platforms using\ntheir APIs. This can be time consuming to build these integrations, but can\nensure your RAG application has all the relevant context it needs from across\nyour organization.\n\n### Chunking documents\n\nWhether you are working with PDF documents, wiki pages you retrieved from a\nknowledge base like Atlassian Confluence or Notion, or account notes from\nSalesforce, you will need to decide how you want to store those documents in\nyour vector database.\n\nRetrieval augmented generation relies heavily on vector search. When you\nperform a similarity search against your vector database, the ideal query\nresults would be the exact information that the language model needs to\nrespond to your prompt.\n\nGenerally speaking, this means you want to vectorize smaller chunks rather\nthan massive ones. For example, pushing a 300 page document into the context\nwindow of your large language model will often produce worse results than if\nyou had broken that document up into page-sized or paragraph-sized chunks.\n\n### Creating vector embeddings\n\nChunking and vector embeddings are closely related. You will most often use\nyour document chunks as the input into a text embedding model. An embedding\nmodel is a specialized type of machine learning model. The output of these\nmodels is a vector which encodes the semantic meaning of the text you supplied\nto the model.\n\nVectors are just arrays of floating point numbers. This means we can use\nstandard mathematical techniques such as cosine similarity and dot product as\ndistance metrics to power semantic search. This allows the vector database to\nidentify chunks from the most similar documents by comparing the distance\nmetric between the search input query vector that is passed in to the vector\nsearch request, and the vector representations that are stored in vector\nindexes within the vector database. In short, we can perform approximate\nnearest neighbor searches to retrieve the most relevant context to the LLM.\n\nEmbedding models are generally created one of two ways. Either they are open\nsource machine learning models that are distributed as python libraries you\ncan download and run from GitHub or Hugging Face. Or, they are proprietary\nmodels that can only be created via REST API calls to a web endpoint. Both\nhave their advantages and disadvantages and you can use either option to\ngenerate vectors that will work very well for vector search and within a\nretrieval augmented generation architecture.\n\n### Populating the vector database\n\nDifferent vector databases have different interfaces you can use to populate\nthem. Modern cloud native vector databases like AstraDB, Pinecone, and Zilliz\nall have REST APIs along with lightweight libraries that wrap the API for\npopular languages like TypeScript and Python.\n\nLikewise, more established databases which have vector support such as MongoDB\nand PostgreSQL will use their existing libraries and/or binary drivers to\npersist vector data and facilitate vector search queries.\n\nFrom an actual vector persistence standpoint, most of these offerings are\ngoing to behave in pretty similar ways. You need to know ahead of time how\nmany dimensions each of your vectors will have. Newer vector databases will\nuse search indexes as their primary data types while more traditional\nrelational and NoSQL databases will use tables or documents with vector-\nspecific data points within these structures.\n\n#### The important role of metadata in vector search\n\nMetadata is important for primarily two reasons. The first is that if you\nexpect the number of vectors in a vector index to grow large, the\n\u201capproximate\u201d part of approximate nearest neighbor gets worse and worse.\n\nIf your vector database is able to return the top 3% most similar vectors when\nyou perform a semantic search, that\u2019s will be fine when your vector indexes\nhave 100 or 1,000 vectors in them. However, if you expect that you might have\na million vectors, then that 3% now encompasses that top 30,000 most similar\nsearches which can be pretty darn different.\n\nMost popular vector databases are able to improve the relevant information\nthat gets retrieved in these high volume similarity search use cases by\nsupporting metadata filtering. Metadata filtering is essentially a pre-\nfiltering step that is used to narrow down the set of vectors to search when\nyou submit a vector search query. This can greatly improve both the speed and\naccuracy of the retrieval process, since there are fewer vector embeddings\nthat need to be compared in order to identify the relevant knowledge to\nreturn.\n\nThe other reason metadata is important is because in retrieval augmented\ngeneration (RAG), you will often want to surface key information about the\nchunks that get returned such as the document location, unique identifiers,\nand other metadata.\n\n#### Hybrid search and traditional search engine capabilities\n\nWhile vector databases excel at performing semantic search, increasingly\ndevelopers are combining vector search with more traditional full text keyword\nsearch in an approach known as hybrid search.\n\nCertain platforms such as OpenSearch, Elastic search and others offer a\nspecialized database that supports multiple query options and even lets you\nspecify the mixture of vector similarity search and keyword search results\nthat you would like to return.\n\n### Refreshing your document vectors\n\nOnce your vector database is initially populated with vector embeddings you\ncan sail away happily into retrieval augmented generation paradise, right?\nWell, unfortunately no. Even if you have achieved a high performing RAG\napplication, your vector database starts to get stale almost immediately,\nespecially if its sourced from a SaaS platform.\n\nWhen building your RAG architecture, you must not just consider the initial\npopulation, but also have a strategy for keeping vector data fresh.\n\n## The Five Common RAG Vector Database Traps\n\nNow that we\u2019ve deconstructed the typical data ingestion process that you must\nimplement to build solid vector search capabilities to power your RAG\napplications, let\u2019s take a deeper look at the mistakes that many developers\nand companies make along the way.\n\n### Trap 1: Using the wrong chunk size\n\nIf you create poorly sized chunks, you will have a difficult time retrieving\nthe most relevant information to power your retrieval augmented generation\n(RAG) applications. As an extreme example, you could imagine that a sentence-\nchunking strategy would often return very similar responses, especially for\nprompts that contain only short input query strings. However, those chunks\nthat come back in the return data from your vector search won\u2019t be\nparticularly useful to help the LLM provide a relevant, accurate response.\n\nLikewise, if you use chunk sizes that are too big, your query may return\nchunks that cover several pages of text from a document. If your query\nparameters indicate you want to get back the top 10 most similar vectors, you\nmay end up with low context relevancy.\n\nIn these cases, the context provided to the large language models include the\ninformation needed to generate an accurate answer, but your language model may\nstruggle to hone in on the signal through the noise and produce the result you\nwant.\n\nFor this reason, it\u2019s often best to rely on either a semantic chunking\napproach which allows you to capture longer passages of related text.\n\nSemantic chunking has the downside that it relies on natural language\nprocessing to identify the semantic breaks. This often involves machine\nlearning libraries which tend to be more computationally expensive and slower\nthan alternative techniques.\n\nRecursive chunking is often used to create vector embeddings from a given\npiece of text, then provides context before and after that text to capture any\nrelevant context that surrounds each snippet.\n\n### Trap 2: Picking the wrong embedding models\n\nThe starting point for most developers when picking a model is usually the\nHugging Face massive text embedding benchmark (MTEB) leaderboard. For\nretrieval augmented generation use cases, the machine learning models which\nscore best on retrieval benchmarks will be the ones that are best suited for\nRAG applications.\n\nHowever, there are a number of considerations and trade offs you need to think\nthrough as you\u2019re selecting an embedding model as well. Many of these models\nwere specifically designed to perform well with the benchmarks\u2019 original\ntraining data.\n\nIn some cases, the machine learning models that perform best on the\nbenchmarks, may be somewhat overfit for these lab-environment tests, but may\nnot perform as well in real world use cases for you and your specific data.\nFor this reason, it\u2019s important to take a data driven approach to evaluate\nwhich embedding model performs best on metrics such as Context Relevancy and\nNormalized Discounted Cumulative Gain.\n\n### Trap 3: Not designing your metadata\n\nVector databases are well suited to provide both core vector search\ncapabilities as well as metadata filtering. However, many developers are so\nfocused on generating vectors and storing data that they forget that their\nuser query often requires retrieval of metadata as well the embedding data.\n\nYour metadata design should take into account any anticipated partitioning\nkeys that can narrow down your search query as well as relevant details from\nyour knowledge base that you want to get back when you perform a vector\nsimilarity search.\n\n### Trap 4: Building fragile vector data pipelines\n\nThere are many mature, battle tested data engineering platforms in the market,\nbut most of these excel at working with structured data. Common RAG\napplications like chatbots that answer a user\u2019s question in human like text,\nor recommendation systems that generate recommendations from knowledge base\ndata, rely much more heavily on unstructured data.\n\nHowever, building vector ingestion pipelines from knowledge bases is fraught\nwith many of the same potential points of failure as traditional structured\ndata pipelines. API calls to generate embeddings can fail, leaving you with a\nhalf populated vector index. Your vector database can have an outage or\ntimeout along with many other common failure conditions.\n\nBuilding resilient data integration capabilities is particularly challenging\nfor RAG systems because many of the go-to tools we reach for for more\ntraditional data engineering solutions \u2013 solutions which have advanced retry\nand error handling capabilities built-in \u2013 aren\u2019t well suited to handle either\nthe unstructured data sources needed for RAG models or the vector generation\nand persistence capabilities needed here either.\n\nFor this reason, you will need to find a specialized product, like Vectorize,\nthat is purpose built to support robust data engineering requirements on\nvector data pipelines, or take care to build these into integration scripts\nyou build on your own.\n\n### Trap 5: Letting your vector search indexes get stale\n\nStale vector data can turn great generative AI systems into useless systems in\nshort order.\n\nImagine that you\u2019ve build a RAG application that uses an LLM to provide\nnatural language processing capabilities on top of your company\u2019s Salesforce\ndata. You\u2019ve generated vector embeddings for customer data, sales\nopportunities, and customer meeting notes.\n\nHowever, you also have sales reps who are continuously updating Salesforce.\nThis means that your RAG application might give great answers about how your\ndeals looked when the vector database was first populated, but each new query\nstrays farther and farther from the truth.\n\nAs you are building your integrations into your vector databases, you want to\ntake care to ensure you are prepared to handle new data as it becomes\navailable. You also want to consider the freshness requirements that you need\nfor your given use case.\n\n## Avoid these traps with Vectorize\n\nIt should come as no surprise that Vectorize was built to solve many of these\ncommon challenges for you out of the box, in many cases using only our free\ntools!\n\n### Data driven decisions with experiments\n\nYou could spend days writing scripts to populate various vector indexes with\ndifferent embedding models and chunking strategies. You could write your own\ntests to score how well each combination performs. You could crunch the number\nand try to get a quantitative assessment of which option works best for you\nand your data. You could do that... OR!\n\nYou could save yourself days of writing code and run a few experiments on\nVectorize and have this information in minutes.\n\nExperiments let you run multiple models to generate text embeddings and\ndifferent chunking strategies in parallel. First you provide a representative\nsample of your data in the form of PDFs, text files, HTML, or other files. We\nthen generate a set of canonical questions that your data would be best suited\nto answer. We help you identify if the data you have will support the use case\nyou are aiming to solve, and we tell you definitively which vectorization\nstrategy is going to produce the most relevant context.\n\nThe end result is that you no longer need to rely on guesswork and gut feel\nwhen deciding on the optimal approach to populate your vector databases.\n\n### Qualitative, collaborative assessments with RAG sandbox\n\nData is great, but it\u2019s often helpful to validate quantitative assessments\nwith actual real world experience. The RAG sandbox lets you chat with your\nexperiment data. You can select the vector index you want to use, pick your\nLLM and submit queries to see how well they perform in your experience. You\ncan invite teammates to try it out with you and compare notes. This can help\nyou pick the right strategy with the best price/performance characteristics\nfor your situation.\n\n### Production-ready vector pipelines\n\nVectorize is built on top of a distributed, real time streaming platform. It\u2019s\nthe same technology which is being used in mission critical enterprise\napplications across the globe. Operated on a cloud native architecture built\non Kubernetes, Vectorize vector pipelines give you the out of the box\nintegration capabilities to connect to unstructured data sources and well as\nvector databases with all of the data engineering best practices built-in.\n\nIf you do not have access to Vectorize yet, sign up for our waitlist or\ncontact us directly if you would like to discuss your project or use case with\nus.\n\n## Conclusion\n\nRAG is the emerging standard for building LLM-powered applications, and vector\ndatabase are an important part of your generative AI tech stack. But they are\nnot without their challenges and many developers have found themselves\ntroubleshooting tricky problems in unfamiliar territory.\n\nWith the right tools and right technology, you can build RAG applications\nfaster, with more accurate context, and that will deliver better results.\n\n### Share this:\n\n  * Twitter\n  * LinkedIn\n  * Facebook\n  * Reddit\n  * Pinterest\n  * Threads\n  * X\n\n### Related\n\n### Leave a ReplyCancel reply\n\n#### Search\n\n#### Categories\n\n#### Recent posts\n\n  * How to Get More from Your Pinecone Vector Database\n\n  * Picking the best embedding model for RAG\n\n  * 5 RAG Vector Database Traps and How to Avoid Them\n\n#### Tags\n\nRAG Retrieval Augmented Generation\n\nThe easiest, fastest way to connect your data to your LLMs.\n\n##### Resources\n\n  * Support center\n  * Documentation\n  * Community\n  * Hosting\n\n##### Company\n\n  * About us\n  * Latest news\n  * Contact us\n  * Resources\n\n\u00a9 Vectorize AI, Inc, All Rights Reserved.\n\n  * Terms & Conditions\n  * Privacy Policy\n\n## Discover more from Vectorize\n\nSubscribe now to keep reading and get access to the full archive.\n\nContinue reading\n\nLoading Comments...\n\n", "frontpage": false}
