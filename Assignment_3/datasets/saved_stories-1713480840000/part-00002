{"aid": "40078241", "title": "Compute Thresholds Are Ineffective", "url": "https://hyperdimensional.substack.com/p/compute-thresholds-are-ineffective", "domain": "hyperdimensional.substack.com", "votes": 2, "user": "eddyzh", "posted_at": "2024-04-18 16:53:20", "comments": 0, "source_title": "Compute Thresholds are Ineffective", "source_text": "Compute Thresholds are Ineffective - by Dean W. Ball\n\n# Hyperdimensional\n\nShare this post\n\n#### Compute Thresholds are Ineffective\n\nhyperdimensional.substack.com\n\n#### Discover more from Hyperdimensional\n\nA newsletter about emerging technology and the future of governance.\n\nContinue reading\n\nSign in\n\n# Compute Thresholds are Ineffective\n\n### Why compute thresholds are a poor tool for policymakers\n\nDean W. Ball\n\nApr 11, 2024\n\n7\n\nShare this post\n\n#### Compute Thresholds are Ineffective\n\nhyperdimensional.substack.com\n\nShare\n\nQuick Hits\n\n  1. A new AI music generation model called Udio was released this week. It is a notable improvement from previous models of this kind. I recommend this one in particular.\n\n  2. Researchers have enhanced DeepMind\u2019s AlphaGeometry system with a geometric theorem-proving algorithm called Wu\u2019s Method. DeepMind\u2019s original AlphaGeometry paper, released in January of this year, scored 25/30 on the International Mathematical Olympiad, just short of human gold medalists in this competition for the world\u2019s best high school math students. With Wu\u2019s Method, the system scores 27/30, the first time an AI system has bested human gold medalists.\n\n  3. Apple unveiled a model called Ferret-UI, which is a multimodal language model intended to understand smartphone user interface screens (I wonder what the application could be). Apple has been releasing a lot of interesting research in recent months, almost all of which points to their efforts to make AI assistants run locally on smartphones, laptops, and VR headsets. This has substantial implications for energy use (no data center required), privacy (no need to send prompts to a third-party provider), and policy (many proposed regulatory schemes would effectively require AI developers to monitor their users\u2019 activity and preserve the ability to shut it down, which would probably be impossible, or at least much thornier, if the model was running locally). Local AI can only be so powerful compared to the frontier models, so they are also rumored to be considering a partnership with Google to use Gemini.\n\n  4. Not AI, but it is apparently now possible to arrest tooth decay, perhaps permanently, using a CRISPR-modified bacteria. It\u2019s being marketed as Lumina and available for purchase. Because it\u2019s marketed as a probiotic supplement, it is subject to far less FDA scrutiny than a typical therapeutic. With CRISPR genome editing of bacteria (and soon, creation from scratch of novel forms of bacteria using AI) becoming more common, I wonder how long the FDA will allow this dynamic to persist.\n\nThe Main Idea\n\nWhen generative AI systems hit the mainstream\u2014and shocked the public\u2014starting\nin 2022, the federal government was caught off guard. Policymakers felt a need\nto get a handle on AI, and to do that, they had to make AI \u2018legible\u2019 to the\nstate.\n\nThe concept of state \u2018legibility\u2019 comes from James C. Scott\u2019s classic book,\nSeeing Like a State. The basic idea is that, in attempting to manage a\nvariegated, shifting, and fractally complex reality, states tend to impose\nsimplistic measures from the top down. For example, in postrevolutionary\nFrance, rulers were faced with a perplexing array of agricultural land uses\npractices and units of measure, each of which had developed for complex\nhistorical, geographic, and economic reasons. The state, however, sought to\nunify this mess by codifying all trade under standardized units of measure.\nThis is the origin of the kilogram, among other common units.\n\nScott\u2019s point is not that legibility is good or bad\u2014it\u2019s that it happens, and\nthat one cannot understand statecraft without understanding the process by\nwhich a state makes reality legible to itself.\n\nVery quickly, western policymakers converged around so-called \u201ccompute\nthresholds\u201d as a means of making AI administratively legible. The idea is\nstraightforward: AI models are trained on large amounts of compute, and by\ncreating thresholds based on this amount, the state can effectively regulate\nor monitor the more powerful models while taking a more laissez-faire approach\nto the less powerful ones. Specifically, most enacted and proposed policies\nuse floating-point operations (flops), essentially the number of mathematical\noperations like multiplication or addition required to train the model.\n\nThe Biden Executive Order on AI imposes a reporting threshold for general-\npurpose foundation models like GPT-4 if they required more than 10^26 (one\nhundred septillion) flops to train, and for DNA or RNA-based models if they\nemploy more than 10^23 (one hundred sextillion) flops. The European Union\u2019s AI\nAct triggers an additional level of regulatory scrutiny at 10^25 flops. Other\nproposed bills at the state and federal level use a threshold in this\nballpark.\n\nThe problem is that, as it turns out, compute thresholds may not be a durably\nuseful means of making AI legible to policymakers.\n\nThe fundamental problem with compute thresholds is that, even absent\ngovernment regulation and all else being equal, compute is a cost for AI\ndevelopers. If they can achieve the same results for less compute, they will\nbe heavily incentivized to do so. Some laws trigger additional levels of\nregulatory scrutiny using metrics like a company\u2019s size. Dodd-Frank, for\nexample, regulates banks differently depending on the amount of capital they\nhold. While this creates numerous incentive problems that economists have\nthoroughly documented, in the broadest sense, companies do want to grow. Firms\ndo not in general try to become smaller.\n\nCompute, however, is more complicated: It is a cost center. Businesses tend to\nlike to reduce those. Yes, frontier model developers aim to use massive scale,\nbut in general, even they are incentivized to make their models more efficient\n(and hence use less compute). Efficiency is even more important for medium and\nsmall-sized AI firms. Compute thresholds, therefore, swim against the current\nof AI development, and in fact make the current stronger: the existence of\ncompute thresholds is yet another incentive to find ways to use less compute\nto avoid additional regulatory scrutiny.\n\nCompute thresholds are thus being squeezed from both the high and low ends of\nthe AI industry. Smaller or compute-limited players, which includes most\nstartups, all global academia, and, because of US-led export controls, most\nChinese companies and researchers, have an obvious incentive to use their\ncompute more efficiently. Such improvements happen weekly and many are freely\navailable as arXiv pre-prints. New model architectures (especially state space\nmodels such as Mamba) are showing substantial empirical performance gains in\nmodel training and inference.\n\nWithin the context of the more traditional transformer architecture (upon\nwhich almost all language models are based), clever tricks, such as the\nrecently proposed Sophia pre-training optimization method, have been saidto\nreduce global compute usage by as much as 10%. Smaller tricks are proposed on\nan almost weekly basis: some of these ideas don\u2019t replicate in other settings,\nand others are not appropriate for most models\u2014others, however, can compound.\n\nTricks like this (and many, many others) are how Cohere, an AI company that\nmakes language models aimed at businesses, was able to deliver an open-source\nmodel that outperforms the original version of GPT-4 from a year ago with\nroughly 1/18^th the number of parameters as that version of GPT-4 had. That\u2019s\nthe kind of efficiency that is possible to achieve in just one year in this\nfast-moving field.\n\nAnd it gets even tougher. Sakana AI, a Japanese AI research company, released\na novel approach of merging open-source models together using evolutionary\nalgorithms. The researchers showed that the resultant merged model performed\nbetter on benchmarks than either of its constituent models, meaning something\nabout the Frankensteinian operation improved the overall model. Because this\nmethod does not require training any model from scratch, it necessitates very\nlittle compute. There are thousands of open-source models on HuggingFace, so\nthe possibilities here are limitless. We don\u2019t yet know how well this approach\ngeneralizes, but if it does, it could be a game-changer.\n\nAt the high end, companies like OpenAI and Google have every incentive to\nincrease efficiency, because even tiny gains can amount to millions of dollars\nsaved. It is rumored that the current versions of GPT-4 and GPT-3.5 that\nOpenAI serves to customers are about 80-90% smaller than they were when they\nfirst shipped. These companies don\u2019t always (or even usually, at this point)\nshare their efficiency gains with the broader world, but the knowledge of how\nto do so exists within employees\u2019 minds, and knowledge tends to spread over\ntime.\n\nThe larger players also have so much compute that they need to innovate to\nfind ways to use it all efficiently. Because Google, Anthropic (via Amazon),\nand OpenAI (via Microsoft) have many data centers across the country and the\nworld at their disposal, they are seeking ways to train models across multiple\nphysical locations\u2014no easy feat. DeepMind recently published an approach\ncalled distributed path composition (DiPaCo), which pushes in this direction.\nWhile this benefits larger players, it could ultimately be used by smaller\nactors to distribute model training\u2014for example, academic researchers at\ndifferent institutions could pool their respective universities\u2019 compute\nclusters to effectively double (or more) the amount of compute at their\ndisposal.\n\nAnd remember that these efficiency gains occur as the cost of compute\ndecreases over time. Nvidia\u2019s latest Blackwell architecture packs as much\ncomputing power into one rack of servers as existed in a state-of-the-art,\nbuilding-sized supercomputer unveiled by the US government in 2022. Today\u2019s\n\u201cfrontier compute\u201d is tomorrow\u2019s old news. Thus any compute threshold set for\nfrontier AI today will cover a much broader range of the industry in just a\nfew short years.\n\nIn the final analysis, then, there are two paths that regulating via compute\nthresholds could take. One would be gradually raise the compute threshold to\nmatch the frontier; this would effectively mean that only the largest domestic\nfirms are subject to any kind of regulation. Because the same level of\ncapability will be reached by smaller players using substantially less compute\nin short order (if recent history is any guide, at least), this does little to\nensure safety. We know that the largest companies take AI safety seriously and\nperform substantial red-teaming on their models, so it\u2019s not clear how much\nvalue a compute threshold that effectively applies only to those firms would\ncreate.\n\nThe other path would be to keep the current threshold where it is around 10^26\nflops, effectively stating that any mathematical operations above that level\nare inherently dangerous. This will create incentives for continued efficiency\nimprovements, but ultimately will cause much of the AI industry to be subject\nto government oversight. Given the lack of manifest harms from advanced AI\nsystems so far and the substantial burdens that would impose on this\ngeostrategically and economically crucial industry, this approach comes with\nhuge costs.\n\nA better approach would be to ditch the idea of compute thresholds for\ntriggering model regulation at all. Reporting requirements could still be used\nat \u2018frontier compute\u2019 levels to give policymakers some peace of mind, but\nultimately, we need a smarter and more durable way to manage AI risk.\n\nTo do this, policymakers should:\n\n  * Start thinking seriously about how AI fits into existing law and make amendments to that law as appropriate;\n\n  * Accelerate the creation of voluntary technical standards for model safety, reliability, and performance evaluation (via NIST for general-purpose systems or agencies like the FDA for industry-specific systems); by the way, regulatory pre-approval of models could very slow down this process, because open science and regulatory regimes tend not to be so compatible;\n\n  * Create standards and best practices for model red-teaming and safety evaluation (led by an agency like CISA).\n\nMany AI developers want to do the right thing and are seeking guidance and\nresources for how best to achieve that goal. The government can play an active\nrole in giving them the tools they need to do their work as well as possible,\nand what\u2019s more, they can do it without subjecting a general-purpose\ntechnology to a sweeping regulatory regime.\n\n### Subscribe to Hyperdimensional\n\nBy Dean W. Ball \u00b7 Launched 3 months ago\n\nA newsletter about emerging technology and the future of governance.\n\n7 Likes\n\n\u00b7\n\n3 Restacks\n\n7\n\nShare this post\n\n#### Compute Thresholds are Ineffective\n\nhyperdimensional.substack.com\n\nShare\n\nComments\n\nCalifornia's Effort to Strangle AI\n\nOn SB 1047, a dangerous attempt at safety\n\nFeb 9 \u2022\n\nDean W. Ball\n\n6\n\nShare this post\n\n#### California's Effort to Strangle AI\n\nhyperdimensional.substack.com\n\n1\n\nLet's Talk About AI 'X-Risk'\n\nWhy I don't lose sleep about existential risk from AI\n\nJan 11 \u2022\n\nDean W. Ball\n\n16\n\nShare this post\n\n#### Let's Talk About AI 'X-Risk'\n\nhyperdimensional.substack.com\n\n4\n\nSoftware's Romantic Era\n\nConsciousness, \"consciousness,\" and Anthropic's latest opus\n\nMar 5 \u2022\n\nDean W. Ball\n\n4\n\nShare this post\n\n#### Software's Romantic Era\n\nhyperdimensional.substack.com\n\n2\n\nReady for more?\n\n\u00a9 2024 Dean W. Ball\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
