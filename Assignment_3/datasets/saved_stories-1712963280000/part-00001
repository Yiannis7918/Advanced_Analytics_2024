{"aid": "40015185", "title": "Lessons after a Half-billion GPT Tokens", "url": "https://kenkantzer.com/lessons-after-a-half-billion-gpt-tokens/", "domain": "kenkantzer.com", "votes": 1, "user": "lordofmoria", "posted_at": "2024-04-12 17:06:38", "comments": 0, "source_title": "Lessons after a half-billion GPT tokens - Ken Kantzer's Blog", "source_text": "Lessons after a half-billion GPT tokens - Ken Kantzer's Blog\n\nSkip to the content\n\nKen Kantzer's Blog\n\nlogging my thoughts on technology, security & management\n\n  * Home\n  * About\n\n  * Home\n  * About\n\n# Lessons after a half-billion GPT tokens\n\nApril 11, 2024 / Ken / 0 Comments\n\nCTO @ Truss | Former VP of Engineering and Head of Security @ FiscalNote | ex-PKC co-founder | princeton tiger '11 | writes on engineering, management, and security.\n\nMy startup (gettruss.io) released a few LLM-heavy features in the last six\nmonths, and the narrative around LLMs that I read on Hacker News is now\nstarting to diverge from my reality, so I thought I\u2019d share some of the more\n\u201csurprising\u201d lessons after churning through just north of 500 million tokens,\nby my estimate.\n\nSome details first:\n\n\u2013 we\u2019re using the OpenAI models, see the Q&A at the bottom if you want my\nopinion of the others\n\n\u2013 our usage is 85% GPT-4, and 15% GPT-3.5\n\n\u2013 we deal exclusively with text, so no gpt-4-vision, Sora, whisper, etc.\n\n\u2013 we have a B2B use case \u2013 strongly focused on summarize/analyze-extract, so\nYMMV\n\n\u2013 500M tokens actually isn\u2019t as much as it seems \u2013 it\u2019s about 750,000 pages of\ntext, to put it in perspective\n\nLesson 1: When it comes to prompts, less is more\n\nWe consistently found that not enumerating an exact list or instructions in\nthe prompt produced better results, if that thing was already common\nknowledge. GPT is not dumb, and it actually gets confused if you over-specify.\n\nThis is fundamentally different than coding, where everything has to be\nexplicit.\n\nHere\u2019s an example where this bit us:\n\nOne part of our pipeline reads some block of text and asks GPT to classify it\nas relating to one of the 50 US states, or the Federal government. This is not\na hard task \u2013 we probably could have used string/regex, but there\u2019s enough\nweird corner cases that that would\u2019ve taken longer. So our first attempt was\n(roughly) something like this:\n\n    \n    \n    Here's a block of text. One field should be \"locality_id\", and it should be the ID of one of the 50 states, or federal, using this list: [{\"locality: \"Alabama\", \"locality_id\": 1}, {\"locality: \"Alaska\", \"locality_id\": 2} ... ]\n\nThis worked sometimes (I\u2019d estimate >98% of the time), but failed enough that\nwe had to dig deeper.\n\nWhile we were investigating, we noticed that another field, name, was\nconsistently returning the full name of the state...the correct state \u2013 even\nthough we hadn\u2019t explicitly asked it to do that.\n\nSo we switched to a simple string search on the name to find the state, and\nit\u2019s been working beautifully ever since.\n\nI think in summary, a better approach would\u2019ve been \u201cYou obviously know the 50\nstates, GPT, so just give me the full name of the state this pertains to, or\nFederal if this pertains to the US government.\u201d\n\nWhy is this crazy? Well, it\u2019s crazy that GPT\u2019s quality and generalization can\nimprove when you\u2019re more vague \u2013 this is a quintessential marker of higher-\norder delegation / thinking.\n\n(Random side note one: GPT was failing most often with the M states \u2014\nMaryland, Maine, Massachusettes, Michigan \u2014 which you might expect of a\nfundamentally stochastic model.)\n\n(Random side note two: when we asked GPT to choose an ID from a list of items,\nit got confused a lot less when we sent the list as prettified JSON, where\neach state was on its own line. I think \\n is a stronger separator than a\ncomma.)\n\nLesson 2: You don\u2019t need langchain. You probably don\u2019t even need anything else\nOpenAI has released in their API in the last year. Just chat. That\u2019s it.\n\nLangchain is the perfect example of premature abstraction. We started out\nthinking we had to use it because the internet said so. Instead, millions of\ntokens later, and probably 3-4 very diverse LLM features in production, and\nour openai_service file still has only one, 40-line function in it:\n\n    \n    \n    def extract_json(prompt, variable_length_input, number_retries)\n\nThe only API we use is chat. We always extract json. We don\u2019t need JSON mode,\nor function calling, or assistants (though we do all that). Heck, we don\u2019t\neven use system prompts (maybe we should...). When a gpt-4-turbo was released,\nwe updated one string in the codebase.\n\nThis is the beauty of a powerful generalized model \u2013 less is more.\n\nMost of the 40 lines in that function are around error handling around OpenAI\nAPI\u2019s regular 500s/socket closed (though it\u2019s gotten better, and given their\nload, it\u2019s not surprising).\n\nThere\u2019s some auto-truncating we built in, so we don\u2019t have to worry about\ncontext length limits. We have my own proprietary token-length estimator. Here\nit is:\n\n    \n    \n    if s.length > model_context_size * 3 # truncate it! end\n\nIt fails in corner cases when there are a LOT of periods, or numbers (the\ntoken ratio is < 3 characters / token for those). So there\u2019s another very\nproprietary try/catch retry logic:\n\n    \n    \n    if response_error_code == \"context_length_exceeded\" s.truncate(model_context_size * 3 / 1.3)\n\nWe\u2019ve gotten quite far with this approach, and it\u2019s been flexible enough for\nour needs.\n\nLesson 3: improving the latency with streaming API and showing users variable-\nspeed typed words is actually a big UX innovation with ChatGPT.\n\nWe thought this was a gimmick, but users react very positively to variable-\nspeed \u201ctyped\u201d characters \u2013 this feels like the mouse/cursor UX moment for AI.\n\nLesson 4: GPT is really bad at producing the null hypothesis\n\n\u201cReturn nothing if you don\u2019t find anything\u201d \u2013 is probably the most error-prone\nprompting language we came across. Not only does GPT often choose to\nhallucinate rather than return nothing, it also causes it to just lack\nconfidence a lot, returning blank more often than it should.\n\nMost of our prompts are in the form:\n\n> \u201cHere\u2019s a block of text that\u2019s making a statement about a company, I want\n> you to output JSON that extracts these companies. If there\u2019s nothing\n> relevant, return a blank. Here\u2019s the text: [block of text]\u201d\n\nFor a time, we had a bug where [block of text] could be empty. The\nhallucinations were bad. Incidentally, GPT loves to hallucinate bakeries, here\nare some great ones:\n\n  * Sunshine Bakery\n  * Golden Grain Bakery\n  * Bliss Bakery\n\nFortunately, the solution was to fix the bug and not send it a prompt at all\nif there was no text (duh!). But it\u2019s harder when \u201cit\u2019s empty\u201d is harder to\ndefine programmatically, and you actually do need GPT to weigh in.\n\nLesson 5: \u201cContext windows\u201d are a misnomer \u2013 and they are only growing larger\nfor input, not output\n\nLittle known fact: GPT-4 may have a 128k token window for input, but it\u2019s\noutput window is still a measly 4k! Calling it a \u201ccontext window\u201d is\nconfusing, clearly.\n\nBut the problem is even worse \u2013 we often ask GPT to give us back a list of\nJSON objects. Nothing complicated mind you: think, an array list of json\ntasks, where each task has a name and a label.\n\nGPT really cannot give back more than 10 items. Trying to have it give you\nback 15 items? Maybe it does it 15% of the time.\n\nWe originally thought this was because of the 4k context window, but we were\nhitting 10 items, and it\u2019d only be maybe 700-800 tokens, and GPT would just\nstop.\n\nNow, you can of course trade in output for input by giving it a prompt, ask\nfor a single task, then give it (prompt + task), ask for the next task, etc.\nBut now you\u2019re playing a game of telephone with GPT, and have to deal with\nthings like Langchain.\n\nLesson 6: vector databases, and RAG/embeddings are mostly useless for us mere\nmortals\n\nI tried. I really did. But every time I thought I had a killer use case for\nRAG / embeddings, I was confounded.\n\nI think vector databases / RAG are really meant for Search. And only search.\nNot search as in \u201coh \u2013 retrieving chunks is kind of like search, so it\u2019ll\nwork!\u201d, real google-and-bing search. Here\u2019s some reasons why:\n\n  1. there\u2019s no cutoff for relevancy. There are some solutions out there, and you can create your own cutoff heuristics for relevancy, but they\u2019re going to be unreliable. This really kills RAG in my opinion \u2013 you always risk poisoning your retrieval with irrelevant results, or being too conservative, you miss important results.\n  2. why would you put your vectors in a specialized, proprietary database, away from all your other data? Unless you are dealing at a google/bing scale, this loss of context absolutely isn\u2019t worth the tradeoff.\n  3. unless you are doing a very open-ended search, of say \u2013 the whole internet \u2013 users typically don\u2019t like semantic searches that return things they didn\u2019t directly type. For most applications of search within business apps, your users are domain experts \u2013 they don\u2019t need you to guess what they might have meant \u2013 they\u2019ll let you know!\n\nIt seems to me (this is untested) that a much better use of LLMS for most\nsearch cases is to use a normal completion prompt to convert a user\u2019s search\ninto a faceted-search, or even a more complex query (or heck, even SQL!). But\nthis is not RAG at all.\n\nLesson 7: Hallucination basically doesn\u2019t happen.\n\nEvery use case we have is essentially \u201cHere\u2019s a block of text, extract\nsomething from it.\u201d As a rule, if you ask GPT to give you the names of\ncompanies mentioned in a block of text, it will not give you a random company\n(unless there are no companies in the text \u2013 there\u2019s that null hypothesis\nproblem!).\n\nSimilarly \u2014 and I\u2019m sure you\u2019ve noticed this if you\u2019re an engineer \u2014 GPT\ndoesn\u2019t really hallucinate code \u2013 in the sense that it doesn\u2019t make up\nvariables, or randomly introduce a typo in the middle of re-writing a block of\ncode you sent it. It does hallucinate the existence of standard library\nfunctions when you ask it to give you something, but again, I see that more as\nthe null hypothesis. It doesn\u2019t know how to say \u201cI don\u2019t know\u201d.\n\nBut if your use case is entirely, \u201chere\u2019s the full context of details, analyze\n/ summarize / extract\u201d \u2013 it\u2019s extremely reliable. I think you can see a lot of\nproduct releases recently that emphasize this exact use case.\n\nSo it\u2019s all about good data in, good GPT tokens responses out.\n\nWhere do I think all this is heading?\n\nRather than responding with some long-form post, here\u2019s a quick Q&A:\n\nAre we going to achieve Gen AI?\n\nNo. Not with this transformers + the data of the internet + $XB infrastructure\napproach.\n\nIs GPT-4 actually useful, or is it all marketing?\n\nIt is 100% useful. This is the early days of the internet still. Will it fire\neveryone? No. Primarily, I see this lowering the barrier of entry to ML/AI\nthat was previously only available to Google.\n\nHave you tried Claude, Gemini, etc?\n\nYeah, meh. Actually in all seriousness, we haven\u2019t done any serious A/B\ntesting, but I\u2019ve tested these with my day to day coding, and it doesn\u2019t feel\neven close. It\u2019s the subtle things mostly, like intuiting intention.\n\nHow do I keep up to date with all the stuff happening with LLMs/AI these days?\n\nYou don\u2019t need to. I\u2019ve been thinking a lot about The Bitter Lesson \u2013 that\ngeneral improvements to model performance outweigh niche improvements. If\nthat\u2019s true, all you need to worry about is when GPT-5 is coming out. Nothing\nelse matters, and everything else being released by OpenAI in the meantime\n(not including Sora, etc, that\u2019s a whooolle separate thing) are basically\nnoise.\n\nSo when will GPT-5 come out, and how good will it be?\n\nI\u2019ve been trying to read the signs with OpenAI, as has everyone else. I think\nwe\u2019re going to see incremental improvement, sadly. I don\u2019t have a lot of hope\nthat GPT-5 is going to \u201cchange everything\u201d. There are fundamental economic\nreasons for that: between GPT-3 and GPT-3.5, I thought we might be in a\nscenario where the models were getting hyper-linear improvement with training:\ntrain it 2x as hard, it gets 2.2x better.\n\nBut that\u2019s not the case, apparently. Instead, what we\u2019re seeing is\nlogarithmic. And in fact, token speed and cost per token is growing\nexponentially for incremental improvements.\n\nIf that\u2019s the case, there\u2019s some Pareto-optimal curve we\u2019re on, and GPT-4\nmight be optimal: whereas I was willing to pay 20x for GPT-4 over GPT-3.5, I\nhonestly don\u2019t think I\u2019d pay 20x per token to go from GPT-4 to GPT-5, not for\nthe set of tasks that GPT-4 is used for.\n\nGPT-5 may break that. Or, it may be the iPhone 5 to the iPhone 4. I don\u2019t\nthink that\u2019s a loss!\n\n### Share this:\n\n  * Twitter\n  * Facebook\n  * Reddit\n  * Email\n\nUncategorized\n\n### Leave a Reply Cancel reply\n\n### About This Site\n\nThis may be a good place to introduce yourself and your site or include some\ncredits.\n\n### Find Us\n\nAddress 123 Main Street New York, NY 10001\n\nHours Monday\u2014Friday: 9:00AM\u20135:00PM Saturday & Sunday: 11:00AM\u20133:00PM\n\n### Categories\n\n  * Hiring\n  * Management\n  * Security\n  * Series: Core Controls for the Transcendent CISO\n  * Technology\n  * Uncategorized\n\n### Recent Posts\n\n  * Lessons after a half-billion GPT tokens\n  * The Parable of the Wise Hiring Manager\n  * Learnings from 5 years of tech startup code audits\n  * The Unreasonable Effectiveness of Secure-by-default\n  * You Don\u2019t Need Hundreds of Engineers to Build a Great Product\n  * Technology ROI Discussions are Broken\n  * 5 Software Engineering Foot-guns\n  * The Backlog Peter Principle\n  * How to find great senior engineers\n  * The Googler\u2019s Dilemma: Why Experience Will Always Have a Premium\n  * 5 Red Flags Signaling Your Rebuild Will Fail\n  * Core Control #6: Log Everything\n  * Core Control #5: Secure by Default\n  * Core Principle #4: Managing Privileged Access\n  * Core Principle #3: Continuous Security\n\n### Pages\n\n  * About\n  * Blog\n\n\u00a9 2024 Ken Kantzer's Blog\n\nTheme by Anders Noren \u2014 Up \u2191\n\n", "frontpage": false}
