{"aid": "40089127", "title": "Does code review speed matter for practitioners?", "url": "https://link.springer.com/article/10.1007/s10664-023-10401-z", "domain": "springer.com", "votes": 2, "user": "cpeterso", "posted_at": "2024-04-19 16:52:56", "comments": 0, "source_title": "Does code review speed matter for practitioners? - Empirical Software Engineering", "source_text": "Does code review speed matter for practitioners? | Empirical Software Engineering\n\nLoading [MathJax]/jax/output/HTML-CSS/config.js\n\nSkip to main content\n\nLog in\n\n## Search\n\n## Navigation\n\n  * Find a journal\n  * Publish with us\n  * Track your research\n\n# Does code review speed matter for practitioners?\n\n  * Open access\n  * Published: 22 November 2023\n\n  * Volume 29, article number 7, (2024)\n  * Cite this article\n\nDownload PDF\n\nYou have full access to this open access article\n\nEmpirical Software Engineering Aims and scope Submit manuscript\n\nDoes code review speed matter for practitioners?\n\nDownload PDF\n\n  * Gunnar Kudrjavets ORCID: orcid.org/0000-0003-3730-4692^1 &\n  * Ayushi Rastogi ORCID: orcid.org/0000-0002-0939-6887^1\n\n  * 1473 Accesses\n\n  * 3 Altmetric\n\n  * Explore all metrics\n\n## Abstract\n\nIncreasing code velocity is a common goal for a variety of software projects.\nThe efficiency of the code review process significantly impacts how fast the\ncode gets merged into the final product and reaches the customers. We\nconducted a qualitative survey to study the code velocity-related beliefs and\npractices in place. We analyzed 75 completed surveys from SurIndustryDevs\nparticipants from the industry and 36 from the open-source community. Our\ncritical findings are (a) the industry and open-source community hold a\nsimilar set of beliefs, (b) quick reaction time is of utmost importance and\napplies to the tooling infrastructure and the behavior of other engineers, (c)\ntime-to-merge is the essential code review metric to improve, (d) engineers\nare divided about the benefits of increased code velocity for their career\ngrowth, (e) the controlled application of the commit-then-review model can\nincrease code velocity. Our study supports the continued need to invest in and\nimprove code velocity regardless of the underlying organizational ecosystem.\n\n### Similar content being viewed by others\n\n### Investigating technical and non-technical factors influencing modern code\nreview\n\nArticle 31 March 2015\n\nOlga Baysal, Oleksii Kononenko, ... Michael W. Godfrey\n\n### The Choice of Code Review Process: A Survey on the State of the Practice\n\nChapter \u00a9 2017\n\n### An empirical study of the impact of modern code review practices on\nsoftware quality\n\nArticle 25 April 2015\n\nShane McIntosh, Yasutaka Kamei, ... Ahmed E. Hassan\n\nUse our pre-submission checklist\n\nAvoid common mistakes on your manuscript.\n\n## 1 Introduction\n\nTraditional software development methodologies, such as the waterfall model,\nfocus on a rigid and highly predictable development and deployment schedule.\nWith the introduction of the Agile Manifesto in 2001, the focus of modern\napproaches to software development has shifted to continuous deployment of\nincremental code changes (Martin 2002). As a result, Continuous Integration\n(CI) and Continuous Deployment (CD) (Fowler 2006) have become default\npractices for most of the projects in the industry and open-source software. A\ncritical objective in the software industry is making code changes reach the\nproduction environment fast. The code review process is a time-consuming part\nof evaluating the quality of code changes and approving their deployment. With\nthe wide adoption of Modern Code Review (Bacchelli and Bird 2013; Sadowski et\nal. 2018) principles, engineers are now under more pressure than ever to\nreview and deploy their code promptly. One of the aspects of Modern Code\nReview that negatively impacts software production is the perception that the\ncode review process is time-consuming (Cunha et al. 2021b).\n\nBased on our experiences in the last two decades with commercial and open-\nsource software development, we have witnessed various practices and beliefs\nrelated to increasing code velocity. An accepted definition of code velocity\nis \u201cthe time between making a code change and shipping the change to\ncustomers\u201d (Microsoft Research 2019). This paper focuses on code review s and\nthe total duration of code review completion and merge time. The \u201ccustomers\u201d\nare other engineers, and \u201cshipping\u201d means a code review was accepted and\ncommitted. We include a detailed description of related terminology in Section\n2.3.\n\nThe opinions related to increasing code velocity range from the willingness to\ndeploy code faster, even if it means increased defect density (Kononenko et\nal. 2016; Kushner 2011), to taking as much time as necessary to \u201cget the code\nright\u201d (The Linux Foundation 2022). These opposing views raise questions about\nthe developer community\u2019s prevalent attitudes and beliefs toward code\nvelocity.\n\nResearchers have investigated the different aspects of the Modern Code Review\nprocess in-depth (Nazir et al. 2020; Wei\u00dfgerber et al. 2008; Bacchelli and\nBird 2013; Czerwonka et al. 2015). We are unaware of any studies focusing\nspecifically on the beliefs, challenges, and trade-offs associated with\nincreasing the code velocity. We describe the work related to increasing code\nvelocity in Section 2.2. The primary goal of this study is to search for the\nbeliefs and practices about code velocity as-is and the context in which they\nhold. We target a variety of experienced practitioners who contribute or\nreview code for commercial and open-source software.\n\nWe formulate the following research questions:\n\n> RQ1 : What beliefs and convictions are related to code velocity for industry\n> and open-source software developers?\n\n> RQ2 : What compromises are engineers willing to make to increase code\n> velocity?\n\n> RQ3 : What are the essential suggestions from practitioners to increase code\n> velocity?\n\nTo gather field data, we survey engineers who either submit or perform code\nreview s as part of their daily work. We describe the recruitment of survey\nparticipants in Section 3.2. Out of 75 respondents, we classify 39 individuals\nas industry participants and 36 as open-source software contributors. We asked\nsurvey participants various Likert-style questions related to the essence of\nour research inquiries. Finally, we solicited free-form suggestions about how\nto increase code velocity.\n\nOur critical findings are that (a) respondents working on both commercial and\nopen-source software respond similarly on Likert-type items (out of 24 items,\nonly 4 have a statistically significant difference between these two groups)\n(b)while there is strong opposition to abandoning the code review process,\nusing the commit-then-review model under some conditions can be acceptable (c)\nrespondents mainly focused on the development process, infrastructure and\ntooling support, response time, and the need to schedule pre-allocated time\nfor code review s to increase code velocity.\n\nOur study suggests that the maximum acceptable size of the code review on the\nmedian is 800 source lines of code (SLOC). This number is an order of a\nmagnitude larger than developer folklore and existing code review guidelines\nsuggest. We find that the metric associated with code review periods that\nengineers find the most useful is time-to-merge, followed by time-to-accept\nand time-to-first-response. That finding confirms what previous studies and\ngrey literature have documented (Izquierdo-Cortazar et al. 2017; Tanna 2021).\n\nIssues of concern include a need for more conviction that increased code\nvelocity benefits an engineer\u2019s career growth, slow response times from either\nauthors or code review ers, and faster fault detection from various parts of\nthe infrastructure.\n\n## 2 Background and Related Work\n\n### 2.1 Motivation for the Study\n\nDeveloper velocity plays a significant role in software projects\u2019 success and\nthe overall job satisfaction of developers. The topic is important enough for\nMicrosoft and GitHub to have a joint research initiative called Developer\nVelocity Lab (Microsoft Research 2023). According to Microsoft, \u201c[i]mproving\ndeveloper velocity is critical to continued satisfaction, iteration, and\ninnovation in software teams\u201d (McMartin 2021). GitLab considers the reduction\nin the code review time as the primary metric that describes the success of\nthe code review process (Armstrong 2022). Data from Meta shows \u201ca correlation\nbetween slow diff review times (P75) and engineer dissatisfaction\u201d (Riggs\n2022). In this context, a diff is a Meta-specific term equivalent to a code\nreview, pull request, or patch (used mainly in open-source software).\n\nIn industry, the drive to increase the code velocity is significant enough\neven to warrant the development of unique bots and tools. These tools\nperiodically remind either an author or reviewer that they block the\ncompletion of a code review. For example, at Microsoft, a bot that\nperiodically nudges developers \u201cwas able to reduce pull request resolution\ntime by 60% for 8500 pull requests\u201d, with 73% of these notifications being\nresolved as positive (Maddila et al. 2022). Meta considers developer velocity\nas one of its critical investments during an economic downturn (Vanian 2022).\nThe company gives engineers a general incentive to \u201c[m]ove fast and break\nthings\u201d (Kushner 2011). In Meta\u2019s development philosophy, engineers expect\ncertain defects to appear if it results in a faster product deployment\n(Feitelson et al. 2013). The startup culture practiced by many software\ncompanies encourages releasing new features \u201cas fast as possible, for the sake\nof fuelling growth\u201d (Frenkel and Kang 2021).\n\nAnother critical point in our inquiry is the differences in opinions about\ncode velocity between industry and open-source software developers.\nFundamentally, the industry and open-source software development process is\nmotivated by different incentives. However, the attitudes vary even in the\ncontext of open-source software. For the Linux kernel, \u201c[t]he goal is to get\nthe code right and not rush it in\u201d, according to the official kernel\ndevelopment guide from the Linux Foundation (The Linux Foundation 2022).\nHowever, we notice the desire for increased code velocity in Mozilla. Based on\na study about code review practices in Mozilla, it is sometimes acceptable to\nbe less thorough during code review s if it speeds up the code review process\n(Kononenko et al. 2016).\n\nCorporate policies are not necessarily dictated by what individual engineers\nthink but by business needs. Research shows that developers and managers have\ndifferent views about productivity and quality (Storey et al. 2022). To\ndiscover the ground truth, we need to understand what engineers think is\n\u201cright\u201d. The topic of code velocity can surface strong emotions in engineers\n(\u201c... I just hate it when things are unreviewed for days\u201d) (S\u00f6derberg et al.\n2022). The survey mechanism that we use provides engineers with anonymity.\nThat anonymity enables engineers to freely share their opinions even if they\ncontradict the company\u2019s or project\u2019s official policies related to code\nvelocity.\n\nEmpirical software engineering involves making daily trade-offs between\nvarious project characteristics. The trade-off between increasing code\nvelocity and product quality has severe consequences because \u201cpoorly-reviewed\ncode has a negative impact on software quality in large systems using modern\nreviewing tools\u201d (McIntosh et al. 2015). We want to study what attributes or\nvalues engineers are willing to compromise to achieve higher code velocity.\n\n### 2.2 Related Work\n\nThe industry mainly drives the research related to increasing the code\nvelocity. The default assumptions in the industry are that increased code\nvelocity is desirable, and the code review s can never be fast enough (Riggs\n2022; Killalea 2019; Greiler 2020). The a priori assumption is that the code\nreview speed does matter for practitioners.\n\nOne focus area in the research related to code velocity is the ability to\npredict the duration of code review s. Existing research has resulted in\ncontradicting findings. An industrial case study from Meta finds that core\ncode review characteristics \u201cdid not provide substantial predictive power\u201d\n(Chen et al. 2022). However, a study based on Gerrit code review s finds that\n\u201cML models significantly outperform baseline approaches with a relative\nimprovement ranging from 7% to 49%\u201d (Chouchen et al. 2023).\n\nAnother part of the research focuses on various tools and techniques to speed\nup code review s. These approaches include optimizing the code review strategy\n(Gon\u00e7alves et al. 2020), investigating the effectiveness of bot usage to\nautomate the code review process (Kim et al. 2022), periodically reminding\nengineers to make progress with their code reviews (Maddila et al. 2022; Shan\net al. 2022), prioritizing the subsets of code review that need attention\n(Hong et al. 2022), targeting the optimal reviewers (Thongtanunam et al.\n2015), and improving automation to suggest reviewers (Zanjani et al. 2016).\n\nWe are unaware of any research about various compromises that engineers are\nwilling to make to improve code velocity or explicit ways to improve code\nvelocity. The closest to our research is a paper that investigates the\n\u201cmisalignments in the code review tooling and process\u201d (S\u00f6derberg et al. 2022)\nand papers about what factors impact the code review decisions (Kononenko et\nal. 2016, 2018).\n\n### 2.3 Terminology and Metrics\n\n#### 2.3.1 Overview of Terminology\n\nMost commercial organizations share a similar goal: reduce the duration of\ncode review s and consequently increase the code velocity.\n\nThe term code velocity can have different meanings depending on the context. A\ncustomer-centric definition of code velocity is \u201cthe time between making a\ncode change and shipping the change to customers\u201d (Microsoft Research 2019).\nAs a quantifier characterizing code churn, it is defined \u201cas the average\nnumber of commits per day for the past year of commit activity\u201d (Tsay 2017).\nIn this paper, our definition focuses on the total duration of code review\ncompletion and merge time. We use time-to-merge as the period from publishing\nthe code review to when accepted code changes are merged to the target branch\n(Izquierdo-Cortazar et al. 2017). Terms like review time (from publishing the\npatch until its acceptance) (Tan and Zhou 2019) and resolve time (Zhu et al.\n2016) that is defined as \u201cthe time spent from submission to the final issue or\npull request status operation (stopped at committed, resolved, merged, closed)\nof a contribution\u201d are used as well. Using the lifetime of a code review\ncoupled with merging time matches the period that the DevOps platforms such as\nGitLab optimize. The formal definition for GitLab\u2019s metric is \u201cduration from\nthe first merge request version to merged\u201d (Armstrong 2022).\n\nDifferent commercial software companies measure various code review periods.\nGoogle\u2019s code review guidance states that \u201c... it is the response time that we\nare concerned with, as opposed to how long it takes a CL to get through the\nwhole review and be submitted\u201d (Google 2023b). The term CL in Google\u2019s\nnomenclature means \u201cone self-contained change that has been submitted to\nversion control or which is undergoing code review\u201d (Google 2023a). A study\nabout code velocity from Microsoft finds that critical points in time for\nengineers are the first comment or sign-off from a reviewer and when the code\nreview has been marked as completed (Bird et al. 2015). A paper that\ninvestigates the performance of code review in the Xen hypervisor project\nfinds that time-to-merge is the metric to optimize for (Izquierdo-Cortazar et\nal. 2017). Anecdotal evidence from grey literature about the challenges in\nCapital One\u2019s code review process presents a similar finding\u2014\u201cthe most\nimportant metric was to calculate the cycle time\u2014that is, how long it takes\nfrom a PR being raised to it being merged (or closed)\u201d (Tanna 2021). Meta\ntracks a Time In Review metric, defined as \u201ca measure of how long a diff is\nwaiting on review across all of its individual review cycles\u201d (Riggs 2022).\nOne of the findings from Meta is that \u201c[t]he longer someone\u2019s slowest 25\npercent of diffs take to review, the less satisfied they were by their code\nreview process\u201d.\n\n#### 2.3.2 Code Review Metrics\n\nIn this paper, we choose the following metrics to characterize code review\nprocess:\n\n  * Time-to-first-response (Bird et al. 2015; MacLeod et al. 2018a): the time from publishing the code review until the first acceptance, comment, inline comment (comment on specific code fragments), rejection, or any other activity by a person other than the author of the code. Survey participants at Microsoft indicated that \u201c[r]eceiving feedback in a timely manner\u201d is the highest-ranked challenge that engineers face during the code review process (MacLeod et al. 2018a). We exclude any code review-related activity by bots.\n\n  * Time-to-accept (Bird et al. 2015): the time from when an engineer publishes code changes for review until someone other than the author accepts the code review. This term is more precise than time to completion (Bird et al. 2015). Similarly, we exclude any code review-related activity by bots.\n\n  * Time-to-merge (GitHub 2021; Kononenko et al. 2016): encompasses the entire duration of the code review process. We define the time-to-merge as \u201c...the time since the proposal of a change (...) to the merging in the code base ...\u201d (Izquierdo-Cortazar et al. 2017).\n\n### 2.4 Expectations Related to Code Velocity\n\nWe find that expected code review response times between industry and various\nopen-source software projects differ by order of magnitude. Google sets an\nexpectation that \u201cwe expect feedback from a code review within 24 (working)\nhours\u201d (Winters et al. 2020). Findings from Meta confirm that \u201creviews start\nto feel slow after they have been waiting for around 24 hour review\u201d (Chen et\nal. 2022). Guidance from Palantir is that \u201ccode reviews need to be prompt (on\nthe order of hours, not days)\u201d and \u201c[i]f you don\u2019t think you can complete a\nreview in time, please let the committer know right away so they can find\nsomeone else\u201d (Palantir 2018). Existing research into code review practices at\nAMD, Google, and Microsoft similarly converges on 24 hours (Rigby and Bird\n2013). The published guidelines and studies match our industry experience of\n24 hours being a de facto expected period for a response.\n\nThe requirements for open-source software are less demanding than in the\nindustry. The code review guidelines from the LLVM project (LLVM Foundation\n2023b) that focuses on various compiler and toolchain technologies set up an\nexpectation that \u201ccode reviews will take longer than you might hope\u201d.\nSimilarly, the expectations for Linux contributors are set to \u201cit might take\nlonger than a week to get a response\u201d during busy times (The Linux Foundation\n2022). The guidance for Mozilla is to \u201cstrive to answer in a couple of days,\nor at least under a week\u201d (Mozilla 2023). For Blender, the expectation is that\n\u201c[d]evelopers are expected to reply to patches [in] 3 working days\u201d (Blender\n2022).\n\nThe etiquette for handling stale code review s in open-source software differs\nfrom the industry. According to the guidelines from various projects,\napproaches like the Nudge bot are unacceptable (Maddila et al. 2022). The\nguidance for inactive code review s for the Linux kernel is to \u201cwait for a\nminimum of one week before requesting a response\u201d (The Linux Foundation 2022).\nThe LLVM project also recommends waiting for a week in case of inactivity\nbefore reminding the reviewers (LLVM Foundation 2023b). The FreeBSD commit\nguidelines state clearly that \u201cthe common courtesy ping rate is one week\u201d (The\nFreeBSD Documentation Project 2022).\n\n## 3 Methodology\n\n### 3.1 Survey Design\n\nThe main goals of our survey are to collect data about the beliefs and\nexperiences about code velocity, the trade-offs engineers are willing to make\nto increase the code velocity, and suggestions from practitioners about how to\nincrease it. Our target audience was both commercial and open-source software\ndevelopers.\n\nThe survey consists of 16 essential questions. The survey starts with a\nquestion that asks participants for consent. The survey finishes with a\nquestion allowing participants to share their email addresses if researchers\ncan contact them to share the findings from the survey. All the questions\nexcept the one that determined the participants\u2019 consent were optional. We\ndisplay the shortened version of the survey questions in Table 1. The entire\nsurvey contents are part of Appendix A.\n\nTable 1 List of survey questions\n\nFull size table\n\nThe first eight questions related to participants\u2019 demographics, such as\nexperience with software development, code review s, their role in the\nsoftware development process, and the application domain they used to answer\nthe survey questions. This question block is followed by questions that ask\nparticipants to rank time-to-first-response, time-to-accept, and time-to-merge\nin order of importance to optimize the code velocity. After that, we present\nfour questions related to the benefits of code velocity and potential\ncompromises related to increasing code velocity. We inquire about the\npossibility of using either a post-commit review model (Rigby and German 2006)\nor no code review process.\n\nQuestions Q11, Q12, Q13, and Q14 contain multiple Likert-type items (Clason\nand Dormody 1994). Because of the survey\u2019s limited number of questions, we do\nnot use classic Likert scales. Likert scales typically contain four or more\nLikert-type items combined to measure a single character or trait. For\nexample, researchers may have 4\u20135 questions about the timeliness of responses\nto a code review. Researchers then merge the results from these questions into\na single composite score. The categories we inquire about come from our\ncombined subjective experiences with software development in the industry and\nopen-source community. Some choices, such as \u201cCareer growth\u201d and \u201cJob\nsatisfaction\u201d, are apparent. Others, such as \u201cDiversity, equity, and\ninclusion\u201d, reflect the changing nature of societal processes.\n\nThe survey ends with asking participants about the maximum acceptable code\nreview size and waiting period, followed by an open-ended question about how\nthe code velocity can be improved. We indicated to participants that the\nsurvey would take 5\u20137 minutes. The complete list of survey questions is\npublicly accessible.^Footnote 1\n\n### 3.2 Survey Participants\n\nEthical Considerations Ethical solicitation of participants for research that\ninvolves human subjects is challenging (Felderer and Horta Travassos 2020).\nThere is no clear consensus in the academic community about sampling strategy\nto solicit survey participants. The topic is under active discussion (Baltes\nand Diehl 2016). In 2021, researchers from the University of Minnesota\nexperimented with the Linux kernel (Feitelson 2023). The approach the\nresearchers used caused significant controversy and surfaced several issues\nsurrounding research ethics on human subjects. The fallout from the \u201chypocrite\ncommits\u201d experiment (Wu and Lu 2021) forced us to approach recruiting the\nsurvey participants with extreme caution.\n\nA typical approach is to use e-mail addresses mined from software repositories\n(e.g., identities of commit authors) to contact software developers. Based on\nthe recent guidance about ethics in data mining (Gold and Krinke 2021;\nGonzalez-Barahona 2020), we decided not to do this. While research related to\ncode review s has in the past utilized an existing relationship with a single\nspecific developer community such as Mozilla (Kononenko et al. 2016) or\nShopify (Kononenko et al. 2018), we wanted to reach out to a broader audience.\nWe also wanted to avoid incentive-based recruitment mechanisms that offer\nrewards. Existing findings suggest that monetary incentives increase survey\nresponse rates (Smith et al. 2019). However, they do not necessarily reduce\nthe non-response bias (Groves 2006). As a potential incentive, we promised\nthat participants who share their contact information with us would be the\nfirst to receive the paper\u2019s preprint.\n\n### Recruitment Strategy\n\nOur goal was to reach a heterogenous collection of practitioners. We used our\nindustry connections to target professional engineers and hobbyists, industry\nand open-source software developers, industry veterans, and junior software\ndevelopers. We used our academic connections to share the survey with software\nengineering researchers and computer science students who participated in the\ncode review process. Geographically, most of our contacts reside in Europe and\nthe United States.\n\nWe started by composing a Medium post in a non-academic style, making the\ncontent reachable to a broader audience. That post contained a link to our\nsurvey to make participation easier. We then used the breadth-first approach\nto share the survey invitation on social media. Our target platforms were\nFacebook, LinkedIn, Medium, Reddit, and Twitter (X).\n\nOur approach did not use the snowball (chain-referral) sampling strategy to\nrecruit additional survey participants. Snowball sampling is an approach where\nalready recruited study participants recruit new people to participate in the\nsurvey. We intended to avoid the community bias and \u201cthe lack of definite\nknowledge as to whether or not the sample is an accurate reading of the target\npopulation\u201d (Raina 2015).\n\nIn addition, we contacted several individuals in commercial software companies\nand various open-source software projects to ask their permission to share the\nsurvey invite with their developer community. We received responses from\nBlender, Gerrit, Free BSD, and Net BSD. These projects gave us explicit\npermission to post in a project\u2019s mailing list, or our contact person\ncirculated the survey internally.\n\n### Survey Summary Statistics\n\nThe survey was published on September 14, 2022, and closed on October 14,\n2022. The survey system received a total of 110 responses. Out of all the\nrespondents, 76 participants completed the survey, with 75 agreeing to the\nconsent form and answering the questions. Of 75 individuals who answered the\nquestions, 25 discovered the survey using social media and 50 via anonymous\nsurvey link. For our analysis, we only used the surveys that participants\nfully completed.\n\n### 3.3 Survey Data Analysis\n\nThe methods used to analyze data from Likert-style items are controversial\nwithout a clear scientific consensus (Brown 2011; Carifio and Perla 2007; Chen\nand Liu 2020). This paper treats Likert-style items as ordinal measurements\nand uses descriptive statistics to analyze them (Allen and Seaman 2007; Boone,\nJr. and Boone 2012). We do not treat ordinal values as metric because it can\nlead to errors (Liddell and Kruschke 2018). For Likert-type items, we define\nthree general categories: negative (\u201cStrongly disagree\u201d, \u201cDisagree\u201d), neutral\n(\u201cNeither agree nor disagree\u201d, \u201cI don\u2019t know\u201d), and positive (\u201cAgree\u201d,\n\u201cStrongly agree\u201d). We added the choice of \u201cI don\u2019t know\u201d based on the feedback\nfrom the pilot tests that we used to refine the survey questions.\n\nWe use the content analysis to analyze the answers to Q17 (\u201cIn your opinion,\nhow can code velocity be improved for your projects?\u201d). Two researchers\nindependently manually coded all the responses to Q17. Once the coding process\nwas finished, the researchers compared their results and tried to achieve a\nconsensus. In case of disagreements, a third researcher acted as a referee.\nSeveral themes and categories emerged as part of the open coding process. We\nrepeated the coding process till we classified all the responses under \\\\(7\n\\pm 2\\\\) labels (Miller 1956).\n\nNumerical data, such as the maximum acceptable size of the code review, was\nanalyzed using custom code written in R. Similarly, the statistical tests\nconducted in Sections 4.3 and 4.4 to evaluate the differences between various\ngroups were implemented in R.\n\n## 4 Results\n\n### 4.1 Demographics\n\nMost of the respondents to our survey are experienced software engineers.\nConsequently, this experience translates to the time spent reviewing other\npeople\u2019s code. We present the participants\u2019 development and code-reviewing\nexperience in Table 2.\n\nTable 2 Survey participant demographics\n\nFull size table\n\nWhen it comes to the number of code review s that participants perform:\n\n  * 36% of respondents submit more than 10 code review s monthly, and 43% submit 3\u201310 code review s.\n\n  * 54% of respondents conduct more than 10 code review s monthly, and 38% conduct 3\u201310 code review s.\n\nFor the type of software the survey participants work on, 58% identified as\ndevelopers working on application software such as mobile or desktop\napplications. 27% of respondents stated that they work on systems software\nsuch as device drivers or kernel development.\n\nRegarding different code review environments, 95% of respondents use a code\ncollaboration tool. That tool may be public, such as Gerrit or GitHub, or the\nprivate instance of a company-specific tool, such as Google\u2019s Critique. Nearly\nevery respondent writes or reviews code as part of their role. Only one\nindividual stated that their role does not require reviewing code, and they\nonly submit patches. Out of the respondents, 89% of developers author new code\nchanges. The rest of the survey participants have a different role, such as\nonly reviewing the code.\n\n### 4.2 Grouping of Respondents\n\nWe divide our participants into two groups based on how they self-identify as\na response to Q8 (\u201cWhat type of software developer are you?\u201d). We use the\nfollowing proxy to understand the difference between industry and open-source\nsoftware developers. If a participant chose only \u201cI work on closed-source and\nget paid\u201d as a response, we classify them as \u201cIndustry\u201d. If one of the choices\nby participants was either \u201cI work on open-source and get paid\u201d or \u201cI work on\nopen-source and do not get paid\u201d, then we classify them as \u201cOSS\u201d. Based on\nthat division, we ended up with 39 participants from the industry and 36\nrespondents from open-source software.\n\nIn Q9 (\u201cChoose an application domain you are most experienced with for the\nremaining questions?\u201d), we asked participants what type of software is their\nprimary area of expertise. We chose not to divide participants based on the\nabstraction level of the software. We base that decision on the number of\nrespondents and the varying size of the different groups. Out of 75\nrespondents, 42 identified as someone working on application software, 20 on\nsystems software, 3 on real-time or critical software, 8 on other types of\nsoftware, and 2 chose not to answer the question.\n\n### 4.3 RQ1: Beliefs and Convictions Related to Code Velocity\n\n#### 4.3.1 Expectations for the Size and Velocity of Code Reviews\n\nOur industry experience related to code review size is highly variable. We\nhave participated in projects where code review s that contained thousands of\nSLOC were standard. Similarly, we have experience with projects where\nengineers required that authors split any review more extensive than 20\u201330\nSLOC into separate reviews. Existing research suggests that the size of code\nreview s impacts their quality and speed (Jiang et al. 2013; Wei\u00dfgerber et al.\n2008). Based on these findings, we investigate what engineers consider a\nmaximum acceptable size for a code review.\n\nMost open-source software projects do not have fixed guidelines for an upper\nbound for a code review. Very little quantitative guidance exists for the size\nof code review s. Most guidelines use qualifiers such as \u201cisolated\u201d (LLVM\nFoundation 2023a), \u201creasonable\u201d (Chromium 2023), and \u201csmall\u201d (PostgreSQL 2019;\nPhabricator 2021; MacLeod et al. 2018b). For stable releases of the Linux\nkernel, the guidance is \u201c[i]t cannot be bigger than 100 lines ...\u201d (Linux\n2023). Google engineering practices specify some bounds: \u201c100 lines is usually\na reasonable size for a CL, and 1000 lines is usually too large\u201d (Google\n2023a). The acronym CL means \u201cone self-contained change that has been\nsubmitted to version control or which is undergoing code review\u201d (Google\n2023a). As anecdotal evidence, a respondent to a survey about code review\npractices states that \u201c[a]nything more than 50 lines of changes, and my brain\ndoesn\u2019t have the capacity to do a good code review\u201d (Alami et al. 2020).\n\nThe sentiment about larger patch sizes is generally negative. A paper that\ninvestigates the efficiency of a code review process finds that \u201cpatch size\nnegatively affects all outcomes of code review that we consider as an\nindication of effectiveness\u201d (dos Santos and Nunes 2017). The existing\nresearch directs developers towards more minor code changes. Anecdotal\nassessment from the Chromium contributor\u2019s guide is that \u201c[r]eview time often\nincreases exponentially with patch size\u201d (Chromium 2023). A study about code\nreview performance finds that \u201creview effectiveness is higher for smaller code\nchanges\u201d (Baum et al. 2019). Another study about participation in Modern Code\nReview finds that patches with smaller sizes receive fewer comments than the\nlarger patches, and larger patches go through more iterations (Thongtanunam et\nal. 2017; Baysal et al. 2015).\n\nFig. 1\n\nSource lines of code\n\nFull size image\n\nFig. 2\n\nFile count\n\nFull size image\n\nWe asked study participants about the maximum acceptable number of SLOC and\nthe number of files in the code review. Figures 1 and 2 display the density\nplots (\u201csmoothed histograms\u201d) of both variables. Before the analysis, we\ncleaned the data and removed entries that were \u201cinconsistent with the\nremainder of the set of data\u201d (Barnett and Lewis 1984) and \u201csurprising or\ndiscrepant to the investigator\u201d (Beckman and Cook 1983). We found only one\nentry for each metric that we considered an outlier. Shapiro-Wilk tests\n(Shapiro and Wilk 1965) confirmed that neither SLOC (\\\\(W = 0.33, p < .001\\\\))\nnor the file count (\\\\(W = 0.15, p < .001\\\\)) were normally distributed. A\nMann-Whitney U test (Mann and Whitney 1947) indicated that the difference\nbetween medians for SLOC was not statistically significant \\\\(U(N_\\text\n{Industry} = {33}, N_{\\textsc {OSS}} = {26}) = {979.5}, z = {-0.16}, p =\n.87\\\\). Similarly, we do not observe differences for the number of files\n\\\\(U(N_\\text {Industry} = {34}, N_{\\textsc {OSS}} = {26}) = {1088.5}, z =\n{0.78}, p = .44\\\\).\n\nSection 4.3.1 discussed the ambiguity of expectations related to an acceptable\ncode review size. One of the problems that we have witnessed in practice is\nthat engineers who switch from one project or team to another will have to\nadjust to a new definition of small. Having to invest time into manually\nsplitting their changes differently to appease reviewers can decrease\ndeveloper productivity. The existing research proposes solutions to\nautomatically decompose the code review s into smaller changesets (Barnett et\nal. 2015). However, we are not aware of any project that actively uses this\napproach or a code collaboration environment that effectively supports\nsplitting code review s. As an early warning mechanism, we propose that\nvarious tools used during the code review process warn engineers when they\nexceed the limit for a particular project. Early notification will prevent the\nunnecessary iteration between a submitter and a reviewer, where the reviewer\nwill request that the submitter split the changes into smaller chunks.\n\nWe discuss different code review periods and how there is no consensus on what\nto optimize in Section 2.1. Our goal is to understand what a heterogeneous\ncollection of practitioners values the most when optimizing code velocity. We\nasked the participants to rank time-to-first-response, time-to-accept, and\ntime-to-merge in the order of importance. A total of 65 participants ranked\ndifferent code review periods. As a feedback, we also received 3 comments. We\npresent the results in Table 3.\n\nOur ranking method that results in the data in Table 3 is subjective. We will\nalso use the Borda count method to objectively evaluate how participants\nranked different code review periods (Llull 1988). The Borda count method is a\nranked election system. In the case of N candidates, the vote for the first\nplace is worth N points, the vote for the second place is worth \\\\(N - 1\\\\)\npoints, the vote for the third place is worth \\\\(N - 2\\\\) points, and the\npattern continues similarly. The vote for the last place is worth 1 point.\nAccording to the Borda count, time-to-merge receives 202 points, time-to-\nfirst-response receives 190 points, time-to-accept receives 187 points, and\nother metrics receive 71 points.\n\nTable 3 Rankings of different code review periods in the order of importance\nto optimize for code velocity\n\nFull size table\n\nIn 49% of cases, the time-to-merge was ranked as the first priority metric to\noptimize. Similarly, time-to-merge wins the Borda count. This result is like\nthe findings from the Xen hypervisor project study (Izquierdo-Cortazar et al.\n2017), anecdotal evidence from the industry (Tanna 2021), and our personal\nexperiences for more than two decades. One participant pointed out that \u201call\nof those metrics are totally irrelevant ...\u201d but did not clarify what else may\nbe relevant. Two other comments suggested a different set of metrics:\n\u201cresponse time on changes in review by both author and reviewers\u201d (like Time\nIn Review that Meta measures (Riggs 2022)) and \u201c[t]ime to re-review\u201d.\n\n#### 4.3.2 Perceived Benefits of Increased Code Velocity\n\nWe compare each Likert-style item between two groups separately using a Mann-\nWhitney U test. No statistically significant differences exist for any of the\nitems between the industry and OSS groups. Figure 3 shows that for most\ncategories, participants perceive increased code velocity as beneficial.\nParticipants think that code velocity benefits aspects such as job\nsatisfaction, reputation, or relationship with peers\n\nFig. 3\n\nLikert scales for the Q11 (\u201cVelocity of your code improves your ...\u201d)\n\nFull size image\n\nThe benefits of a code review process generally have been associated with\ncareer development and growth (Cunha et al. 2021a). However, in our study, the\nitem with the lowest score is \u201cCareer growth\u201d, where only 50% of the industry\nand 44% of OSS group respondents rated it positively. Career growth in the\ncorporate environment generally means increased professional scope, monetary\nrewards, and promotion speed. This finding is somewhat concerning. While\nintrinsic motivation is essential, it is hard to motivate engineers to conduct\nefficient code review s if there is no substantial payoff.\n\nFor the OSS group, one possible explanation is that it is much more\nchallenging to define career progression in the open-source software community\nthan in a traditional corporate environment. However, given that only 50% of\nresponses from the industry rated the \u201cCareer growth\u201d category positively, we\nthink this topic is worth exploring further.\n\nObservation 2 serves as a motivating factor behind investments in the\ndeveloper infrastructure to increase the code velocity. This finding confirms\nour anecdotal observations from industry that \u201cengineers are happier when they\ncan commit their code changes fast\u201d. Similarly, the data from Meta shows that\nincrease in code review time causes engineers to be less satisfied with the\ncode review process (Riggs 2022). We recommend that projects include the\nmetrics related to code velocity as one of the indicators of organizational\nhealth. Historically, Microsoft used the success of the daily build to\nindicate the project\u2019s health and overall state (McCarthy 1995). With the\nprevalence of CI and CD, metrics such as median daily time-to-merge can be a\nsuitable replacement.\n\n### 4.4 RQ2: Compromises that are Acceptable to Increase Code Velocity\n\n#### 4.4.1 Commit-then-Review Model\n\nThe foundation of the Modern Code Review is the review-then-commit model. Some\nprojects use the opposite of that approach. One major software project that\nstarted using the commit-then-review model was Apache (Rigby et al. 2008).\nWhile most of the projects have abandoned commit-then-review for review-then-\ncommit, we wanted to study what developers think about the resurrection of the\ncommit-then-review model. Several data points influence the decision to\nresearch this possibility. We discuss them below.\n\n### Industry Experience\n\nThe primary motivation to survey developers about commit-then-review is our\nindustry experience. We have witnessed several complaints and discussions\nabout the \u201cslowness\u201d of the review-then-commit model. Developers are\nfrustrated that even for trivial changes, such as one or two lines of code\nthat fix formatting issues or compiler warnings, they must wait hours or days\nfor someone to approve the code changes formally. Even organizations that use\ncutting-edge approaches to software development, such as Meta, state that\n\u201cevery diff must be reviewed, without exception\u201d (Riggs 2022). We frequently\nobserve this frustration about the inflexibility of the code review process,\nprimarily in teams of experienced and senior engineers.\n\n### Efficacy of Code Reviews\n\nData from Microsoft shows that \u201c[o]nly about 15% of comments provided by\nreviewers indicate a possible defect, much less a blocking defect\u201d (Czerwonka\net al. 2015). Given this finding, the trade-off between blocking the commits\nuntil the code review finishes to satisfy the process versus optimizing for\ncode velocity and taking some risks needs investigation. Our observations\nindicate that senior engineers consider code review s to be efficient only if\nthe reviewer is as or more senior than the author. The quality of various\nlinters and static analysis tools has improved over the years. We observe that\ntools automatically flag issues related to formatting, coding conventions, and\nfundamental coding issues without any human intervention.\n\n### Development Process\n\nThe establishment of CI and CD as de facto approaches to developing modern\nsoftware shows that industry and open-source software value the speed of\nsoftware delivery as the critical success metric.\n\nFig. 4\n\nLikert scales for the Q12 (\u201cI am willing to compromise on ... if it improves\ncode velocity\u201d)\n\nFull size image\n\nFigure 4 displays how much participants are willing to compromise on various\ncharacteristics to increase the code velocity. We compared each Likert-style\nitem separately using a Mann-Whitney U test. There are significant differences\nonly between two items: \u201cCode quality\u201d and \u201cSecurity\u201d. A Mann-Whitney U test\nfor \u201cCode quality\u201d indicated that the difference in mean ranks is\nstatistically significant \\\\(U(N_\\text {Industry} = {39}, N_{\\textsc {OSS}} =\n{34}) = {1664}, z = {2.65}, p = .008\\\\). Similarly, for \u201cSecurity\u201d, a Mann-\nWhitney U test indicated that the difference in mean ranks is statistically\nsignificant \\\\(U(N_\\text {Industry} = {39}, N_{\\textsc {OSS}} = {34}) =\n{1621}, z = {2.33}, p = .02\\\\).\n\nAn encouraging finding in our survey is what developers think about\ncompromises related to code quality and software security.\n\nCritical security vulnerabilities such as the OpenSSL Heartbleed bug\n(Synopsys, Inc. 2020) could have contributed to the increased awareness that a\nmistake in a single line of code can cause catastrophic damage. This finding\nis very positive, given the impact of zero-day vulnerabilities and society\u2019s\nincreased reliance on software. We interpret it to mean that even given\nexternal factors such as deadlines, pressure from the author of the code, or\nproject needs, the engineers are not willing to compromise on security. There\nis no equivalent of the Hippocratic Oath for software engineers, and licensing\nsoftware engineers is controversial (Bagert 2002). However, this finding\nindicates the presence of an internal code of conduct of \u201cdo no harm\u201d that\nengineers strive to follow.\n\nFig. 5\n\nLikert scales for the Q13 (\u201cI think the post-commit review model (changes are\nfirst committed and then reviewed at some point later) can improve ...\u201d)\n\nFull size image\n\nThe 82% of industry developers who gave negative responses and 10% who gave\nneutral responses to this question share a similar sentiment. Code quality is\nsomething that 91% of OSS developers and 69% of industry participants are not\nwilling to negotiate over. One potential explanation for the differences is\nthat industry developers view code quality and security as one software\ncharacteristic regarding what they can make trade-offs. On the other hand, the\nOSS developers are \u201ctrue believers\u201d who are not willing to compromise to\nrelease less secure software.\n\nAs a next step, we asked participants what aspects of software a commit-then-\nreview model can improve. We display the results in Fig. 5. We compared each\nLikert-style item separately using a Mann-Whitney U test. There are\nsignificant differences only between two items: \u201cCode velocity and \u201cJob\nsatisfaction\u201d. A Mann-Whitney U test for \u201cCode velocity\u201d indicated that the\ndifference in mean ranks is statistically significant \\\\(U(N_\\textrm{Industry}\n= {39}, N_\\mathrm {\\textsc {OSS}} = {33}) = {1603.5}, z = {2.12}, p = .034\\\\).\nThe code velocity is also a category that most participants thought could be\nimproved. Of industry respondents, 64% gave a positive response, with 48% of\nOSS respondents feeling similarly. Application of the commit-then-review model\nmeans that developers no longer have to wait for a code review to be complete.\nThe potential improvements in code velocity are a logical result of this\nprocess change.\n\nA Mann-Whitney U test for \u201cJob satisfaction\u201d indicated that the difference in\nmean ranks is statistically significant \\\\(U(N_\\textrm{Industry} = {39},\nN_\\mathrm {\\textsc {OSS}} = {33}) = {1620}, z = {2.28}, p = .023\\\\). Of\nindustry respondents, 46% gave a positive response, with 24% of OSS\nrespondents feeling similarly. Approximately half of the survey participants\nfrom the industry think that their job satisfaction could improve with the\ncommit-then-review model. The difference between the industry and OSS is as\nsignificant as two times. This finding makes sense because of how the industry\nevaluates the performance of software engineers. Based on our experience, the\nability to complete the assigned tasks on time and reduce code review idle\ntime is directly associated with engineers\u2019 anxiety levels and productivity.\n\nFor \u201cCareer growth\u201d, 51\u201352% of respondents in industry and OSS chose a neutral\nresponse. This finding indicates a need for more clarity regarding the\nrelationship between code velocity and its direct impact on an individual\u2019s\ncareer. While it may be beneficial for an organization or a project to be\nreleased on a faster cadence, there is not necessarily a significant reward\nfor individuals responsible for that cadence. This finding is like the data\nfrom Section 4.3.2, indicating that developers do not view an increase in code\nvelocity as beneficial to their careers.\n\n#### 4.4.2 Abandonment of Code Review s in Favor of Code Velocity\n\nCode reviews are optional in some contexts. For example, the Free BSD project\ndefines a committer as \u201can individual with write access to the Free BSD source\ncode repository\u201d (FreeBSD Foundation 2022). While committers are \u201crequired to\nhave any nontrivial changes reviewed by at least one other person before\ncommitting them to the tree\u201d (McKusick et al. 2015), the definition of\nnontrivial is open to interpretation. Therefore, experienced developers can\ncommit code changes without the review at will. We have also witnessed\nmultiple instances in the industry where a similar practice is employed. To\nincrease the code velocity, senior developers with a significant contribution\nhistory to the project have ignored the official code review process or\naccepted each other\u2019s changes immediately to \u201csatisfy the process\u201d.\n\nBased on the observations from the industry and the committer model used by\nprojects such as Dragon Fly BSD, Free BSD, Net BSD, and Open BSD, we decided\nto survey what engineers think about the possibility of abandoning code review\ns. We asked participants under what conditions engineers can commit code\nwithout someone else reviewing that code. We display the results in Fig. 6 and\ndiscuss them below.\n\nFig. 6\n\nLikert scales for the Q14 (\u201cShould engineers be allowed to commit their code\nwithout the code review depending on ...\u201d)\n\nFull size image\n\nWe compared each Likert-style item separately using a Mann-Whitney U test.\nThere were no statistically significant differences for any of the items\nbetween the \u201cIndustry\u201d and \u201cOSS\u201d groups.\n\nObservation 4 implies that even given the industry\u2019s relentless drive toward\nincreasing the code velocity, engineers do not perceive the model where anyone\ncan commit code freely as something genuinely beneficial. At the same time,\nthe committer model that Dragon Fly BSD, Free BSD, Net BSD, and Open BSD use\nshows that there is a point where an engineer can \u201cgraduate\u201d to a level where\nthey are no longer required to have their code reviewed. A worthwhile research\navenue is to investigate and estimate the cost of becoming a committer in one\nof the open-source software projects. For example, the Free BSD guidelines\n(FreeBSD Foundation 2022) say that \u201c[t]he process to gain FreeBSD source\naccess is a long one and \u201c[i]t can take months of contributions and\ninteraction with the project\u201d. Is the cost-benefit still there for the\nengineers who become committers and maintain the committer status?\n\nThe highest items with positive responses are \u201cProject\u2019s needs\u201d and\n\u201cSeniority\u201d. For \u201cProject\u2019s needs\u201d, 56% of the industry and 41% of OSS\nrespondents thought it permissible to commit code without conducting a code\nreview. Based on our industry experience, this sounds reasonable. Engineers\nmust exercise their judgment in cases like build breaks or issues blocking an\nentire project and not blindly follow the process. For example, suppose an\napplication is not building, and an engineer has a potential fix. In that\ncase, it is reasonable to take a calculated risk to commit the changes\nimmediately without waiting for hours for a code review.\n\nThe choice of \u201cSeniority\u201d is reasonable as well. Senior engineers typically\nhave more tribal knowledge, related experience, and in-depth knowledge than\njunior engineers. Therefore, if anyone can occasionally \u201cbreak the rules\u201d, it\nmakes the most sense for them to do that. In our industry experience, code\nreview s can find apparent mistakes. However, finding problems in either\ncomplex algorithms or design nuances works best if a reviewer has a similar or\nhigher level of knowledge. Code reviews where a junior engineer reviews the\nsenior engineer\u2019s code are effective in detecting defects only in a subset of\ncases. Suppose the goal is to improve code velocity. In that case, we\nrecommend that a project explicitly discuss the risk versus reward in a\nsituation where senior engineers can exercise their judgment on when to\nrequire reviews for their changes.\n\nFig. 7\n\nA word cloud of suggestions about how to improve code velocity\n\nFull size image\n\n### 4.5 RQ3: Suggestions to Increase Code Velocity\n\nTo solicit feedback from developers, we asked respondents, \u201cIn your opinion,\nhow can code velocity be improved for your projects?\u201d We display the word\ncloud that summarizes the comments from the survey participants in Fig. 7. A\nword cloud is a widely used visualization technique to analyze and summarize\nqualitative data. The rendering of the most frequently used words (e.g.,\n\u201ccommit\u201d, \u201ctime\u201d, \u201ctooling\u201d, and \u201csmaller\u201d) causes them to be displayed more\nprominently. While the word cloud is an initial indicator of the themes that\nemerge after analyzing the text corpus, a more detailed grouping of the\nresults is necessary.\n\nTwo researchers manually analyzed and coded the 47 comments received from the\nsurvey participants. Each comment was assigned one or more labels depending on\nits content. After the coding, researchers discussed the results and\ndisagreements (less than ten), normalized the labels used for coding, and\nsummarized the findings. Our goal was to reach \\\\(7 \\pm 2\\\\) labels that\nadequately describe the nature of the suggestions (Miller 1956). We display\nthe labels and their distribution in Table 4.\n\n#### 4.5.1 Improved Conventions and Standards\n\nWe notice the desire for more formalized standards and the establishment of\ncoding conventions. Going forward, we use the notation Ri to indicate a\nrespondent i. An R5 states that \u201c[e]stablishing and following company-wide\nstandards for coding style and code review\u201d can be helpful. Similarly, R29\nsuggests that \u201c[t]eam needs established coding conventions or tooling that\nenforces conventions to reduce debate\u201d. The standards help to set expectations\nand reduce the number of round-trips between an author and reviewer. According\nto R11, it will be helpful to \u201cdecrease the number of surprises - have the\nreview criteria prepared and explicit beforehand as much as\npossible/sensible\u201d. An R56 points out that \u201c[w]e need to improve our general\ncode review guidelines - both for reviewer and reviewee (like \"reviewer is not\ntester!\")\u201d. The sentiment is shared in R59 by asking for \u201cstricter\nguidelines\u201d.\n\n#### 4.5.2 Prioritization of Follow-Up Changes\n\nDetermining clearly what is critical and what is not is another suggested\nimprovement. Feedback from R56 suggests that \u201ccode reviews shall be\nprioritised over new feature requests\u201d. An R30 suggests a potential two-phased\napproach to code review s: \u201c[b]e crisp on what is critical to fix and what can\nbe done in a follow up\u201d and \u201c[s]top wasting time in LINT feedback and\npreferences about what code should look like and focus only on functionality\nand correctness\u201d. We have noticed similar behavior in the industry where\nreviewers try to separate the feedback into mandatory and optional. The\noptional items are improvements that can be made in an independent code review\nor later.\n\nTable 4 Different themes that result from coding the survey responses\n\nFull size table\n\n#### 4.5.3 Infrastructure and Tooling Improvements\n\nThe critical requirement from infrastructure is fast capabilities for early\nfault detection. The general requirement is for \u201cgood code review tooling\u201d for\nvarious tools \u201cto perform more checks before actual \"peer\" review\u201d (R37).\nDevelopers want \u201c[a]utomated testing, tooling\u201d (R59). The individual responses\ndescribe this need as \u201c[f]aster automated signals with more actionable error\u201d\n(R6), \u201c[a]utomatic code linters, style cops, CI builds and CI test passes\u201d\n(R9), and \u201c...automated CI/CD environments for initial checks, builds, tests\n...\u201d (R28).\n\n#### 4.5.4 Response Time\n\nCompared to the status quo, the responses indicate the need for faster\nresponses. The desire to respond quickly to code review s is hardly a\nsurprise. Various existing studies and code review guidelines specify that\ndifferent periods of development processes should optimize (MacLeod et al.\n2018b; Google 2023b; Izquierdo-Cortazar et al. 2017). In addition to the\nincreased anxiety caused by waiting for feedback, there are other drawbacks,\nsuch as the cost of a context switch and the potential for introducing new\ndefects (Czerwonka et al. 2015). Respondents use phrases such as \u201cmore\nresponsive engineers\u201d (R63), \u201cvalidation turn-around time\u201d (R41),\n\u201c[c]ommunicating with reviewers promptly\u201d (R1), and \u201creducing the feedback\nloop\u201d (R12) to describe the potential improvements. One of the respondents\n(R12) mentions that reducing time-to-first-response is crucial. While tooling\nis essential, one respondent (R4) points out that \u201cthe limiting factor is not\nreally about tooling, but if reviewers are willing to spend the time it takes\nto review other people\u2019s changes\u201d.\n\n#### 4.5.5 Scheduling Time for Code Reviews\n\nExisting research shows that developers spend 6.4 hours per week on reviewing\ncode (Bosu and Carver 2013). That is almost 20% of the typical 40-hour work\nweek. In our industry experience, the management often classifies the time\nspent performing code review s as the \u201ccost of doing the business\u201d.\nConsequently, nobody accounts for this work during the formal planning process\n(if any). The feedback from survey participants indicates that the time for\nconducting code review s is an activity that planners need to include in the\nschedule formally. Various responses demonstrate the need for better\nscheduling: \u201chave dedicated time to spend on code review\u201d (R67), \u201ctime\nallocated on their calendar to review the code\u201d (R8), and\u201c[m]ore time\nallocated for reviews\u201d (R10). Planning for the code review time can (a) reduce\nthe potential anxiety developers have (b) expose the cost of the Modern Code\nReview process (c) help to increase code review quality because engineers can\nnow take time to review code thoroughly. We are not aware of any organizations\nin the industry that use the practice of accounting for a specific amount of\ncode review time. According to R37, \u201ccommitment from engineers and the\norganization\u201d can help to improve the code velocity.\n\n#### 4.5.6 Too Much Focus on Speed\n\nAn R33 provides an interesting observation: \u201c[i]f anything development should\nbe slowed down. Being in a (constant) hurry is a red flag\u201d. While we agree\nwith this sentiment, we question if decreasing the code velocity is possible\nfor commercial software development. We have rarely observed cases where\nindustrial software projects attempt to decelerate the pace of code changes.\nThe rare situations in which that happens falls into two categories. The first\ncase is related to a stabilization period, such as weeks and months leading to\nshipping the product. A second case results from the fallout from significant\nevents, such as discovering repeated zero-day vulnerabilities in the code.\n\n#### 4.5.7 Size of the Code Review\n\nOther noteworthy themes include the size of the code review and communication.\nRequests such as \u201c[s]maller sized patches\u201d (R1), \u201csmaller pieces to review\u201d\n(R37), \u201c[s]maller merge requests\u201d (R47), \u201c[h]ave less code to review\u201d (R61),\nand \u201c[b]reaking up the use cases into smaller chunks\u201d (R36) indicate the\ndesire for the upper bound of the code review size. While there is no\ndefinitive evidence to show that smaller sizes increase code velocity, the\nresponses indicate that size is associated with overall job satisfaction. The\nsmaller code review size is a strong preference amongst the engineers.\nAnecdotal guidance from FreeBSD suggests that \u201c[t]he smaller your patch, the\nhigher the probability that somebody will take a quick look at it\u201d (The\nFreeBSD Documentation Project 2022).\n\n## 5 Discussion\n\nWe summarize the frequent themes that resulted from the analysis of the survey\nresults. The concrete application of suggestions and potential solutions to\nproblems depends on the context, such as corporate culture or the project\nstage.\n\n### Commonalities Between Different Groups\n\nThe beliefs and willingness to make trade-offs are very similar between the\npractitioners in the industry and the open-source software community.\nTherefore, whatever solutions will manifest will be helpful for both groups.\nThe results from our survey indicate that for a majority of answers to Likert-\nstyle items, there are no statistically significant differences between\nindustry and open-source software developers. The differences focus on career\ngrowth, job satisfaction, and what trade-offs engineers are willing to make to\nincrease code velocity. The financial incentive structure is conceptually\ndifferent between industry and largely unpaid open-source software\ndevelopment. We expected divergent views in these areas.\n\n### The Need for Speed\n\nSpeed is the central theme across all the categories we cover in the survey.\nEngineers expect the code review process and the infrastructure to validate\nthe code changes to be fast and responsive. Most importantly, engineers need\nthe code review ers to promptly pay attention to the new code changes, follow-\nup questions, and any other open issues. In our experience, some of these\nexpectations and behavior are motivated by organizational culture. Companies\nevaluate engineers\u2019 performance using metrics such as the number of open pull\nrequests and their age, features deployed to the production environment, and\neven the SLOC that an engineer has committed. These metrics can positively or\nnegatively impact an engineer\u2019s career. Therefore, it is reasonable to assume\nthat engineers will focus on improving these metrics.\n\n### Engineer\u2019s Career and Code Velocity\n\nThe impact of code velocity on an engineer\u2019s career is unidirectional. Actions\nsuch as missing deadlines, not completing the work items on time, or being\nunable to deploy the code in production to meet agreed-upon milestones\nnegatively impact the engineer\u2019s career. The item \u201cCareer growth\u201d ranks lowest\nin items that increase in code velocity impacts positively. This finding is\nconcerning at multiple levels. Engineers can perform various actions to\nincrease code velocity. For example, they can split their commits into\nisolated units, write concise commit messages, and establish an effective\nworking relationship with their peers. All these tasks are non-trivial and\ntake time. Based on the survey feedback, there is no objective payoff\nregarding career growth when engineers invest all that effort into increasing\ncode velocity.\n\n### Splitting the Code Changes\n\nPrevious research shows that patches introducing new features will receive\nslow feedback due to their size (Thongtanunam et al. 2017). There is no clear\nsolution to mitigate this except splitting the code changes and sacrificing\nthe cohesiveness of the code review. The current trend in the industry is to\nsplit more prominent features into smaller ones and incrementally enable a\nsubset of functionality. However, implementation in small chunks is beneficial\nfor only some features and products. For example, the commonly used software\nMicrosoft Office has 12000 feature flags (Schr\u00f6der et al. 2022). Each feature\nflag can be enabled or disabled. It is not immediately evident that the\nintroduction of \\\\(2^n\\\\) of different configurations software can operate\nunder is beneficial for its maintenance.\n\n### Potential for the Return of the Commit-then-Review Model\n\nWe recommend that projects consider the application of the commit-then-review\nmodel under some conditions. In situations where most engineers have\nnoticeable experience, in-depth knowledge about the product, and can make\ninformed trade-offs, letting developers use their judgment to decide when to\nask for code review s seems a good use of time. A potential set of issues\nassociated with this can be a decisions-process related to who is qualified\nenough to be permitted to act this way. For example, can only engineers at a\ncertain level make changes without a code review? That in itself may be\ndivisive among the engineers. Another issue could be engineers getting used to\nthe \u201cedit-compile-debug-commit-push\u201d cycle and not requesting code review s\neven for more extensive changes. Industry can use a process that open-source\nsoftware uses to designate an individual as a committer (McKusick et al.\n2015).\n\n## 6 Threats to Validity\n\nLike any other study, the results we present in this paper are subject to\nspecific categories of threats (Shull et al. 2008).\n\nA primary threat to internal validity is that a survey, due to its nature, is\na self-report instrument. We mitigate this threat by making the survey\nanonymous, indicating that all the questions are optional (except the\nconsent), and avoiding any monetary incentives.\n\nFor conclusion validity, we rely purely on what our participants reported. Our\nprimary data source is a collection of survey results. We rely on the correct\nself-identification of survey participants to draw a subset of our\nconclusions. To draw conclusions from our sample size and analyze the Likert-\nstyle items, we used non-parametric statistical methods recommended in the\nliterature (Allen and Seaman 2007; Boone, Jr. and Boone 2012; Mann and Whitney\n1947; Shapiro and Wilk 1965). Our survey could have reached a very homogeneous\naudience because we reached out to our contacts. As a result, the views\nexpressed may be like the ones that the authors of this paper hold. We\nmitigated this concern by soliciting responses from Blender, Gerrit, Free BSD,\nand Net BSD developer communities.\n\nAnother threat in this category relates to readability. While the intent of\nthe questions may be apparent to the authors, they may be ambiguous to\nparticipants. We tried to mitigate this threat by asking a native speaker to\nreview the questions and adjust them based on the feedback from a pilot group\nof participants. Similarly, there are no assurances that all the survey\nparticipants are native English speakers. Therefore, they can misinterpret\nsome of the questions or terminology. For example, the terms like \u201cCareer\ngrowth\u201d and \u201cJob satisfaction\u201d can have different meanings depending on the\ncontext and individual. We tried to mitigate this threat by composing pointed\nquestions and highlighting the key terms. We did not provide formal\ndefinitions for each term to limit the survey\u2019s verbosity and attempt to\nincrease the completion rate.\n\nFor external validity, the concern is if our findings are relevant in\ndifferent contexts. Because we decided not to solicit participants based on\nthe data mined from various source code repositories, such as GitHub or\nGerrit, we could not target specific demographics precisely. We mitigated this\nby reaching out to several open-source software projects and our connections\nin the industry to solicit responses.\n\n## 7 Conclusions and Future Work\n\nThis paper presents the qualitative results from a survey about code velocity\nand the beliefs and practices surrounding it. We analyzed the responses to 75\ncompleted surveys. Ethical solicitation of survey participants was a\npainstaking process requiring exhaustive usage of various social media\nchannels. Demographically, 39 participants were from the industry, and 36\nrespondents were from the open-source software community. Based on what we\nknow, this is the first paper that studies the trade-offs engineers make to\nincrease the code velocity and critical impediments that block engineers from\nincreasing code velocity even more.\n\nThe software development processes in the industry and open-source community\nhave conceptual differences. However, our survey suggests that most beliefs\nand trade-offs related to increasing code velocity in these ecosystems are\nsimilar. Engineers\u2019 critical concern is the payoff towards their career growth\nif code velocity improves. A controlled application of the commit-then-review\nmodel scored the highest as a potential means to increase code velocity.\nReduced software security is something that 100% of open-source and 82% of\nindustry developers will^Footnote 2 not compromise, even if it means increased\ncode velocity.\n\nIn our future research, we plan to investigate the following topics: (a) the\nselective application of the commit-then-review model in the industry, (b) the\nbenefit of reward-based incentives to motivate engineers to react faster to\ncode reviews, and (c) the benefit of scheduling dedicated code review time to\nachieve a more precise planning outcome.\n\n## Data Availability\n\nThe datasets generated and analyzed during the current study are available in\nthe Zenodo repository. The dataset includes the anonymized responses to the\nsurvey, survey contents, and the R script that analyzes the survey data.\n\n## Notes\n\n  1. https://doi.org/10.5281/zenodo.8242289\n\n  2. https://doi.org/10.5281/zenodo.8242289\n\n## References\n\n  * Alami A, Cohn ML, W\u0105isowski A (2020) How do FOSS communities decide to accept pull requests? In: Proceedings of the evaluation and assessment in software engineering EASE \u201920. Association for Computing Machinery, New York, pp 220\u2013229. https://doi.org/10.1145/3383219.3383242\n\n  * Allen IE, Seaman CA (2007) Likert scales and data analyses. Qual Prog 40:64\u201365. http://rube.asq.org/quality-progress/2007/07/statistics/likert-scales-and-data-analyses.html\n\n  * Armstrong K (2022) Category direction\u2013code review 4. https://about.gitlab.com/direction/create/code_review/\n\n  * Bacchelli A, Bird C (2013) Expectations, outcomes, and challenges of modern code review. In: Proceedings of the 2013 international conference on software engineering ICSE \u201913. IEEE Press, pp 712\u2013721. https://doi.org/10.1109/ICSE.2013.6606617\n\n  * Bagert DJ (2002) Texas licensing of software engineers: all\u2019s quiet, for now. Commun ACM 45(11):92\u201394. https://doi.org/10.1145/581571.581603\n\nArticle Google Scholar\n\n  * Baltes S, Diehl S (2016) Worse than spam: issues in sampling software developers. In: Proceedings of the 10th ACM/IEEE international symposium on empirical software engineering and measurement ESEM \u201916. Association for Computing Machinery, New York. https://doi.org/10.1145/2961111.2962628\n\n  * Barnett M, Bird C, Brunet JA, Lahiri SK (2015) Helping developers help themselves: automatic decomposition of code review changesets. In: Proceedings of the 37th international conference on software engineering ICSE \u201915. IEEE Press, Florence, vol 1, pp 134\u2013144. https://doi.org/10.1109/ICSE.2015.35\n\n  * Barnett V, Lewis T (1984) Outliers in statistical data. Biom J 30(7):866\u2013867. https://doi.org/10.1002/bimj.4710300725\n\nArticle Google Scholar\n\n  * Baum T, Schneider K, Bacchelli A (2019) Associating working memory capacity and code change ordering with code review performance. Empir Softw Eng 24(4):1762\u20131798. https://doi.org/10.1007/s10664-018-9676-8\n\nArticle Google Scholar\n\n  * Baysal O, Kononenko O, Holmes R, Godfrey MW (2015) Investigating technical and non-technical factors influencing modern code review. Empir Softw Eng 21(3):932\u2013959. https://doi.org/10.1007/s10664-015-9366-8\n\nArticle Google Scholar\n\n  * Beckman RJ, Cook RD (1983) Outlier.........s. Technometrics 25(2):119\u2013149. http://www.tandfonline.com/doi/abs/10.1080/00401706.1983.10487840\n\n  * Bird C, Carnahan T, Greiler M (2015) Lessons learned from building and deploying a code review analytics platform. In: 2015 IEEE/ACM 12th working conference on mining software repositories (MSR). IEEE Computer Society, Los Alamitos, pp 191\u2013201. https://doi.org/10.1109/MSR.2015.25\n\n  * Blender (2022) Code review. https://wiki.blender.org/wiki/Tools/CodeReview\n\n  * Boone Jr HN, Boone DA (2012) Analyzing likert data. J Ext 50. https://archives.joe.org/joe/2012april/tt2.php\n\n  * Bosu A, Carver JC (2013) Impact of peer code review on peer impression formation: a survey. In: 2013 ACM/IEEE international symposium on empirical software engineering and measurement, pp 133\u2013142. https://doi.org/10.1109/ESEM.2013.23\n\n  * Brown JD (2011) Likert items and scales of measurement? Shiken: JALT Testing & Evaluation SIG Newsletter 15(1):10\u201314. https://hosted.jalt.org/test/PDF/Brown34.pdf\n\n  * Carifio J, Perla RJ (2007) Ten common misunderstandings, misconceptions, persistent myths and urban legends about likert scales and likert response formats and their antidotes. J Soc Sci 3(3):106\u2013116. https://thescipub.com/pdf/jssp.2007.106.116.pdf\n\n  * Chen L, Rigby PC, Nagappan N (2022) Understanding why we cannot model how long a code review will take: an industrial case study. In: Proceedings of the 30th ACM joint European software engineering conference and symposium on the foundations of software engineering ESEC/FSE 2022. Association for Computing Machinery, New York, pp 1314\u20131319. https://doi.org/10.1145/3540250.3558945\n\n  * Chen LT, Liu L (2020) Methods to analyze likert-type data in educational technology research. J Educ Tech Dev Exch 13(2). https://doi.org/10.18785/jetde.1302.04\n\n  * Chouchen M, Ouni A, Olongo J, Mkaouer MW (2023) Learning to predicts code review completion time in modern code review. Empir Softw Eng 28(4):82. https://doi.org/10.1007/s10664-023-10300-3\n\nArticle Google Scholar\n\n  * Chromium (2023) Contributing to chromium. https://chromium.googlesource.com/chromium/src/+/HEAD/docs/contributing.md#Creating-a-change\n\n  * Clason D, Dormody T (1994) Analyzing data measured by individual likert-type items. J Agric Educ 35(4). https://doi.org/10.5032/jae.1994.04031\n\n  * Cunha AC, Conte T, Gadelha B (2021a) Code review is just reviewing code? A qualitative study with practitioners in industry. In: Proceedings of the XXXV Brazilian symposium on software engineering SBES \u201921. Association for Computing Machinery, New York, pp 269\u2013274. https://doi.org/10.1145/3474624.3477063\n\n  * Cunha AC, Conte T, Gadelha B (2021b) What really matters in code review? A study about challenges and opportunities related to code review in industry. In: XX Brazilian symposium on software quality SBQS \u201921. Association for Computing Machinery, New York. https://doi.org/10.1145/3493244.3493255\n\n  * Czerwonka J, Greiler M, Tilford J (2015) Code reviews do not find bugs. How the current code review best practice slows us down. In: 2015 IEEE/ACM 37th IEEE international conference on software engineering, vol 2, pp 27\u201328. https://doi.org/10.1109/ICSE.2015.131\n\n  * Feitelson DG (2023) We do not appreciate being experimented on: developer and researcher views on the ethics of experiments on open-source projects. J Syst Softw 204:111774. https://doi.org/10.1016/j.jss.2023.111774\n\nArticle Google Scholar\n\n  * Feitelson DG, Frachtenberg E, Beck KL (2013) Development and deployment at facebook. IEEE Internet Comput 17(4):8\u201317. https://doi.org/10.1109/MIC.2013.25\n\nArticle Google Scholar\n\n  * Felderer M, Horta Travassos G (eds) (2020) Contemporary empirical methods in software engineering, 1st edn. Springer Nature, Cham\n\nGoogle Scholar\n\n  * Fowler M (2006) Continuous integration. https://martinfowler.com/articles/continuousIntegration.html\n\n  * FreeBSD Foundation (2022) Obtaining write access to the freeBSD source tree. https://wiki.freebsd.org/BecomingACommitter\n\n  * Frenkel S, Kang C (2021) An ugly truth: inside Facebook\u2019s battle for domination. Harper, New York\n\nGoogle Scholar\n\n  * GitHub (2021) Metrics available with GitHub insights\u2014GitHub docs. https://docs.github.com/en/enterprise-server@2.21/insights/exploring-your-usage-of-github-enterprise/metrics-available-with-github-insights#code-review-turnaround\n\n  * Gold NE, Krinke J (2021) Ethics in the mining of software repositories. Empir Softw Eng 27(1). https://doi.org/10.1007/s10664-021-10057-7\n\n  * Gon\u00e7alves PW, Fregnan E, Baum T, Schneider K, Bacchelli A (2020) Do explicit review strategies improve code review performance? In: Proceedings of the 17th international conference on mining software repositories MSR \u201920. Association for Computing Machinery, New York, pp 606\u2013610. https://doi.org/10.1145/3379597.3387509\n\n  * Gonzalez-Barahona JM (2020) Mining software repositories while respecting privacy. https://2020.msrconf.org/details/msr-2020-Education/1/Mining-Software-Repositories-While-Respecting-Privacy\n\n  * Google (2023a) Google engineering practices documentation. https://google.github.io/eng-practices/\n\n  * Google (2023b) Speed of code reviews. https://google.github.io/eng-practices/review/reviewer/speed.html\n\n  * Greiler M (2020) Code reviews\u2014from bottleneck to superpower with Michaela Greiler. https://learning.acm.org/techtalks/codereviews\n\n  * Groves RM (2006) Nonresponse rates and nonresponse bias in household surveys. Public Opin Q 70(5):646\u2013675. https://doi.org/10.1093/poq/nfl033\n\nArticle Google Scholar\n\n  * Hong Y, Tantithamthavorn CK, Thongtanunam PP (2022) Where should i look at? Recommending lines that reviewers should pay attention to. In: 2022 IEEE international conference on software analysis, evolution and reengineering (SANER), pp 1034\u20131045. https://doi.org/10.1109/SANER53432.2022.00121\n\n  * Izquierdo-Cortazar D, Sekitoleko N, Gonzalez-Barahona JM, Kurth L (2017) Using metrics to track code review performance. In: Proceedings of the 21st international conference on evaluation and assessment in software engineering EASE\u201917. Association for Computing Machinery, Karlskrona, pp 214\u2013223. https://doi.org/10.1145/3084226.3084247\n\n  * Jiang Y, Adams B, German DM (2013) Will my patch make it? And how fast?: Case study on the Linux kernel. In: Proceedings of the 10th working conference on mining software repositories MSR \u201913. IEEE Press, pp 101\u2013110. https://doi.org/10.1109/MSR.2013.6624016\n\n  * Killalea T (2019) Velocity in software engineering. Commun ACM 62(9):44\u201347. https://doi.org/10.1145/3345626\n\nArticle Google Scholar\n\n  * Kim H, Kwon Y, Joh S, Kwon H, Ryou Y, Kim T (2022) Understanding automated code review process and developer experience in industry. In: Proceedings of the 30th ACM joint European software engineering conference and symposium on the foundations of software engineering ESEC/FSE 2022. Association for Computing Machinery, New York, pp 1398\u20131407. https://doi.org/10.1145/3540250.3558950\n\n  * Kononenko O, Baysal O, Godfrey MW (2016) Code review quality: how developers see it. In: Proceedings of the 38th international conference on software engineering ICSE \u201916. Association for Computing Machinery, Austin, pp 1028\u20131038. https://doi.org/10.1145/2884781.2884840\n\n  * Kononenko O, Rose T, Baysal O, Godfrey MW, Theisen D, de Water B (2018) Studying pull request merges: a case study of shopify\u2019s active merchant. In: Proceedings of the 40th international conference on software engineering: software engineering in practice ICSE-SEIP \u201918. Association for Computing Machinery, New York, pp 124\u2013133. https://doi.org/10.1145/3183519.3183542\n\n  * Kushner D (2011) Facebook philosophy: move fast and break things. https://spectrum.ieee.org/facebook-philosophy-move-fast-and-break-things\n\n  * Liddell TM, Kruschke JK (2018) Analyzing ordinal data with metric models: what could possibly go wrong? J Exp Soc Psychol 79:328\u2013348. https://doi.org/10.1016/j.jesp.2018.08.009\n\nArticle Google Scholar\n\n  * Linux (2023) Everything you ever wanted to know about Linux -stable releases. https://www.kernel.org/doc/html/v4.15/process/stable-kernel-rules.html\n\n  * Llull R (1988) Blanquerna, 2nd edn. Dedalus Hippocrene books, Sawtry, Cambs, United Kingdom, Dedalus European classics\n\nGoogle Scholar\n\n  * LLVM Foundation (2023a) Contributing to LLVM\u2014LLVM 12 documentation. https://llvm.org/docs/Contributing.html#format-patches\n\n  * LLVM Foundation (2023b) LLVM code-review policy and practices. https://llvm.org/docs/CodeReview.html\n\n  * MacLeod L, Greiler M, Storey MA, Bird C, Czerwonka J (2018) Code reviewing in the trenches: challenges and best practices. IEEE Softw 35(4):34\u201342. https://doi.org/10.1109/MS.2017.265100500\n\nArticle Google Scholar\n\n  * MacLeod L, Greiler M, Storey MA, Bird C, Czerwonka J (2018) Code reviewing in the trenches: challenges and best practices. IEEE Softw 35(4):34\u201342. https://doi.org/10.1109/MS.2017.265100500\n\nArticle Google Scholar\n\n  * Maddila C, Upadrasta SS, Bansal C, Nagappan N, Gousios G, Av Deursen (2022) Nudge: accelerating overdue pull requests towards completion. ACM Trans Softw Eng Methodol. https://doi.org/10.1145/3544791\n\n  * Mann HB, Whitney DR (1947) On a test of whether one of two random variables is stochastically larger than the other. Ann Math Stat 18(1):50\u201360. https://doi.org/10.1214/aoms/1177730491\n\nArticle MathSciNet Google Scholar\n\n  * Martin RC (2002) Agile software development, principles, patterns, and practices. Alan Apt Series, Pearson\n\nGoogle Scholar\n\n  * McCarthy J (1995) Dynamics of software development. Microsoft Press, Redmond\n\nGoogle Scholar\n\n  * McIntosh S, Kamei Y, Adams B, Hassan AE (2015) An empirical study of the impact of modern code review practices on software quality. Empir Softw Eng 21(5):2146\u20132189. https://doi.org/10.1007/s10664-015-9381-9\n\nArticle Google Scholar\n\n  * McKusick MK, Neville-Neil GV, Watson RNM (2015) The design and implementation of the FreeBSD operating system, 2nd edn. Addison Wesley, Upper Saddle River\n\nGoogle Scholar\n\n  * McMartin A (2021) Introducing developer velocity lab\u2014a research initiative to amplify developer work and well-being. https://techcommunity.microsoft.com/t5/azure-developer-community-blog/introducing-developer-velocity-lab-a-research-initiative-to/ba-p/2333140\n\n  * Microsoft Research (2019) 14th IEEE/ACM international workshop on automation of software test. https://www.microsoft.com/en-us/research/event/14th-ieee-acm-international-workshop-on-automation-of-software-test/\n\n  * Microsoft Research (2023) Developer velocity lab. https://www.microsoft.com/en-us/research/group/developer-velocity-lab/\n\n  * Miller GA (1956) The magical number seven, plus or minus two: some limits on our capacity for processing information. Psychol Rev 63(2):81\u201397. https://doi.org/10.1037/h0043158\n\nArticle Google Scholar\n\n  * Mozilla (2023) Code reviews\u2014Firefox source docs documentation. https://firefox-source-docs.mozilla.org/devtools/contributing/code-reviews.html\n\n  * Nazir S, Fatima N, Chuprat S (2020) Modern code review benefits-primary findings of a systematic literature review. In: Proceedings of the 3rd international conference on software engineering and information management ICSIM \u201920. Association for Computing Machinery, New York, pp 210\u2013215. https://doi.org/10.1145/3378936.3378954\n\n  * Palantir (2018) Code review best practices. https://blog.palantir.com/code-review-best-practices-19e02780015f\n\n  * Phabricator (2021) Writing reviewable code. https://secure.phabricator.com/book/phabflavor/article/writing_reviewable_code/#many-small-commits\n\n  * PostgreSQL (2019) Submitting a patch - PostgreSQL Wiki. https://wiki.postgresql.org/wiki/Submitting_a_Patch\n\n  * Raina S (2015) Establishing association. Indian J Med Res 141(1):127. https://doi.org/10.4103/0971-5916.154519\n\nArticle Google Scholar\n\n  * Rigby PC, Bird C (2013) Convergent contemporary software peer review practices. In: Proceedings of the 2013 9th joint meeting on foundations of software engineering ESEC/FSE 2013. Association for Computing Machinery, New York, pp 202\u2013212. https://doi.org/10.1145/2491411.2491444\n\n  * Rigby PC, German DM (2006) A preliminary examination of code review processes in open source projects. Tech. rep., Concordia University, https://users.encs.concordia.ca/pcr/paper/Rigby2006TechReport.pdf\n\n  * Rigby PC, German DM, Storey MA (2008) Open source software peer review practices: a case study of the apache server. In: Proceedings of the 30th international conference on software engineering ICSE \u201908. Association for Computing Machinery, New York, pp 541\u2013550. https://doi.org/10.1145/1368088.1368162\n\n  * Riggs P (2022) Move faster, wait less: improving code review time at Meta. https://engineering.fb.com/2022/11/16/culture/meta-code-review-time-improving/\n\n  * Sadowski C, S\u00f6derberg E, Church L, Sipko M, Bacchelli A (2018) Modern code review: a case study at google. In: Proceedings of the 40th international conference on software engineering: software engineering in practice ICSE-SEIP \u201918. Association for Computing Machinery, Gothenburg, pp 181\u2013190. https://doi.org/10.1145/3183519.3183525\n\n  * dos Santos EW, Nunes I (2017) Investigating the effectiveness of peer code review in distributed software development. In: Proceedings of the XXXI Brazilian symposium on software engineering SBES \u201917. Association for Computing Machinery, New York, pp 84\u201393. https://doi.org/10.1145/3131151.3131161\n\n  * Schr\u00f6der M, Kevic K, Gopstein D, Murphy B, Beckmann J (2022) Discovering feature flag interdependencies in Microsoft Office. In: Proceedings of the 30th ACM joint European software engineering conference and symposium on the foundations of software engineering ESEC/FSE 2022. Association for Computing Machinery, New York, pp 1419\u20131429. https://doi.org/10.1145/3540250.3558942\n\n  * Shan Q, Sukhdeo D, Huang Q, Rogers S, Chen L, Paradis E, Rigby PC, Nagappan N (2022) Using nudges to accelerate code reviews at scale. In: Proceedings of the 30th ACM joint European software engineering conference and symposium on the foundations of software engineering ESEC/FSE 2022. Association for Computing Machinery, New York, pp 472\u2013482. https://doi.org/10.1145/3540250.3549104\n\n  * Shapiro SS, Wilk MB (1965) An analysis of variance test for normality (complete samples). Biometrika 52(3\u20134):591\u2013611. https://doi.org/10.1093/biomet/52.3-4.591\n\nArticle MathSciNet Google Scholar\n\n  * Shull F, Singer J, Sj\u00f8berg DIK (2008) Guide to advanced empirical software engineering. Springer, London\n\nBook Google Scholar\n\n  * Smith MG, Witte M, Rocha S, Basner M (2019) Effectiveness of incentives and follow-up on increasing survey response rates and participation in field studies. BMC Med Res Methodol 19(1). https://doi.org/10.1186/s12874-019-0868-8\n\n  * S\u00f6derberg E, Church L, B\u00f6rstler J, Niehorster D, Rydenf\u00e4lt C (2022) Understanding the experience of code review: misalignments, attention, and units of analysis. In: Proceedings of the international conference on evaluation and assessment in software engineering EASE \u201922. Association for Computing Machinery, New York, pp 170\u2013179. https://doi.org/10.1145/3530019.3530037\n\n  * Storey MA, Houck B, Zimmermann T (2022) How developers and managers define and trade productivity for quality. In: Proceedings of the 15th international conference on cooperative and human aspects of software engineering CHASE \u201922. Association for Computing Machinery, New York, pp 26\u201335. https://doi.org/10.1145/3528579.3529177\n\n  * Synopsys Inc (2020) The heartbleed bug. https://heartbleed.com/\n\n  * Tan X, Zhou M (2019) How to communicate when submitting patches: an empirical study of the Linux Kernel. Proc ACM Hum-Comput Interact 3(CSCW). https://doi.org/10.1145/3359210\n\n  * Tanna J (2021) Improving team efficiency by measuring and improving code review cycle time. https://www.jvt.me/posts/2021/10/27/measure-code-review/\n\n  * The FreeBSD Documentation Project (2022) Committer\u2019s guide. https://docs.freebsd.org/en/articles/committers-guide/#pre-commit-review\n\n  * The Linux Foundation (2022) A beginner\u2019s guide to Linux Kernel development. https://trainingportal.linuxfoundation.org/learn/course/a-beginners-guide-to-linux-kernel-development-lfd103/\n\n  * Thongtanunam P, Tantithamthavorn C, Kula RG, Yoshida N, Iida H, Matsumoto Ki (2015) Who should review my code? A file location-based code-reviewer recommendation approach for modern code review. In: 2015 IEEE 22nd international conference on software analysis, evolution, and reengineering (SANER), pp 141\u2013150. https://doi.org/10.1109/SANER.2015.7081824\n\n  * Thongtanunam P, Mcintosh S, Hassan AE, Iida H (2017) Review participation in modern code review. Empirical Softw Eng 22(2):768\u2013817. https://doi.org/10.1007/s10664-016-9452-6\n\nArticle Google Scholar\n\n  * Tsay JT (2017) Software developers using signals in transparent environments. PhD Thesis, Carnegie Mellon University. https://doi.org/10.1184/R1/6723026.v1\n\nBook Google Scholar\n\n  * Vanian J (2022) Internal Facebook memo warns company must be disciplined, prioritize ruthlessly. https://www.cnbc.com/2022/06/30/internal-facebook-memo-warns-company-must-be-disciplined-prioritize.html\n\n  * Wei\u00dfgerber P, Neu D, Diehl S (2008) Small patches get in! In: Proceedings of the 2008 international working conference on mining software repositories MSR \u201908. Association for Computing Machinery, Leipzig, pp 67\u201376. https://doi.org/10.1145/1370750.1370767\n\n  * Winters T, Manshreck T, Wright H (2020) Software engineering at google: lessons learned from programming over time, 1st edn. O\u2019Reilly, Beijing Boston Farnham Sebastopol Tokyo\n\nGoogle Scholar\n\n  * Wu Q, Lu K (2021) On the feasibility of stealthily introducing vulnerabilities in open-source software via hypocrite commits. https://github.com/QiushiWu/QiushiWu.github.io/blob/main/papers/OpenSourceInsecurity.pdf\n\n  * Zanjani MB, Kagdi H, Bird C (2016) Automatically recommending peer reviewers in modern code review. IEEE Trans Softw Eng 42(6):530\u2013543. https://doi.org/10.1109/TSE.2015.2500238\n\nArticle Google Scholar\n\n  * Zhu J, Zhou M, Mockus A (2016) Effectiveness of code contribution: from patch-based to pull-request-based tools. In: Proceedings of the 2016 24th ACM SIGSOFT international symposium on foundations of software engineering FSE 2016. Association for Computing Machinery, New York, pp 871\u2013882. https://doi.org/10.1145/2950290.2950364\n\nDownload references\n\n## Acknowledgements\n\nWe thank all the survey participants for their insightful comments and\nsuggestions. We are incredibly grateful to the Blender, Gerrit, Free BSD, and\nNet BSD developer communities. Our contacts in these teams either circulated\nthe survey internally or allowed us to use their forum platforms or mailing\nlists to solicit survey participants.\n\n## Author information\n\n### Authors and Affiliations\n\n  1. Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence, University of Groningen, 9712 CP, Groningen, Netherlands\n\nGunnar Kudrjavets & Ayushi Rastogi\n\nAuthors\n\n  1. Gunnar Kudrjavets\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  2. Ayushi Rastogi\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n### Corresponding author\n\nCorrespondence to Gunnar Kudrjavets.\n\n## Ethics declarations\n\n### Funding and conflicts of interests\n\nThe authors declare that they have no conflict of interest. The authors did\nnot receive support from any organization for the submitted work.\n\n## Additional information\n\nCommunicated by: Tayana Conte.\n\n### Publisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional affiliations.\n\n## Appendix A: Survey Questionnaire\n\n### Appendix A: Survey Questionnaire\n\n### 1.1 A.1 Q1\n\nDear participant,\n\nThis survey solicits your beliefs and experiences about code velocity (the\nspeed with which code changes are reviewed and merged) as a software engineer.\nThe results from this survey will help to improve developer productivity and\nthe daily lives of large numbers of software engineers. The survey should take\napproximately 5\u20137 minutes to complete.\n\nFor questions, contact Gunnar Kudrjavets (g.kudrjavets@rug.nl) and Dr. Ayushi\nRastogi (a.rastogi@rug.nl) at the University of Groningen.\n\nYou may refuse to take part in the research or exit the survey at any time\nwithout penalty. You may skip any question you do not wish to answer for any\nreason. Please refer to the link below for more details about the\nparticipation.\n\nConsent form for code velocity survey.pdf.\n\nChoosing \u201cAgree\u201d indicates that\n\n  * You have read the above information\n\n  * You voluntarily agree to participate\n\n  * You are 18 years of age or older\n\n\\\\(\\bigcirc \\\\) Agree \\\\(\\bigcirc \\\\) Disagree\n\n### 1.2 A.2 Q2\n\nHow many years of experience do you have with collaborative software\ndevelopment?\n\n  * \\\\(\\bigcirc<\\\\) 3 year\n\n  * \\\\(\\bigcirc \\\\) 3\u201310 years\n\n  * \\\\(\\bigcirc>\\\\) 10 years\n\n### 1.3 A.3 Q3\n\nHow many years have you been reviewing other people\u2019s code?\n\n  * \\\\(\\bigcirc < 1\\\\) year\n\n  * \\\\(\\bigcirc \\\\) 1 to 5 years\n\n  * \\\\(\\bigcirc \\\\) 5 to 10 years\n\n  * \\\\(\\bigcirc > 10\\\\) years\n\n  * I do not review code (e.g., only contribute code, moderate code review discussions)\n\n### 1.4 A.4 Q4\n\nWhat is your role in the code review (e.g., diff, patch, pull request)\nprocess? Select all that apply.\n\n  * \\\\(\\square \\\\) Author (e.g., contribute to a software project, fix a bug, implement a feature)\n\n  * \\\\(\\square \\\\) Moderator (e.g., resolve conflicts between participants, make final decisions)\n\n  * \\\\(\\square \\\\) Reviewer (e.g., a maintainer for a subsystem, owner of a feature)\n\n  * \\\\(\\square \\\\) Other, please specify\n\n### 1.5 A.5 Q5\n\nHow many times in a month do you submit code changes for review?\n\n  * \\\\(\\bigcirc \\\\) 1\u20132 times\n\n  * \\\\(\\bigcirc \\\\)3\u20136 times\n\n  * \\\\(\\bigcirc \\\\)6\u201310 times\n\n  * \\\\(\\bigcirc 10+\\\\) times\n\n  * None, my role is different (e.g., I only review code). Please specify:\n\n### 1.6 A.6 Q6\n\nHow many times in a month do you review other people\u2019s code changes?\n\n  * \\\\(\\bigcirc \\\\)1\u20132 times\n\n  * \\\\(\\bigcirc \\\\)3\u20136 times\n\n  * \\\\(\\bigcirc \\\\)6\u201310 times\n\n  * \\\\(\\bigcirc 10+\\\\) times\n\n  * None, my role is different (e.g., I only submit patches. Please specify:\n\n### 1.7 A.7 Q7\n\nWhat code reviewing environments do you use? Select all that apply.\n\n  * \\\\(\\square \\\\) Code collaboration tools (e.g., Gerrit, GitHub, Phabricator)\n\n  * \\\\(\\square \\\\) Tools internal to a company (e.g., Google\u2019s Critique, Meta\u2019s Phabricator, Microsoft\u2019s CodeFlow)\n\n  * \\\\(\\square \\\\) Mailing lists to review diffs/patches\n\n  * \\\\(\\square \\\\) Pair programming\n\n  * \\\\(\\square \\\\) Other, please specify\n\n### 1.8 A.8 Q8\n\nWhat type of software developer are you? Select all that apply.\n\n  * \\\\(\\square \\\\) I work on closed-source and get paid\n\n  * \\\\(\\square \\\\) I work on open-source and get paid\n\n  * \\\\(\\square \\\\) I work on open-source and do not get paid\n\n  * \\\\(\\square \\\\) Other, please specify\n\n### 1.9 A.9 Q9\n\nChoose an application domain you are most experienced with for the remaining\nquestions?\n\n  * \\\\(\\bigcirc \\\\) Application software (e.g., mobile and desktop applications)\n\n  * \\\\(\\bigcirc \\\\) Systems software (e.g., drivers, kernel development)\n\n  * \\\\(\\bigcirc \\\\) Real-time or critical software (e.g., aeronautics, embedded systems)\n\n  * \\\\(\\bigcirc \\\\) Other, please specify\n\n### 1.10 A.10 Q10\n\nRank the following metrics in the order of importance to optimize code\nvelocity. Drag the items \\\\(\\uparrow \\\\) or \\\\(\\downarrow \\\\) with 1 being the\nmost important metric.\n\n  1. 1.\n\nTime-to-first-response (first indication when someone reacted to your code\nreview or provided any actionable feedback)\n\n  2. 2.\n\nTime-to-accept (someone formally accepted your code changes and the code\nchanges can now be officially committed and merged)\n\n  3. 3.\n\nTime-to-merge (the accepted code changes have finally ended up in the\ndestination branch)\n\n  4. 4.\n\nOther, please specify\n\n### 1.11 A.11 Q11\n\nVelocity of your code improves your \\\\(\\dots \\\\)\n\nTable 5 Survey question Q11\n\nFull size table\n\n### 1.12 A.12 Q12\n\nI am willing to compromise on ...if it improves code velocity.\n\nTable 6 Survey question Q12\n\nFull size table\n\n### 1.13 A.13 Q13\n\nI think the post-commit review model (changes are first committed and then\nreviewed at some point later) can improve ...\n\nTable 7 Survey question Q13\n\nFull size table\n\n### 1.14 A.14 Q14\n\nShould engineers be allowed to commit their code without the code review\ndepending on ...\n\nTable 8 Survey question Q14\n\nFull size table\n\n### 1.15 A.15 Q15\n\nIn your opinion, what is the maximum acceptable size of the code review?\n\n  1. 1.\n\nSource lines of code:\n\n  2. 2.\n\nNumber of files:\n\n### 1.16 A.16 Q16\n\nWhat is a desired time range for someone to either accept your code review or\ngive you detailed feedback?\n\n  * \\\\(\\bigcirc \\\\) Same day\n\n  * \\\\(\\bigcirc \\le 24\\\\) hours\n\n  * \\\\(\\bigcirc \\\\) 1\u20132 days\n\n  * \\\\(\\bigcirc \\\\) A week\n\n  * \\\\(\\bigcirc \\\\) Other, please specify\n\n### 1.17 A.17 Q17\n\nIn your opinion, how can code velocity be improved for your projects?\n\nFor example, is the tooling (branch management, code review infrastructure)\nlacking, do other engineers need to be more responsive, are stricter\nguidelines for the code review process needed, etc.\n\n### 1.18 A.18 Q18\n\nIf you want to share more of your opinions about the the role of code velocity\nin software development or code review process then contact the researcher\ndirectly.\n\nIf you want to be notified about the findings from the survey then feel free\nto enter your email address below. We won\u2019t spam you ;-)\n\nEmail address:\n\n## Rights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0\nInternational License, which permits use, sharing, adaptation, distribution\nand reproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's\nCreative Commons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article's Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this licence, visit\nhttp://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\n## About this article\n\n### Cite this article\n\nKudrjavets, G., Rastogi, A. Does code review speed matter for practitioners?.\nEmpir Software Eng 29, 7 (2024). https://doi.org/10.1007/s10664-023-10401-z\n\nDownload citation\n\n  * Accepted: 02 October 2023\n\n  * Published: 22 November 2023\n\n  * DOI: https://doi.org/10.1007/s10664-023-10401-z\n\n### Share this article\n\nAnyone you share the following link with will be able to read this content:\n\nSorry, a shareable link is not currently available for this article.\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\n### Keywords\n\n  * Code review\n  * Code velocity\n  * Developer productivity\n  * Time-to-merge\n\nUse our pre-submission checklist\n\nAvoid common mistakes on your manuscript.\n\nAdvertisement\n\n### Discover content\n\n  * Journals A-Z\n  * Books A-Z\n\n### Publish with us\n\n  * Publish your research\n  * Open access publishing\n\n### Products and services\n\n  * Our products\n  * Librarians\n  * Societies\n  * Partners and advertisers\n\n### Our imprints\n\n  * Springer\n  * Nature Portfolio\n  * BMC\n  * Palgrave Macmillan\n  * Apress\n\n128.140.102.183\n\nNot affiliated\n\n\u00a9 2024 Springer Nature\n\n", "frontpage": false}
