{"aid": "40027236", "title": "Domo Arigato, Mr. Debugfs", "url": "https://notes.pault.ag/debugfs/", "domain": "pault.ag", "votes": 1, "user": "JNRowe", "posted_at": "2024-04-13 23:40:05", "comments": 0, "source_title": "Domo Arigato, Mr. debugfs", "source_text": "Paul's Notes | Domo Arigato, Mr. debugfs\n\n# CECI N'EST PAS UNE -EPIPE\n\n## Notes from paultag\n\n# Domo Arigato, Mr. debugfs\n\n### by\n\n## Paul Tagliamonte\n\nYears ago, at what I think I remember was DebConf 15, I hacked for a while on\ndebhelper to write build-ids to debian binary control files, so that the\nbuild-id (more specifically, the ELF note .note.gnu.build-id) wound up in the\nDebian apt archive metadata. I\u2019ve always thought this was super cool, and\nseeing as how Michael Stapelberg blogged some great pointers around the\necosystem, including the fancy new debuginfod service, and the find-dbgsym-\npackages helper, which uses these same headers, I don\u2019t think I\u2019m the only\none.\n\nAt work I\u2019ve been using a lot of rust, specifically, async rust using tokio.\nTo try and work on my style, and to dig deeper into the how and why of the\ndecisions made in these frameworks, I\u2019ve decided to hack up a project that\nI\u2019ve wanted to do ever since 2015 \u2013 write a debug filesystem. Let\u2019s get to it.\n\n# Back to the Future\n\nIt shouldn't shock anyone to learn I'm a huge fan of Go, right?\n\nTime to admit something. I really love Plan 9. It\u2019s just so good. So many\nideas from Plan 9 are just so prescient, and everything just feels right. Not\njust right like, feels good \u2013 like, correct. The bit that I\u2019ve always liked\nthe most is 9p, the network protocol for serving a filesystem over a network.\nThis leads to all sorts of fun programs, like the Plan 9 ftp client being a 9p\nserver \u2013 you mount the ftp server and access files like any other files. It\u2019s\nkinda like if fuse were more fully a part of how the operating system worked,\nbut fuse is all running client-side. With 9p there\u2019s a single client, and\ndifferent servers that you can connect to, which may be backed by a hard\ndrive, remote resources over something like SFTP, FTP, HTTP or even purely\nsynthetic.\n\nI even triggered a weird bug in vim when writing a 9p filesystem that wound up\nimpacting WSL -- although it seems like maybe not due to 9p (rather, SMB)\n\nThe interesting (maybe sad?) part here is that 9p wound up outliving Plan 9 in\nterms of adoption \u2013 9p is in all sorts of places folks don\u2019t usually expect.\nFor instance, the Windows Subsystem for Linux uses the 9p protocol to share\nfiles between Windows and Linux. ChromeOS uses it to share files with\nCrostini, and qemu uses 9p (virtio-p9) to share files between guest and host.\nIf you\u2019re noticing a pattern here, you\u2019d be right; for some reason 9p is the\ngo-to protocol to exchange files between hypervisor and guest. Why? I have no\nidea, except maybe due to being designed well, simple to implement, and it\u2019s a\nlot easier to validate the data being shared and validate security boundaries.\nSimplicity has its value.\n\nAs a result, there\u2019s a lot of lingering 9p support kicking around. Turns out\nLinux can even handle mounting 9p filesystems out of the box. This means that\nI can deploy a filesystem to my LAN or my localhost by running a process on\ntop of a computer that needs nothing special, and mount it over the network on\nan unmodified machine \u2013 unlike fuse, where you\u2019d need client-specific software\nto run in order to mount the directory. For instance, let\u2019s mount a 9p\nfilesystem running on my localhost machine, serving requests on 127.0.0.1:564\n(tcp) that goes by the name \u201cmountpointname\u201d to /mnt.\n\nUnfortunately, this requires root to mount and feels very un-plan9, but it\ndoes work and the protocol is good.\n\n    \n    \n    $ mount -t 9p \\ -o trans=tcp,port=564,version=9p2000.u,aname=mountpointname \\ 127.0.0.1 \\ /mnt\n\nLinux will mount away, and attach to the filesystem as the root user, and by\ndefault, attach to that mountpoint again for each local user that attempts to\nuse it. Nifty, right? I think so. The server is able to keep track of per-user\naccess and authorization along with the host OS.\n\n# WHEREIN I STYX WITH IT\n\n\"Simple\" here is intended as my highest form of praise. Writing complex things\nis easy. Taking your work, and simplifying it down the core is the most\ndifficult part of our work.\n\nSince I wanted to push myself a bit more with rust and tokio specifically, I\nopted to implement the whole stack myself, without third party libraries on\nthe critical path where I could avoid it. The 9p protocol (sometimes called\nStyx, the original name for it) is incredibly simple. It\u2019s a series of client\nto server requests, which receive a server to client response. These are,\nrespectively, \u201cT\u201d messages, which transmit a request to the server, which\ntrigger an \u201cR\u201d message in response (Reply messages). These messages are TLV\npayload with a very straight forward structure \u2013 so straight forward, in fact,\nthat I was able to implement a working server off nothing more than a handful\nof man pages.\n\nThere's also a 9P2000.L 9p variant which has more Linux specific extensions.\nThere's a good chance I port this forward when I get the chance.\n\nLater on after the basics worked, I found a more complete spec page that\ncontains more information about the unix specific variant that I opted to use\n(9P2000.u rather than 9P2000) due to the level of Linux specific support for\nthe 9P2000.u variant over the 9P2000 protocol.\n\n# MR ROBOTO\n\nIt really bothers me rust libraries that deal with I/O need to support\nstd::io, but to add support for async runtimes, you need to implement support\nfor tokio::io and every other runtime; but them's the breaks I guess. I really\nmiss Go's built-in async support and io module.\n\nThe backend stack over at zoo is rust and tokio running i/o for an HTTP and\nWebRTC server. I figured I\u2019d pick something fairly similar to write my\nfilesystem with, since 9P can be implemented on basically anything with I/O.\nThat means tokio tcp server bits, which construct and use a 9p server, which\nhas an idiomatic Rusty API that partially abstracts the raw R and T messages,\nbut not so much as to cause issues with hiding implementation possibilities.\nAt each abstraction level, there\u2019s an escape hatch \u2013 allowing someone to\nimplement any of the layers if required. I called this framework arigato which\ncan be found over on docs.rs and crates.io.\n\n    \n    \n    /// Simplified version of the arigato File trait; this isn't actually /// the same trait; there's some small cosmetic differences. The /// actual trait can be found at: /// /// https://docs.rs/arigato/latest/arigato/server/trait.File.html trait File { /// OpenFile is the type returned by this File via an Open call. type OpenFile: OpenFile; /// Return the 9p Qid for this file. A file is the same if the Qid is /// the same. A Qid contains information about the mode of the file, /// version of the file, and a unique 64 bit identifier. fn qid(&self) -> Qid; /// Construct the 9p Stat struct with metadata about a file. async fn stat(&self) -> FileResult<Stat>; /// Attempt to update the file metadata. async fn wstat(&mut self, s: &Stat) -> FileResult<()>; /// Traverse the filesystem tree. async fn walk(&self, path: &[&str]) -> FileResult<(Option<Self>, Vec<Self>)>; /// Request that a file's reference be removed from the file tree. async fn unlink(&mut self) -> FileResult<()>; /// Create a file at a specific location in the file tree. async fn create( &mut self, name: &str, perm: u16, ty: FileType, mode: OpenMode, extension: &str, ) -> FileResult<Self>; /// Open the File, returning a handle to the open file, which handles /// file i/o. This is split into a second type since it is genuinely /// unrelated -- and the fact that a file is Open or Closed can be /// handled by the `arigato` server for us. async fn open(&mut self, mode: OpenMode) -> FileResult<Self::OpenFile>; } /// Simplified version of the arigato OpenFile trait; this isn't actually /// the same trait; there's some small cosmetic differences. The /// actual trait can be found at: /// /// https://docs.rs/arigato/latest/arigato/server/trait.OpenFile.html trait OpenFile { /// iounit to report for this file. The iounit reported is used for Read /// or Write operations to signal, if non-zero, the maximum size that is /// guaranteed to be transferred atomically. fn iounit(&self) -> u32; /// Read some number of bytes up to `buf.len()` from the provided /// `offset` of the underlying file. The number of bytes read is /// returned. async fn read_at( &mut self, buf: &mut [u8], offset: u64, ) -> FileResult<u32>; /// Write some number of bytes up to `buf.len()` from the provided /// `offset` of the underlying file. The number of bytes written /// is returned. fn write_at( &mut self, buf: &mut [u8], offset: u64, ) -> FileResult<u32>; }\n\n# Thanks, decade ago paultag!\n\nIf this isn't my record for longest idea-to-wip-project time, it's close.\n\nLet\u2019s do it! Let\u2019s use arigato to implement a 9p filesystem we\u2019ll call debugfs\nthat will serve all the debug files shipped according to the Packages metadata\nfrom the apt archive. We\u2019ll fetch the Packages file and construct a filesystem\nbased on the reported Build-Id entries. For those who don\u2019t know much about\nhow an apt repo works, here\u2019s the 2-second crash course on what we\u2019re doing.\nThe first is to fetch the Packages file, which is specific to a binary\narchitecture (such as amd64, arm64 or riscv64). That architecture is specific\nto a component (such as main, contrib or non-free). That component is specific\nto a suite, such as stable, unstable or any of its aliases (bullseye,\nbookworm, etc). Let\u2019s take a look at the Packages.xz file for the unstable-\ndebug suite, main component, for all amd64 binaries.\n\n    \n    \n    $ curl \\ https://deb.debian.org/debian-debug/dists/unstable-debug/main/binary-amd64/Packages.xz \\ | unxz\n\nThis will return the Debian-style rfc2822-like headers, which is an export of\nthe metadata contained inside each .deb file which apt (or other tools that\ncan use the apt repo format) use to fetch information about debs. Let\u2019s take a\nlook at the debug headers for the netlabel-tools package in unstable \u2013 which\nis a package named netlabel-tools-dbgsym in unstable-debug.\n\n    \n    \n    Package: netlabel-tools-dbgsym Source: netlabel-tools (0.30.0-1) Version: 0.30.0-1+b1 Installed-Size: 79 Maintainer: Paul Tagliamonte <paultag@debian.org> Architecture: amd64 Depends: netlabel-tools (= 0.30.0-1+b1) Description: debug symbols for netlabel-tools Auto-Built-Package: debug-symbols Build-Ids: e59f81f6573dadd5d95a6e4474d9388ab2777e2a Description-md5: a0e587a0cf730c88a4010f78562e6db7 Section: debug Priority: optional Filename: pool/main/n/netlabel-tools/netlabel-tools-dbgsym_0.30.0-1+b1_amd64.deb Size: 62776 SHA256: 0e9bdb087617f0350995a84fb9aa84541bc4df45c6cd717f2157aa83711d0c60\n\nSo here, we can parse the package headers in the Packages.xz file, and store,\nfor each Build-Id, the Filename where we can fetch the .deb at. Each .deb\ncontains a number of files \u2013 but we\u2019re only really interested in the files\ninside the .deb located at or under /usr/lib/debug/.build-id/, which you can\nfind in debugfs under rfc822.rs. It\u2019s crude, and very single-purpose, but I\u2019m\nfeeling a bit lazy.\n\n# Who needs dpkg?!\n\nHilariously, the fourth? fifth? non-serious time (second serious time) I've\nhad to do this for a new language.\n\nFor folks who haven\u2019t seen it yet, a .deb file is a special type of .ar file,\nthat contains (usually) three files inside \u2013 debian-binary, control.tar.xz and\ndata.tar.xz. The core of an .ar file is a fixed size (60 byte) entry header,\nfollowed by the specified size number of bytes.\n\n    \n    \n    [8 byte .ar file magic] [60 byte entry header] [N bytes of data] [60 byte entry header] [N bytes of data] [60 byte entry header] [N bytes of data] ...\n\nI can't believe it's already been over a decade since my NM process, and\nnearly 16 years since I became an Ubuntu member.\n\nFirst up was to implement a basic ar parser in ar.rs. Before we get into using\nit to parse a deb, as a quick diversion, let\u2019s break apart a .deb file by hand\n\u2013 something that is a bit of a rite of passage (or at least it used to be? I\u2019m\ngetting old) during the Debian nm (new member) process, to take a look at\nwhere exactly the .debug file lives inside the .deb file.\n\n    \n    \n    $ ar x netlabel-tools-dbgsym_0.30.0-1+b1_amd64.deb $ ls control.tar.xz debian-binary data.tar.xz netlabel-tools-dbgsym_0.30.0-1+b1_amd64.deb $ tar --list -f data.tar.xz | grep '.debug$' ./usr/lib/debug/.build-id/e5/9f81f6573dadd5d95a6e4474d9388ab2777e2a.debug\n\nSince we know quite a bit about the structure of a .deb file, and I had to\nimplement support from scratch anyway, I opted to implement a (very!) basic\ndebfile parser using HTTP Range requests. HTTP Range requests, if supported by\nthe server (denoted by a accept-ranges: bytes HTTP header in response to an\nHTTP HEAD request to that file) means that we can add a header such as range:\nbytes=8-68 to specifically request that the returned GET body be the byte\nrange provided (in the above case, the bytes starting from byte offset 8 until\nbyte offset 68). This means we can fetch just the ar file entry from the .deb\nfile until we get to the file inside the .deb we are interested in (in our\ncase, the data.tar.xz file) \u2013 at which point we can request the body of that\nfile with a final range request. I wound up writing a struct to handle a\nread_at-style API surface in hrange.rs, which we can pair with ar.rs above and\nstart to find our data in the .deb remotely without downloading and unpacking\nthe .deb at all.\n\nI really like HTTP Range requests a lot.I did some stats to figure out what\ncompression dbgsym packages use these days; my LAN debug mirror contains\n113459 xz compressed tarfiles, and 9 gzip compressed tarfiles at the time of\nwriting.\n\nAfter we have the body of the data.tar.xz coming back through the HTTP\nresponse, we get to pipe it through an xz decompressor (this kinda sucked in\nRust, since a tokio AsyncRead is not the same as an http Body response is not\nthe same as std::io::Read, is not the same as an async (or sync) Iterator is\nnot the same as what the xz2 crate expects; leading me to read blocks of data\nto a buffer and stuff them through the decoder by looping over the buffer for\neach lzma2 packet in a loop), and tarfile parser (similarly troublesome). From\nthere we get to iterate over all entries in the tarfile, stopping when we\nreach our file of interest. Since we can\u2019t seek, but gdb needs to, we\u2019ll pull\nit out of the stream into a Cursor<Vec<u8>> in-memory and pass a handle to it\nback to the user.\n\nFrom here on out its a matter of gluing together a File traited struct in\ndebugfs, and serving the filesystem over TCP using arigato. Done deal!\n\n# A quick diversion about compression\n\nI was originally hoping to avoid transferring the whole tar file over the\nnetwork (and therefore also reading the whole debug file into ram, which\nobjectively sucks), but quickly hit issues with figuring out a way around\nseeking around an xz file. What\u2019s interesting is xz has a great primitive to\nsolve this specific problem (specifically, use a block size that allows you to\nseek to the block as close to your desired seek position just before it, only\ndiscarding at most block size - 1 bytes), but data.tar.xz files generated by\ndpkg appear to have a single mega-huge block for the whole file. I don\u2019t know\nwhy I would have expected any different, in retrospect. That means that this\nnow devolves into the base case of \u201cHow do I seek around an lzma2 compressed\ndata stream\u201d; which is a lot more complex of a question.\n\nAfter going through a lot of this, I realized just how complex the xz format\nis -- it's a lot more than just lzma2!\n\nThankfully, notoriously brilliant tianon was nice enough to introduce me to\nJon Johnson who did something super similar \u2013 adapted a technique to seek\ninside a compressed gzip file, which lets his service oci.dag.dev seek through\nDocker container images super fast based on some prior work such as soci-\nsnapshotter, gztool, and zran.c. He also pulled this party trick off for apk\nbased distros over at apk.dag.dev, which seems apropos. Jon was nice enough to\npublish a lot of his work on this specifically in a central place under the\nname \u201ctargz\u201d on his GitHub, which has been a ton of fun to read through.\n\nThe gist is that, by dumping the decompressor\u2019s state (window of previous\nbytes, in-memory data derived from the last N-1 bytes) at specific\n\u201ccheckpoints\u201d along with the compressed data stream offset in bytes and\ndecompressed offset in bytes, one can seek to that checkpoint in the\ncompressed stream and pick up where you left off \u2013 creating a similar \u201cblock\u201d\nmechanism against the wishes of gzip. It means you\u2019d need to do an O(n) run\nover the file, but every request after that will be sped up according to the\nnumber of checkpoints you\u2019ve taken.\n\nGiven the complexity of xz and lzma2, I don\u2019t think this is possible for me at\nthe moment \u2013 especially given most of the files I\u2019ll be requesting will not be\nloaded from again \u2013 especially when I can \u201cjust\u201d cache the debug header by\nBuild-Id. I want to implement this (because I\u2019m generally curious and Jon has\na way of getting someone excited about compression schemes, which is not a\nsentence I thought I\u2019d ever say out loud), but for now I\u2019m going to move on\nwithout this optimization. Such a shame, since it kills a lot of the work that\nwent into seeking around the .deb file in the first place, given the debian-\nbinary and control.tar.gz members are so small.\n\n# The Good\n\nFirst, the good news right? It works! That\u2019s pretty cool. I\u2019m positive my\nyounger self would be amused and happy to see this working; as is current day\npaultag. Let\u2019s take debugfs out for a spin! First, we need to mount the\nfilesystem. It even works on an entirely unmodified, stock Debian box on my\nLAN, which is huge. Let\u2019s take it for a spin:\n\n    \n    \n    $ mount \\ -t 9p \\ -o trans=tcp,version=9p2000.u,aname=unstable-debug \\ 192.168.0.2 \\ /usr/lib/debug/.build-id/\n\nAnd, let\u2019s prove to ourselves that this actually mounted before we go trying\nto use it:\n\n    \n    \n    $ mount | grep build-id 192.168.0.2 on /usr/lib/debug/.build-id type 9p (rw,relatime,aname=unstable-debug,access=user,trans=tcp,version=9p2000.u,port=564)\n\nSlick. We\u2019ve got an open connection to the server, where our host will keep a\nconnection alive as root, attached to the filesystem provided in aname. Let\u2019s\ntake a look at it.\n\n    \n    \n    $ ls /usr/lib/debug/.build-id/ 00 0d 1a 27 34 41 4e 5b 68 75 82 8E 9b a8 b5 c2 CE db e7 f3 01 0e 1b 28 35 42 4f 5c 69 76 83 8f 9c a9 b6 c3 cf dc E7 f4 02 0f 1c 29 36 43 50 5d 6a 77 84 90 9d aa b7 c4 d0 dd e8 f5 03 10 1d 2a 37 44 51 5e 6b 78 85 91 9e ab b8 c5 d1 de e9 f6 04 11 1e 2b 38 45 52 5f 6c 79 86 92 9f ac b9 c6 d2 df ea f7 05 12 1f 2c 39 46 53 60 6d 7a 87 93 a0 ad ba c7 d3 e0 eb f8 06 13 20 2d 3a 47 54 61 6e 7b 88 94 a1 ae bb c8 d4 e1 ec f9 07 14 21 2e 3b 48 55 62 6f 7c 89 95 a2 af bc c9 d5 e2 ed fa 08 15 22 2f 3c 49 56 63 70 7d 8a 96 a3 b0 bd ca d6 e3 ee fb 09 16 23 30 3d 4a 57 64 71 7e 8b 97 a4 b1 be cb d7 e4 ef fc 0a 17 24 31 3e 4b 58 65 72 7f 8c 98 a5 b2 bf cc d8 E4 f0 fd 0b 18 25 32 3f 4c 59 66 73 80 8d 99 a6 b3 c0 cd d9 e5 f1 fe 0c 19 26 33 40 4d 5a 67 74 81 8e 9a a7 b4 c1 ce da e6 f2 ff\n\nOutstanding. Let\u2019s try using gdb to debug a binary that was provided by the\nDebian archive, and see if it\u2019ll load the ELF by build-id from the right .deb\nin the unstable-debug suite:\n\n    \n    \n    $ gdb -q /usr/sbin/netlabelctl Reading symbols from /usr/sbin/netlabelctl... Reading symbols from /usr/lib/debug/.build-id/e5/9f81f6573dadd5d95a6e4474d9388ab2777e2a.debug... (gdb)\n\nYes! Yes it will!\n\n    \n    \n    $ file /usr/lib/debug/.build-id/e5/9f81f6573dadd5d95a6e4474d9388ab2777e2a.debug /usr/lib/debug/.build-id/e5/9f81f6573dadd5d95a6e4474d9388ab2777e2a.debug: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, interpreter *empty*, BuildID[sha1]=e59f81f6573dadd5d95a6e4474d9388ab2777e2a, for GNU/Linux 3.2.0, with debug_info, not stripped\n\n# The Bad\n\nLinux\u2019s support for 9p is mainline, which is great, but it\u2019s not robust.\nNetwork issues or server restarts will wedge the mountpoint (Linux can\u2019t\nreconnect when the tcp connection breaks), and things that work fine on local\nfilesystems get translated in a way that causes a lot of network chatter \u2013 for\ninstance, just due to the way the syscalls are translated, doing an ls, will\nresult in a stat call for each file in the directory, even though linux had\njust got a stat entry for every file while it was resolving directory names.\nOn top of that, Linux will serialize all I/O with the server, so there\u2019s no\nconcurrent requests for file information, writes, or reads pending at the same\ntime to the server; and read and write throughput will degrade as latency\nincreases due to increasing round-trip time, even though there are offsets\nincluded in the read and write calls. It works well enough, but is frustrating\nto run up against, since there\u2019s not a lot you can do server-side to help with\nthis beyond implementing the 9P2000.L variant (which, maybe is worth it).\n\n# The Ugly\n\nUnfortunately, we don\u2019t know the file size(s) until we\u2019ve actually opened the\nunderlying tar file and found the correct member, so for most files, we don\u2019t\nknow the real size to report when getting a stat. We can\u2019t parse the tarfiles\nfor every stat call, since that\u2019d make ls even slower (bummer). Only hiccup is\nthat when I report a filesize of zero, gdb throws a bit of a fit; let\u2019s try\nwith a size of 0 to start:\n\n    \n    \n    $ ls -lah /usr/lib/debug/.build-id/e5/9f81f6573dadd5d95a6e4474d9388ab2777e2a.debug -r--r--r-- 1 root root 0 Dec 31 1969 /usr/lib/debug/.build-id/e5/9f81f6573dadd5d95a6e4474d9388ab2777e2a.debug $ gdb -q /usr/sbin/netlabelctl Reading symbols from /usr/sbin/netlabelctl... Reading symbols from /usr/lib/debug/.build-id/e5/9f81f6573dadd5d95a6e4474d9388ab2777e2a.debug... warning: Discarding section .note.gnu.build-id which has a section size (24) larger than the file size [in module /usr/lib/debug/.build-id/e5/9f81f6573dadd5d95a6e4474d9388ab2777e2a.debug] [...]\n\nThis obviously won\u2019t work since gdb will throw away all our hard work because\nof stat\u2019s output, and neither will loading the real size of the underlying\nfile. That only leaves us with hardcoding a file size and hope nothing else\nbreaks significantly as a result. Let\u2019s try it again:\n\n    \n    \n    $ ls -lah /usr/lib/debug/.build-id/e5/9f81f6573dadd5d95a6e4474d9388ab2777e2a.debug -r--r--r-- 1 root root 954M Dec 31 1969 /usr/lib/debug/.build-id/e5/9f81f6573dadd5d95a6e4474d9388ab2777e2a.debug $ gdb -q /usr/sbin/netlabelctl Reading symbols from /usr/sbin/netlabelctl... Reading symbols from /usr/lib/debug/.build-id/e5/9f81f6573dadd5d95a6e4474d9388ab2777e2a.debug... (gdb)\n\nMuch better. I mean, terrible but better. Better for now, anyway.\n\n# Kilroy was here\n\nDo I think this is a particularly good idea? I mean; kinda. I\u2019m probably going\nto make some fun 9p arigato-based filesystems for use around my LAN, but I\ndon\u2019t think I\u2019ll be moving to use debugfs until I can figure out how to ensure\nthe connection is more resilient to changing networks, server restarts and\nfixes on i/o performance. I think it was a useful exercise and is a pretty\ngreat hack, but I don\u2019t think this\u2019ll be shipping anywhere anytime soon.\n\nAlong with me publishing this post, I\u2019ve pushed up all my repos; so you should\nbe able to play along at home! There\u2019s a lot more work to be done on arigato;\nbut it does handshake and successfully export a working 9P2000.u filesystem.\nCheck it out on on my github at arigato, debugfs and also on crates.io and\ndocs.rs.\n\nAt least I can say I was here and I got it working after all these years.\n\n", "frontpage": false}
