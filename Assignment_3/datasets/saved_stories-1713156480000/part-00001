{"aid": "40035185", "title": "Patchscopes: A framework for viewing hidden representations of language models", "url": "https://research.google/blog/patchscopes-a-unifying-framework-for-inspecting-hidden-representations-of-language-models/", "domain": "research.google", "votes": 1, "user": "panabee", "posted_at": "2024-04-14 22:47:36", "comments": 0, "source_title": "Patchscopes: A unifying framework for inspecting hidden representations of language models", "source_text": "Patchscopes: A unifying framework for inspecting hidden representations of\nlanguage models\n\nresearch.google uses cookies from Google to deliver and enhance the quality of\nits services and to analyze traffic. Learn more.\n\nJump to Content\n\nResearch\n\nResearch\n\n# Patchscopes: A unifying framework for inspecting hidden representations of\nlanguage models\n\nApril 11, 2024\n\nAvi Caciularu and Asma Ghandeharioun, Research Scientists, Google Research\n\nPatchscopes is a new framework that aims to unify a variety of previous\nmethods for interpreting the inner workings of LLMs by leveraging their\ninherent language abilities to provide intuitive, natural language\nexplanations of their internal hidden representations.\n\n## Quick links\n\n  * Paper\n  *     * \u00d7\n\nThe remarkable advancements in large language models (LLMs) and the concerns\nassociated with them, such as factuality and transparency, highlight the\nimportance of comprehending their mechanisms, particularly in instances where\nthey produce errors. By exploring the way a machine learning (ML) model\nrepresents what it has learned (the model's so called hidden representations),\nwe can gain better control over a model's behavior and unlock a deeper\nscientific understanding of how these models really work. This question has\nbecome even more important as deep neural networks grow in complexity and\nscale. Recent advances in interpretability research show promising results in\nusing LLMs to explain neuron patterns within another model.\n\nThese findings motivate our design of a novel framework to investigate hidden\nrepresentations in LLMs with LLMs, which we call Patchscopes. The key idea\nbehind this framework is to use LLMs to provide natural language explanations\nof their own internal hidden representations. Patchscopes unifies and extends\na broad range of existing interpretability techniques, and it enables\nanswering questions that were difficult or impossible before. For example, it\noffers insights into how an LLM's hidden representations capture nuances of\nmeaning in the model's input, making it easier to fix certain types of\nreasoning errors. While we initially focus the application of Patchscopes to\nthe natural language domain and the autoregressive Transformer model family,\nits potential applications are broader. For example, we are excited about its\napplications to detection and correction of model hallucinations, the\nexploration of multimodal (image and text) representations, and the\ninvestigation of how models build their predictions in more complex scenarios.\n\n## Patchscopes walkthrough with an example\n\nConsider the task of understanding how an LLM processes co-references to\nentities within a text. An implementation of Patchscopes is a specialized tool\ncrafted to address the specific problem of co-reference resolution. For\ninstance, to investigate a model's contextual understanding of whom a pronoun\nlike \u201cit\u201d refers to, a Patchscopes configuration can be created as follows\n(also illustrated below):\n\n  1. The Setup: A standard prompt (namely, the \u201csource prompt\u201d) containing relevant contextual information is presented to the model under investigation. The full source prompt in the example below is \u201cPatchscopes is robust. It helps interpret...\u201d.\n  2. The Target: A secondary prompt (namely, the \u201ctarget prompt\u201d) is designed to extract specific hidden information. In this example, a simple word-repetition prompt can reveal information from the hidden representation. The target prompt in the example below is \u201ccat->cat; 135->135; hello->hello; ?\u201d. Note that these words are randomly selected and so the prompt might look like gibberish at first, but there is also a pattern: it is composed of a few examples, where each example is a word, an arrow, and a repetition of the same word. If we feed this text to a language model trained to predict the next word, it is expected to continue to follow this pattern. That is, if we replace \"?\" with any random word and let the model generate the next word, it should repeat whatever we used to replace \"?\".\n  3. The Patch: Inference is performed on the source prompt. The hidden representation at the layer of interest in the token for \"It\" (green dot in the example below) is injected into the target prompt (the orange dot in the example below). Optionally, a transformation can be applied (the \u201cf\u201d in the example below) to align the representation with other layers or models.\n  4. The Reveal: The model processes the augmented input, and its output provides insights into how the original model internally comprehends the word \"It\" within its specific context. In the example below, the model generates \u201cPatchscopes\u201d, explaining the hidden representation at the 4th layer of the model above the \u201cIt\u201d token. This shows that after 4 layers of computation, the model has incorporated information from previous words into this hidden representation above the \u201cIt\u201d token, and has concluded that it is no longer referring to a generic object, but rather to \u201cPatchscopes\u201d. While this token representation (green dot) might otherwise look like an inscrutable vector of floating point numbers, Patchscopes can translate it into human-understandable text, showing that it refers to \u201cPatchscopes\u201d. This is in line with prior work that suggests information about a subject gets accrued in its last token.\n\n## Patchscopes in action\n\nPatchscopes has a broad range of applications for understanding and\ncontrolling LLMs. Here are a few examples we explored:\n\n  1. Next-token prediction: How early in the computation might the model have concluded its final prediction from the given context? Next token prediction from intermediate hidden representations is a widely used task to evaluate interpretability methods that look at the internals of transformers. Application of the Patchscope used in the previous section is surprisingly effective at this also, even in the usually much trickier early- to mid-layers of processing. Across different language models, it uniformly outperforms prior methods like Tuned Lens and Logit Lens from layer 10 onward.\n\nEvaluating various interpretability methods using the next token prediction\ntask from the intermediate hidden representations of an LLM. This shows our\napplication of Patchscopes with a simple \"Token Identity\" target prompt (that\nis, a target prompt composed of k demonstrations representing an identity-like\nfunction, formatted as \"tok_1 \u2192 tok_1 ; tok_2 \u2192 tok_2 ; . . . ; tok_k\")\ncompared to Tuned Lens and Logit Lens methods. The x-axis is the hidden\nrepresentations' layer that is being examined in the LLM. The y-axis shows\nprecision@1, which measures the proportion of examples where the highest\nprobability predicted token matches the highest probability token in the\noriginal distribution.  \n---  \n  2. Pulling out facts: How early in a model's computation does it have attribute information (e.g., the currency of a country)? In this experiment, we consider the task of extracting attributes from texts that originate from commonsense and factual knowledge tasks compiled by Hernandez et al., 2024. Here we use a target prompt that is a simple verbalization of the relation under investigation, followed by a placeholder for the subject. For example, to extract the official currency of the United States from the representation of \u201cStates\u201d, we use the target prompt, \"The official currency of x\". Given that this application of Patchscopes does not use any training examples, it's notable that it significantly outperforms other techniques.\n\nAttribute extraction accuracy across source layers (l). Left: Task done by\ntool (commonsense), 54 Source prompts, 12 Classes. Right: Country currency\n(factual), 83 Source prompts, 14 Classes.  \n---  \n  3. Explaining entities: Beyond just \"Yes\" or \"No\" How does a model understand a multi-word input like \"Alexander the Great\" as it processes input? Patchscopes goes beyond simple \u201chas it figured this out yet\u201d answers to reveal how the model gradually understands an entity, even in the very beginning stages. We use the following few-shot target prompt to decode the model\u2019s gradual processing: \"Syria: Country in the Middle East, Leonardo DiCaprio: American actor, Samsung: South Korean multinational major appliance and consumer electronics corporation, x\". The table below shows that as we go through the layers of two different models, Vicuna 13B and Pythia 12B, more words from the context get integrated into the current representation as reflected in the generations.\n\nIllustrating entity resolution via qualitative examples. The expressive\ngenerations show that as we go through the layers, more tokens from the\ncontext get integrated into the current representation. \"Explanation\" is what\nthe generation seems to be referring to and how that relates to the source\nprompt. Both these examples use the same target prompt described above.  \n---  \n  4. Teamwork makes the dream work: Models explaining models The Patchscopes framework lets us use a powerful language model to decode the process of a smaller one. Here, we leveraged Vicuna 13B to explain Vicuna 7B input processing by patching hidden representations of entities from the smaller model into the larger one. We measure the lexical similarity (using the RougeL score) between the model-generated text and the actual reference description sourced from Wikipedia. Vicuna 7B \u2192 13B (green line) is almost always above the Vicuna 7B \u2192 7B (blue line) and has a higher area under the curve. This shows that cross-model patching into a larger and more expressive model results in improved lexical similarity between generations and the reference text, and demonstrates that the process of cross-model patching significantly enhances the model's ability to generate text that is contextually aligned with the input representation from another model.\n\nRougeL (lexical similarity) scores of the generated descriptions against\ndescriptions from Wikipedia, using the Vicuna models. Patched representations\nfrom Vicuna 7B to Vicuna 13B result in a more expressive verbalization of\nentity resolution, both for popular and rare entities.  \n---  \n  5. Fixing faulty reasoning The most advanced LLMs can still struggle with multi-step reasoning, even if they are able to solve each reasoning step in isolation. Patchscopes can help address this by rerouting the intermediate hidden representations, significantly boosting accuracy. In this experiment, we systematically generate multi-hop factual and commonsense reasoning queries, and show that with prior knowledge about the input\u2019s structure, errors can be fixed by patching the hidden representations from one part of the query into another. We call this a chain-of-thought (CoT) Pathcscope, because it enforces sequential reasoning using the same prompt for source and target, but patching the hidden representation of one position into another. We show that CoT Patchscope improves accuracy from 19.57% to 50%. Our goal with this experiment is to demonstrate that it is feasible to use Patchscopes for intervention and correction, but caution that the CoT Pathscope is meant as an illustration rather than a generic correction method.\n\nAn illustration of CoT Patchscope on a single example, focusing on a response\nneeding correction with the prompt \"The current CEO of the company that\ncreated Visual Basic Script\".  \n---  \n\n## The takeaway\n\nThe Patchscopes framework is a breakthrough in understanding how language\nmodels work. It helps answer a wide range of questions from simple predictions\nto extracting knowledge from hidden representations and fixing errors in LLMs\u2019\ncomplex reasoning. This has intriguing implications for improving the\nreliability and transparency of the powerful language models we use every day.\nWant to see Patchscopes in action? Find more details in the paper.\n\nLabels:\n\n  * Machine Intelligence\n\n  * Natural Language Processing\n\n  * Responsible AI\n\n## Quick links\n\n  * Paper\n  *     * \u00d7\n\n### Other posts of interest\n\n  * April 12, 2024\n\nContrastive neural audio separation\n\n    * Machine Intelligence \u00b7\n    * Sound & Accoustics\n\n  * March 28, 2024\n\nAutoBNN: Probabilistic time series forecasting with compositional bayesian\nneural networks\n\n    * Algorithms & Theory \u00b7\n    * Machine Intelligence \u00b7\n    * Open Source Models & Datasets\n\n  * March 20, 2024\n\nComputer-aided diagnosis for lung cancer screening\n\n    * Health & Bioscience \u00b7\n    * Human-Computer Interaction and Visualization \u00b7\n    * Machine Intelligence\n\nFollow us\n\n", "frontpage": false}
