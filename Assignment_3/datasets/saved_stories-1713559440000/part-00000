{"aid": "40087440", "title": "Stateful Migrations Using Mutations", "url": "https://stack.convex.dev/migrating-data-with-mutations", "domain": "convex.dev", "votes": 1, "user": "FleetAdmiralJa", "posted_at": "2024-04-19 14:42:04", "comments": 0, "source_title": "Stateful Migrations using Mutations", "source_text": "Stateful Migrations using Mutations\n\nPatternsPerspectivesWalkthroughsAI\n\nBright ideas and techniques for building with Convex.\n\nIan Macartney\n\n3 days ago\n\n# Stateful Migrations using Mutations\n\nMigrations are inevitable. Initial schemas aren't perfect on the first try. As\nyour understanding of the problem evolves, you will inevitably change your\nmind about the ideal way to store information. So how do you do it at scale,\nwhere you might not be able to change everything in a single transaction?\n\nIn this post, we\u2019ll look at strategies for migrating data. In particular,\nscalable online migrations that don't require downtime or block pushes. We\u2019ll\nbe working specifically with Convex, but the concepts are universal.\n\nTo learn about migrations at a high level and some best practices, see this\nintro to migrations.\n\n## Schema Migrations\n\nOne thing to call out explicitly is that with Convex, you don\u2019t have to write\nmigration code like \u201cadd column\u201d or \u201cadd index\u201d explicitly. All you need to do\nis update your schema.ts file and Convex handles it. Convex isn\u2019t rigidly\nstructured like most SQL databases are. If you change your field from\nv.string() to v.union(v.string(), v.number()), Convex doesn\u2019t have to reformat\nthe data or table. However, it will enforce the schema you define, and will\nnot let you deploy a schema that doesn't match the data at rest. Or you can\nturn off schema validation and throw unstructured data into Convex and it will\nalso work^1.\n\nWith schema validation enabled, Convex will help your code and data stay in\nsync by only letting you push schemas that match the current data. To add a\nstring field to an object, for instance, you can push a schema where that\nfield is v.optional(v.string()). Once there is a string on every object,\nConvex will let you push a schema that is just v.string() and future writes\nwill enforce that the field will always be set and be a string.\n\nIn this way, Convex gives you the ease of just defining your types\ndeclaratively, while also guaranteeing that they match the reality of the data\nat rest when you deploy your code and schema. It\u2019s also worth mentioning that\ntransitions from one schema definition and code version to the next are\natomic, thanks to Convex coordinating both the functions and the database.\n\nThe rest of this post is about how you go about changing the underlying data.\n\n## Data Migrations using Mutations\n\nTo migrate data in Convex, you can use a mutation to transform your data. In\nparticular, you'd likely use an internalMutation so it isn't exposed on your\npublic API.\n\nI\u2019ve made some helpers you can use in your project so you only have to write\nthe code relevant to updating documents. It allows you to run migrations over\nyour data in batches. We'll use them in the following examples. See below for\nsteps to install and configure them.\n\n### Common use cases\n\nTo illustrate what writing migrations looks like in Convex, let's use the\nmigration helper. Below we'll show where this comes from, but the gist is that\nit runs a specified function over each document in your table, in batches.\n\nHere's how to achieve common migration patterns:\n\n#### Adding a new field with a default value\n\n    \n    \n    export const setDefaultPlan = migration({ table: \"teams\", migrateOne: async (ctx, team) => { if (!team.plan) { await db.patch(team._id, { plan: \"basic\" }); } }, });\n\nIf you\u2019re using a schema and validation, you\u2019d likely update the team\u2019s schema\nfirst to define \u201cplan\u201d as:\n\nplan: v.optional(v.union(v.literal(\"basic\"), v.literal(\"pro\")))\n\nThen, after all the fields have a value, you\u2019d change it to:\n\nplan: v.union(v.literal(\"basic\"), v.literal(\"pro\"))\n\nConvex won\u2019t let you deploy a schema that doesn\u2019t conform to the data unless\nyou turn off schema validation. As a result, you can safely trust that the\ntypescript types inferred from your schema match the actual data.\n\nNote: this doesn\u2019t have to be a static value. You could write the value based\non other fields in the document, or whatever custom logic you like.\n\nAs a reminder for those who skipped the primer, to do this correctly, you\u2019d\nalso want to update your code to start writing the default field value on new\ndocuments before running this mutation to avoid missing any documents.\n\n#### Deleting a field\n\nIf you\u2019re sure you want to get rid of data, you would modify the schema in\nreverse: making the field optional before you can delete the data.\n\nisPro: v.boolean() -> isPro: v.optional(v.boolean())\n\nThen you can run the following:\n\n    \n    \n    export const removeBoolean = migration({ table: \"teams\", migrateOne: async (ctx, team) => { if (team.isPro !== undefined) { await db.patch(team._id, { isPro: undefined }); } }, });\n\nAs mentioned in the migration primer, I advise deprecating fields over\ndeleting them when real user data is involved.\n\n#### Changing the type of a field\n\nYou can both add and delete fields in the same migration - we could have done\nboth the setting a default plan and deleting the deprecated isPro plan:\n\n    \n    \n    export const updatePlanToEnum = migration({ table: \"teams\", migrateOne: async (ctx, team) => { if (!team.plan) { await db.patch(team._id, { plan: team.isPro ? \"pro\" : \"basic\", isPro: undefined, }); } }, });\n\nI'd recommend new fields when types change, but if you want to use the same\nfield, you can do it with a union: zipCode: v.number() -> field:\nv.union(v.string(), v.number())\n\n    \n    \n    export const zipCodeShouldBeAString = migration({ table: \"addresses\", migrateOne: async (ctx, address) => { if (typeof address.zipCode === \"number\") { // Note: as a convenience, it will apply a patch you return. return { zipCode: address.zipCode.toString() }; } }, });\n\n#### Inserting documents based on some state\n\nLet's say you're changing user preferences from being an object in the users\nschema to its own document - you might consider doing this as preferences\ngrows to be a lot of options, or to avoid accidentally returning preference\ndata to clients for queries that return users. You can walk the users table\nand insert into another table:\n\n    \n    \n    export const changePreferencesToDocument = migration({ table: \"users\", migrateOne: async (ctx, user) => { const prefs = await ctx.db .query(\"preferences\") .withIndex(\"userId\", (q) => q.eq(\"userId\", user._id)) .first(); if (!prefs) { await ctx.db.insert(\"preferences\", user.preferences); await ctx.db.patch(user._id, { preferences: undefined }); } }, });\n\nYou'd want to also have code that is adding perferences documents by default\nfor new users, so the migration is only responsible for older users. You'd\nalso update your code to first check the user for preferences, and if it's\nunset, fetch it from the table. Later, once you're confident there are\npreferences for all users, remove the preferences object from the users\nschema, and the code can just read preferences from the table.\n\n#### Deleting documents based on some state\n\nIf you had a bug where you didn't delete related documents correctely, you\nmight want to clean up documents based on the existence of another document.\nFor example, one gotcha with vector databases is forgetting to delete\nembedding documents linked to chunks of documents that have been deleted. When\nyou do a vector search, you'd get results that no longer exist. To delete the\nrelated documents you could do:\n\n    \n    \n    export const deleteOrphanedEmbeddings = migration({ table: \"embeddings\", migrateOne: async (ctx, doc) => { const chunk = await ctx.db .query(\"chunks\") .withIndex(\"embeddingId\", (q) => q.eq(\"embeddingId\", doc._id)) .first(); if (!chunk) { await ctx.db.delete(doc._id); } }, });\n\n### Setting up convex-helpers/server/migrations\n\nTo use the above migration helper, first install convex-helpers:\n\n    \n    \n    npm i convex-helpers@latest\n\nIt can optionally keep track of migration state, allowing you to resume or\nskip already-completed migrations. If you don't do this, you can still run\nmigrations but you'll have to look at logs to know when it's done or what\ncursor to resume from in the case of failure. If you want persistence, add the\nmigrations table in convex/schema.ts:\n\n    \n    \n    // In convex/schema.ts import { migrationsTable } from \"convex-helpers/server/migrations\"; export default defineSchema({ migrations: migrationsTable, // other tables... });\n\nYou can pick any table name for this, but it should match migrationTable used\nbelow.\n\nTo define the migration helper, use makeMigration. In convex/migrations.ts (or\nwherever you want to define it):\n\n    \n    \n    import { makeMigration } from \"convex-helpers/server/migrations\"; import { internalMutation } from \"./_generated/server\"; const migration = makeMigration(internalMutation, { migrationTable: \"migrations\", });\n\nWe'll assume the migrations are stateful for the rest of the post.\n\n#### Defining migrations\n\nAs shown in previous sections, you use the migration wrapper to define\ninternal mutations that run your migration function over all documents. In\naddition to the syntax above, you can also just return a patch from a\nmigration:\n\n    \n    \n    export const myMigration = migration({ table: \"users\", migrateOne: async (ctx, doc) => ({ someField: \"some value\" }), batchSize: 10, });\n\nIf you don't provide a batchSize it will default to \ud83d\udcaf.\n\n#### Running a migration from code\n\nYou can start a migration from a Convex mutation or action with the\nstartMigration function.\n\n  * If it is already running it will refuse to start another duplicate worker.\n  * If it had previously failed on some batch, it will continue from that batch unless you manually specify startCursor.\n  * If you provide an explicit startCursor (null means to start at the beginning), it will start from there.\n  * If you set dryRun: true then it will run and then throw, so no changes are committed, and you can see what it would have done. This is good for validating it does what you expect before running it on your data. Note: I often just run dry runs from the command line.\n\n    \n    \n    import { startMigration } from \"convex-helpers/server/migrations\"; //... within a mutation or action await startMigration(ctx, internal.migrations.myMigration, { startCursor: null, // optional override batchSize: 10, // optional override });\n\n#### Running a series of default migrations from code\n\nIt's sometimes handy to just add the migration you want to run to a list, and\nhave them all run after a deploy, or via some script. startMigrationsSerially\nwill run each migration that hasn't finished, one at a time.\n\n  * If a migration had already completed, it will skip it.\n  * If a migration had partial progress, it will resume from where it left off.\n  * If a migration is already in progress when attempted, it will no-op.\n  * If a migration fails, it will not continue to the next migration, in case you had some dependencies between the migrations. Call the series again to retry.\n\n    \n    \n    import { startMigrationsSerially } from \"convex-helpers/server/migrations\"; import { internalMutation } from \"./_generated/server\"; export default internalMutation(async (ctx) => { await startMigrationsSerially(ctx, [ internal.migrations.myMigration, internal.migrations.myOtherMigration, //... ]); });\n\nNote: if you start multiple serial migrations, the behavior is:\n\n  * If they don't overlap on functions, they will happily run in parallel.\n  * If they have a function in common and one completes before the other attempts it, the second will just skip it.\n  * If they have a function in common and one is in progress, the second will no-op and not run any further migrations in its series.\n\n#### Running migrations from the CLI or dashboard\n\nYou can run migrations manually from the CLI or dashboard.\n\nTo run a single migration that will start or resume where it previously left\noff, run:\n\n    \n    \n    npx convex run migrations:myMigration '{\"fn\": \"migrations:myMigration\"}'\n\nTo run a series of migrations, like the example above where there's a default\nexport in convex/migrations.ts running startMigrationsSerially, run:\n\n    \n    \n    npx convex run migrations\n\nIn production you could run this after a deploy:\n\n    \n    \n    npx convex deploy --cmd 'npm run build' && npx convex run migrations --prod\n\nNote you pass --prod to run these commands in production.\n\n#### Test a migration before running it to completion from the CLI\n\n    \n    \n    npx convex run migrations:myMigration '{\"dryRun\": true, \"fn\": \"migrations:myMigration\"}' # --prod\n\n#### Restart a migration from the beginning from the CLI\n\n    \n    \n    npx convex run migrations:myMigration '{\"cursor\": null, \"fn\": \"migrations:myMigration\"}' # --prod\n\nOr you can pass in any cursor to start from, e.g. where a previous migration\nleft off, if you haven't configured it to be stateful with a table.\n\n#### Stop a migration\n\nYou can stop a migration with the cancelMigration function call. The currently\nrunning batch will complete, but it will not schedule further batches. This\nrequires stateful migrations - here passing in the table name \"migrations\".\n\n    \n    \n    import { cancelMigration } from \"convex-helpers/server/migrations\"; await cancelMigration(ctx, \"migrations\", internal.migrations.myMigration);\n\nYou can also write an internal mutation that calls cancel for some job, so you\ncan cancel a migration without pushing new code:\n\n    \n    \n    export const cancel = internalMutation({ args: { fn: v.string() }, handler: async (ctx, { fn }) => { return await cancelMigration(ctx, \"migrations\", fn); }, });\n\nAnd call it (assuming here that it's in convex/migrations.ts):\n\n    \n    \n    npx convex run migrations:cancel '{\"fn\": \"migrations:myMigration\" }' # --prod\n\n#### Get the status of migrations\n\nTo see how a migration has progressed, you can use the getStatus function,\neither for specific migrations:\n\n    \n    \n    import { getStatus, MigrationStatus } from \"convex-helpers/server/migrations\"; // We annotate the type here to avoid circular references if we use this // value in the return of a function (part of the internal.* type). const status: MigrationStatus<\"migrations\"> = await getStatus(ctx, { migrationTable: \"migrations\", migrations: [internal.migrationsExample.increment], });\n\nOr you can get the status of the most recent migrations (defaults to 10):\n\n    \n    \n    export const status = internalQuery(async (ctx) => { return await getStatus(ctx, { migrationTable: \"migrations\", limit: 10 }); });\n\nIf you define an internalQuery like this, you can watch the status of your\nmigration live from the CLI:\n\n    \n    \n    npx convex run --watch migrations:status # --prod\n\n### Defining your own migrations\n\nHow would you do this without the migration helper? The rest of this post is\nhere if you want to know how to build some of this yourself. If you're happy\nwith the helpers, you can stop reading here.\n\nIf your table is small enough (let\u2019s say a few thousand rows, as a guideline),\nyou could just do it all in one mutation. For example:\n\n    \n    \n    export const doMigration = internalMutation(async ({ db }) => { const teams = await db.query(\"teams\").collect(); for (const team of teams) { // modify the team and write it back to the db here } });\n\nThis would define the doMigration mutation, which you could run from the\ndashboard or via npx convex run.\n\n#### Big tables\n\nFor larger tables, reading the whole table becomes impossible. Even with\nsmaller tables, if there are a lot of active writes happening to the table,\nyou might want to break the work into smaller chunks to avoid conflicts.\nConvex will automatically retry failed mutations up to a limit, and mutations\ndon\u2019t block queries, but it\u2019s still best to avoid scenarios that make them\nlikely.\n\nThere are a few ways you could break up the work. For the helper, I use\npagination. Each mutation will only operate on a batch of documents and keep\ntrack of how far it got, so the next worker can efficiently pick up the next\nbatch. One nice benefit of this is you can keep track of your progress, and if\nit fails on some batch of data, you can keep track of the cursor it started\nwith and restart the migration at that batch. Thanks to Convex\u2019s transactional\nguarantees, either all of the batch or none of the batch\u2019s writes will have\ncommitted. A mutation that works with a page of data might look like this:\n\n    \n    \n    export const myMigrationBatch = internalMutation( async ({ db }, { cursor, numItems }) => { const data = await db.query(\"mytable\").paginate({ cursor, numItems }); const { page, isDone, continueCursor } = data; for (const doc of page) { // modify doc } return { cursor: continueCursor, isDone }; } );\n\n#### Running a batch\n\nTo try out your migration, you might try running it on one chunk of data via\nthe CLI or by going to the functions panel on the dashboard and clicking \u201cRun\nfunction.\u201d To run from the beginning of the table, you\u2019d pass as an argument:\n\n{ cursor: null, numItems: 1 }\n\nOn the CLI it would be:\n\n    \n    \n    npx convex run mutations:myMigrationBatch '{ \"cursor\": null, \"numItems\": 1 }'\n\nIt would then run and return the next cursor (and print it to the console so\nyou can look back if you lose track of it). To run the next batch, just update\nthe parameter to the cursor string instead of null.\n\nYou could keep running it from here, but it might start to feel tedious. Once\nyou have confidence in the code and batch size, you can start running the\nrest. You can even pass in the cursor you got from testing on the dashboard to\nskip the documents you\u2019ve already processed \u261d\ufe0f.\n\n#### Looping batches from an action\n\nTo iterate through chunks, you can call it from an action in a loop:\n\n    \n    \n    export const runMigration = internalAction( async ({ runMutation }, { name, cursor, batchSize }) => { let isDone = false; while (!isDone) { const args = { cursor, numItems: batchSize }; ({ isDone, cursor } = await runMutation(name, args)); } } );\n\nYou can then go to the dashboard page for the runMigration function and test\nrun the mutation with the arguments { name: \"myMigrationBatch\", cursor: null,\nbatchSize: 1 }\n\nHere \"myMigrationBatch\" is whatever your mutation\u2019s path is, e.g. if it\u2019s in\nthe file convex/migrations/someMigration.js, it would be\n\"migrations/someMigration:myMigrationBatch\".\n\nTo use the CLI, you could run:\n\n    \n    \n    npx convex run migrations:runMigration '{ \"name\": \"myMigrationBatch\", \"cursor\": null, \"batchSize\": 1 }'\n\nIt is also possible to loop from a client, such as the ConvexHttpClient, if\nyou make it a public mutation. You could also recursively schedule a mutation\nto run, as an exercise left to the reader.\n\n#### Batching via recursive scheduling\n\nIn the helpers, we use recursive scheduling for batches. A mutation keeps\nscheduling itself until the pagination is done. This is simpler as you don't\nneed to use a separate runMigration function, you can just call it itself.\nThis is why it takes a fn parameter: to know how to call itself. Read the code\nto see it in action.\n\n#### An aside on serial vs. parallelizing\n\nYou might be wondering whether we should be doing all of this in parallel. I\u2019d\nurge you to start doing it serially, and only add parallelization gradually if\nit\u2019s actually too slow. As a general principle with backend systems, avoid\nsending big bursts of traffic when possible. Even without causing explicit\nfailures, it could affect latencies for user requests if you flood the\ndatabase with too much traffic at once. This is a different mindset from an\nanalytics database where you\u2019d optimize for throughput. I think you\u2019ll be\nsurprised how fast a serial approach works in most cases. The helpers run\nserially. Reach out if you want to explore more parallelism.\n\n## Summary\n\nIn this post, we looked at a strategy for migrating data in Convex using\nmutation functions. As with other posts, the magic is in composing helper\nfunctions and leveraging the fact that you get to write javascript or\ntypescript rather than divining the right SQL incantation. The code for the\nhelpers is available in the convex-helpers package and visible on GitHub, and\nif you have any questions don\u2019t hesitate to reach out in Discord.\n\nget-convex/convex-helpers\n\n### Footnotes\n\n  1. Technically, there are some restrictions on Convex values, such as array lengths and object key names that you can read about here. \u21a9\n\nBuild in minutes, scale forever.\n\nConvex is the backend application platform with everything you need to build\nyour project. Cloud functions, a database, file storage, scheduling, search,\nand realtime updates fit together seamlessly.\n\nGet started\n\n  * Patterns\n\nJoin the Convex Community\n\nAsk the team questions, learn from others, and stay up-to-date on the latest\nwith Convex.\n\nJoin the Discord community\n\nShare this article\n\nRead next\n\nHow Convex Works\n\nThe full, unabridged story on how the Convex internals work.\n\nSujay Jayakar\n\nConvex: The Software-Defined Database\n\nWhich to choose, the expressive power of code, or the robustness of built-in\ndatabase features? With Convex, you can have both. By eliminating the boundary\nbetween the application and the database, Convex provides a uniform and\npowerful way to model your entire backend data flow and security using plain\nol' code.\n\nJamie Turner\n\nAutomatically Retry Actions\n\nLearn how to automatically retry actions in Convex while also learning a\nlittle about scheduling, system tables, and function references.\n\nJames Cowling\n\nDatabases are Spreadsheets\n\nI want to share my mental model of databases: - Databases are just big\nspreadsheets - An index is just a view of the spreadsheet sorted by one or\nmore columns - Binary search over a sorted list is faster than a linear scan\n(for large lists)\n\nSarah Shader\n\nConvexDocsGitHubDashboardJobsLegal\n\n\u00a92024 Convex, Inc.\n\n", "frontpage": false}
