{"aid": "40034491", "title": "Fast Development in Rust, Part One", "url": "https://blog.sdf.com/p/fast-development-in-rust-part-one", "domain": "sdf.com", "votes": 2, "user": "AndrewDucker", "posted_at": "2024-04-14 21:08:02", "comments": 0, "source_title": "Fast Development In Rust, Part One", "source_text": "Fast Development In Rust, Part One\n\n# SDF Blog\n\nShare this post\n\n#### Fast Development In Rust, Part One\n\nblog.sdf.com\n\n#### Discover more from SDF Blog\n\nWelcome to the official SDF blog. Here you can learn all things data\ndevelopment.\n\nContinue reading\n\nSign in\n\n# Fast Development In Rust, Part One\n\n### Like... really f***ing fast.\n\nBo Lin\n\n,\n\nAlexander Bogdanowicz\n\n, and\n\nLukas Schulte\n\nMar 13, 2024\n\n19\n\nShare this post\n\n#### Fast Development In Rust, Part One\n\nblog.sdf.com\n\n2\n\nShare\n\n\u201cI Am Speed\u201d...\n\nOver the last 12 months, we\u2019ve been working on building a SQL compiler,\nsemantic analyzer, and data build system. It\u2019s called SDF, and it\u2019s written\nentirely in Rust. Our initial decision to adopt Rust was not without\nreservations -- despite all the hype around Rust's features (the great\n\"Rewrite in Rust\" movement had long become a meme by the time we hopped on the\nbandwagon), the language is also well known for having a \"steep learning\ncurve\". While we were a team of seasoned engineers, none of us had any prior\nproduction experience with Rust. For an early stage startup, iterating fast is\nliterally a matter of life and death, and if the bottleneck to productivity\nturned out to be our chosen programming language, we would be regretting our\ndecision for a very long time indeed.\n\nLooking back from this one year mark, we're happy to report that not only did\nwe not die on the proverbial hill of learning Rust, the (much hyped) benefits\nof the language started materializing into tangible returns far sooner than we\nanticipated. While there is undeniably an upfront investment to learn Rust, by\nthis point for us it has easily paid for itself many, many times over. Rust\nmakes certain tasks easy that would have been exceedingly challenging in other\nlanguages. More importantly, our experience with Rust has turned out to be\nevery bit as fun as we thought it would, and perhaps even increasingly so as\nwe progress further on this journey:)\n\nThanks for reading SDF Blog! Subscribe for free to receive new posts and\nsupport my work.\n\nIn this two-part blog series, we'd like to add our own perspective to what is\nnow surely an overdone topic, in the hopes that this may prove helpful to\nother rustaceans.\n\nWe\u2019re going to focus on three main areas:\n\n  1. Language -- how we managed to quickly build MVPs and hit milestones while learning the intricacies of Rust itself\n\n  2. Tooling -- how we leverage and build on Rust's excellent tooling ecosystem to further enhance team velocity\n\n  3. And finally, miscellaneous tricks we learned along the way that helped keep our project scalable.\n\nWe will cover the first topic in this article, and the remainder in part two.\nLet\u2019s go. \ud83e\udd80\n\n###\n\nLanguage\n\n####\n\nFighting the Borrow Checker\n\nThe first major obstacle one must overcome on the road to productivity, and\nindeed the primary contributor to Rust's famed \"learning curve\", is the borrow\nchecker itself. This is a process that's been aptly termed \"fighting the\nborrow checker\", where the \"fighting\" involves copious amounts of angry\ncussing along with some amount of hair-pulling. Eventually, there would come a\npoint where the concept of compiler-enforced ownership sinks in and\nincorporated into one's mental model, then the whole thing \"clicks\" and one\ncomes to the realization that \"fighting the borrow checker\" was nothing more\nthan a symptom of Rust manifesting the very complexities of software\nengineering upfront at compile time. From then on, the borrow checker becomes\ntheir formidable ally, instead of a frustrating adversary seemingly trying to\nthwart you at every turn.\n\nPOV: Fighting the Borrow Checker\n\nBut such revelations only come much later, with time and experience, and in\nour early days of developing in Rust, under real pressure to build a working\nproduct, the borrow checker was very much an adversary we had to overcome.\nWhat worked for us in that situation was doing the simplest and most\nstraightforward thing -- `Clone`. Yes, whenever the borrow checker screeched\n\u201clifetime does not live long enough!!\u201d, we would just `clone()` and `clone()`\nuntil the compiler stopped complaining, and then `clone()`d some more. As a\nresult, our early codebase was riddled with `clone()`s. But hey, it worked! By\nadopting this simple yet effective strategy, we were able to keep the project\nmoving without agonizing over Rust's ownership semantics too much.\n\nYou may now be wondering if the performance of our program suffered because of\nthis. (It would be tempting here to default to the clich\u00e9d, and frequently\nmisapplied, adage that \"premature optimization is the root of all evil.\", but\nwe'll spare you from that common retort and tell the rest of our story :) ) As\nwe gained proficiency with the borrow checker, we did incrementally refactor\naway the many unnecessary clones by adopting improved ownership models in our\ndata structures and function signatures, perhaps hundreds of them.\n\nAnd guess how many of those instances actually resulted in tangible real world\nperformance gains? 0. Yes, zilch, nada -- all those clones added up to no more\nthan a minuscule overhead in real world workloads. This was true even for the\nmany instances where we were so sure that cloning was bad as to leave a `//\nTODO: refactor this clone` note in the code! Imagine our disappointment when\nwe meticulously cleaned up such TODOs, only to measure a net zero in\nperformance gains...\n\nWhen in doubt: clones are the solution\n\nSo there you have it -- if you ever find yourself \"fighting\" the Rust borrow\nchecker, keep calm, clone, and move on. And if you feel guilty about cloning\ntoo much -- don't, because in all likelihood, the clones are not the root\ncause of your performance problems.\n\n####\n\nChasing Performance\n\nThis brings us to everyone's favorite topic -- performance. After all, if you\nwere drawn here by the keyword \"Rust\", then it's probably a safe bet that\n\"building fast programs\" is one of the top things on your mind :)\n\nWe'll start off by saying that, making your program run fast is hard. Unlike\n\"fighting the borrow checker\", there's really no fast and easy rule to follow\nwhen it comes to building performant applications. And no, if your expectation\nis that \"if I build it in Rust then it will be fast\", you may be setting\nyourself up for a quick disappointment -- as far as our story goes, we've hit\nperformance bottlenecks on multiple production scenarios. One major\ncomplicating factor is that \"performance\" in practice is often not a simple,\nfixed objective criteria as many would like to believe, but rather a highly\nscenario dependent one -- how your program perform can vary drastically from\none type of workload to the next. And ultimately which type of workload\n\u201cmatters more\u201d, very much comes down to subjective opinions.\n\nWith that out of the way, we'll talk about types of optimizations that did\nyield significant results for us:\n\n  * Squashing asymptotic complexity\n\n  * Maximizing parallelism\n\n  * Shunning synchronization locks (to maximize the parallelism)\n\n  * Discovering the right memory allocator (unexpected, but massively impactful)\n\nEach of these optimizations alone at some point improved our real world\nperformance by 2x ~ 20x, depending on the type of workload and the number of\nCPU cores in the system -- the margin of difference between \"it\u2019s alright\" and\n\"holy sh** that\u2019s fast!!\".\n\n##### On Asymptotic Complexity - and why understanding your workload is\ncrucial \u26d1\ufe0f\n\nIn the last section, we described how sanitizing redundant cloning had little\nto no impact on real world performance. In asymptotic terms, all that's really\nsaying is simply that optimizing away some low constant factor probably isn't\nworth the effort.\n\nHowever, when it comes to super-linear run time, the small costs will\nabsolutely dominate on larger inputs. Here you are faced with just the cold,\nhard theory of computational complexity, and no amount of compiler magic is\ngoing to save you. In practice pinpointing such performance bottlenecks is\nprobably 80% of the work. Once the offending logic is found, the solution is\nusually self-evident -- either it's going to be \"very straightforward\", or\n\"theoretically impossible\". The latter case may sound like some ominous\ndoomsday scenario, but it rarely is. Recall that performance is actually a\nsubjective criteria, and all subjective criterion can be \"molded\". In\npractice, \"theoretically impossible\" just means relaxing the right constraint,\nor sacrificing less important workloads to make the important ones run faster.\nIn all likelihood, \"theoretically impossible\" might end up being even less\nactual work than \"very straightforward\"!\n\nBack to the story of SDF. At one point, we were choking on some benign looking\nqueries. After a quick investigation, we found that those queries were\noperating on a large number of tables, each with more than 10k columns. Since\nour column resolution logic was using a linear scan to find matching columns,\nthe overall compilation was quadratic in the size of input schemas. By using a\nlookup table for column resolution, we observed an average 3x speedup on such\nqueries. Note again the very scenario-dependent nature of performance here --\nthe speedup was observed only on queries over very large schemas, the lookup\ntable had no effect on (the far more common) queries over normal sized schemas\n(say, <100 columns). Had the large schemas never showed up in our workloads,\nthen there would be no need to optimize -- the simple linear scan algorithm\nwould have worked completely fine.\n\n##### Parallelization - and why compilers \u2764\ufe0f many core architectures\n\nWe built SDF in the beginning as a single threaded application. This kept\nthings simple and allowed us to focus on rapidly building up the basic\nfunctionalities. But we always knew that parallelization had to come at some\npoint, given the \"embarrassingly parallelizable\" nature of compilation.\nPerhaps we were even quietly dreading the exercise in the back of our minds --\nwe were of course aware of Rust's well-hyped reputation for \"safe and easy\"\nconcurrency, but remained somewhat skeptical given our past experiences with\nbuilding concurrent applications.\n\nThat point came roughly 5 months into the project -- when it became clear that\nwe would not be able to scale to handle a full production warehouse if we only\nutilize a single CPU core -- and that's when we truly appreciated the power of\nRust for the first time. As things turned out, the effectiveness of Rust in\nthis domain is one of those rare cases where the hype matched the reality.\nGone were the countless hours sunk into tracking down weird segmentation\nfaults, intermittent bugs from race conditions, and deadlocks. Instead, once\nwe hammered our code enough to make it pass the compiler, things mostly just\nworked! Though it should be noted that it did take a lot of hammering to get\nour parallelized code to pass the compiler -- SDF at that point was already\nfar from a toy prototype -- but all in all it was orders of magnitude less\ntime and stress than debugging concurrency bugs at runtime.\n\nAs for how we got it to pass the compiler? Mainly by putting things in\n`Arc`s/`RwLocks`, and, you guessed it, by copious amounts of `clone`s (see\nsection Fighting the Borrow Checker). And that's the beauty of Rust's\nconcurrency model -- there is no separate \"thread safety\" mechanism in Rust,\nit's all just the same underlying object ownership model. IOW, turns out\nmemory safety is also thread safety!\n\nParallelizing gave us an instant 10~20x speed boost on the end-to-end workload\n(on a consumer grade 16-core CPU), no other optimization came close.\n\n##### Synchronization locks - and why you should avoid them in Rust \ud83d\ude45\u2642\ufe0f\n\nAn immediately related topic to parallelization is that of locking. Locks are\nperhaps the go-to mechanism to safely introduce parallelism to an existing\ncode base, especially for people coming from an imperative-paradigm\nbackground. A neat thing about Rust is that the type system tells you which\nobjects need to be locked, via the `Send`/`Sync` traits, which makes the whole\nexercise a lot less stressful.\n\nBut there is a dark side to locking. For one, locks have relatively high\nconstant overhead on each invocation, often involving some mix of busy spins\nand calls into OS primitives. More importantly, locks absolutely wreck the\npotential parallel factor of your program. In a sense, locking is the \"anti-\nparallelization\" -- adding locks is like working to spoil your hard-earned\nfruits from the labor of parallelization. In the extreme case, locks can\nreduce your parallel factor to ~1, effectively reducing your parallel code to\nmerely an illusion (i.e. like Python programs). Furthermore, it's just really\nhard to reason about how much of a negative effect any given lock actually has\non your parallel factor, sometimes it might not matter much, but some\ninconspicuous lock can make all the difference.\n\nYour best bet, is therefore to avoid locks whenever possible. Here we found\nthat the functional way of thinking really helps -- instead of having\nresources to be shared among the workers, clone a \"privately-owned\" copy of\nall the required resources for each work \"unit\", then aggregate all the\nresults from workers at the end. The tradeoff is here is using some additional\ncloning (which as we mentioned, is usually very cheap) before starting the\nworkers, to avoid locking while the workers are running.\n\n##### The Memory Allocator - and a lesson on hidden \ud83e\udd77 performance\n\nMUSL is a powerful C library to allow static linking of a library. It allows\nyou to create an isolated, self contained, binary that works across almost any\nLinux distribution. All of our releases are built for MUSL. But, one day we\nwere informed by one of our customers that they were seeing massive\nperformance degradation of SDF between a 10-core M1 MacBook pro and their\n64-core X86 Linux server. The server was not much faster.\n\nTo the Apple fanboys: M1s are good, but they\u2019re not that good.\n\nIt turns out that MUSL incorporates its own global memory allocator, and, in\nheavily multithreaded situations this allocator is extremely not performant.\nThe benefits have lessened over time as we\u2019ve made more performance\nimprovements, but is still 2-3X depending on workload. To read more, check out\nAndy Groves more extensive writeup on the matter and how it impacted\nDatafusion\u2019s performance.\n\n    \n    \n    ## COMPILE TIME 16K QUERIES ## ## 7950X3D, 16 cores ## ## AVG 500 columns per query ## ## Date: 03/10/2024 ## +-----------+-------------+ | JEMALLOC | NO JEMALLOC | +-----------+-------------+ | 0m19.088s | 0m40.706s | +-----------+-------------+\n\nSwitching to a modern global allocator like Jemalloc not only solves the\nissue, but provides performance improvements on every SKU we offer! Add\nJemallocator to your cargo.toml and use it as your program\u2019s entry point as\nbelow.\n\n    \n    \n    /* jemalloc used to be the default Rust allocator til circa November 2018. Here we explicitly opt back into it to avoid the abysmal musl allocator */ #[cfg(not(target_env = \"msvc\"))] #[global_allocator] static ALLOC: jemallocator::Jemalloc = jemallocator::Jemalloc;\n\n####\n\nFearless Refactors\n\nAn often under appreciated benefit of using a strongly-typed language like\nRust in a fast-paced startup environment, is how much it helps with the task\nof refactoring. In the Parallelization section we briefly touched on this\npoint from the perspective of adding parallelism, but the point extends far\nbeyond that specific context. By now, aside from parallelization SDF has\nundergone perhaps three or four broad sweeping refactors touching almost every\npart of the codebase. Each one was spurred by some key objective we had to hit\nat the time, all were accomplished within matter of days and without causing\nnoticeable regressions on existing workflows.\n\nIt may seem counter-intuitive to claim that a language with stronger type\nrestrictions results in faster development cycles, but such has been our\nexperience with Rust. There are two related aspects here.\n\n  1. First is the fact that it's generally improbable that you can design a \"perfect\" system from the get-go. And even if you somehow can, you can't foresee the future -- software requirements change and evolve over time, what may be a \"perfect\" design for yesterday's use case may not be able to handle tomorrow's scenario. It is therefore better to embrace (large) refactors as a regular and necessary part of the software development cycle, rather than something that is \"abnormal\" and to be avoided.\n\n  2. The other perhaps more subtle aspect, is the mentality shift that comes as a result of low-stress refactors -- because we know that better abstractions can be easily added later, it frees us to be much more bold in experimenting with new features! In Rust we are actually far more comfortable doing the \"quick and dirty\" thing to rapidly implement some new feature, knowing that it can all be easily cleaned up into better abstractions later when the requirements become better understood.\n\nIn contrast, when working with dynamically typed languages one would generally\nbe more inclined to try and \"get it right\" the first time, in the hopes of\navoiding nightmare refactors later (or, much worse, \"deprecation\" and \"legacy\nsystems\"), which often leads to over thinking and inaction. Simply put, Rust\nenables us to adopt a \u201cGet things done now, add abstractions later\u201d approach\nto evolving our product and meeting our clients\u2019 needs.\n\n###\n\nClosing Thoughts\n\nWe\u2019ve gotten immense value out of the Rust ecosystem. It\u2019s not a perfect\nlanguage, and the community is still small. But for developing complex\nsystems, Rust and its ecosystem removes an extraordinary amount of mental\nburden allowing you to focus on what matters - delivering an exceptional\nproduct.\n\n> Our team built their careers on the development of compilers & programming\n> languages. We can confidently say that we have benefitted greatly from\n> buying into this ecosystem. Rust has allowed us to move faster, more safely,\n> and with greater confidence than we would have with any other language.\n\nNext time we will dive into how we leverage and build on Rust\u2019s excellent\ntooling ecosystem and miscellaneous tricks we learned along the way.\n\nInterested in contributing to this series? If you have any other tips, tricks,\nor resources to share, please do so in the comments below. Or, if you\u2019re\nthinking of adopting Rust for your project, we\u2019d be happy to talk, share our\nexperiences, and grow the ecosystem.\n\nThanks for reading SDF Blog! Subscribe for free to receive new posts and\nsupport my work.\n\n19 Likes\n\n\u00b7\n\n1 Restack\n\n19\n\nShare this post\n\n#### Fast Development In Rust, Part One\n\nblog.sdf.com\n\n2\n\nShare\n\n2 Comments\n\nCijo ThomasMar 15Liked by Lukas SchulteVery nice writeup! Loved everything\nabout it.My own experience with clone and its costs were different - cloning\ndid affect the performance significantly, and avoiding that boosted\nperformance significantly! So it really depends on the application\nitself.Expand full commentLike (1)ReplyShare  \n---  \n  \n1 reply\n\n1 more comment...\n\nIntroducing SDF - The Semantic Data Fabric\n\nModern SQL Development for Enterprise Scale Big Data\n\nJul 25, 2023 \u2022\n\nSDF Labs\n\n24\n\nShare this post\n\n#### Introducing SDF - The Semantic Data Fabric\n\nblog.sdf.com\n\n2\n\nFast Development In Rust, Part 2\n\nTools, Tricks, and Recommendations.\n\nMar 21 \u2022\n\nLukas Schulte\n\n,\n\nBo Lin\n\n, and\n\nAlexander Bogdanowicz\n\n12\n\nShare this post\n\n#### Fast Development In Rust, Part 2\n\nblog.sdf.com\n\nAutomating Data Classification for the 21st Century\n\nIntroduction Imagine a warehouse with hundreds, thousands, or millions of\ntables. How do you know what purpose each table has, or what each column...\n\nFeb 15 \u2022\n\nWolfram Schulte\n\n,\n\nMichael Levin\n\n, and\n\nSDF Labs\n\n12\n\nShare this post\n\n#### Automating Data Classification for the 21st Century\n\nblog.sdf.com\n\nReady for more?\n\n\u00a9 2024 SDF Team\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
