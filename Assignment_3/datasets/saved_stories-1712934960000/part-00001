{"aid": "40010751", "title": "Effective compression using frame-of-reference and delta coding", "url": "https://lemire.me/blog/2012/02/08/effective-compression-using-frame-of-reference-and-delta-coding/", "domain": "lemire.me", "votes": 1, "user": "jolj", "posted_at": "2024-04-12 09:15:24", "comments": 0, "source_title": "Effective compression using frame-of-reference and delta coding", "source_text": "Effective compression using frame-of-reference and delta coding \u2013 Daniel\nLemire's blog\n\nSkip to content\n\nDaniel Lemire's blog\n\nDaniel Lemire is a computer science professor at the Data Science Laboratory\nof the Universit\u00e9 du Qu\u00e9bec (T\u00c9LUQ) in Montreal. His research is focused on\nsoftware performance.\n\n## Support my work!\n\nI do not accept any advertisement. However, you can you can sponsor my open-\nsource work on GitHub.\n\nJoin over 12,500 email subscribers:\n\nYou can follow this blog on telegram. You can find me on twitter as @lemire or\non Mastodon.\n\n## Recent Posts\n\n## Recent Comments\n\n## Pages\n\n## Archives\n\n## Boring stuff\n\n# Effective compression using frame-of-reference and delta coding\n\nMost generic compression techniques are based on variations on run-length\nencoding (RLE) and Lempel-Ziv compression. Compared to these techniques and on\nthe right data set, frame-of-reference and delta coding can be faster for a\ncomparable compression rate.\n\nMathematically, frame-of-reference and delta coding use the same principle: we\napply an invertible transformation that maps a set of (relatively) large\nintegers to mostly smaller integers. (This is a common pattern when\ncompressing data).\n\nSuppose that you wish to compress a sequence of (non-negative) integers.\nConsider the following sequence:\n\n107,108,110,115,120,125,132,132,131,135.\n\nWe could store these 10 numbers as 8-bit integers using 80 bits in total. For\nexample, we have that 135 is 10000111 in binary notation.\n\nThe frame-of-reference approach begins by computing the range and minimum of\nthe array. We see that the numbers range from 107 and 135. Thus, instead of\ncoding the original sequence, we can subtract 107 from each value and code\nthis difference instead:\n\n0, 1, 3, 8, 13, 18, 25, 25, 24, 28.\n\nWe can code each offset value using no more than 5 bits. For example, 28 is\n11100 in binary notation. Of course, we still need to store the minimum value\n(107) using 8 bits, and we need at least 3 bits to record the fact that only 5\nbits per value are used. Nevertheless, the total (8+3+9*5=45) is much less\nthan the original 80 bits. In actual compression software, you would decompose\nthe data into blocks that are maybe larger than 10 values (say 16, 128 or 2048\nvalues). The overhead of storing the minimal value would be small. Moreover,\nthere are computational side benefits to this format: if we seek the value\n1000, we know it cannot be in the block if its minimum is 107 and we use only\n5 bits to store the offset from 107.\n\nFrame-of-reference works when the range of values in each block is relatively\nsmall. We can sometimes get better compression if the difference between the\nvalues is small. In this case, it is useful to look at the differences between\nsuccessive values (e.g., 108-107=1, 110-108=2, 115-110=5):\n\n1,2,5,5,5,7,0,-1,4.\n\nGiven this set of differences and the initial value (107), we can reconstruct\nthe original sequence. Delta coding is the compression strategy where we store\nthese differences instead of the original values. Some people like to think of\ndelta coding as a predictive scheme: you constantly predict that the next\nvalue will be like the previous one, and you just code the difference between\nyour prediction and the observed value.\n\nIn binary, the values 1,2,5,7 and 4 can be written as 001, 010, 101, 111, 100.\nIf we did not have a negative value (-1), we could store these differences\nusing only 3 bits per value. The negative value comes from the fact that our\nvalues are not entirely sorted (just locally so). However, as we shall see,\nthis single negative value will cause us some trouble. How do we code the -1?\n\n  * The original values are 8-bit values. This means that -1 and 256-1 are the same numbers (modulo 256). That is 25+255 modulo 256 is 24. In effect, we compute differences in an integer ring. The differences become 1,2,5,5,5,7,0,255,4. Computing the modulo with a power of two is fast because computers use the binary format natively.\n  * If you know the value that was predicted (25 in our case). You know that the range of differences goes from -25 to 230. Thus for differences x between -25 and 25, we store them as 2x if it is positive and as -2x-1 if it is negative. Otherwise, we store it as x+25. One problem with this approach is that it may require much branching: the processor has to constantly check conditions before proceeding further. There may be a substantial penalty to pay when using modern superscalar processors. Thankfully, you can use a trick called zig-zag encoding to avoid the branching. In effect, you map x to (x << 1) ^ (x >> 31) (in C or Java). This transformation can then be reversed by mapping the result y to ((y >>> 1) ^ ((y << 31) >> 31) where >>> is the unsigned right shift.\n  * We can replace subtractions by bitwise exclusive or (xor) operations. It bypasses the issue entirely because xoring integers never generates negative values. The successive xor values are 7,2,29,11,5,249,0,7,4. A benefit of the xor operation is that it is symmetric: x xor y is y xor x. This means that inverting the order of the original list, we would simply invert the order of the list of differences. Obviously, computing the xor is quite fast.\n\nOnce we have the list of differences as non-negative numbers, we can then try\nto store them by using as few bits as possible. Unfortunately, in our case, we\ncould to the conclusion that we need 8 bits to store the differences. We\nremarked however that for all but one value, 3 bits per difference would\nsuffice.\n\nSo a sensible solution is to code the first 3 bits of each differences: 001,\n010, 101, 101, 101, 111, 000, 111, 100. And then we add a pointer to the\nsecond last difference to indicate that we are missing 5 bits (11111). The\ncost of coding this exception is about 13 bits. So the total storage cost\nwould be (8+3+9*3+13=51). In this case, frame-of-reference is preferable to\ndelta coding, but both are preferable to the original 8-bit coding which used\n80 bits.\n\nThere are many possible variations. For example, you can also use exception\ntechnique with the frame-of-reference approach when almost all values fit in a\nrange of values, except for a few.\n\nFurther reading: the document SZIP Compression in HDF Products and the\ncorresponding CCSDS 120.0-G-2 data compression standard describe the\napplication of delta coding for scientific data. Michael Dipperstein\u2019s page\nprovides a nice overview of generic compression techniques. The specific\nexception technique I described is from the NewPFD scheme first described in:\n\n> H. Yan, S. Ding, T. Suel, Inverted index compression and query processing\n> with optimized document ordering, in: WWW \u201909, 2009.\n\nSee also my blog post How fast is bit packing?\n\n## Published by\n\n### Daniel Lemire\n\nA computer science professor at the University of Quebec (TELUQ). View all\nposts by Daniel Lemire\n\nPosted on February 8, 2012February 12, 2016Author Daniel LemireCategories\n\n## 12 thoughts on \u201cEffective compression using frame-of-reference and delta\ncoding\u201d\n\n  1. Itman says:\n\nFebruary 10, 2012 at 9:47 am\n\nI would also recommend a a link to\nhttp://www.springerlink.com/content/j66851228120170t/ Delta and Golomb codes\nare kinda slow to be used in a high-throughput search engine (though they\ndefinitely make sense in other apps). Byte and word-aligned codes work much\nfaster in this case.\n\n> Note by D. Lemire: The link above is to the following paper\n>\n> Anh, Vo Ngoc and Moffat, Alistair, Inverted Index Compression Using Word-\n> Aligned Binary Codes, Information Retrieval 8 (1), 151\u2013166, 2005.\n\nReply\n\n  2. Daniel Lemire says:\n\nFebruary 10, 2012 at 10:42 am\n\n@Itman\n\nGolomb-Rice is definitively a bit slow for high-throughput applications. I am\nnot sure that it is true for Yan et al.\u2019s NewPFD. According to Delbru et al.,\nit is just as fast, and even faster, than word-aligned codes (S9-64):\n\nR. Delbru, S. Campinas, K. Samp Giovanni Tummarello, Adaptive frame of\nreference for compressing inverted lists, DERI Technical Report 2010-12-16,\n2010.\n\nDelbru also posted a comparison on the Lucene mailing list.\n\nReply\n\n  3. Itman says:\n\nFebruary 10, 2012 at 10:50 am\n\nCool, I am certainly going to check it out. With respect to the Golomb and\nDelta codes, by slow I mean really slow. Even the not-so-efficient VBC is\ntwice is fast as Golomb.\n\nReply\n\n  4. Daniel Lemire says:\n\nFebruary 10, 2012 at 10:57 am\n\n@Itman\n\nI agree regarding Golomb-Rice coding, and I believe that\u2019s because it\nintroduces a lot of branching. But the Delbru reference hints that delta\ncoding can be quite fast when properly implemented. Moreover, szip which uses\ndelta coding can be quite a bit faster than gzip during compression.\n\nWhy do you think that delta coding is necessarily slow? For example, if I\nnotice that within blocks successive xors are small integers, and I pack them,\nthis ought to be very fast (no branching, and only very fast operations). In\nfact, I am pretty sure I can implement it using no more than 1 CPU cycle per\nvalue on average with pipelining, and maybe less. Of course, it may compress\npoorly, but that\u2019s another story.\n\nReply\n\n  5. Itman says:\n\nFebruary 10, 2012 at 11:11 am\n\nFrom my experience: just re-ran a test on my laptop. Well, of course, I cannot\nguarantee that this holds true for all implementations and platforms. New\narchitectures have amazing surprises like 1) Unfolding loops does not help 2)\nAligned reading is as good as unaligned Anyways, for my implementations the\ndifferences between VBC and Delta is not even two-fold, it is 10+-fold.\nPerhaps, I should rexamine my code some day.\n\nReply\n\n  6. Daniel Lemire says:\n\nFebruary 10, 2012 at 11:14 am\n\n@Itman\n\nWould you share your code with me? I\u2019d like to examine it. (I don\u2019t need a\nlicense to it, I just would like to see exactly what you are comparing.)\n\nReply\n\n  7. Itman says:\n\nFebruary 10, 2012 at 11:21 am\n\nSure, no problem. I will it do it next week, because I want to do a quick\ncheck myself.\n\nReply\n\n  8. Oscar says:\n\nNovember 4, 2015 at 9:48 pm\n\nJust ran into this page after googling \u201cdelta compression xor\u201d. It seems to me\nthe hoops to jump through in the unsorted case for delta encoding (negative\ndeltas) go away if you encode your deltas using base -2 . Then, all you need\nis for the absolute value of the differences to be small. There are then\ntricks to compute using the -2 encoded numbers directly.\n\nReply\n\n    1. Daniel Lemire says:\n\nNovember 4, 2015 at 11:36 pm\n\nCan you elaborate?\n\nReply\n\n      1. Oscar says:\n\nNovember 5, 2015 at 12:34 am\n\nWhat I mean is, related to the problem with coding up the -1, You offered\nthree solutions: xor rather than delta which is good except in cases where the\nhamming distance is large (even though the delta is small), a second solution\nthat I didn\u2019t fully follow and one where \u201cwe add a pointer to the second last\ndifference to indicate that we are missing 5 bits (11111).\u201d.\n\nAnother approach is the following: Using base -2 to encode the deltas.\nHacker\u2019s delight explains base -2 better than I can. But roughly: a number\nb_k, ... , b1 b0 corresponds to b0 \u2013 2 (b1) + 4 (b2) \u2013 8 (b3) .... + (-2)^k\n(b_k). In practice, this means 0 maps to \u20180\u2019, 1 maps to \u20181\u2019, -1 maps to \u201911\u2019,\n2 maps to \u2018110\u2019, -2 maps to \u201910\u2019, 3 maps to \u2018111\u2019. The property I care about\nin this case is that numbers with small absolute values will have small\nnumbers non-zero bits. A positive number may be penalized in this case since\nit requires slightly more bits.\n\nI haven\u2019t done a full analysis of when this will be a win over the other\nmethods (how many bits does it actually take for your sequence, how much does\nencoding/ decoding cost), but there are tricks to operate add the numbers\nwithout fully decoding them. It was just exciting because it looks like a\npossible application of this encoding\n\nReply\n\n        1. Daniel Lemire says:\n\nNovember 5, 2015 at 12:48 pm\n\nI have updated the blog post with zig-zag encoding. I think it is closely\nrelated to your concept of base -2.\n\nReply\n\n  9. saddam says:\n\nNovember 21, 2017 at 11:37 pm\n\nhow it(delta compression) works on sensor received data, which is a 16 bit\ncontinuous data\u2019s . The standard delta encoding doesnt work, because i have\nrandom values. ?\n\nReply\n\n### Leave a Reply Cancel reply\n\nYou may subscribe to this blog by email.\n\nTerms of use Proudly powered by WordPress\n\n", "frontpage": false}
