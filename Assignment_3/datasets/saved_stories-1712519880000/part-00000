{"aid": "39960795", "title": "Evaluating the Language Consistency of Local and Cloud-Based LLMs", "url": "https://nikolas.blog/evaluating-the-language-consistency-of-local-and-cloud-based-llms/", "domain": "nikolas.blog", "votes": 1, "user": "ngar", "posted_at": "2024-04-07 13:56:40", "comments": 0, "source_title": "Evaluating the Language Consistency of Local and Cloud-Based LLMs - Nikolas' Blog", "source_text": "Evaluating the Language Consistency of Local and Cloud-Based LLMs - Nikolas'\nBlog\n\nDirekt zum Inhalt wechseln\n\nNikolas' Blog\n\n# Evaluating the Language Consistency of Local and Cloud-Based LLMs\n\nFoto von Google DeepMind auf Unsplash\n\nOne year ago, in my first article about OpenAI Whisper I made a prediction: I\nstill think that one day we will have LLMs in the capability range between\nGPT-3 and GPT-3.5 (maybe up to GPT-4) \u2013 on our mobile devices. Now, let\u2019s see\nhow good the sentence has aged and how far we have come.\n\nBefore getting too excited, it should be made clear that no smartphone\ncurrently has a GPT-3.5 local model, which would still be quite remarkable.\nHowever, significant progress has been made towards this goal. The models have\nimproved significantly, with Llama 2 being a major breakthrough in the summer\nof 2023. Mistral and Mixtral have further solidified the foundation for open-\nsource LLMs. Additionally, the continuous optimization of this technology has\nled to the development of Ollama. Ollama is currently the easiest way to try\nout LLMs on your own computer, providing access to a variety of LLMs that can\nrun on a moderate machine. That\u2019s leaves us with the questions which LLM is\nsuited best for our needs.\n\n## Let\u2019s build the Benchmark!\n\nOne needs to know the performance differences between changes when building an\nRAG application or anything beyond a simple Prompt interface. We have metrics\nlike speed, memory consumption and many more in the classical software\nengineering. But when working with LLMs you have a new strange one: answer\nquality.\n\nMy particular problem was the answer language consistency. As a tech-savvy\nperson, I use English on a daily basis for my work. Less tech-savvy persons\nare often limited to their native language. A big advantage of ChatGPT at the\ntime was that it already had a very good multilingual capability. This lowers\nthe barrier to entry if you don\u2019t have to switch languages. So, if I want to\nrecommend the local LLMs in the private and business sector, I have to ensure\nthat the LLM can answer in the local language.\n\nAt is core, it is a pretty interesting task. Most modern LLMs are trained on a\nlarge, unsupervised corpus of English text. Many modern smaller open source\nLLMs are trained using English at is core language. Multilingual capabilities\nstill somehow develop.\n\n#### Building the Benchmark\n\nFifty general questions were prepared for the various LLMs to answer. All\nquestions are in German, but it is not explicitly stated that the answers must\nalso be in German. The model itself should simply follow the source language.\nDue to the large number of answers and tested models (20 in total), I opted\nnot to conduct the evaluation myself and instead used a little trick. There\nare models that are very good at detecting the language of a sentence. In my\ncase, I am using xlm-roberta-base-language-detection from HuggingFace.\n\n\u00dcbersetzen Sie den Satz ins Deutsche: \"I'm going on holiday next week.\"  \n---  \nKorrigieren Sie den fehlerhaften Satz: \"Ich haben ein Apfel gegessen.\"  \nFormulieren Sie den folgenden Satz im Futur I: \"Er liest ein Buch.\"  \nWas bedeuten die folgenden W\u00f6rter: \"Zirbelnuss\", \"unisono\"?  \nGeben Sie ein Synonym f\u00fcr das Wort \"klein\".  \nZeigen Sie anhand des folgenden Textes, ob Sie den Textzusammenhang verstanden\nhaben: \"Thomas ist flei\u00dfiger als sein Bruder. Er wurde zum Schulsprecher\ngew\u00e4hlt. Wer k\u00f6nnte der Flei\u00dfigere der beiden sein?\"  \nK\u00f6nnen Sie die implizite Information in der folgenden Aussage identifizieren:\n\"Lisa hat bekommen, wonach sie gesucht hat. Sie freut sich so sehr \u00fcber die\nHochschulzulassung. Was hat Lisa bekommen?\"  \nWas ist die Essenz aus dem folgenden Text: \"Anna ist eine K\u00fcnstlerin. Sie malt\nBilder und fertigt Skulpturen an. Was macht Anna beruflich?\"  \nWer war Bundeskanzlerin Deutschlands vor Angela Merkel?  \nWelcher Fluss flie\u00dft durch Berlin?  \nWer komponierte die 9. Symphonie?  \nWas bedeuten diese umgangssprachlichen Ausdr\u00fccke: 'Mit jemandem ein H\u00fchnchen\nrupfen', 'Das ist mir schnuppe'?  \nWas bedeuten diese umgangssprachlichen Ausdr\u00fccke: 'Bock haben', 'Einen Zahn\nzulegen'?  \nWas ist ein SWOT-Analyse?  \nWas versteht man unter KPIs im Unternehmenskontext?  \nBeschreiben Sie den Begriff \"Marketing-Mix\".  \nWas bedeutet der Begriff \"Cash Flow\" im Unternehmenskontext?  \nWas k\u00f6nnte jemand meinen, wenn er sagt: \"Mir ist ein Stein vom Herzen\ngefallen.\"  \nWas ist die Bedeutung von \u201eGem\u00fctlichkeit\u201d in der deutschen Kultur? Kann diese\nad\u00e4quat ins Englische \u00fcbersetzt werden?  \nWas feiert man beim Oktoberfest?  \nWie viele Planeten gibt es in unserem Sonnensystem?  \nWas ist der menschliche K\u00f6rper in der Lage, rote Blutk\u00f6rperchen zu\nproduzieren?  \nK\u00f6nnen Sie das Prinzip der k\u00fcnstlichen Intelligenz erkl\u00e4ren?  \nWas ist der Hauptunterschied zwischen Prokaryoten und Eukaryoten?  \nIn welcher Zeitperiode lebte Albert Einstein?  \nWas ist der Hauptunterschied zwischen klassischer und Quantenphysik?  \nWas sind die grundlegenden Prinzipien der Demokratie?  \nWie kam es zur Entstehung des Internets?  \nWie funktioniert ein Elektronenmikroskop?  \nWas sind die grundlegenden Prinzipien der globalen Erw\u00e4rmung?  \nWas sind die Key Features von Python als Programmiersprache?  \nWie erzeugt eine Solarzelle Strom?  \nWas war der Hauptgrund f\u00fcr den Ausbruch des Ersten Weltkrieges?  \nWas ist der Zweck des menschlichen Immunsystems?  \nK\u00f6nnen Sie die Funktionsweise der menschlichen DNA erkl\u00e4ren?  \nWas sind die Hauptmerkmale des Barockstils in der Kunst?  \nWie funktioniert ein Verbrennungsmotor?  \nWas sind die wichtigsten wirtschaftlichen Aspekte der Globalisierung?  \nWie beeinflusst die Schwerkraft das Leben auf der Erde?  \nWie wurde das Periodensystem entwickelt und wie wird es in der modernen\nWissenschaft genutzt?  \nWas sind die neun Nachbarl\u00e4nder Deutschlands?  \nWelche zwei St\u00e4dte in Deutschland haben die gr\u00f6\u00dfte Bev\u00f6lkerung?  \nWelcher ist der l\u00e4ngste Fluss, der durch Europa flie\u00dft?  \nWelche Gebirgsregion befindet sich im S\u00fcden Deutschlands?  \nK\u00f6nnen Sie die drei gr\u00f6\u00dften Inseln Deutschlands nennen?  \nSchreiben Sie einen kurzen Dialog zwischen zwei Personen. Sie sollten dabei\ngrundlegende Konversationsthemen wie Begr\u00fc\u00dfungen, Hobbys und Wetter\neinbeziehen.  \nEntwerfen Sie ein Verkaufsgespr\u00e4ch f\u00fcr ein Produkt oder eine Dienstleistung\nIhrer Wahl. Nutzen Sie dabei \u00fcberzeugende, sprachliche Techniken und\nFachausdr\u00fccke.  \nVerfassen Sie einen imagin\u00e4ren Blogpost \u00fcber eine Reise zu einem europ\u00e4ischen\nReiseziel. Beschreiben Sie dabei sowohl die kulturelle Erfahrung als auch die\nphysische Landschaft und Umgebung.  \nSchreiben Sie eine virtuelle F\u00fchrung durch ein bekanntes europ\u00e4ischen Museum,\nin der Sie drei Hauptexponate ausf\u00fchrlich beschreiben.  \nStellen Sie sich vor, Sie halten eine Rede auf einer Konferenz zum Thema\n\"Nachhaltigkeit in der Industrie\". Erstellen Sie eine Gliederung f\u00fcr die Rede,\nmit drei Hauptpunkten und entsprechenden Unterthemen.  \n  \nview raw questions.txt hosted with by GitHub\n\nHowever, this is where the escalation spiral began. At first, I wanted to test\nonly models with up to about 3 billion parameters. But what about a comparison\nwith models with 7 billion parameters? What about closed source models like\nGPT-3.5 and others? And while we\u2019re at it, how can you test the language\nwithout checking the content? It is possible that an LLM speaks German but\ntalks nonsense. Something that one would technically predict under the given\ncircumstance. So the answers had to be checked.\n\nHere comes the next trick into play. Most of the LLMs I wanted to test still\nhad weaknesses. This is known from the English benchmarks. So we can just use\na smarter LLM for evaluating the content. Initially, my first approach was to\nuse only GPT-4 as the evaluation model. But after including GPT-3.5 and GPT-4\nin the suite of tested models, several evaluation models were required to\navoid self-evaluation by the teacher. The advantage here is that there are now\nseveral models that are like GPT-4, namely Claude 3 Sonnet and Mistral Large.\n\nIn the end, each of the 20 LLMs were evaluated by 3 LLMs.\n\n##### Model selection\n\nOpen Source| tinyllama (1B), stablelm2 (2B), phi-2 (3B), gemma-2b (3B), llama2\n(7B), mistral (7B), gemma (9B), open-mixtral-8x7b (47B)  \n---|---  \nFine Tuned| orca-mini (LLama, 3B), mistral-openorca (Mistral, 7B), nous-\nhermes2 (Llama 2, 11B), dolphin-mistral (Mistral, 7B), openhermes (Mistral,\n7B), neural-chat (Mistral, 7B)  \nClosed Source| claude-3-haiku, claude-3-sonnet, gpt-3.5-turbo-0125,\ngpt-4-0125-preview, mistral-large-latest, mistral-small-latest  \nParameter count from Ollama\n\nAll open-source models (except Mixtral, which was unfortunately too large)\nwere tested using Ollama on an RTX 2060 with 6 GB of VRAM, while the closed-\nsource models were used with their respective providers (except Claude, which\nwas used via AWS). The responses were generated and evaluated using GPT-4\nTurbo, Claude 3 Sonnet, and Mistral Large in their newest editions. The time\ntaken to answer all 50 questions was also recorded.\n\n#### Checking the quality of responses\n\nFor each answer, a evaluation JSON object was created. Points were awarded on\na scale from \u201eexcellent\u201c to \u201einsufficient\u201c for\n\n  * accuracy (Is the answer factually correct and relevant to the question?),\n  * linguistic quality (How good is the grammar, syntax, and spelling in the answer?),\n  * contextual sensitivity (Does the model demonstrate a good understanding of the question\u2019s context? Is it able to switch between different contexts and respond appropriately?)\n  * comprehensiveness (Does the answer cover all aspects of the question, provide relevant information, and is it thorough without being digressive or irrelevant?)\n\nThe results were converted into numerical grades from 1 (best) to 6 (worst).\nIn a second step, each generated sentence was checked for its language,\nresulting in a ratio of how many sentences were in German, English, or another\nlanguage.\n\nTechnically, this step was not 100% clean, as I used simple sentence splitting\nas the separation system. Enumerations or other stylistic elements were\nunfortunately sometimes classified as separate sentences during the process,\nleading to noise in the results. Many different languages occurred only once\neach. Therefore, even with a perfect answer in the target language, 100%\naccuracy cannot be achieved. These results should not be taken too literally.\nNow, let\u2019s move on to the results.\n\n### Language\n\nPhi and Llama 2 models have a preference for responding in English, while\nMistral has a slight bias towards English with a 60% preference. It is\ninteresting to note that none of these models were explicitly trained on\nGerman. Similarly, Gemma was not trained on German, but its larger embedding\nspace and more tokens may explain its good performance. Stablelm 2, on the\nother hand, was explicitly trained on multiple languages, including German,\nwhich explains its excellent results. It is possible that Mixtral\u2019s size\nallows it to perform well in German.\n\nThe fascinating thing is that the number of parameters has nothing to do with\nthe language. Whether the response is in the same language depends on the\ndata.\n\nThis is further evident when examining fine tuning. The average performance of\nall models has significantly improved, except for the Orca Mini, which is\nstill based on the Llama 1 in the 3B variant.\n\nThe situation is similar with the closed source models. Everywhere you get an\nanswer in German.\n\nWhen examining models such as StableLM 2, it is evident that data plays a\ncrucial role in their success. It is important to note that this chart should\nnot be viewed as a definitive ranking due to the presence of noise.\n\n### Content\n\nWhat about the content of the answers? Let\u2019s start again with the open-source\nmodels:\n\n  * The more parameters, the better the answers. The smallest models perform the worst. This is to be expected as there is a factor of 3 between 7B and 2B. Nevertheless, the difference between the worst model 7B and the best model 2B is not very large.\n  * As the answer gets better, the quality of the language improves in the same ratio. There is no dumb answer that is linguistically perfect. The order between the different categories is almost identical.\n  * The data also plays an important role. Although Llama 2, Mistral, and Gemma are similar in size, they have very different results.\n  * The jump to 47B is also not very large in this case. Mistral 7B achieves an average score of 2 when it comes to answer accuracy.\n\nHow let\u2019s look at the fine-tuned models\n\nSome observations:\n\n  * Fine-tuning of models usually only changes performance marginally.\n  * This means that, for example, sentence structure can be improved, but not the content.\n\nA direct comparison with the base model 7B shows that fine-tuning is not\nnecessarily required when answering general questions. The performance of the\nbase model has not been significantly improved. However, considering the\nlanguage consistency evaluation, it may aid in enhancing the consistency of\nthe response. In particular, Llama 2 has received a boost in speech evaluation\nthrough fine tuning, but less in the quality of the answer.\n\n##### Close Source LLMs\n\nThe closed-source models are not much different when it comes to their\nperformance results. The response quality is usually very high.\n\nAll in all, the following picture emerges:\n\nHere, the results are grouped according to the valuation model used:\n\n### A brief look at time\n\nIt is also interesting to take a look at the time. This was measured for all\n50 answers. This means that it can be seen as a bit of an average. At the same\ntime, individual anomalies, such as a very slow generation of a response, can\ndistort the overall result a little. Also because local hardware was used, a\ndistinction must be made between the Ollama and Cloud models. Let\u2019s start with\nthe cloud:\n\nCan we take a moment to talk about the fact that GPT-4 is 2.5 times slower\nthan the competition for some reason? This is something I can very strongly\nconfirm from my own experience. For this experiment, each answer had to be\nchecked by 3 models. Claude and Mistral had finished similarly quickly, GPT-4\ntook ages. It\u2019s something that should definitely be a priority for the next\ngeneration of models. But otherwise you can recognize a few groups. Mistral\nsmall, Claude 3 Haiku and GPT-3.5 are similarly fast. Claude 3 Sonnet and\nMistral Large are fast, too. Especially for Claude 3 Haiku I find it a very\nimpressive performance, as in some benchmarks it approaches GPT-4 in answer\nquality.\n\n#### Local LLMs\n\nLet\u2019s move on to the open LLMs. Nous Hermes 2 was identified as an outlier due\nto its size exceeding the capacity of the GPU (RTX 2060, 6 GB VRAM). As a\nresult, the CPU had to be utilized, which significantly impacted performance.\nOnce again, this proves the great importance of GPUs.\n\nThe time also increases with the number of parameters, but optimization also\nplays an important role. There is no other way to explain why Mistral and the\nassociated models perform so well. Llama 2 and Gemma still need a lot of\noptimization in this area. There are similarly wild results for the small\nmodels with few parameters. Gemma-2b is the largest in this class, but is one\nof the fastest. On the other hand, it must also be said that the data only\ncomes from one run. Perhaps the inference times of the models at Ollama are\nvery different, which can lead to these differences.\n\n## Conclusion\n\nSo, what can we take away from all these results? One of the most fascinating\nfindings was that the language used does not necessarily have to be related to\nthe content. Even if the answers were in English, they were still correct in\nterms of content, despite the question being in German. There is already some\nresearch in this area, as shown in this paper. In summary, the LLM operates\nwithin a conceptual space (that tend to be structured around the English\nlanguage). The final answer is only converted into the target language at the\nend. Although there may be instances where this conversion does not occur, the\ncontent remains accurate in the concept space and is typically translated into\nEnglish. That also explains why fine-tuning can help in this case. It doesn\u2019t\nrequire major changes to trigger this translation process.\n\nMultilingualism is already covered in most of the usual benchmarks today. MMLU\nand other benchmarks are examples of this. However, I wanted to have a closer\nlook at the exact numbers myself. This topic is not unimportant. When working\nwith SMEs in a more rural area, it\u2019s important to keep in mind that they are\nfar from the big cities and their internationality. In such areas, you can\u2019t\nassume an understanding of the English language, especially when it comes to a\nnew and unfamiliar technology. The answer has to come as expected, and that\nunfortunately means it has to be in the same language.\n\n### The perfect moment\n\nThis is why I appreciate the release of OpenAI ChatGPT. It met the necessary\ncriterion at the time of its release. For a long time, GPT-3.5 and GPT-4 were\nsome of the only valid options when multilingualism was a topic. That changed\nsignificantly in the last year, along with the higher quality of the answers.\n\nAn LLM must also be able to handle specific knowledge for such a use case. The\nquestions posed to the LLMs could have been written by a German teacher with a\nfondness for quiz shows, which is not unexpected. The sweet spot for this was\nGPT-3.5, as confirmed by the data above, where we almost have a perfect result\nfrom this model level.\n\nI am aware that some people have already dismissed this level of performance.\nWe discuss more about which LLM can write better code or reason. But\nespecially for the smaller models, this is still a big challenge. There was a\nfocus on the \u201esmall\u201c models in this experiment because an SME certainly does\nnot have the money for an H100. Therefore, inference time and resource\nconsumption were two metrics that I did not want to ignore.\n\n### Old and new problems\n\nIt was also a lesson to run all these benchmarks and collect the data. On\npaper, I had known for a long time that one could use GPT-4-like LLMs as\nevaluation modules, but I didn\u2019t know how laborious it would be. In the end,\nit was easier than expected, but not without its own problems. The consistency\nof the results from Claude, Sonnet, GPT-4, and Mistral Large is still lacking.\nFor the evaluation, except for Claude, the JSON mode was activated and a fixed\nJSON structure was provided. But as it always is with prompting, it is\nsometimes only taken as an ignorable recommendation by the model and breaks\nthe data pipeline.\n\nSo, it still means that sometimes the data has to be manually formatted. We\ndidn\u2019t have many requests here but there were outliers among less than 100\nresults. And here we see the discrepancy between research and product\ndevelopment. The more complex the task becomes, the more chaotic the quality.\nSome people forget this when it comes to LLMs.\n\n## Opinion\n\nBut even if you have perfected the prompts, it doesn\u2019t mean you are safe\nforever. This primarily concerns the closed-source models and is inherent in\nthe nature of the services. What is the true offering of LLM operators such as\nOpenAI, Claude, and Google? In practice, users submit a request, similar to\nsearch engines, and receive the most suitable answer. The quality is variable,\nand usually, some adjustments are given to the user in the form of different\nmodels to provide personalization options. But if one day OpenAI were to\ndowngrade their GPT-4 option to the level of a BERT Transformer, users can do\nvery little about it.\n\nOf course, then no one would give OpenAI any more money, and big companies\nlike Microsoft would exert a strong influence on OpenAI to reverse that\ndecision. This is an extreme example, but it would be theoretically possible.\nThe quality of the API is not necessarily guaranteed. So, if GPT-4 becomes\nworse, reads prompts differently, or proactively rejects requests, there is\nvery little one can do to resist.\n\nEach prompt wrapper is dependent on LLM providers success or failure.\nPrecisely for this reason, as shown in this report, one of the most popular\nreasons for using open-source LLMs is control and adaptability. There is a\ngood reason why several providers are being considered and used for enterprise\nproducts. You don\u2019t want to be dependent on only one supplier. This is where I\nsee the strength of open source.\n\n### New models\n\nSo, it was all the more encouraging to hear the news earlier this year when\nother providers like Google, Mistral, and Claude introduced models similar in\nstrength to OpenAI\u2019s. At present, there is a choice and the monopoly on\nquality by GPT-4 has been somewhat disrupted. Of course, none of these models\nof this class are open source because it is still too lucrative to offer this\ntechnology freely.\n\nThere is also the possibility of a GPT-5 in the near future that is expected\nto outshine everything. But one should not count their chickens before they\nhatch. OpenAI has a significant concentration of know-how, computing power,\nmarket power, and partly financial resources that enable them to deliver great\nproducts. But this call for GPT-5 reminds me a little of the same call for\nGoogle when ChatGPT was first introduced. Even with the best conditions,\nmarket leadership in LLM products is not guaranteed unless it is put into\napplication. As with Google, the public pressure on OpenAI is undeniable high.\nThey are no longer just a good research group from Silicon Valley, but one of\nthe most significant companies currently, even if OpenAI still has its own\nstruggles with it.\n\n# Closing words\n\nSo, what is the state of the models and my prognosis from the beginning? Well,\none year later, the commercial market has caught up to the level of GPT-4. At\nthe same time, open models are gradually reaching the level of GPT-3.5, but\nnot on mobile devices. But honestly, it\u2019s just a matter of time, optimization,\nand hardware. The implementation of this technology is expected to be the\nfocus of the upcoming year. It is important to note that despite its\nunconventional nature, many individuals outside of technology space have yet\nto experience significant advancements over the standard ChatGPT. This is also\napplicable to individuals who have not yet tried LLMs.\n\nMany people simply do not use GPT-4 because it is only available as a\nsubscription. In enthusiast communities, there are debates about which model\nis truly better in certain edge cases, but the outside world is not aware of\nthe problems behind it. There was a small chance for a collapsing AI market if\nOpenAI had completely lost its way in November of last year. Much of the gains\nfrom last year only exists on paper and have not yet arrived in reality. Their\ntrial is still pending. The story is just beginning.\n\nBeitrag ver\u00f6ffentlicht\n\n7\\. April 2024\n\nin\n\nK\u00fcnstliche Intelligenz, Technik\n\nvon\n\nNikolas Garske\n\nSchlagw\u00f6rter:\n\nAI, LLM, Mistral, Ollama, OpenAI\n\n## Kommentare\n\n### Schreibe einen Kommentar Antworten abbrechen\n\nNikolas' Blog\n\nImpressum\n\nDatenschutz\n\nOpt-Out\n\nStolz pr\u00e4sentiert von WordPress\n\n", "frontpage": false}
