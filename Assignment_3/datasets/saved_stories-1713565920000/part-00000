{"aid": "40088826", "title": "Microsoft's VASA-1 can deepfake a person with one photo and one audio track", "url": "https://arstechnica.com/information-technology/2024/04/microsofts-vasa-1-can-deepfake-a-person-with-one-photo-and-one-audio-track/", "domain": "arstechnica.com", "votes": 26, "user": "Brajeshwar", "posted_at": "2024-04-19 16:31:22", "comments": 17, "source_title": "Microsoft\u2019s VASA-1 can deepfake a person with one photo and one audio track", "source_text": "Microsoft\u2019s VASA-1 can deepfake a person with one photo and one audio track | Ars Technica\n\nSkip to main content\n\nSubscribe\n\nClose\n\n### Navigate\n\n  * Store\n  * Subscribe\n  * Videos\n  * Features\n  * Reviews\n\n  * RSS Feeds\n  * Mobile Site\n\n  * About Ars\n  * Staff Directory\n  * Contact Us\n\n  * Advertise with Ars\n  * Reprints\n\n### Filter by topic\n\n  * Biz & IT\n  * Tech\n  * Science\n  * Policy\n  * Cars\n  * Gaming & Culture\n  * Store\n  * Forums\n\n### Settings\n\nFront page layout\n\nGrid\n\nList\n\nSite theme\n\nlight\n\ndark\n\nSign in\n\n#### pics and it didn't happen \u2014\n\n# Microsoft\u2019s VASA-1 can deepfake a person with one photo and one audio track\n\n## YouTube videos of 6K celebrities helped train AI model to animate photos in\nreal time.\n\nBenj Edwards - 4/19/2024, 1:07 PM\n\nEnlarge / A sample image from Microsoft for \"VASA-1: Lifelike Audio-Driven\nTalking Faces Generated in Real Time.\"\n\nMicrosoft\n\n#### reader comments\n\n126\n\nOn Tuesday, Microsoft Research Asia unveiled VASA-1, an AI model that can\ncreate a synchronized animated video of a person talking or singing from a\nsingle photo and an existing audio track. In the future, it could power\nvirtual avatars that render locally and don't require video feeds\u2014or allow\nanyone with similar tools to take a photo of a person found online and make\nthem appear to say whatever they want.\n\n### Further Reading\n\nAI image generation tech can now create life-wrecking deepfakes with ease\n\n\"It paves the way for real-time engagements with lifelike avatars that emulate\nhuman conversational behaviors,\" reads the abstract of the accompanying\nresearch paper titled, \"VASA-1: Lifelike Audio-Driven Talking Faces Generated\nin Real Time.\" It's the work of Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong\nYang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo.\n\nThe VASA framework (short for \"Visual Affective Skills Animator\") uses machine\nlearning to analyze a static image along with a speech audio clip. It is then\nable to generate a realistic video with precise facial expressions, head\nmovements, and lip-syncing to the audio. It does not clone or simulate voices\n(like other Microsoft research) but relies on an existing audio input that\ncould be specially recorded or spoken for a particular purpose.\n\nMicrosoft claims the model significantly outperforms previous speech animation\nmethods in terms of realism, expressiveness, and efficiency. To our eyes, it\ndoes seem like an improvement over single-image animating models that have\ncome before.\n\nAdvertisement\n\nAI research efforts to animate a single photo of a person or character extend\nback at least a few years, but more recently, researchers have been working on\nautomatically synchronizing a generated video to an audio track. In February,\nan AI model called EMO: Emote Portrait Alive from Alibaba's Institute for\nIntelligent Computing research group made waves with a similar approach to\nVASA-1 that can automatically sync an animated photo to a provided audio track\n(they call it \"Audio2Video\").\n\n## Trained on YouTube clips\n\nMicrosoft Researchers trained VASA-1 on the VoxCeleb2 dataset created in 2018\nby three researchers from the University of Oxford. That dataset contains\n\"over 1 million utterances for 6,112 celebrities,\" according to the VoxCeleb2\nwebsite, extracted from videos uploaded to YouTube. VASA-1 can reportedly\ngenerate videos of 512x512 pixel resolution at up to 40 frames per second with\nminimal latency, which means it could potentially be used for realtime\napplications like video conferencing.\n\nTo show off the model, Microsoft created a VASA-1 research page featuring many\nsample videos of the tool in action, including people singing and speaking in\nsync with pre-recorded audio tracks. They show how the model can be controlled\nto express different moods or change its eye gaze. The examples also include\nsome more fanciful generations, such as Mona Lisa rapping to an audio track of\nAnne Hathaway performing a \"Paparazzi\" song on Conan O'Brien.\n\nThe researchers say that, for privacy reasons, each example photo on their\npage was AI-generated by StyleGAN2 or DALL-E 3 (aside from the Mona Lisa). But\nit's obvious that the technique could equally apply to photos of real people\nas well, although it's likely that it will work better if a person appears\nsimilar to a celebrity present in the training dataset. Still, the researchers\nsay that deepfaking real humans is not their intention.\n\nAdvertisement\n\n\"We are exploring visual affective skill generation for virtual, interactive\ncharactors [sic], NOT impersonating any person in the real world. This is only\na research demonstration and there's no product or API release plan,\" reads\nthe site.\n\nWhile the Microsoft researchers tout potential positive applications like\nenhancing educational equity, improving accessibility, and providing\ntherapeutic companionship, the technology could also easily be misused. For\nexample, it could allow people to fake video chats, make real people appear to\nsay things they never actually said (especially when paired with a cloned\nvoice track), or allow harassment from a single social media photo.\n\nRight now, the generated video still looks imperfect in some ways, but it\ncould be fairly convincing for some people if they did not know to expect an\nAI-generated animation. The researchers say they are aware of this, which is\nwhy they are not openly releasing the code that powers the model.\n\n### Further Reading\n\nDeepfake scammer walks off with $25 million in first-of-its-kind AI heist\n\n\"We are opposed to any behavior to create misleading or harmful contents of\nreal persons, and are interested in applying our technique for advancing\nforgery detection,\" write the researchers. \"Currently, the videos generated by\nthis method still contain identifiable artifacts, and the numerical analysis\nshows that there's still a gap to achieve the authenticity of real videos.\"\n\nVASA-1 is only a research demonstration, but Microsoft is far from the only\ngroup developing similar technology. If the recent history of generative AI is\nany guide, it's potentially only a matter of time before similar technology\nbecomes open source and freely available\u2014and they will very likely continue to\nimprove in realism over time.\n\n### Ars Video\n\n### What Happens to the Developers When AI Can Code? | Ars Frontiers\n\n#### reader comments\n\n126\n\nBenj Edwards Benj Edwards is an AI and Machine Learning Reporter for Ars\nTechnica. In his free time, he writes and records music, collects vintage\ncomputers, and enjoys nature. He lives in Raleigh, NC.\n\nAdvertisement\n\n### Channel Ars Technica\n\n#### Unsolved Mysteries Of Quantum Leap With Donald P. Bellisario\n\nToday \"Quantum Leap\" series creator Donald P. Bellisario joins Ars Technica to\nanswer once and for all the lingering questions we have about his enduringly\npopular show. Was Dr. Sam Beckett really leaping between all those time\nperiods and people or did he simply imagine it all? What do people in the\nwaiting room do while Sam is in their bodies? What happens to Sam's loyal ally\nAl? 30 years following the series finale, answers to these mysteries and more\nawait.\n\n  * ##### Unsolved Mysteries Of Quantum Leap With Donald P. Bellisario\n\n  * ##### Unsolved Mysteries Of Warhammer 40K With Author Dan Abnett\n\n  * ##### SITREP: F-16 replacement search a signal of F-35 fail?\n\n  * ##### Sitrep: Boeing 707\n\n  * ##### Steve Burke of GamersNexus Reacts To Their Top 1000 Comments On YouTube\n\n  * ##### Modern Vintage Gamer Reacts To His Top 1000 Comments On YouTube\n\n  * ##### How The NES Conquered A Skeptical America In 1985\n\n  * ##### Scott Manley Reacts To His Top 1000 YouTube Comments\n\n  * ##### How Horror Works in Amnesia: Rebirth, Soma and Amnesia: The Dark Descent\n\n  * ##### LGR's Clint Basinger Reacts To His Top 1000 YouTube Comments\n\n  * ##### The F-35's next tech upgrade\n\n  * ##### How One Gameplay Decision Changed Diablo Forever\n\n  * ##### Unsolved Mortal Kombat Mysteries With Dominic Cianciolo From NetherRealm Studios\n\n  * ##### US Navy Gets an Italian Accent\n\n  * ##### How Amazon\u2019s \u201cUndone\u201d Animates Dreams With Rotoscoping And Oil Paints\n\n  * ##### Fighter Pilot Breaks Down Every Button in an F-15 Cockpit\n\n  * ##### How NBA JAM Became A Billion-Dollar Slam Dunk\n\n  * ##### Linus \"Tech Tips\" Sebastian Reacts to His Top 1000 YouTube Comments\n\n  * ##### How Alan Wake Was Rebuilt 3 Years Into Development\n\n  * ##### How Prince of Persia Defeated Apple II's Memory Limitations\n\n  * ##### How Crash Bandicoot Hacked The Original Playstation\n\n  * ##### Myst: The challenges of CD-ROM | War Stories\n\n  * ##### Markiplier Reacts To His Top 1000 YouTube Comments\n\n  * ##### How Mind Control Saved Oddworld: Abe's Oddysee\n\n  * ##### Bioware answers unsolved mysteries of the Mass Effect universe\n\n  * ##### Civilization: It's good to take turns | War Stories\n\n  * ##### SITREP: DOD Resets Ballistic Missile Interceptor program\n\n  * ##### Warframe's Rebecca Ford reviews your characters\n\n  * ##### Subnautica: A world without guns | War Stories\n\n  * ##### How Slay the Spire\u2019s Original Interface Almost Killed the Game | War Stories\n\n  * ##### Amnesia: The Dark Descent - The horror facade | War Stories\n\n  * ##### Command & Conquer: Tiberian Sun | War Stories\n\n  * ##### Blade Runner: Skinjobs, voxels, and future noir | War Stories\n\n  * ##### Dead Space: The Drag Tentacle | War Stories\n\n  * ##### Teach the Controversy: Flat Earthers\n\n  * ##### Delta V: The Burgeoning World of Small Rockets, Paul Allen's Huge Plane, and SpaceX Gets a Crucial Green-light\n\n  * ##### Chris Hadfield explains his 'Space Oddity' video\n\n  * ##### The Greatest Leap, Episode 1: Risk\n\n  * ##### Ultima Online: The Virtual Ecology | War Stories\n\nMore videos\n\n\u2190 Previous story Next story \u2192\n\n### Related Stories\n\n### Today on Ars\n\nCNMN Collection WIRED Media Group \u00a9 2024 Cond\u00e9 Nast. All rights reserved. Use of and/or registration on any portion of this site constitutes acceptance of our User Agreement (updated 1/1/20) and Privacy Policy and Cookie Statement (updated 1/1/20) and Ars Technica Addendum (effective 8/21/2018). Ars may earn compensation on sales from links on this site. Read our affiliate link policy. Your California Privacy Rights | Manage Preferences The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond\u00e9 Nast. Ad Choices\n\n## We Care About Your Privacy\n\nWe and our 167 partners store and/or access information on a device, such as\nunique IDs in cookies to process personal data. You may accept or manage your\nchoices by clicking below or at any time in the privacy policy page. These\nchoices will be signaled to our partners and will not affect browsing\ndata.More information about your privacy\n\n### We and our partners process data to provide:\n\nUse precise geolocation data. Actively scan device characteristics for\nidentification. Store and/or access information on a device. Personalised\nadvertising and content, advertising and content measurement, audience\nresearch and services development.\n\n", "frontpage": true}
