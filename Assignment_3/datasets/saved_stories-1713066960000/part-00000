{"aid": "40026499", "title": "Debugging the little SSH that sometimes couldn't", "url": "http://beza1e1.tuxen.de/lore/little_ssh_couldnt.html", "domain": "tuxen.de", "votes": 4, "user": "goranmoomin", "posted_at": "2024-04-13 21:53:47", "comments": 0, "source_title": "The little ssh that (sometimes) couldn't", "source_text": "The little ssh that (sometimes) couldn't \u2015 Andreas Zwinkau\n\n# The little ssh that (sometimes) couldn't\n\nSource: Mina Naguib http://mina.naguib.ca/blog/2012/10/22/the-little-ssh-that-\nsometimes-couldnt.html\n\n## Preface\n\nThis is a technical article chronicling one of the most interesting bug hunts\nI\u2019ve had the pleasure of chasing down.\n\nAt AdGear Technologies Inc. where I work, ssh is king. We use it for\nmanagement, monitoring, deployments, log file harvesting, even real-time event\nstreaming. It\u2019s solid, reliable, has all the predictability of a native unix\ntool, and just works.\n\nUntil one day, random cron emails started flowing about it not working.\n\n## The timeout\n\nThe machines in our London data center were randomly failing to send their log\nfiles to our data machines in our Montreal data center. This job is initiated\nperiodically from cron, and the failure manifested itself as:\n\n  * cron emails stating that the ssh was unsuccessful\n\n    * Sometimes hangs\n    * Sometimes exits with a timeout error\n  * nagios warnings down the line for in-house sanity checks detecting the missing data in Montreal\n\nWe logged into the London machines, manually run the push command, and it\nworked successfully. We brushed it off as temporary network partitions.\n\n## The timeouts\n\nBut the failures kept popping up randomly. Once a day, a couple of times a\nday, then one Friday morning, several times an hour. It was clear something\u2019s\ngetting worse. We kept up with manually pushing the files until we figure out\nwhat the problem was.\n\nThere were 17 hops between London and Montreal. We built a profile of latency\nand packet loss for them, and found that a couple were losing 1-3% of packets.\nWe filed a ticket with our London DC ops to route away from them.\n\nWhile London DC ops were verifying the packet loss, we started seeing random\ntimeouts from London to our SECOND data center in Montreal, and hops to that\ndata center did not share the same routes we observed the packet loss at. We\nconcluded packet loss is not the main problem around the same time London DC\nops replied saying they\u2019re not able to replicate the packet loss or timeouts\nand that everything looked healthy on their end.\n\n## The revelation\n\nWhile manually keeping up with failed cron uploads, we noticed an interesting\npattern. A file transfer either succeeded at a high speed, or didn\u2019t succeed\nat all and hung/timed out. There were no instances of a file uploading slowly\nand finishing successfully.\n\nRemoving the large volume of data from the equation, we were able to recreate\nthe scenario via simple vanilla ssh. On a London machine an \u201cssh mtl-machine\u201d\nwould either work immediately, or hang and never establish a connection.\nEyebrows started going up.\n\n## Where the wild packets are\n\nWe triple-checked the ssh server configs and health in Montreal:\n\n  * DNS servers were responding fast\n  * DNS reverse lookup was not enabled\n  * Maximum client connections was high enough\n  * We were not under attack\n  * Bandwidth usage was nowhere near saturation\n\nBesides, even if something was off, we were observing the hangs talking to 2\ncompletely distinct data centers in Montreal. Furthermore, our other data\ncenters (non-London) were talking happily to Montreal. Something about London\nwas off.\n\nWe fired up tcpdump and started looking at the packets, both in summary and in\ncaptured pcaps loaded into wireshark. We saw telltale signs of packet loss and\nretransmission, but it was minimal and not particularly worrisome.\n\nWe then captured full connections from cases where ssh established\nsuccessfully, and full connections from cases where the ssh connection hung.\n\nHere\u2019s what we logically saw when a connection from London to Montreal hung:\n\n  * Normal TCP handshake\n  * Bunch of ssh-specific back and forth, with normal TCP ack packets where they should be\n  * A particular packet sent from London and received in Montreal\n  * The same packet re-sent (and re-sent, several times) from London and received in Montreal\n  * Montreal\u2019s just not responding to it!\n\nIt didn\u2019t make sense why Montreal was not responding (hence London re-\ntransmitting it). The connection was stalled at this point, as the layer 4\nprotocol was at a stalemate. More infuriatingly, if you kill the ssh attempt\nin London and re-launched it immediately, odds are it worked successfully.\nWhen it did, tcpdump showed Montreal receiving the packet but responding to\nit, and things moved on.\n\nWe enabled verbose debugging (-vvv) on the ssh client in London, and the hang\noccurred after it logged:\n\n    \n    \n    debug2: kex_parse_kexinit: first_kex_follows 0 debug2: kex_parse_kexinit: reserved 0 debug2: mac_setup: found hmac-md5 debug1: kex: server->client aes128-ctr hmac-md5 none debug2: mac_setup: found hmac-md5 debug1: kex: client->server aes128-ctr hmac-md5 none debug1: SSH2_MSG_KEX_DH_GEX_REQUEST(1024<1024<8192) sent debug1: expecting SSH2_MSG_KEX_DH_GEX_GROUP\n\nGoogling \u201cssh hang SSH2_MSG_KEX_DH_GEX_GROUP\u201d has many results \u2013 from bad\nWiFi, to windows TCP bugs, to buggy routers discarding TCP fragments. One\nsolution for LANs was to figure out the path\u2019s MSS and set that as the MTU on\nboth ends.\n\nI kept decrementing the MTU on a London server down from 1500 \u2013 it didn\u2019t help\nuntil I hit the magic value 576. At that point, I was no longer able to get\nthe ssh hanging behavior replicated. I had an ssh loop script running, and it\nwas on-demand that I could cause timeouts by bringing the MTU back up to 1500,\nor make them disappear by setting it to 576.\n\nUnfortunately these are public ad servers and globally setting the MTU to 1500\nwon\u2019t cut it, but the above did suggest that perhaps packet fragmentation or\nreassembly is broken somewhere.\n\nGoing back to check the received packets with tcpdump, there was no evidence\nof fragmentation. The received packet size matched exactly the packet size\nsent. If something did fragment the packet at byte 576+, something else\nreassembled it successfully.\n\n## Twinkle twinkle little mis-shapen star\n\nDigging in some more, I was now looking at full packet dumps (tcpdump -s 0 -X)\ninstead of just the headers. Comparing that magic packet in instances of ssh\nsuccess vs ssh hang showed very little difference aside from TCP/IP header\nvariations. It was however clear that this is the first packet in the TCP\nconnection that had enough data to bypass the 576-byte mark \u2013 all previous\npackets were much smaller.\n\nComparing the same packet, during a hanging instance, as it left London, and\nas captured in Montreal, something caught my eye. Something very subtle, and I\nbrushed it off as fatigue (it was late Friday at this point), but sure enough\nafter a few refreshes and comparisons, I wasn\u2019t imagining things.\n\nHere\u2019s the packet as it left London (minus the first few bytes identifying the\nIP addresses):\n\n    \n    \n    0x0040: 0b7c aecc 1774 b770 ad92 0000 00b7 6563 .|...t.p......ec 0x0050: 6468 2d73 6861 322d 6e69 7374 7032 3536 dh-sha2-nistp256 0x0060: 2c65 6364 682d 7368 6132 2d6e 6973 7470 ,ecdh-sha2-nistp 0x0070: 3338 342c 6563 6468 2d73 6861 322d 6e69 384,ecdh-sha2-ni 0x0080: 7374 7035 3231 2c64 6966 6669 652d 6865 stp521,diffie-he 0x0090: 6c6c 6d61 6e2d 6772 6f75 702d 6578 6368 llman-group-exch 0x00a0: 616e 6765 2d73 6861 3235 362c 6469 6666 ange-sha256,diff 0x00b0: 6965 2d68 656c 6c6d 616e 2d67 726f 7570 ie-hellman-group 0x00c0: 2d65 7863 6861 6e67 652d 7368 6131 2c64 -exchange-sha1,d 0x00d0: 6966 6669 652d 6865 6c6c 6d61 6e2d 6772 iffie-hellman-gr 0x00e0: 6f75 7031 342d 7368 6131 2c64 6966 6669 oup14-sha1,diffi 0x00f0: 652d 6865 6c6c 6d61 6e2d 6772 6f75 7031 e-hellman-group1 0x0100: 2d73 6861 3100 0000 2373 7368 2d72 7361 -sha1...#ssh-rsa 0x0110: 2c73 7368 2d64 7373 2c65 6364 7361 2d73 ,ssh-dss,ecdsa-s 0x0120: 6861 322d 6e69 7374 7032 3536 0000 009d ha2-nistp256.... 0x0130: 6165 7331 3238 2d63 7472 2c61 6573 3139 aes128-ctr,aes19 0x0140: 322d 6374 722c 6165 7332 3536 2d63 7472 2-ctr,aes256-ctr 0x0150: 2c61 7263 666f 7572 3235 362c 6172 6366 ,arcfour256,arcf 0x0160: 6f75 7231 3238 2c61 6573 3132 382d 6362 our128,aes128-cb 0x0170: 632c 3364 6573 2d63 6263 2c62 6c6f 7766 c,3des-cbc,blowf 0x0180: 6973 682d 6362 632c 6361 7374 3132 382d ish-cbc,cast128- 0x0190: 6362 632c 6165 7331 3932 2d63 6263 2c61 cbc,aes192-cbc,a 0x01a0: 6573 3235 362d 6362 632c 6172 6366 6f75 es256-cbc,arcfou 0x01b0: 722c 7269 6a6e 6461 656c 2d63 6263 406c r,rijndael-cbc@l 0x01c0: 7973 6174 6f72 2e6c 6975 2e73 6500 0000 ysator.liu.se... 0x01d0: 9d61 6573 3132 382d 6374 722c 6165 7331 .aes128-ctr,aes1 0x01e0: 3932 2d63 7472 2c61 6573 3235 362d 6374 92-ctr,aes256-ct 0x01f0: 722c 6172 6366 6f75 7232 3536 2c61 7263 r,arcfour256,arc 0x0200: 666f 7572 3132 382c 6165 7331 3238 2d63 four128,aes128-c 0x0210: 6263 2c33 6465 732d 6362 632c 626c 6f77 bc,3des-cbc,blow 0x0220: 6669 7368 2d63 6263 2c63 6173 7431 3238 fish-cbc,cast128 0x0230: 2d63 6263 2c61 6573 3139 322d 6362 632c -cbc,aes192-cbc, 0x0240: 6165 7332 3536 2d63 6263 2c61 7263 666f aes256-cbc,arcfo 0x0250: 7572 2c72 696a 6e64 6165 6c2d 6362 6340 ur,rijndael-cbc@ 0x0260: 6c79 7361 746f 722e 6c69 752e 7365 0000 lysator.liu.se.. 0x0270: 00a7 686d 6163 2d6d 6435 2c68 6d61 632d ..hmac-md5,hmac- 0x0280: 7368 6131 2c75 6d61 632d 3634 406f 7065 sha1,umac-64@ope 0x0290: 6e73 7368 2e63 6f6d 2c68 6d61 632d 7368 nssh.com,hmac-sh 0x02a0: 6132 2d32 3536 2c68 6d61 632d 7368 6132 a2-256,hmac-sha2 0x02b0: 2d32 3536 2d39 362c 686d 6163 2d73 6861 -256-96,hmac-sha 0x02c0: 322d 3531 322c 686d 6163 2d73 6861 322d 2-512,hmac-sha2- 0x02d0: 3531 322d 3936 2c68 6d61 632d 7269 7065 512-96,hmac-ripe 0x02e0: 6d64 3136 302c 686d 6163 2d72 6970 656d md160,hmac-ripem 0x02f0: 6431 3630 406f 7065 6e73 7368 2e63 6f6d d160@openssh.com 0x0300: 2c68 6d61 632d 7368 6131 2d39 362c 686d ,hmac-sha1-96,hm 0x0310: 6163 2d6d 6435 2d39 3600 0000 a768 6d61 ac-md5-96....hma 0x0320: 632d 6d64 352c 686d 6163 2d73 6861 312c c-md5,hmac-sha1, 0x0330: 756d 6163 2d36 3440 6f70 656e 7373 682e umac-64@openssh. 0x0340: 636f 6d2c 686d 6163 2d73 6861 322d 3235 com,hmac-sha2-25 0x0350: 362c 686d 6163 2d73 6861 322d 3235 362d 6,hmac-sha2-256- 0x0360: 3936 2c68 6d61 632d 7368 6132 2d35 3132 96,hmac-sha2-512 0x0370: 2c68 6d61 632d 7368 6132 2d35 3132 2d39 ,hmac-sha2-512-9 0x0380: 362c 686d 6163 2d72 6970 656d 6431 3630 6,hmac-ripemd160 0x0390: 2c68 6d61 632d 7269 7065 6d64 3136 3040 ,hmac-ripemd160@ 0x03a0: 6f70 656e 7373 682e 636f 6d2c 686d 6163 openssh.com,hmac 0x03b0: 2d73 6861 312d 3936 2c68 6d61 632d 6d64 -sha1-96,hmac-md 0x03c0: 352d 3936 0000 0015 6e6f 6e65 2c7a 6c69 5-96....none,zli 0x03d0: 6240 6f70 656e 7373 682e 636f 6d00 0000 b@openssh.com... 0x03e0: 156e 6f6e 652c 7a6c 6962 406f 7065 6e73 .none,zlib@opens 0x03f0: 7368 2e63 6f6d 0000 0000 0000 0000 0000 sh.com.......... 0x0400: 0000 0000 0000 0000 0000 0000 ............\n\nAnd here\u2019s the same packet as it arrived in Montreal:\n\n    \n    \n    0x0040: 0b7c aecc 1774 b770 ad92 0000 00b7 6563 .|...t.p......ec 0x0050: 6468 2d73 6861 322d 6e69 7374 7032 3536 dh-sha2-nistp256 0x0060: 2c65 6364 682d 7368 6132 2d6e 6973 7470 ,ecdh-sha2-nistp 0x0070: 3338 342c 6563 6468 2d73 6861 322d 6e69 384,ecdh-sha2-ni 0x0080: 7374 7035 3231 2c64 6966 6669 652d 6865 stp521,diffie-he 0x0090: 6c6c 6d61 6e2d 6772 6f75 702d 6578 6368 llman-group-exch 0x00a0: 616e 6765 2d73 6861 3235 362c 6469 6666 ange-sha256,diff 0x00b0: 6965 2d68 656c 6c6d 616e 2d67 726f 7570 ie-hellman-group 0x00c0: 2d65 7863 6861 6e67 652d 7368 6131 2c64 -exchange-sha1,d 0x00d0: 6966 6669 652d 6865 6c6c 6d61 6e2d 6772 iffie-hellman-gr 0x00e0: 6f75 7031 342d 7368 6131 2c64 6966 6669 oup14-sha1,diffi 0x00f0: 652d 6865 6c6c 6d61 6e2d 6772 6f75 7031 e-hellman-group1 0x0100: 2d73 6861 3100 0000 2373 7368 2d72 7361 -sha1...#ssh-rsa 0x0110: 2c73 7368 2d64 7373 2c65 6364 7361 2d73 ,ssh-dss,ecdsa-s 0x0120: 6861 322d 6e69 7374 7032 3536 0000 009d ha2-nistp256.... 0x0130: 6165 7331 3238 2d63 7472 2c61 6573 3139 aes128-ctr,aes19 0x0140: 322d 6374 722c 6165 7332 3536 2d63 7472 2-ctr,aes256-ctr 0x0150: 2c61 7263 666f 7572 3235 362c 6172 6366 ,arcfour256,arcf 0x0160: 6f75 7231 3238 2c61 6573 3132 382d 6362 our128,aes128-cb 0x0170: 632c 3364 6573 2d63 6263 2c62 6c6f 7766 c,3des-cbc,blowf 0x0180: 6973 682d 6362 632c 6361 7374 3132 382d ish-cbc,cast128- 0x0190: 6362 632c 6165 7331 3932 2d63 6263 2c61 cbc,aes192-cbc,a 0x01a0: 6573 3235 362d 6362 632c 6172 6366 6f75 es256-cbc,arcfou 0x01b0: 722c 7269 6a6e 6461 656c 2d63 6263 406c r,rijndael-cbc@l 0x01c0: 7973 6174 6f72 2e6c 6975 2e73 6500 0000 ysator.liu.se... 0x01d0: 9d61 6573 3132 382d 6374 722c 6165 7331 .aes128-ctr,aes1 0x01e0: 3932 2d63 7472 2c61 6573 3235 362d 6374 92-ctr,aes256-ct 0x01f0: 722c 6172 6366 6f75 7232 3536 2c61 7263 r,arcfour256,arc 0x0200: 666f 7572 3132 382c 6165 7331 3238 2d63 four128,aes128-c 0x0210: 6263 2c33 6465 732d 6362 632c 626c 6f77 bc,3des-cbc,blow 0x0220: 6669 7368 2d63 6263 2c63 6173 7431 3238 fish-cbc,cast128 0x0230: 2d63 6263 2c61 6573 3139 322d 6362 632c -cbc,aes192-cbc, 0x0240: 6165 7332 3536 2d63 6263 2c61 7263 666f aes256-cbc,arcfo 0x0250: 7572 2c72 696a 6e64 6165 6c2d 6362 7340 ur,rijndael-cbs@ 0x0260: 6c79 7361 746f 722e 6c69 752e 7365 1000 lysator.liu.se.. 0x0270: 00a7 686d 6163 2d6d 6435 2c68 6d61 732d ..hmac-md5,hmas- 0x0280: 7368 6131 2c75 6d61 632d 3634 406f 7065 sha1,umac-64@ope 0x0290: 6e73 7368 2e63 6f6d 2c68 6d61 632d 7368 nssh.com,hmac-sh 0x02a0: 6132 2d32 3536 2c68 6d61 632d 7368 7132 a2-256,hmac-shq2 0x02b0: 2d32 3536 2d39 362c 686d 6163 2d73 7861 -256-96,hmac-sxa 0x02c0: 322d 3531 322c 686d 6163 2d73 6861 322d 2-512,hmac-sha2- 0x02d0: 3531 322d 3936 2c68 6d61 632d 7269 7065 512-96,hmac-ripe 0x02e0: 6d64 3136 302c 686d 6163 2d72 6970 756d md160,hmac-ripum 0x02f0: 6431 3630 406f 7065 6e73 7368 2e63 7f6d d160@openssh.c.m 0x0300: 2c68 6d61 632d 7368 6131 2d39 362c 786d ,hmac-sha1-96,xm 0x0310: 6163 2d6d 6435 2d39 3600 0000 a768 7d61 ac-md5-96....h}a 0x0320: 632d 6d64 352c 686d 6163 2d73 6861 312c c-md5,hmac-sha1, 0x0330: 756d 6163 2d36 3440 6f70 656e 7373 782e umac-64@openssx. 0x0340: 636f 6d2c 686d 6163 2d73 6861 322d 3235 com,hmac-sha2-25 0x0350: 362c 686d 6163 2d73 6861 322d 3235 362d 6,hmac-sha2-256- 0x0360: 3936 2c68 6d61 632d 7368 6132 2d35 3132 96,hmac-sha2-512 0x0370: 2c68 6d61 632d 7368 6132 2d35 3132 3d39 ,hmac-sha2-512=9 0x0380: 362c 686d 6163 2d72 6970 656d 6431 3630 6,hmac-ripemd160 0x0390: 2c68 6d61 632d 7269 7065 6d64 3136 3040 ,hmac-ripemd160@ 0x03a0: 6f70 656e 7373 682e 636f 6d2c 686d 7163 openssh.com,hmqc 0x03b0: 2d73 6861 312d 3936 2c68 6d61 632d 7d64 -sha1-96,hmac-}d 0x03c0: 352d 3936 0000 0015 6e6f 6e65 2c7a 7c69 5-96....none,z|i 0x03d0: 6240 6f70 656e 7373 682e 636f 6d00 0000 b@openssh.com... 0x03e0: 156e 6f6e 652c 7a6c 6962 406f 7065 6e73 .none,zlib@opens 0x03f0: 7368 2e63 6f6d 0000 0000 0000 0000 0000 sh.com.......... 0x0400: 0000 0000 0000 0000 0000 0000 ............\n\nDid something there catch your eye ? If not, I don\u2019t blame you. Feel free to\ncopy each into a text editor and rapidly switch back-and-forth to see some\ncharacters dance.\n\nWell well well. It\u2019s not packet loss, it\u2019s packet corruption! Very subtle,\nvery predictable packet corruption.\n\nSome interesting notes:\n\n  * The lower part of the packet (<576 bytes) is unaffected\n  * The affected portion is predictably corrupted on the 15th byte of every 16\n  * The corruption is predictable. All instances of \u201ch\u201d become \u201cx\u201d, all instances of \u201cc\u201d become \u201cs\u201d\n\nSome readers might have already checked ASCII charts and reached the\nconclusion: There\u2019s a single bit statically stuck at \u201c1\u201d somewhere. Flipping\nthe 4th bit in a byte to 1 would reliably corrupt the above letters on the\nleft side to the value on the right side.\n\nThe obvious culprits within our control (NIC cards, receiving machines) are\nnot suspect due to the pattern of failure observed (several London machines \u2192\nSeveral Montreal data centers and machines). It\u2019s got to be something upstream\nand close to London.\n\nGoing back to validate, things started to make sense. I also noticed a little\nhint in tcpdump verbose mode (tcp cksum bad) which was missed before. A\nMontreal machine receiving this packet discarded it at the kernel level after\nrealizing it\u2019s corrupt, never passing it to the userland ssh daemon. London\nthen re-transmitted it, going through the same corruption, getting the same\nsilent treatment. From ssh and sshd\u2019s perspective, the connection was at a\nstalemate. From tcpdump\u2019s perspective, there was no loss, and Montreal\nmachines appeared to be just ignoring data.\n\nWe sent these findings to our London DC ops, and within a few minutes they\nchanged outbound routes dramatically. The first router hop, and most hops\nafterwards, were different. The hanging problem disappeared.\n\nLate Friday night fixes are nice because you can relax and not carry problems\nand support staff into the weekend :)\n\n## Where\u2019s Waldo\n\nHappy that we were no longer suffering from this problem and that our systems\nare caught up with the backlog, I decided I\u2019d try my hand at actually finding\nthe device causing the corruption.\n\nHaving the London routes updated to not go through the old path meant that I\ncouldn\u2019t reproduce the problem easily. I asked around until I found a friend\nwith a FreeBSD box in Montreal I could use, which was still accessed through\nthe old routes from London.\n\nNext, I wanted to make sure that the corruption is predictable even without\nssh involvement. This was trivially proven with a few pipes.\n\nIn Montreal:\n\n    \n    \n    nc -l -p 4000 > /dev/null\n\nThen in London:\n\n    \n    \n    cat /dev/zero | nc mtl 4000\n\nAgain, accounting for the randomness factor and settings things up in a retry\nloop, I got a few packets which remove any doubt about the previous\nconclusions. Here\u2019s part of one \u2013 remember that we\u2019re sending just a stream of\nnulls:\n\n    \n    \n    0x0210 ..... 0x0220 0000 0000 0000 0000 0000 0000 0000 0000 ................ 0x0230 0000 0000 0000 0000 0000 0000 0000 0000 ................ 0x0240 0000 0000 0000 0000 0000 0000 0000 0000 ................ 0x0250 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0260 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0270 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0280 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0290 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x02a0 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x02b0 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x02c0 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x02d0 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x02e0 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x02f0 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0300 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0310 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0320 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0330 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0340 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0350 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0360 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0370 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0380 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x0390 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x03a0 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x03b0 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x03c0 0000 0000 0000 0000 0000 0000 0000 1000 ................ 0x03d0 0000 0000 0000 0000 0000 0000 0000 0000 ................ 0x03e0 .....\n\nWith the bug replicated, I needed to find a way to isolate which of the 17\nhops along that path cause the corruption. There was simply no way to call up\nthe provider of each cluster to ask them to check their systems.\n\nI decided pinging each router, incrementally, might be the way to go. I\ncrafted special ICMP packets that are large enough to go over the 576 safety\nmargin, and filled entirely with NULLs. Then pinged the Montreal machine with\nthem from London.\n\nThey came back perfectly normal. There was no corruption.\n\nI tried all variations of speed, padding, size \u2013 to no avail. I simply could\nnot observe corruption in the returned ICMP ping packets.\n\nI replaced the netcat pipes with UDP instead of TCP. Again there was no\ncorruption.\n\nThe corruption needed TCP to be reproducible \u2013 and TCP needs 2 cooperating\nendpoints. I tried in vain to see if all the routers had an open TCP port I\ncan talk to directly, to no avail.\n\nIt seemed there was no easy way an external party can pinpoint the bad apple.\nOr was there ?\n\n## Mirror mirror on the wall\n\nTo detect whether corruption occurred or not, we need one of these scenarios:\n\n  * Control over the TCP peer we\u2019re talking to inspect the packet at the destination\n\n    * Not just in userland, where the packet would not get delivered if the TCP checksum failed, but root + tcpdump to inspect it as it arrives\n  * A TCP peer that acts as an echo server to mirror back the data it received, so we get inspect it at the sending node and detect corruption there\n\nIt suddenly occurred to me that the second data point is available to us. Not\nper-se, but consider this: In our very first taste of the problem, we observed\nssh clients hanging when talking to ssh servers over the corrupting hop. This\nis a good passive signal that we can use instead of the active \u201cecho\u201d signal.\n\n... and there are lots of open ssh servers out there on the internet to help\nus out.\n\nWe don\u2019t need actual accounts on these servers \u2013 we just need to kickstart the\nssh connection and see if the cipher exchange phase succeeds or hangs (with a\nreasonable number of retries to account for corruption randomness).\n\nSo this plan was hatched:\n\n  * Use the wonderful nmap tool \u2013 specifically \u2013 its \u201crandom IP\u201d mode \u2013 to make a list of geographically distributed open ssh servers\n  * Test each server to determine whether it is:\n\n    * Unresponsive/unpredictable/firewalled \u2192 Ignore it\n    * Negotiates successfully after being retried N times \u2192 mark as \u201cgood\u201d\n    * Negotiates with hangs at the telltale phase after being retried N times \u2192 mark as \u201cbad\u201d\n  * For both \u201cgood\u201d and \u201cbad\u201d servers, remember the traceroute to them\n\nThe idea was this: All servers marked as \u201cbad\u201d will share a few hops in their\ntraceroute. We can then take that set of suspect hops, and subtract from it\nany that appear in the traceroutes of the \u201cgood\u201d servers. Hopefully what\u2019s\nleft is only one or two.\n\nAfter spending an hour manually doing the above exercise, I stopped to inspect\nthe data. I had classified 16 servers as \u201cBAD\u201d and 25 servers as \u201cGOOD\u201d.\n\nThe first exercise was to find the list of hops that appear in all the\ntraceroutes of the \u201cBAD\u201d servers. As I cleaned and trimmed the list, I\nrealized I won\u2019t even need to get to the \u201cGOOD\u201d list to remove false\npositives. Within the \u201cBAD\u201d lists alone, there remained only 1 that was common\nto all of them.\n\nFor what it\u2019s worth, it was 2 providers away: London \u2192 N hops upstream1 \u2192 Y\nhops upstream2\n\nIt was the first in Y hops of upstream2 \u2013 right at the edge between upstream1\nand upstream2, corrupting random TCP packets, causing many retries, and,\ndepending on the protocol\u2019s logical back-and-forth, hangs, or reduced\ntransmission rates.\n\nI followed up with our London DC ops with the single hop\u2019s IP address.\nHopefully with their direct relationship with upstream1 they can escalate\nthrough there and get it fixed.\n\n## Update\n\nThrough upstream1, I got confirmation that the hop I pointed out (first in\nupstream2) had an internal \u201cmanagement module failure\u201d which affected BGP and\nrouting between two internal networks. It\u2019s still down (they\u2019ve routed around\nit) until they receive a replacement for the faulty module.\n\nMore such crazy stories\n\n## Share page on\n\nTwitter Facebook\n\n\u00a9 2023-11-22\n\nartikel (\u00e4ltere) articles (older) homepage publications\n\nAndreas Zwinkau appreciates email to zwinkau@mailbox.org, if you have a\ncomment.\n\nYou can get updates via Twitter @azwinkau.\n\ndatenschutz\n\n", "frontpage": true}
