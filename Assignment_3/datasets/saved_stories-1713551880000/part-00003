{"aid": "40086046", "title": "Building AI assistants that can write code in production", "url": "https://rehance.ai/blog/building-ai-assistants-that-can-execute-code-safely", "domain": "rehance.ai", "votes": 1, "user": "jasontlouro", "posted_at": "2024-04-19 12:36:41", "comments": 0, "source_title": "Building AI Assistants That Can Execute Code in Production (Safely)", "source_text": "Building AI Assistants That Can Execute Code in Production (Safely)\n\nRehance\n\nSign InBlogDocumentationPricingFAQDemos\n\nSign InBlogDocumentationPricingFAQDemos\n\n# Building AI Assistants That Can Execute Code in Production (Safely)\n\n## This is a proposal for how you might build an AI assistant that can safely\nwrite and execute code in production on behalf of users.\n\nApr 11, 2024\n\n\u00b710 minute read\n\nI know what you're thinking. LLMs must never touch any production code, APIs,\nor, God forbid, databases. I get it. LLMs can hallucinate and generate all\nsorts of incorrect or dangerous outputs.\n\nBut hear me out. There is a way to make this work, where we give AI systems\nthe power to execute code and make data transformations on behalf of users,\nwithout blowing everything up.\n\nThese learnings come from our experience working on an AI assistant platform.\nWhile our assistant isn't yet as robust as we describe in this post, we'll get\nthere soon. This serves as, essentially, our roadmap.\n\nHere's the summary:\n\n  * Create a high-level programming language or framework that you can compile and execute in production with safeguards.\n  * Pick an AI model that can generate great code in this language/framework.\n  * Provide a way for users to make requests in plain English, and have the model generate code to handle the user request.\n  * Allow users to review and approve the code before it runs (in plain text or visually, not the actual code).\n\nBefore we get into the details, a note on who this is for: you don't need to\nbe an AI expert to understand this post, as most of the technical details\naround the 'AI' part are already abstracted away by the folks at OpenAI and\nother companies. This is a tactical guide for using those abstractions in\npractice to create AI assistants and copilots of real value to users.\n\n## Create a High-Level Language or Framework\n\nThis sounds more complicated than it is. You don't need to reinvent a\nprogramming language from scratch. I recommend taking an existing\nlanguage\u2014TypeScript is a great fit\u2014and using it as a base. Then add guardrails\nto prevent the AI from generating code that could be dangerous.\n\nThe particular framework you choose can vary, but the key is to include in the\nprompt a strictly-typed set of functions and data structures available to the\nLLM. These should be very high-level to begin with (e.g., getUsers(), not\ndatabase.query()) and should match your software's features and APIs. They\nshould be features that an individual user would be able to access in your\nsoftware.\n\nHere's how you might get started prompting GPT4 to write valid Typescript in\nyour app:\n\n    \n    \n    You are a Typescript expert, writing a simple program that can resolve a user's request. You cannot use any external libraries or APIs (including `fetch`, `axios`, etc.). You can only use the build-in data structures, such as arrays, objects, dates, and strings. You can use `if` statements, `for` loops, and `functions`. You cannot use `eval` or any other method that would allow you to execute arbitrary code. You must write code that is safe to run in a production environment. Your code will be run in a sandboxed environment in [SoftwareName], a [SoftwareCategory] platform for [SoftwarePurpose]. As such, you have access to the following data structures and functions (provided to you in Typescript for convenience): interface User { id: string; name: string; email: string; } interface UserFilter { id?: string; email?: string; } type GetUsersType = (userFilter: UserFilter) => Promise<User[]>; const getUsers: GetUsersType = async (userFilter) => { // Implementation details are hidden from you. // You can assume this function will return an array of User objects. return []; }; // Etc. Provide all the data structures and functions // that the AI can use. Use getter functions instead // of providing raw data to prevent the AI from // accessing sensitive information. Here is the user's input: \"[UserInput]\". Write a program that can resolve this request.\n\nWhen we ran something like this, GPT4 spit out a program that was much too\nlong, re-defining the existing types and attempting to write implementations\nof the functions we provided (instead of understanding that they were already\nimplemented, and accessible to the AI). But we resolved that by adding some\nmore constraints to the end of the prompt:\n\n    \n    \n    Do not include the provided types, or implement any functions that are already defined above. Only add the code necessary to execute the request, assuming the above functions and types are already available. Do not encapsulate your code in a function, do not provide example usage. You may assume your code will be executed in an async environment in which you can use `async/await`.\n\nIt's a rudimentary start, but you'd be surprised how much you can accomplish\nwith an approach just like this. For example, here's the output of GPT4 when\nprompted with the above, using a project management software as an example,\nwhere the user wants to see all tasks assigned to them.\n\nFirst, the types we provided in the prompt:\n\n    \n    \n    interface User { id: string; name: string; email: string; } interface Issue { ownerId: string; title: string; description: string; tags: string[]; } interface UserFilter { email?: string; id?: string; } interface IssueFilter { ownerId?: string; title?: string; description?: string; matchesAllTags?: string[]; matchesAnyTags?: string[]; } type GetUsersType = (userFilter: UserFilter) => Promise<User[]>; const getUsers: GetUsersType = async (userFilter) => { // Implementation details are hidden from you. // You can assume this function will return an array of User objects. return []; }; type GetIssuesType = (issueFilter: IssueFilter) => Promise<Issue[]>; const getIssues: GetIssuesType = async (issueFilter) => { // Implementation details are hidden from you. // You can assume this function will return an array of Issue objects. return []; };\n\nAnd the output:\n\n    \n    \n    const currentUser = await getUsers({ email: 'john@example.com' }); if (currentUser.length > 0) { const userIssues = getIssues({ ownerId: currentUser[0].id }); console.log(userIssues); } else { console.log('User not found.'); }\n\nThat's pretty good! Simple code that uses the provided functions appropriately\nto resolve the user's request. We'll revisit this later to see how we can make\nit even better and handle more complex requests and edge cases, but first we\nneed to deal with the compilation and execution part.\n\n## Compile and Execute in Production (Safely)\n\nAssuming we've got a valid Typescript code snippet that we want to run, we\nneed to ensure it doesn't do anything malicious and then execute it in\nproduction. This is perhaps the trickiest part, but it's manageable.\n\nFirst of all, we need to get from Typescript to Javascript. That's easy\nenough, as typescript provides a transpileModule function that can do this for\nus. So we can take our Typescript file that we use in the LLM prompt, append\nthe generated code to it, and transpile the whole thing to Javascript in\nmemory. During this step, we should also catch any type or compile-time errors\nthat the AI may have made, and re-prompt the AI to fix them or tell the user\nthat we were unable to process their request.\n\nThat gets us to the point where we have a Javascript string that we can\nexecute, and that is type-safe, making use of our functions/APIs\nappropriately. Now we just need to run the code in a sandbox. Again, we don't\nneed to reinvent the wheel here. This problem has been solved before. One\npotential solution is safe-eval, or if you want to go lower level you can look\ninto Isolate in the V8 API (used by Cloudflare Workers, for example). There's\na node library for this called isolated-vm.\n\n### Sidebar: User Approval\n\nPivoting briefly from the technical side of this to the UX side, we need to\nallow users to review and approve the code before it runs, since the code may\nperform operations that the user didn't intend, like deleting some of their\ndata.\n\nThe approach that should work for this is to first run the sandboxed code in a\ndry-run mode, where you don't actually execute the functions that would have\nside effects. Instead you record the function calls in chronological order and\nthe arguments passed to them. Then you present this to the user in a human-\nreadable format, and ask them to approve the execution. If they do, you run\nthe sandboxed code again, but this time you actually execute the functions.\n\nThe tricky part here is that the user needs to understand what the code is\ndoing, and why it's doing it. I won't get into too much detail here as it's\nmore of a design question than a technical one, but suffice to say that this\nis a critical part of the system that needs to be designed well.\n\nHere's our current (very rudimentary) approach to user approval. We plan to\nimprove this significantly with time:\n\nThe user needs to know what's going on, and why, and be able to stop it or\nundo it if they don't like the result.\n\n## Interactivity\n\nFor a system like this to serve as a proper assistant that users can interact\nwith, it needs to be able to have some back-and-forth with the user, asking\nfor more information or clarification when needed, and communicating any\nissues with producing a valid result.\n\nSince we've set up a programming framework for the assistant, this is actually\nquite easy. We can simply define a function that the AI can use to send a\nmessage to the user and await their input. You could even use console.log!\nOnce the user replies with the necessary information, the model can then\ngenerate the code to handle the request.\n\nFor example, say a user makes a request that's too vague for the system to\ninterpret. It can generate the following output code:\n\n    \n    \n    sendUserMessage( \"I'm sorry, I don't understand your request. Could you please explain it in more detail?\", ); await waitForUserResponse();\n\nOn your end, you'd just hook up the sendUserMessage function to your messaging\nsystem, and interpret waitForUserResponse to mean: take the user's reply, work\nit back into the prompt, and run the new prompt through the model.\n\nI will add that this flips the usual paradigm of LLMs a bit. Rather than\ngenerating text in natural language first, which may include code or anything\nelse, instead we want to use a model that's always generating code, and work\nin natural language into function parameters or variables as it makes sense to\ndo so. Code first, natural language as needed.\n\n## Optimize the System\n\nThe reason this approach is so powerful is that over time, LLMs that are\noptimized for writing code will get better and better. There is so much open\nsource and publicly available code that even now, Copilot, CodeLlama, and\nsimilar models are able to generate code that is quite good.\n\nBy building a sandboxed environment and providing access to our own (high-\nlevel) APIs/functions, we also ensure that the assistant doesn't need to\ngenerate complex, low-level code that is potentially destructive. It only\nneeds to do the basics\u2014combining the existing features in the right ways to\nperform tasks for the users and automate their point-and-click flows.\n\nSetting up a system like this means that to improve the system you basically\njust need to upgrade your LLMs and add more APIs/functions for them to use. Of\ncourse, once you've added all the features available to your software's users,\nall that's left is to keep upgrading to the latest models (and, of course,\nmeasuring the system's performance and user satisfaction).\n\n## Final Notes\n\nThis is a very high-level overview of how you might build an AI assistant that\ncan execute code in production. The details will vary depending on your\nsoftware and your users, but the strategy should be applicable to basically\nall user-facing software.\n\nRemember, the goal is to draw a line in the sand between your software/APIs\nand the AI assistant's generated code, so that the assistant can only access\nthe parts of your software that are appropriate, and in a strictly-typed,\nhigh-level, carefully controlled way.\n\n### What's Next For Us\n\nWe're building this out here at Rehance, and we're excited to see where it\ngoes. This process is largely reusable from software to software, so we're\nhopeful that we can build a platform that makes it easy for any tech company\nto integrate and add an AI assistant to their software that's not just a\ngimmick, but a true assistant that can help users get things done.\n\nWe're also very intrigued to see how AI models, specifically those optimized\nfor code generation, evolve in the coming months and years.\n\nRehance\n\n\u00a9 Copyright 2024 Rehance, Inc. All Rights Reserved.\n\n###### About\n\n  * Founders' Letter\n  * Blog\n  * FAQ\n  * Contact\n\n###### Product\n\n  * Documentation\n  * Demos\n  * Use Cases\n  * Affiliate Program\n\n###### Legal\n\n  * Terms of Service\n  * Privacy Policy\n\n", "frontpage": false}
