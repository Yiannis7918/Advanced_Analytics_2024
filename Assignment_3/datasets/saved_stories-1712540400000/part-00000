{"aid": "39963202", "title": "Building LLM application using RAG", "url": "https://mindfulmatrix.substack.com/p/build-a-simple-llm-application-with", "domain": "mindfulmatrix.substack.com", "votes": 1, "user": "sagarg55", "posted_at": "2024-04-07 19:38:23", "comments": 0, "source_title": "Building LLM application using RAG", "source_text": "Building LLM application using RAG - by Sagar Gandhi\n\n# Mindful Matrix\n\nShare this post\n\n#### Building LLM application using RAG\n\nmindfulmatrix.substack.com\n\n#### Discover more from Mindful Matrix\n\nWeekly newsletter to simplify the complex world of technology and personal\ngrowth. In this newsletter, I'll share wealth of insightful reflections and\nvaluable lessons gleaned from my experiences. Written by a Senior SDE at\nAmazon!\n\nContinue reading\n\nSign in\n\n# Building LLM application using RAG\n\n### How to query your document using LLM\n\nSagar Gandhi\n\nApr 07, 2024\n\n7\n\nShare this post\n\n#### Building LLM application using RAG\n\nmindfulmatrix.substack.com\n\n7\n\nShare\n\n\ud83d\udc4b Hi, this is Sagar with this week\u2019s edition of the Mindful Matrix newsletter.\nThis is my second edition on GenAI/LLM learning series.\n\nIn this edition, I\u2019ll be discussing a key architectural approach known as RAG,\nthat improves the efficacy of LLMs by leveraging custom data. I\u2019ll explain how\nto build a simple LLM application in 5 easy steps to query your data,\nutilizing RAG architecture.\n\nBefore we begin, let\u2019s take a look at what you're about to get from this\narticle -\n\n  1. Limitations of LLMs and need for RAG.\n\n  2. What exactly is RAG and how does RAG pipeline looks?\n\n  3. Build a simple LLM application using RAG in 5 easy steps.\n\n  4. Common use cases for RAG\n\nNote : For starting points in diving into LLMs/GenAI, please check out this\npost.\n\n###\n\nLimitations of LLMs and need for RAG.\n\nLLMs learn language patterns by analyzing vast text datasets, predicting the\nnext word in a sentence based on previous words. However, they face key\nlimitations:\n\n  1. Once trained, LLMs can't update with new information beyond their training cutoff, leading to inaccuracies or hallucination with new or unseen data.\n\n  2. LLMs, typically trained on general data, struggle with domain-specific queries. For precise, relevant answers, they need training on specific organizational data rather than providing broad, generalized responses.\n\nThis is where significance of Retrieval Augmented Generation (RAG) comes into\nplay.\n\n###\n\nSo What is Retrieval Augmented Generation(RAG)?\n\nRAG, is a technique to ground your LLMs to generate responses to your queries\nbased on a custom knowledge-base that you provide.\n\nThis is done by retrieving data/documents relevant to a question or task and\nproviding them as context for the LLM.\n\nWith RAG architecture, organizations can deploy any LLM model and augment it\nto return relevant results for their organization by giving it a small amount\nof their data without the costs and time of fine-tuning or pretraining the\nmodel.\n\nHere\u2019s simple RAG pipeline -\n\nRAG Architecture\n\nLet\u2019s understand the above architecture in detail as we build our LLM\napplication.\n\n###\n\nBuild a simple LLM application in 5 easy steps\n\nThere are many ways to implement a RAG system, depending on specific needs and\ndata nuances. We will build simple RAG application using our custom knowledge\nbase and then query it using an LLM.\n\nI used langchain framwork which provides the building blocks for RAG\napplications.\n\n####\n\nInitial Set up -\n\nI\u2019ve used Amazon Titan Embeddings text model as embedding model and Claud V2\nmodel as LLM, through Amazon Bedrock, which require to have AWS account set\nup.\n\nI\u2019ve used AWS Sagemaker notebook instance to run my python application.\n\nNote : You can also run it locally as well with all the required permissions\nto invoke external APIs (For ex - setting up API Keys if you are using OpenAI\nAPIs for LLM or setting up required IAM permissions if using AWS )\n\nNext, install all the required python modules - langhain, pypdf, boto3, and\nfaiss-cpu.\n\n####\n\nStep - 1 : Load Document\n\nData is gathered and subjected to initial preprocessing. For this tutorial I\nwill be grounding the LLM on my resume (custom knowledge) in pdf format (it\ncan be any other format and langchain has more than 80 different document\nloaders)\n\n    \n    \n    ## Your directory structure should look like this: ## \u2514\u2500\u2500 data ## \u2514\u2500\u2500 my_resume.pdf ## \u2514\u2500\u2500 rag.ipynb (For your application code)\n    \n    \n    from langchain.document_loaders import PyPDFLoader pdf_loader = PyPDFLoader(\"data/my_resume.pdf\") # Load data from the pdf pages = pdf_loader.load()\n\n> Document loaders deal with the specifics of accessing and converting data\n> from a variety of different formats and sources into a standardized format.\n\n####\n\nStep - 2 : Document Splitting/Chunking\n\nBefore creating embedding split large documents into smaller chunks which also\nallows the retriever to select the more relevant chunks from the document\ninstead of feeding the entire data to an LLM.\n\n    \n    \n    from langchain.text_splitter import RecursiveCharacterTextSplitter splitter_pdf = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=50, separators=[\"\\n\\n\", \"\\n\"], ) pdf_splits = splitter_pdf.split_documents(pages)\n\n> RecursiveCharacterTextSplitter takes a large text and splits it based on a\n> specified chunk size. In above it splits by double newlines and then splits\n> chunks by single newlines.\n\n####\n\nStep - 3 : Embedding and indexing with Vector Store\n\nEmbeddings take a piece of text and create a numerical representation of the\ntext, and Embedding model is used to generate the embeddings. Vector stores\nand embeddings come after text splitting as we need to store our documents in\nan easily accessible format.\n\nA vector store is a database where you can easily look up similar vectors\nlater on. This becomes useful when we try to find documents that are relevant\nto a question.\n\nThus, text with semantically similar content will have similar vectors in\nembedding space. So we can compare embeddings(vectors) and find texts that are\nsimilar.\n\n    \n    \n    from langchain.embeddings import BedrockEmbeddings from langchain.vectorstores import FAISS import boto3 # Defining bedrock client bedrock = boto3.client( service_name=\"bedrock\", region_name=\"us-east-1\", endpoint_url=\"https://bedrock.us-east-1.amazonaws.com\" ) # Defining bedrock-runtime client that will be used for predictions bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\") # Define the bedrock embeddings model bedrock_embeddings = BedrockEmbeddings( model_id=\"amazon.titan-embed-text-v1\", client=bedrock_runtime ) # Define Vector DB vectordb = FAISS.from_documents( pdf_splits, bedrock_embeddings, )\n\n> FAISS (Facebook AI Similarity Search)is a library for efficient similarity\n> search and clustering of dense vectors. It\u2019s used as in-memory vector DB.\n\n####\n\nStep - 4 : Define Bedrock model for LLM inference :\n\nWe will initialize the LLM interface via Bedrock and set inference parameters,\nwhich are adjustable values that can restrict or guide the model's responses.\n\n    \n    \n    # Each model has a different set of inference parameters inference_params = { \"temperature\": 0.0 } # Define the langchain module with the selected bedrock model bedrock_llm = Bedrock( model_id='anthropic.claude-v2', client=bedrock_runtime, model_kwargs=inference_params )\n\n> If I simply query LLM without RAG\n>  \n>  \n>     llm_response = bedrock_llm(\"What is Sagar's education?\")\n>\n> Output \u00bb I'm afraid I don't have enough information to know details about\n> someone named Sagar's education. I would need more context to determine\n> that.\n\n####\n\nStep - 5 : Retrieve, Augment and Generate\n\nLet\u2019s use pre-processed knowledge data from above steps and utilize RAG\npipeline.\n\n  * Retrieve parts of our data that are relevant to a user's query.\n\n    * Create embeddings of the question, then compare this embeddings with all the different vectors in the vector store and pick the k most similar.\n\n  * Augment the context of the prompt/query with retrieved data\n\n    * We take k most similar chunks and pass these chunks as a context along with the question into an LLM.\n\n  * LLM generates the response based on the prompt and retrieved data\n\n> RetrievalQA chain as part of langchain framework provide a interface to\n> abstract all these different steps.\n    \n    \n    from langchain.chains import RetrievalQA # Define the RetrievalQA chain qa_chain = RetrievalQA.from_chain_type( bedrock_llm, retriever=vectordb.as_retriever(search_kwargs={\"k\": 5}), ) # Perform retrieval Q&A qa_response = qa_chain({\"query\": \"What is Sagar's education?\"})\n\n> Output \u00bb Based on the resume, Sagar Gandhi earned a B.Tech degree in\n> Computer Science and Engineering from MNIT Jaipur between 2010-2014.\n\nNote : I\u2019ll share the complete code in my github repo also.\n\nThat\u2019s it folks! It\u2019s a simple LLM application designed to demonstrate the use\nof the RAG concepts.\n\n###\n\nCommon use cases for RAG :\n\n  1. Question and answer chatbots: Incorporating LLMs with chatbots allows them to automatically derive more accurate answers from company documents and knowledge bases.\n\n  2. Search augmentation: Incorporating LLMs with search engines that augment search results with LLM-generated answers can better answer informational queries.\n\n  3. Knowledge engine - ask questions on your data: Company data can be used as context for LLMs and allow employees to get answers to their questions easily. (e.g., HR, compliance documents)\n\nStay tuned for my next week\u2019s edition where I\u2019ll cover Prompt engineering,\nincluding techniques such as ReAct and ZeroShot prompting, and build another\nLLM application utilizing these concepts.\n\nI'd also love to hear your suggestions on topics you'd like me to address in\nfuture edition of this series.\n\n###\n\nInteresting reads you don\u2019t want to miss\n\n  1. How to Use Your Mentor Effectively? by\n\nRaviraj Achar\n\n  2. How I Upgraded My Conflict Resolution Skills (Part 2) by\n\nAkash Mukherjee\n\n  3. Why Job Titles Matter by\n\nJunaid Effendi\n\n###\n\nIn case you missed my previous articles on this series...\n\nUnveiling the Revolutionary Architecture behind LLMs - \"Attention is all you\nneed\"\n\nIf you found this useful, please share it with your network and consider\nsubscribing for more such insights.\n\nIf you haven\u2019t subscribed, or followed me on LinkedIn, I\u2019d love to connect\nwith you. Please share your thoughts, feedback, and ideas, or even just to say\nhello!\n\nFollow me on LinkedIn\n\nThanks for reading Mindful Matrix! Subscribe for free to receive new posts and\nsupport my work.\n\n7 Likes\n\n\u00b7\n\n3 Restacks\n\n7\n\nShare this post\n\n#### Building LLM application using RAG\n\nmindfulmatrix.substack.com\n\n7\n\nShare\n\n7 Comments\n\nAkash MukherjeeLeadership Letters1 hr ago\u00b7edited 1 hr agoLiked by Sagar\nGandhiEnjoyed the read, it was at a great detail but I loved your RAG\nexplanation flow diagram!Also, thanks for signal boosting my post!Expand full\ncommentLike (1)ReplyShare  \n---  \n  \n2 replies by Sagar Gandhi and others\n\nJunaid EffendiJunaid Effendi | Sharing knowle...2 hrs agoLiked by Sagar GandhiNice one, definitely this is something people are looking for and don't have enough knowledge.Expand full commentLike (1)ReplyShare  \n---  \n  \n1 reply by Sagar Gandhi\n\n5 more comments...\n\nUnveiling the Revolutionary Architecture behind LLMs - \"Attention is all you\nneed\"\n\nPaper that changed the trajectory of language AI forever\n\nMar 17 \u2022\n\nSagar Gandhi\n\n11\n\nShare this post\n\n#### Unveiling the Revolutionary Architecture behind LLMs - \"Attention is all\nyou need\"\n\nmindfulmatrix.substack.com\n\n1\n\nAttitude: The Secret Ingredient in Software Engineering and Beyond\n\nThe Silent Architect of Your Success!\n\nJan 29 \u2022\n\nSagar Gandhi\n\n11\n\nShare this post\n\n#### Attitude: The Secret Ingredient in Software Engineering and Beyond\n\nmindfulmatrix.substack.com\n\n2\n\nBeyond Guesswork: The Art and Science of Project Estimation\n\nElevating your project estimation game\n\nFeb 8 \u2022\n\nSagar Gandhi\n\n9\n\nShare this post\n\n#### Beyond Guesswork: The Art and Science of Project Estimation\n\nmindfulmatrix.substack.com\n\n2\n\nReady for more?\n\n\u00a9 2024 Sagar Gandhi\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great writing\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
