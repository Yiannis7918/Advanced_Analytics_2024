{"aid": "40014171", "title": "Calculating the Likelihood of Programs", "url": "https://lunaverus.com/programLikelihoods", "domain": "lunaverus.com", "votes": 2, "user": "memorygrep", "posted_at": "2024-04-12 15:42:18", "comments": 0, "source_title": "AnthemScore - ML Blog", "source_text": "AnthemScore - ML Blog\n\nLoading [MathJax]/jax/output/HTML-CSS/jax.js\n\n## The Likelihood of Programs\n\nThere is a neat way to frame unsupervised learning as likelihood maximization,\nbut not in the usual way where you just compute the likelihood of the data\nusing a model and ignore the likelihood of the model itself. Rather, this is\nthe combined likelihood of model and data, where the model is treated as a\nsequence of code symbols. Imagine concatenating the model's code symbols with\nthe data to form one long sequence. If every symbol in that sequence can be\nassigned a probability from the preceding symbols, then the total likelihood\nwould be the product of those probabilities. The data is explained as a\nsequence of probabilistic 'events', the symbols in the sequence. In this\ncontext, the goal of unsupervised learning would be to find the most likely\nexplanation of the data by finding the most likely sequence of events that\ninclude the data.\n\nBut how would you compute the probability of the model's code symbols? You can\nuse the model to compute the probabilities of the data, but what do you use\nfor the model? There are at least two different way to do it:\n\nFirst, this is the realm of Algorithmic Information Theory and the usual\napproach in this field is to use the length of the model's code, like in bits.\nConsider if the model's binary code was randomly generated using a uniform\ndistribution over the possible values (\"0\", \"1\", and \"END\"). As soon as we\nsample \"END\" the program is done. The log3 likelihood of a single binary code\nsymbol is log3(1/3) = -1 and so the log3 likelihood of a model N bits long is\n\u2212N. So if you wanted the combined log3 likelihood of model and data you would\ncompute the log3 likelihood of the data from the model and subtract N.\n(Technical side note: There are many ways to get a normalized distribution\nover models. The \"END\" symbol can be omitted if the program specifies its own\nlength, giving a uniform distribution over just (\"0\" and \"1\"), but I think\nit's easier to think in terms of an \"END\" symbol.)\n\nBefore jumping into the second method for computing model probability, there\nare some interesting observations we can make. The fact that the log\nlikelihood of a model is its negative length means that longer programs are\nalways less likely. That seems like a good thing, since there are infinitely\nmany possible programs and most of them are very long, but the probability\ndistribution over all of them must sum to 1. It's reminiscent of Occam's\nrazor. You can come up with a big, complicated model to explain some data, but\nthe act of making your model bigger makes it less likely. Smaller models are\ninherently more likely, but when you account for the fact that larger models\ncan explain the data better the optimal model hits a sweet spot that isn't too\nlarge (overfit) or too small (underfit). A benefit of considering the combined\nmodel+data likelihood in machine learning is that you wouldn't need to worry\nabout any explicit regularization. There is only more or less likely as you\nsearch over different models. Unfortunately, it's an endless search, since the\nonly way to 100% know for sure we found the most likely model is to evaluate\nevery possible model, which is the space of all possible programs. Some\nprograms never halt (finish their computation). We can't even know which ones\nwill or won't halt\u2014this is the well-known halting problem and it's\nundecidable. So, we'll have to be content with \"the most likely model found so\nfar\".\n\n## Self-Modeling Programs\n\nOk, so a second method for computing the probability of a model is to use the\nearlier code symbols of the model to compute the probabilities of the later\nones, similar to how we use the model to compute the probabilities of later\ndata symbols from the preceding data symbols. First, to be clear on the\nterminology, let's define the program to be the concatenation of model code\nand data:\n\nIn a self-modeling program, both model and data are considered code symbols in\na special programming language. The probability distribution for the Nth\nprogram symbol is computed by executing the partial program formed by the\npreceding symbols 0 to N-1.\n\nThis sounds weird at first, but it has a kind of elegance. Most large language\nmodels work by executing the model on the past inputs to compute a probability\ndistribution over the possible next symbols. We usually think of the model as\nthe program and the past data symbols as the program inputs, but it's also\nvalid to think of the past data as the program code and the model as a kind of\ninterpreter for that code. So let's create a new programming language that\naccepts as valid code symbols both the regular code symbols of any Turing\ncomplete language and all of the possible symbols in the data sequence. We\nwant to be able to always generate an output probability distribution from the\npast symbols of the combined model+data sequence, so let's add a restriction\nthat the execution of any program in the language the must always return a\nprobability distribution over symbols, even for partial or invalid programs.\nLet's call these programs self-modeling programs.\n\nThere's a lot to unpack here. The program can contain complex expressions\ndefining how to compute the probabilities of future symbols from past ones,\nbut it also includes regular data symbols. Consider this example in natural\nlanguage:\n\nI will count to ten. 1 2 3 4 5 6 7 8 9 10.\n\nIn this example, 1 2 3 4 5 6 7 8 9 10 is the data sequence and I will count to\nten is the model code we are pre-pending to the data to form the entire\nprogram. To be clear, natural language isn't a well-defined self-modeling\nprogramming language, but it works well to illustrate. If this were a self-\nmodeling program, we would compute the probability of every symbol (character)\nby executing the program formed by the preceding characters. The programming\nlanguage would define how to do this, but it could start with a uniform\ndistribution over all characters until we finish parsing a full expression. In\nthis case I will count to ten might be a valid expression in our programming\nlanguage, which has the effect of making the symbols in the sequence 1 2 3 4 5\n6 7 8 9 10 have very high probabilities, possibly even at or near 1.\n\nIn self-modeling programs, the data sequence by itself is always a valid\nprogram, but it may not be the most likely one. It's often possible to\nincrease the likelihood by making the program longer. This is in contrast to\nthe method of using program length, where shorter programs are always more\nlikely. To summarize:\n\n  * The self-modeling programming language must be Turing complete so it can define any possible computation.\n  * The language must include the data symbols as valid code symbols.\n  * The execution of any expression (even an empty, partial, or invalid one) must always result in a valid probability distribution over code symbols.\n  * The probability of each code symbol is computed by executing the partial program formed from all preceding symbols.\n  * The program likelihood is the product of all code symbols in the program.\n  * The most likely program for some data is the program with the highest likelihood among all possible self-modeling programs that ends with that data.\n\nThe upshot of all of this is that it lets us explain the data as a sequence of\nprobabilistic events, where each event can influence the probabilities of\nthose that follow in any arbitrary way that a program can describe.\nEssentially we have some observable data and are hypothesizing the events that\nled to its creation. We can evaluate different hypotheses by computing their\nlikelihoods and pick the most likely one. A fun fact is that the most likely\npossible self-modeling program for some given data could be compressed to the\nshortest possible program, if perfect compression is achieved. This length is\ncalled the Kolmogorov Complexity of the data.\n\nDoesn't it matter which programming language we use? Yes, but Kolmogorov made\nthe observation that you can construct an emulator in any Turing complete\nlanguage for any other language. The emulator will have a constant length,\nwhich means that converting from one language to another will change the\nprogram length by a constant.\n\nSo which method for measuring model likelihood is better? Measuring the length\nof a program is easy, but it's an indirect measure of model likelihood and you\nneed to compress the model first as much as possible to get the best measure.\nIf you are searching for the most likely program it is more straightforward to\niterate through all short programs for the first method, but it's\ncomputationally impractical to get very far doing this. You could iterate\nthrough the most likely self-modeling programs by sampling code symbols, but\nthis is also computationally impractical. A downside of self-modeling programs\nis that they require the construction of a special programming language, but\nthey also provide explicit code symbol probabilities and provide a very\nintuitive way to reason about program likelihood, as shown with the\nobservations below.\n\n## Insights From Self-Modeling Programs\n\nHere is a fun observation: if you use some hidden program to generate\nobservable data, then given enough of this data, the hidden program can be\nreverse engineered by searching for the most likely program for that data. To\nsee why, let's use the self-modeling program paradigm described above. We have\nsome hidden program that generates observable code one symbol at a time by\ncomputing the next symbol's probability distribution and randomly sampling a\nnew symbol. If the hidden program generates a deterministic sequence of data,\nthen the data symbols will have been generated with probabilities of 1. If the\ndata sequence is long enough, then the most likely program we could find must\nalso assign a probability of 1 to every data symbol. Otherwise, it would not\nbe the most likely program and we could construct an even more likely program\nprogram by adding some extra code to our model that happens to match the\nhidden program so that the probabilities of the data symbols are 1. This extra\ncode will decrease the likelihood of the program by some constant amount, but\nif the data sequence is long enough the gains from adding the extra code will\noutweigh its cost.\n\nThis result holds even if the data sequence isn't generated deterministically.\nThink, like, quantum mechanics where we may assume that the observable data\nlike position and momentum comes from randomly sampling a probability\ndistribution. In this case, the most likely program will still assign\nprobabilities to the observations that match those computed by the hidden\nprogram. Otherwise, we could construct a more likely program that does. If the\nprobability of a recurring event is p, then the most likely program is the one\nthat also predicts probability p for that event. Likelihood is maximized when\nthe predicted probability matches the empirical probability of the event.\n\nSo the most likely program will be functionally equivalent to the hidden one,\nin that they both will compute the same probabilities for observable symbols.\nBut what if the hidden program randomly sampled unlikely symbols? This could\nmake the most likely program functionally different from the hidden program,\nbut the effect diminishes the more data you have. As more data is generated,\nthe empirical probabilities converge to the true hidden ones and the most\nlikely program's symbol probabilities also converges to the empirical\nprobabilities.\n\nSo, it shouldn't be surprising that we can reverse engineer the laws of\nnature. Nature generates a lot of observable data and the goal of physics\nresearchers is essentially to find the most likely program for the observed\ndata. Maybe learning itself could even be defined as the process of finding\nthe more likely programs to explain observed data. It's plausible that in\nvisual processing, for example, concepts like 'cat', 'dog', or 'human' emerge\nnaturally as code symbols in the program that explains our observations.\nHaving these code symbols allows you to assign higher probabilities to the\ndata you are observing.\n\nWhat about practical implications for machine learning? One useful idea is\nthat programs are fundamentally more likely when fewer events occur more often\nas opposed to many events less often. In other words, programs with more\nrepetition are more likely. Thus, if you construct a machine learning model\nwhere each layer is the same function, that is inherently more likely than a\nmodel of the same code size where each layer is a different function. They may\nhave the same program lengths, but the one with more repetition is more\ncompressible. Of course that's just the probability of the model. To get the\nprogram likelihood you would have to evaluate the model on the data, but when\nevaluating models is so computationally expensive it makes sense to try the\nmodels with higher probabilities first. Any kind of symmetry in general is\nmore likely for the same reason. We may see so much symmetry in biology, for\nexample, because genetic code containing symmetry is more likely to form than\nany near-equivalent code without symmetric repetitions.\n\nProgram likelihood is the best metric to use when comparing different models\non the same data. Other metrics may still provide useful information, but for\nunsupervised/self-supervised learning or even supervised learning where the\nmodel output is a discrete probability distribution, the best model can be\nselected based on program likelihood.\n\nSo what about these large language models that ignore the likelihood of the\nmodel itself? Clearly, when the amount of data is small, then it's important\nto consider the likelihood of your model. The more data you have, the less the\nmodel likelihood matters to the total program likelihood, assuming the model\nstays constant. In that case, you may be able to approximately ignore the\nmodel likelihood. However, longer data sequences naturally support bigger\nmodels, so program likelihood is still useful to compare models of different\nsizes.\n\nUltimately, finding the most likely program is a search problem, which is\ncomputationally expensive. But even if you only search over a small number of\nmodels, program likelihood is the metric you want to use to select the best\none. Computing program likelihood does take a little extra work, though. For a\nfirst approximation you could use the length of the model code, but this\nignores the fact that some models are more compressible than others. Another\noption is to use a self-modeling programming language that can explicitly\ncalculate program likelihood. In any case, I think this is a fascinating\nsubject area and a useful tool that seems a little underappreciated.\n\nApril 11, 2024 Contact:\n\n  * AnthemScore\n  * Overview\n  * Download\n  * Compare Editions\n  * Machine Learning\n\n  * Shop\n  * Buy\n  * Upgrade\n  * SW Updates\n  * Trial extension\n\n  * Support\n  * Documentation\n  * FAQ\n  * About\n  * Contact\n\n  * Legal\n  * Privacy Policy\n  * Terms of Use\n\nCopyright \u00a9 2024 Lunaverus. All rights reserved.\n\n", "frontpage": false}
