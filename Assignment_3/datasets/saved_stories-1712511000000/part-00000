{"aid": "39960070", "title": "Open-Sora-Plan v1.0.0 Release", "url": "https://github.com/PKU-YuanGroup/Open-Sora-Plan", "domain": "github.com/pku-yuangroup", "votes": 1, "user": "lnyan", "posted_at": "2024-04-07 11:27:36", "comments": 0, "source_title": "GitHub - PKU-YuanGroup/Open-Sora-Plan: This project aim to reproduce Sora (Open AI T2V model), but we only have limited resource. We deeply wish the all open source community can contribute to this project.", "source_text": "GitHub - PKU-YuanGroup/Open-Sora-Plan: This project aim to reproduce Sora\n(Open AI T2V model), but we only have limited resource. We deeply wish the all\nopen source community can contribute to this project.\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nPKU-YuanGroup / Open-Sora-Plan Public\n\n  * Notifications\n  * Fork 593\n  * Star 6.5k\n\nThis project aim to reproduce Sora (Open AI T2V model), but we only have\nlimited resource. We deeply wish the all open source community can contribute\nto this project.\n\n### License\n\nMIT license\n\n6.5k stars 593 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# PKU-YuanGroup/Open-Sora-Plan\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n4 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nLinB203Merge pull request #178 from qqingzheng/causalvae_release_pr8fbaa47 \u00b7\n\n## History\n\n368 Commits  \n  \n### .github/workflows\n\n|\n\n### .github/workflows\n\n| add CI for docker autobuild  \n  \n### assets\n\n|\n\n### assets\n\n| Add files via upload  \n  \n### docker\n\n|\n\n### docker\n\n| update docker scripts and configs  \n  \n### docs\n\n|\n\n### docs\n\n| Update Report-v1.0.0.md  \n  \n### examples\n\n|\n\n### examples\n\n| Merge pull request #178 from qqingzheng/causalvae_release_pr  \n  \n### opensora\n\n|\n\n### opensora\n\n| Merge pull request #178 from qqingzheng/causalvae_release_pr  \n  \n### scripts\n\n|\n\n### scripts\n\n| Merge pull request #178 from qqingzheng/causalvae_release_pr  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| release  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Create LICENSE  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md  \n  \n### pyproject.toml\n\n|\n\n### pyproject.toml\n\n| update train script  \n  \n## Repository files navigation\n\n# Open-Sora Plan\n\nWe are thrilled to present Open-Sora-Plan v1.0.0, which significantly enhances\nvideo generation quality and text control capabilities. See our report. We are\ntraining for higher resolution (>1024) as well as longer duration (>10s)\nvideos, here is a preview of the next release. We show compressed .gif on\ngithub, which loses some quality.\n\n257\u00d7512\u00d7512 (10s)| 65\u00d71024\u00d71024 (2.7s)| 65\u00d71024\u00d71024 (2.7s)  \n---|---|---  \nTime-lapse of a coastal landscape transitioning from sunrise to nightfall...|\nA quiet beach at dawn, the waves gently lapping at the shore and the sky\npainted in pastel hues....| Sunset over the sea.  \n65\u00d7512\u00d7512 (2.7s)| 65\u00d7512\u00d7512 (2.7s)| 65\u00d7512\u00d7512 (2.7s)  \n---|---|---  \nA serene underwater scene featuring a sea turtle swimming...| Yellow and black\ntropical fish dart through the sea.| a dynamic interaction between the ocean\nand a large rock...  \nThe dynamic movement of tall, wispy grasses swaying in the wind...| Slow pan\nupward of blazing oak fire in an indoor fireplace.| A serene waterfall\ncascading down moss-covered rocks...  \n  \n## \ud83d\udcaa Goal\n\nThis project aims to create a simple and scalable repo, to reproduce Sora\n(OpenAI, but we prefer to call it \"ClosedAI\" ) and build knowledge about\nVideo-VQVAE (VideoGPT) + DiT at scale. However, since we have limited\nresources, we deeply wish all open-source community can contribute to this\nproject. Pull requests are welcome!!!\n\n\u672c\u9879\u76ee\u5e0c\u671b\u901a\u8fc7\u5f00\u6e90\u793e\u533a\u7684\u529b\u91cf\u590d\u73b0Sora,\u7531\u5317\u5927-\n\u5154\u5c55AIGC\u8054\u5408\u5b9e\u9a8c\u5ba4\u5171\u540c\u53d1\u8d77,\u5f53\u524d\u6211\u4eec\u8d44\u6e90\u6709\u9650\u4ec5\u642d\u5efa\u4e86\u57fa\u7840\u67b6\u6784,\u65e0\u6cd5\u8fdb\u884c\u5b8c\u6574\u8bad\u7ec3,\u5e0c\u671b\u901a\u8fc7\u5f00\u6e90\u793e\u533a\u9010\u6b65\u589e\u52a0\u6a21\u5757\u5e76\u7b79\u96c6\u8d44\u6e90\u8fdb\u884c\u8bad\u7ec3,\u5f53\u524d\u7248\u672c\u79bb\u76ee\u6807\u5dee\u8ddd\u5de8\u5927,\u4ecd\u9700\u6301\u7eed\u5b8c\u5584\u548c\u5feb\u901f\u8fed\u4ee3,\u6b22\u8fcePull\nrequest!!!\n\nProject stages:\n\n  * Primary\n\n  1. Setup the codebase and train a un-conditional model on a landscape dataset.\n  2. Train models that boost resolution and duration.\n\n  * Extensions\n\n  3. Conduct text2video experiments on landscape dataset.\n  4. Train the 1080p model on video2text dataset.\n  5. Control model with more conditions.\n\n## \ud83d\udcf0 News\n\n[2024.04.07] \ud83d\ude80\ud83d\ude80\ud83d\ude80 Today, we are thrilled to present Open-Sora-Plan v1.0.0,\nwhich significantly enhances video generation quality and text control\ncapabilities. See our report. Thanks to HUAWEI NPU for supporting us.\n\n[2024.03.27] \ud83d\ude80\ud83d\ude80\ud83d\ude80 We release the report of VideoCausalVAE, which supports both\nimages and videos. We present our reconstructed video in this demonstration as\nfollows. The text-to-video model is on the way.\n\n[2024.03.10] \ud83d\ude80\ud83d\ude80\ud83d\ude80 This repo supports training a latent size of 225\u00d790\u00d790\n(t\u00d7h\u00d7w), which means we are able to train 1 minute of 1080P video with 30FPS\n(2\u00d7 interpolated frames and 2\u00d7 super resolution) under class-condition.\n\n[2024.03.08] We support the training code of text condition with 16 frames of\n512x512. The code is mainly borrowed from Latte.\n\n[2024.03.07] We support training with 128 frames (when sample rate = 3, which\nis about 13 seconds) of 256x256, or 64 frames (which is about 6 seconds) of\n512x512.\n\n[2024.03.05] See our latest todo, pull requests are welcome.\n\n[2024.03.04] We re-organizes and modulizes our code to make it easy to\ncontribute to the project, to contribute please see the Repo structure.\n\n[2024.03.03] We opened some discussions to clarify several issues.\n\n[2024.03.01] Training code is available now! Learn more on our project page.\nPlease feel free to watch \ud83d\udc40 this repository for the latest updates.\n\n## \u270a Todo\n\n#### Setup the codebase and train a unconditional model on landscape dataset\n\n  * Fix typos & Update readme. \ud83e\udd1d Thanks to @mio2333, @CreamyLong, @chg0901, @Nyx-177, @HowardLi1984, @sennnnn, @Jason-fan20\n  * Setup environment. \ud83e\udd1d Thanks to @nameless1117\n  * Add docker file. \u231b [WIP] \ud83e\udd1d Thanks to @Mon-ius, @SimonLeeGit\n  * Enable type hints for functions. \ud83e\udd1d Thanks to @RuslanPeresy, \ud83d\ude4f [Need your contribution]\n  * Resume from checkpoint.\n  * Add Video-VQGAN model, which is borrowed from VideoGPT.\n  * Support variable aspect ratios, resolutions, durations training on DiT.\n  * Support Dynamic mask input inspired by FiT.\n  * Add class-conditioning on embeddings.\n  * Incorporating Latte as main codebase.\n  * Add VAE model, which is borrowed from Stable Diffusion.\n  * Joint dynamic mask input with VAE.\n  * Add VQVAE from VQGAN. \ud83d\ude4f [Need your contribution]\n  * Make the codebase ready for the cluster training. Add SLURM scripts. \ud83d\ude4f [Need your contribution]\n  * Refactor VideoGPT. \ud83e\udd1d Thanks to @qqingzheng, @luo3300612, @sennnnn\n  * Add sampling script.\n  * Add DDP sampling script. \u231b [WIP]\n  * Use accelerate on multi-node. \ud83e\udd1d Thanks to @sysuyy\n  * Incorporate SiT. \ud83e\udd1d Thanks to @khan-yin\n  * Add evaluation scripts (FVD, CLIP score). \ud83e\udd1d Thanks to @rain305f\n\n#### Train models that boost resolution and duration\n\n  * Add PI to support out-of-domain size. \ud83e\udd1d Thanks to @jpthu17\n  * Add 2D RoPE to improve generalization ability as FiT. \ud83e\udd1d Thanks to @jpthu17\n  * Compress KV according to PixArt-sigma.\n  * Support deepspeed for videogpt training. \ud83e\udd1d Thanks to @sennnnn\n  * Train a low dimension Video-AE, whether it is VAE or VQVAE.\n  * Extract offline feature.\n  * Train with offline feature.\n  * Add frame interpolation model. \ud83e\udd1d Thanks to @yunyangge\n  * Add super resolution model. \ud83e\udd1d Thanks to @Linzy19\n  * Add accelerate to automatically manage training.\n  * Joint training with images. \ud83d\ude4f [Need your contribution]\n  * Implement MaskDiT technique for fast training. \ud83d\ude4f [Need your contribution]\n  * Incorporate NaViT. \ud83d\ude4f [Need your contribution]\n  * Add FreeNoise support for training-free longer video generation. \ud83d\ude4f [Need your contribution]\n\n#### Conduct text2video experiments on landscape dataset.\n\n  * Implement PeRFlow for improving the sampling process. \ud83d\ude4f [Need your contribution]\n  * Finish data loading, pre-processing utils.\n  * Add T5 support.\n  * Add CLIP support. \ud83e\udd1d Thanks to @Ytimed2020\n  * Add text2image training script.\n  * Add prompt captioner.\n\n    * Collect training data.\n\n      * Need video-text pairs with caption. \ud83d\ude4f [Need your contribution]\n      * Extract multi-frame descriptions by large image-language models. \ud83e\udd1d Thanks to @HowardLi1984\n      * Extract video description by large video-language models. \ud83d\ude4f [Need your contribution]\n      * Integrate captions to get a dense caption by using a large language model, such as GPT-4. \ud83e\udd1d Thanks to @HowardLi1984\n    * Train a captioner to refine captions. \ud83d\ude80 [Require more computation]\n\n#### Train the 1080p model on video2text dataset\n\n  * Looking for a suitable dataset, welcome to discuss and recommend. \ud83d\ude4f [Need your contribution]\n  * Add synthetic video created by game engines or 3D representations. \ud83d\ude4f [Need your contribution]\n  * Finish data loading, and pre-processing utils. \u231b [WIP]\n  * Support memory friendly training.\n\n    * Add flash-attention2 from pytorch.\n    * Add xformers. \ud83e\udd1d Thanks to @jialin-zhao\n    * Support mixed precision training.\n    * Add gradient checkpoint.\n    * Support for ReBased and Ring attention. \ud83e\udd1d Thanks to @kabachuha\n    * Train using the deepspeed engine. \ud83e\udd1d Thanks to @sennnnn\n    * Integrate with Colossal-AI for a cheaper, faster, and more efficient. \ud83d\ude4f [Need your contribution]\n  * Train with a text condition. Here we could conduct different experiments: \ud83d\ude80 [Require more computation]\n\n    * Train with T5 conditioning.\n    * Train with CLIP conditioning.\n    * Train with CLIP + T5 conditioning (probably costly during training and experiments).\n\n#### Control model with more condition\n\n  * Load pretrained weights from Latte.\n  * Incorporating ControlNet. \ud83d\ude4f [Need your contribution]\n\n## \ud83d\udcc2 Repo structure (WIP)\n\n    \n    \n    \u251c\u2500\u2500 README.md \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 Data.md -> Datasets description. \u2502 \u251c\u2500\u2500 Contribution_Guidelines.md -> Contribution guidelines description. \u251c\u2500\u2500 scripts -> All scripts. \u251c\u2500\u2500 opensora \u2502 \u251c\u2500\u2500 dataset \u2502 \u251c\u2500\u2500 models \u2502 \u2502 \u251c\u2500\u2500 ae -> Compress videos to latents \u2502 \u2502 \u2502 \u251c\u2500\u2500 imagebase \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 vae \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 vqvae \u2502 \u2502 \u2502 \u2514\u2500\u2500 videobase \u2502 \u2502 \u2502 \u251c\u2500\u2500 vae \u2502 \u2502 \u2502 \u2514\u2500\u2500 vqvae \u2502 \u2502 \u251c\u2500\u2500 captioner \u2502 \u2502 \u251c\u2500\u2500 diffusion -> Denoise latents \u2502 \u2502 \u2502 \u251c\u2500\u2500 diffusion \u2502 \u2502 \u2502 \u251c\u2500\u2500 dit \u2502 \u2502 \u2502 \u251c\u2500\u2500 latte \u2502 \u2502 \u2502 \u2514\u2500\u2500 unet \u2502 \u2502 \u251c\u2500\u2500 frame_interpolation \u2502 \u2502 \u251c\u2500\u2500 super_resolution \u2502 \u2502 \u2514\u2500\u2500 text_encoder \u2502 \u251c\u2500\u2500 sample \u2502 \u251c\u2500\u2500 train -> Training code \u2502 \u2514\u2500\u2500 utils\n\n## \ud83d\udee0\ufe0f Requirements and Installation\n\n  1. Clone this repository and navigate to Open-Sora-Plan folder\n\n    \n    \n    git clone https://github.com/PKU-YuanGroup/Open-Sora-Plan cd Open-Sora-Plan\n\n  2. Install required packages\n\n    \n    \n    conda create -n opensora python=3.8 -y conda activate opensora pip install -e .\n\n  3. Install additional packages for training cases\n\n    \n    \n    pip install -e \".[train]\" pip install flash-attn --no-build-isolation\n\n  4. Install optional requirements such as static type checking:\n\n    \n    \n    pip install -e '.[dev]'\n\n## \ud83d\udddd\ufe0f Usage\n\n### \ud83e\udd17 Demo\n\n#### Gradio Web UI\n\nHighly recommend trying out our web demo by the following command. We also\nprovide online demo in Huggingface Spaces.\n\n    \n    \n    python -m opensora.serve.gradio_web_server\n\n#### CLI Inference\n\n    \n    \n    sh scripts/text_condition/sample_video.sh\n\n### Datasets\n\nRefer to Data.md\n\n### Evaluation\n\nRefer to the document EVAL.md.\n\n### Causal Video VAE\n\n#### Reconstructing\n\n    \n    \n    python examples/rec_video_vae.py --rec-path test_video.mp4 --video-path video.mp4 --resolution 512 --num-frames 1440 --sample-rate 1 --sample-fps 24 - -device cuda --ckpt <Your ckpt>\n\n### VideoGPT VQVAE\n\nPlease refer to the document VQVAE.\n\n### Video Diffusion Transformer\n\n#### Training\n\n    \n    \n    sh scripts/text_condition/train_videoae_17x256x256.sh\n    \n    \n    sh scripts/text_condition/train_videoae_65x256x256.sh\n    \n    \n    sh scripts/text_condition/train_videoae_65x512x512.sh\n\n## \ud83d\ude80 Improved Training Performance\n\nIn comparison to the original implementation, we implement a selection of\ntraining speed acceleration and memory saving features including gradient\ncheckpointing, mixed precision training, and pre-extracted features, xformers,\ndeepspeed. Some data points using a batch size of 1 with a A100:\n\n### 64\u00d732\u00d732 (origin size: 256\u00d7256\u00d7256)\n\ngradient checkpointing| mixed precision| xformers| feature pre-extraction|\ndeepspeed config| compress kv| training speed| memory  \n---|---|---|---|---|---|---|---  \n\u2714| \u2714| \u2714| \u2714| \u274c| \u274c| 0.64 steps/sec| 43G  \n\u2714| \u2714| \u2714| \u2714| Zero2| \u274c| 0.66 steps/sec| 14G  \n\u2714| \u2714| \u2714| \u2714| Zero2| \u2714| 0.66 steps/sec| 15G  \n\u2714| \u2714| \u2714| \u2714| Zero2 offload| \u274c| 0.33 steps/sec| 11G  \n\u2714| \u2714| \u2714| \u2714| Zero2 offload| \u2714| 0.31 steps/sec| 12G  \n  \n### 128\u00d764\u00d764 (origin size: 512\u00d7512\u00d7512)\n\ngradient checkpointing| mixed precision| xformers| feature pre-extraction|\ndeepspeed config| compress kv| training speed| memory  \n---|---|---|---|---|---|---|---  \n\u2714| \u2714| \u2714| \u2714| \u274c| \u274c| 0.08 steps/sec| 77G  \n\u2714| \u2714| \u2714| \u2714| Zero2| \u274c| 0.08 steps/sec| 41G  \n\u2714| \u2714| \u2714| \u2714| Zero2| \u2714| 0.09 steps/sec| 36G  \n\u2714| \u2714| \u2714| \u2714| Zero2 offload| \u274c| 0.07 steps/sec| 39G  \n\u2714| \u2714| \u2714| \u2714| Zero2 offload| \u2714| 0.07 steps/sec| 33G  \n  \n## \ud83d\udca1 How to Contribute to the Open-Sora Plan Community\n\nWe greatly appreciate your contributions to the Open-Sora Plan open-source\ncommunity and helping us make it even better than it is now!\n\nFor more details, please refer to the Contribution Guidelines\n\n## \ud83d\udc4d Acknowledgement\n\n  * Latte: The main codebase we built upon and it is an wonderful video gererated model.\n  * VideoGPT: Video Generation using VQ-VAE and Transformers.\n  * DiT: Scalable Diffusion Models with Transformers.\n  * FiT: Flexible Vision Transformer for Diffusion Model.\n  * Positional Interpolation: Extending Context Window of Large Language Models via Positional Interpolation.\n\n## \ud83d\udd12 License\n\n  * See LICENSE for details.\n\n## \ud83e\udd1d Community contributors\n\n## About\n\nThis project aim to reproduce Sora (Open AI T2V model), but we only have\nlimited resource. We deeply wish the all open source community can contribute\nto this project.\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\nCustom properties\n\n### Stars\n\n6.5k stars\n\n### Watchers\n\n124 watching\n\n### Forks\n\n593 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 33\n\n\\+ 19 contributors\n\n## Languages\n\n  * Python 98.1%\n  * Shell 1.4%\n  * Other 0.5%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
