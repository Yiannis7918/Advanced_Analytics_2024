{"aid": "40010606", "title": "Ten Years and Counting: My Affair with Microservices", "url": "https://blog.allegro.tech/2024/04/ten-years-microservices.html", "domain": "allegro.tech", "votes": 2, "user": "mkosmul", "posted_at": "2024-04-12 08:52:01", "comments": 0, "source_title": "Ten Years and Counting: My Affair with Microservices", "source_text": "Ten Years and Counting: My Affair with Microservices \u00b7 allegro.tech\n\nBack to\n\nApr 12 2024\n\n# Ten Years and Counting: My Affair with Microservices\n\n  * allegro.tech blog\n\n  * Micha\u0142 Kosmulski\n\n  * Ten Years and Counting: My Affair with Microservices\n\nIn early 2024, I hit ten years at Allegro, which also happens to be how long\nI\u2019ve been working with microservices. This timespan also roughly corresponds\nto how long the company as a whole has been using them, so I think it\u2019s a good\ntime to outline the story of project Rubicon: a very ambitious gamble which\ncompletely changed how we work and what our software is like. The idea\nprobably seemed rather extreme at the time, yet I am certain that without this\nchange, Allegro would not be where it is today, or perhaps would not be there\nat all.\n\n## Background\n\nAllegro is one of the largest e-commerce sites in Central Europe, with 20\nmillion users and over 300 million offers. It was founded in 1999, originally\nwith just the Polish market in mind. The story I want to tell you starts in\n2013, a year before I joined.\n\nIn 2013, the site was already large and relevant, but its commercial success\nand further growth led to development bottlenecks emerging. The codebase was a\nmonolithic PHP application, with some auxiliary processes written in C.\nChecked out, the git monorepo weighed about 2 GB, and the number of pull\nrequests produced daily by a few hundred developers was so large that if you\nstarted a new branch in the morning, you were almost sure to get some\nconflicts if you wanted to merge in the afternoon. The system was centered\naround a single, huge database, with all the performance and architectural\nchallenges you can imagine. Tests were brittle and took ages to finish.\nDeployment was a mostly manual and thus time-consuming processes that required\nlots of attention and ran the risk of causing a serious problem in production\nif something went wrong. It was so demanding and stressful I still remember my\nteam having to run the deployment once (in place of the usual deployers) as a\nbig event.\n\n## Rubicon Rises\n\nIt was becoming clear that we would hit a wall if we continued working this\nway. So, around 2012/2013, the idea for a complete overhaul of the\narchitecture started to emerge. We began experimenting with SOA (Service-\nOriented Architecture) by creating a small side project, the so-called New\nPlatform, in PHP, as a proof-of-concept. We also decided we would start doing\nAgile, TDD, and Cloud. After a short while, on top of this, we decided to\nswitch to Java for backend development. It was becoming clear that it would be\na revolution indeed, requiring everyone to change the way they worked, and to\nswitch out the whole development ecosystem, starting with the core programming\nlanguage. Once we got this going, there would be no turning back, so a\nmatching name also appeared: Project Rubicon.\n\nThe project had such a broad scope that it even came with its own\nconstitution, a set of high-level guidelines to be used in case of doubt. It\nfocused mostly on ways of working and highlighted the value of learning (on\npersonal, team, and company level), testing, reuse, empirical approach to\nsoftware development, and active participation in the open-source community\nboth as users and as contributors. Specific technical assumptions included:\n\n  * focus on quality\n  * microservices\n  * distributed, multi-regional, active-active architecture\n  * Java\n  * cloud deployment\n  * using open-source technologies\n\nThere was also a list of success criteria for the project:\n\n  * the monolith is gone\n  * we have Java gurus on board\n  * we have services\n  * development is faster\n  * we have continuous delivery\n  * we don\u2019t have another monolith\n  * we still make money\n\nFaster development was probably the most important goal, since slow delivery\nwas the direct reason we embarked on this long journey.\n\nOn top of these lists, more detailed plans were made as well, of course. For\nexample, various parts of the system were prioritized for moving out of the\nmonolith as we were well aware we would not be able to work on everything at\nonce. Being Agile does not mean planning is to be avoided, only that plans\nhave to be flexible. So, armed with a plan, we got off to work.\n\n## Execution\n\nToo much has happened during the 10+ years to report here. The initial period\nwas really frantic since we had to set up everything, and, first of all, teams\nhad to switch to a new mindset. This was also a period of intense hiring, and\nthe time I joined the recently opened office in Warsaw. Microservices were at\nthat time only starting to gain traction, so while we used the experiences of\nothers as much has possible, we had to learn many things ourselves, sometimes\nlearning them the hard way.\n\nTo give you an idea of the pace, here are just some of the things that\nhappened in 2013:\n\n  * outline of the common architecture (service discovery, logging) created\n  * a set of common libraries created (presentation from 2016)\n  * training in Java and JVM for PHP developers\n  * recruitment of Java developers started\n  * first Java code got written\n  * fierce discussions about technology choices (Guice vs Spring, Maven vs Gradle, Jetty vs Undertow)\n\nWhat followed in 2014 (this is the part I could already experience in person):\n\n  * various self-service tools allowing developers to handle common tasks such as creating databases themselves rather than by involving specialized support teams\n  * automation tools\n  * development of Hermes, our open-source message broker built on top of Kafka, started\n  * strategic DDD training with Eric Evans\n  * migration to Java 8\n  * global architecture improvements\n  * allegro.tech, the project to coordinate the visibility of our tech division online and offline, of which this blog is a part, started\n  * SRE team created\n  * CQK (Code Quality Keepers) guild opened\n  * first Java services deployed to production\n  * intense recruitment and learning\n\nThe number of both production services and of tools supporting developers\u2019\nwork that got created thereafter is staggering. It should be clear from just\nthe list above that this was a huge investment, and could only proceed due to\nfull buy-in of both technology and business parts of the company. It was\nindeed a gamble, well-informed, but still a gamble that carried big risk\nshould it fail, but an even greater risk if we were to stay with the old\narchitecture.\n\nAt this point you probably can see that actually building microservices seems\nlike a minor part of the whole undertaking. There was a lot of work to writing\nso many parts of this huge system anew, but indeed the amount of work that we\nhad to invest into infrastructure, tooling, and learning, was immense. It was\nalso absolutely critical for the project\u2019s success. A lot has been said about\nmicroservices, and it is true that for them to be beneficial, you need the\nright scale and the right kind of system. We had both, and so the decision to\nmove to microservices proved to be worthwhile, but despite knowing the theory,\nI think no one expected the amount of auxiliary work to be so huge. Indeed,\nwhile microservices themselves may be simple, the glue that holds them\ntogether is not.\n\n## Flashbacks\n\nSummarizing ten years of rapid development is tough, so instead of trying to\ntell you the full story, I decided to share a few flashbacks: moments which I\nremembered for one reason or another.\n\n### No-ing SQL\n\nWhen refactoring our huge monolith into smaller microservices, we needed to\nalso choose the database to use for each of them. Since horizontal scalability\nwas our focus, we preferred NoSQL databases when possible. This was a big\nchange since the monolithic solution relied on a single, huge SQL database. On\ntop of that, it was not modularized well, and in many places there was little\nor no separation between domain and persistence layers. If the monolith was\nstructured well, splitting it into separate services would have been much\neasier. Unfortunately, this was not the case, so we had to perform the\ntransition to NoSQL together with other refactorings and cleanup. Usually, we\nhad to deeply remodel data and operations handling it, especially\ntransactional, so that they could be executed in the new environment. This was\noften a significant effort even if we could divide the code in such way that\nthe transaction or set of related operations ended up within the same service.\nThings became even more complicated if an operation spanned multiple services\n(and databases) in the new architecture. This is one of the reasons why\ndividing a big application into smaller chunks is much harder than it may seem\nat first.\n\nCassandra was initially our preferred NoSQL database for most tasks. Only\nafter a while did we learn that each database is good for some use cases and\nbad for others, and that we needed polyglot persistence to achieve high\nperformance and get the required flexibility in all cases. The team I worked\non was among the first Cassandra adopters at the company, and as is often the\ncase when you run something in production for the first time, we uncovered a\nnumber of issues in our Cassandra deployment which was \u201cready\u201d but not tested\nin production yet. The team responsible for the DB was learning completely new\nstuff just as we were.\n\nAn argument sometimes put up against the need to separate your application\u2019s\npersistence layer from the domain logic is \u201cyou\u2019re never going to switch out\nthe DB for another one anyway\u201d. Most of the time that\u2019s true, but in one\nservice we did have to switch from Cassandra to MongoDB after we found out our\naccess patterns were not very well aligned with Cassandra\u2019s data model. We\nmanaged to do it inside a single two-week sprint, and apart from the service\nbecoming faster, its clients would not notice any difference as the external\nAPI stayed the same. While the (usually theoretical) prospect of switching\ndatabases is not the only reason for decoupling domain and persistence layers,\nit did help a lot in this case, and it is about this time I started to\nunderstand why we were creating so many classes even though you could just\ncram all that code into one.\n\nI also managed to kill our Cassandra instance once when I was learning about\nbig data processing and created a job that was supposed to process some data\nfrom the DB. The job was so massively parallel that the barrage of requests it\ngenerated overwhelmed even Cassandra. Fortunately, this situation also showed\nthe advantage of having separate databases for each service, as only that\nsingle service experienced an outage.\n\n### Into the cloud\n\nBefore joining Allegro, I had only deployed to physical servers, so moving to\nthe cloud was a big change. At first, we deployed our services to virtual\nmachines configured in OpenStack. What a convenience it was to be able to just\nset up a complete virtual server with a few clicks rather than wait days for a\nphysical machine. We used Puppet to fully configure the virtual machines for\neach service, so while you had to write some configuration once, you could\nspin up a new server configured for your service almost instantly afterwards.\n\nThis IaaS (Infrastructure as a Service) approach was very convenient, and\nquite a change, but in many ways it still resembled what I had known before:\nyou had a machine, even if virtual, and you could ssh and run any commands\nthere if you wanted, even if it was rarely needed since Puppet set up\neverything for you.\n\nThe real revolution came when we switched to PaaS (Platform as a Service)\nmodel, at that time based on Mesos and Marathon. Suddenly, there were no more\nvirtual machines, and you could not ssh to the server where your software was\nrunning. For me, this was a real culture shock, and even though up to this\npoint I was very enthusiastic about all the cool technology we were\nintroducing, the thought of no more ssh freaked me out. How would I know what\nwas going on in the system if I couldn\u2019t even access it? Despite my\nreservations, I gradually found out you could indeed deploy and monitor\nsoftware despite not being able to access the machine via ssh. It sounds weird\nin retrospect, but this was one of the most difficult technological\ntransitions in my career.\n\nAfter a while, we built some abstraction layers on top of Mesos, including a\ncustom app console that allows you to deploy a service and perform all\nmaintenance tasks. It isolates you from most details of the underlying system,\nand is so effective that when we migrated from Mesos to Kubernetes later on,\nthe impact on most teams was much smaller that you could imagine for such a\nbig change. Our App Console is an internal project, but if you are familiar\nwith Backstage, it should give you an idea of what kind of tool we\u2019re talking\nabout here.\n\n### Monitoring\n\nBelieve it or not, initially all monitoring was centralized and handled by a\nsingle team. If you wanted to have any non-standard charts in Zabbix or any\ncustom alerts (and obviously, you wanted to), you had to create a ticket in\nJIRA, describe exactly what you wanted, and after a while, the monitoring team\nwould set it up for you. The whole process took about a week, and quite often,\nright after seeing the new chart you knew you wanted to improve it, so you\nwould file another ticket and wait another week. Needless to say, this was\nincredibly frustrating, and I consider it one of my early big successes when I\nkept pushing the monitoring team until they finally gave in and allowed\ndevelopment teams to configure all of their observability settings themselves.\n\n### Going polyglot\n\nWhile Rubicon started out with the premise of rewriting our software in Java,\nwe quickly started experimenting with other JVM languages. The team I worked\non considered Scala for a while, but after some experimentation decided\nagainst using it as our main language. Some other teams, however, did choose\nit, and even though they are a minority at Allegro, we have some microservices\nwritten in Scala to this day. On the other hand, Scala is the dominant\nlanguage at Allegro when it comes to writing Spark jobs.\n\nSome time around 2015, a teammate found out there was a relatively new, but\npromising, language called Kotlin. It so happened that we were just starting\nwork on a new microservice which was still very simple and not quite critical.\nHe decided to use it as a testbed, and within I think two days rewrote the\nwhole thing in Kotlin. Thanks to the services being independent and this one\nnot being very important yet, we could safely experiment in production and\nassess the stability of the rewritten service. Learning the language by\nwriting actual production-ready code rather than just playing with throw-away\ncode allowed us to check what advantages and disadvantages the language\noffered under realistic usage scenarios. Kotlin caught on, and gradually we\nstarted to use it for more and more new services and to use it for new\nfeatures in existing Java services as mixing the two was easy. Many services\nalready used Groovy and Spock for tests anyway. At this point, Kotlin is more\npopular than Java at Allegro, and on our blog we published some articles about\nKotlin, including one which unfortunately stirred a lot of controversy, and\ncaused a (deserved IMO) shitstorm both inside and outside the company.\n\nBesides JVM languages, we by now have also microservices written in C#, Go,\nPython, Elixir, and probably a few more languages I forgot to mention. This is\njust the backend, but our frontend architecture also allows for components\nwritten in various languages. And besides customer-facing business code, there\nare also internal tools and utilities, sometimes written using yet other\ngeneral-purpose languages and DSLs. Finally, there\u2019s the whole world of AI,\nincluding prompting for generative AI that you can also consider a programming\nlanguage of sorts.\n\nThe main point I want to make here is that using microservices has allowed us\nto safely experiment with various programming languages, to consciously limit\nthe blast radius of those experiments should anything go wrong, and to perform\nall transitions gradually. Of course, this all has a purpose: finding the best\ntool for the job, and using all the different languages\u2019 strengths where they\ncan help us most. It is not about introducing new tools just for the sake of\nit, which would cause but chaos and introduce risks related to future\nmaintenance. I think the autonomy teams get in making technical decisions, yet\ncombined with responsibility for the outcomes, is what allows us to learn and\nfind new ways of doing things while at the same time it limits the risks\nassociated with experimenting. As in many other cases, things work well when\nthe organization\u2019s ways of working (team autonomy) are aligned with technical\nsolutions (microservices).\n\n### Using antipatterns wisely\n\nGood practices are heuristics: most of the time, following them is a good\nidea. For example, two microservices should not share database tables since\nthis introduces tight coupling: you can\u2019t introduce a change to the schema and\ndeploy just one service but not the other. Your two services are not\nindependent, but form a distributed monolith instead. Avoiding such situations\nis just common sense.\n\nStill, you should always keep in mind the reasons why a good practice exists,\nwhat it protects you from and what costs it introduces. At one point we had a\ndiscussion within our team about how to best handle a peculiar performance\nissue. Our service connected to an Elasticsearch instance and performed two\nkinds of operations: reads and writes. Reads were much more numerous, but\nwrites introduced heavy load (on the service itself \u2014 Elastic could handle\nit). Writes came in bursts, so most of the time things worked well, but when a\nburst of writes arrived, performance of the whole service suffered and read\ntimes were affected. We tried various mechanisms for isolating the two kinds\nof operation, but couldn\u2019t do it effectively.\n\nA colleague suggested we split the service in two, one responsible for\nhandling reads and the other for writes. We had a long discussion, in which I\npresented arguments for having a single service as the owner of the data,\nresponsible for both reads and writes, and highlighted what issues could arise\ndue to the split. While keeping the service intact seemed to be the elegant\nthing to do, I didn\u2019t have a good solution for the performance issue. My\ncolleague\u2019s idea to split the service, on the other hand, while somewhat\nmessy, did offer a chance to solve it.\n\nSo, we decided to just try it and see whether this approach would solve the\nperformance issue and how bad the side effects would be. We did just that, and\nthe antipattern-based solution worked great: performance hiccups went away,\nand despite sharing the common Elasticsearch cluster, the two services\nremained maintainable. We were not able to fully assess this aspect right\naway, but time proved my colleague right as well: during the 3+ years we\nworked with that codebase later on, we only ran into issues related to sharing\nElasticsearch once, and we managed to fix that case quickly. It certainly did\nhelp, though, that both services kept being developed by the same team, and\nthat by the time we introduced the split, the schema was already quite stable\nand did not change often. Nonetheless, had I insisted on keeping things clean,\nwe would have probably spent much more time fighting performance issues than\nwe lost during the single issue that resulted from sharing Elasticsearch\nbetween services. Know when to use patterns, know when to use antipatterns,\nand use both wisely.\n\n### One size does not fit all\n\nI think we\u2019ve always been quite pragmatic about sizing our microservices. It\u2019s\nhard to define a set of specific rules for finding the right size, but going\ntoo far in one direction or the other causes considerable pain. Make a service\nhuge, and it becomes too hard for a single team to maintain and develop, or\nscaling issues arise similar to those you could experience with a monolith.\nMake it very small, and you might get overwhelmed by the overhead of having\nyour logic split between too many places, issues with debugging, and the\nperformance penalty of the system being distributed to the extreme.\n\nMost services I got to work on at Allegro were not too tiny, and contained\nsome non-trivial amount of logic. There were sometimes agitated discussions\nabout where to implement a certain feature, in particular whether it should be\nin an existing service or in a new one. In hindsight, I think most decisions\nmade sense, but there were certainly cases where a feature that we believed\nwould grow ended up in a new service which then never took off and remained\ntoo small, and cases where something was bolted onto an existing service\nbecause it was easier to implement this way, but which caused some pain later\non.\n\nI think I only once saw a team fall into the nanoservice trap where services\nwere designed so small the split caused more trouble than it was worth. On the\nother hand, there were certainly services which you could no longer call micro\nby any stretch. This was not necessarily a bad thing. As long as a service\nfulfilled a well-defined role, a single team was enough to take care of it,\nand it was OK that you had to deploy and scale the whole thing together,\nthings were fine. In some cases of services which grew really much too big\n(indicators being that they contained pieces of logic only very loosely\nrelated to each other, and that at some point multiple teams were regularly\ninterested in contributing), we did get back to them and split them up. It was\nnot very easy, but doable, and the second-hardest part was usually finding the\nright lines along which to divide. The only thing harder was finding the time\nto perform such operations, but with a bit of negotiation and persistence,\nafter a while we usually succeeded.\n\nThere is an ongoing discussion of whether we have too many microservices. It\u2019s\nnot an urgent thing, but there are reasons to not go too high, such as certain\ntechnical limitations in the infrastructure and the cost of overprovisioning\n(each service allocates resources such as memory or CPUs with a margin, and\nthose margins add up). Still, the fact that we are well above a thousand\nservices and yet their number is only a minor nuisance, speaks well of our\ntooling and organization. Indeed, thanks to some custom tools, creating a new\nservice is very easy (maybe too easy?), and managing those already there is\nalso quite pleasant. This is possible due to huge investments we made early on\n(and continue): we knew right from the start that while each microservice may\nbe relatively simple, a lot of complexity goes into the glue that holds the\nwhole system together. Without it, things would not quite work so well.\nAnother factor is, obviously, that our system has an actual use case for\nmicroservices: we have hundreds of teams, a system that keeps growing in\ncapacity and complexity, and scale that makes a truly distributed system\nnecessary. I think much of the anti-microservice sentiment you see around the\ninternet today stems from treating microservices as a silver bullet that you\ncan apply to any problem regardless of whether they actually make sense in\ngiven situation, or from not being aware that they can bring huge payoffs but\nalso require great investments. There is a good summary of the advantages and\ndisadvantages of microservices in this Gitlab blog post.\n\n### Service Mesh and common libraries\n\nProbably the most recent really significant change related to our microservice\necosystem was the migration to service mesh. From developers\u2019 perspective it\ndid not seem all that radical, but it required a lot of work from\ninfrastructure teams. The most important gain is the possibility to control\nsome aspects of services\u2019 behavior in a single place. For example, originally\nif you wanted to have secure connections between services, you had to support\nTLS in code, using common libraries. With service mesh, you can just enable it\nglobally without the developers even having to know. This makes maintaining\nthe huge ecosystem that consists of more than a thousand services much more\nbearable.\n\nEach microservice needs certain behaviors in order to work well within our\nenvironment. For example, it needs a healthcheck endpoint which allows\nKubernetes to tell if the service instance is working or not. We have a\nwritten Microservice Contract which defines those requirements. There are also\nfeatures that are not strictly necessary, but which many services will find\nuseful, for example various metrics. Our initial approach was to have a set of\ncommon libraries that provided both the required and many of the nice-to-have\nfeatures. Of course, if you can\u2019t or don\u2019t want to use those libraries, you\nare free to do so, as long as your service implements the Microservice\nContract some other way.\n\nOver time, the role of those libraries has changed, with the general direction\nbeing that of reducing their scope. There are several reasons.\n\nReason number one is more and more features can be moved to infrastructure\nlayer, of which Service Mesh is an important part. For example, originally\ncommunicating with another service required a service discovery client,\nimplemented in a shared library. Now, all this logic has been delegated to the\nService Mesh and requires no special support in shared libraries or service\ncode.\n\nAnother reason is that open source libraries have caught on and some features\nwe used to need to implement ourselves, such as certain metrics, are now\navailable out of the box in Spring Boot or other frameworks. There is no point\nin reinventing the wheel and having more code to maintain.\n\nFinally, the problem with libraries is that updating a library in 1000+\nservices is a slow and costly process. Meanwhile, a feature that the Service\nMesh provides can be switched on or reconfigured for all services almost\ninstantly.\n\nDespite common libraries falling out of favor with us, there are some features\nthat are hard to implement in infrastructure alone. Even with a simple feature\nsuch as logging, sometimes we need data that only code running within the\nservices has access to. When we want to fill in certain standard fields in\norder to make searching logs easier, some fields, such as host or dc, can\neasily be filled in by the infrastructure, but some, such as thread_name are\nonly known inside the service and can\u2019t be handled externally. Thus, the role\nof libraries is diminished but not completely eliminated. In order to make\nworking with shared libraries less cumbersome, we are working on ways to\nautomate upgrades as much as possible so that we can keep all versions up to\ndate without it costing too much developers\u2019 time.\n\n### Learning\n\nDuring the transition, Allegro invested in learning and development heavily.\nDaily work was full of learning opportunities since everything we were doing\nwas quite new, and many approaches and technologies were not mature yet. We\nwere really on the cutting edge of technology, so for many problems there were\nsimply no run-of-the-mill solutions yet. We were already several years into\nthe microservice transition when microservices became a global hype.\n\nSince everybody was well aware of what an ambitious plan we were pursuing, it\nwas also well understood that some things took experimenting, and while of\ncourse we were expected to ship value, there was a company-wide understanding\nthat time for learning, team tourism, trying out new things, and sometimes\nfailing, were necessary for success. One of the things I really enjoyed was\nthe focus on quality and doing things right. Business understood this as well,\nand actually at one point when Rubicon was quite advanced, developers were\ngranted a 6-month grace period during which we could focus on just technical\nchanges without having to deliver any business value. As a matter of fact,\nmany business logic changes were delivered anyway. For example, the team I was\non created a microservice-based approach to handling payments which was much\nmore flexible than the old solution, so it was not just a refactoring, but\nrather a rewrite that took new business requirements into account.\n\nApart from learning by doing, we also invested in organized training and\nconferences. We bought a number of dedicated training sessions with\nestablished experts from Silicon Valley on topics such as software\narchitecture and JVM performance. Pretty much everyone could attend at least\none good conference each year, and we also sponsored a number of developer-\ncentric events in order to gain visibility and attract good hires. About a\nyear into my job, I got to attend JavaOne in San Francisco, whose scale and\ndepth trumped even the biggest conferences I knew from Europe. After attending\na few conferences, I decided to give speaking myself a try, and was able to\ntake advantage of a number of useful trainings to help me with that. We also\nstarted the allegro.tech initiative in order to organize all the activity used\nto promote our brand, and this blog is one of the projects that we run under\nthe allegro.tech umbrella to this day.\n\n### The cycle of life\n\nIn 2022, a service I had worked on when I first started at Allegro was shut\ndown due to being replaced with a newer solution. This way, I witnessed the\nfull lifecycle of a service: building it from scratch, adding more features to\nthe mature solution, maintenance, and finally seeing it discontinued. It was\nreally a great experience to see that something I had built had run its course\nand I could be there to see the whole cycle.\n\n## Takeaways\n\nWhen we started out working with microservices, we were well aware of their\nbenefits but also of their cost. The famous You must be this tall to use\nmicroservices image adorned many of our presentations at that time. By taking\na realistic stance, we avoided many pitfalls. Our transition to the\nmicroservice world took several years, but was successful, and I am certain we\nwould be in a much worse place had the company not made that bold decision.\nApart from being a huge technical challenge, it was also a great\ntransformation in our way of thinking and in the way we work together.\nConway\u2019s Law applies and the change in system architecture was possible only\ntogether with a change in company architecture. It was also possible thanks to\nmany smart people with whom I had the pleasure to work over these years.\n\nWhen I look back, I see how far we have come. Creating a new service used to\ntake a week or two at first, and now it takes minutes. Scaling a service\nrequired a human operator, creating virtual machines, and manually adding them\nto the monitoring system. Today, an autoscaler handles most services and\ndevelopers do not even need to know that instances were added or removed. Our\ntooling is really convenient, even though there are things we could improve,\nand some components are already showing signs of aging. Nonetheless, many\nthings that used to be a challenge, are trivial today. New joiners at the\ncompany can benefit from all these conveniences right from the start, and\nsometimes I think they might not fully appreciate them since they never had to\nperform all that work manually.\n\nThe world does not stand still, though. Technologies change, and some\nassumptions we made when planning our architecture ten years ago, have already\nhad to be updated. Our system has grown, and so has the company, so many\nissues we are dealing with now are different from those that troubled us in\nthe beginning of Project Rubicon. Initially, everything was a greenfield\nproject, but by now, some places have accumulated bit rot and need cleanup.\nThe system is much bigger (which microservices enabled) so introducing changes\ngets harder (still, much easier than it would be within a monolith). And since\nten years is a lot of time, many people have moved through the company, so\nknowledge transfer and continued learning are still essential. Only change is\ncertain, and this has not changed a bit. I\u2019m happy I could experience the\nheroic age of microservices myself, and I\u2019m looking forward to whatever comes\nnext.\n\n### Micha\u0142 Kosmulski\n\nMicha\u0142 is interested in all things high-performance, whether it be low-level\nstuff like analyzing how OS caches affect I/O operations or high-level like\nproper application design and functional programming. At Allegro, he has\nworked both as a software engineer and as a team leader, in finance and\nadvertising divisions.\n\nsee 16 posts by Micha\u0142 Kosmulski\n\n  * Share this post\n\nProudly built by engineers\n\n", "frontpage": false}
