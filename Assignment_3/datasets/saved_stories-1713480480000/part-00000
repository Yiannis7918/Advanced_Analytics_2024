{"aid": "40078147", "title": "From data warehouse to data science: adopting Arrow Flight SQL", "url": "https://doris.apache.org/blog/arrow-flight-sql-in-apache-doris-for-10x-faster-data-transfer/", "domain": "apache.org", "votes": 1, "user": "ShawnL30", "posted_at": "2024-04-18 16:45:31", "comments": 0, "source_title": "Arrow Flight SQL in Apache Doris for 10X faster data transfer", "source_text": "Arrow Flight SQL in Apache Doris for 10X faster data transfer - Apache Doris\n\nSkip to main content\n\nDo you \u2764\ufe0f Doris? Give us a \ud83c\udf1f on GitHub\n\nSkip to main content\n\nBlog/Tech Sharing\n\n# Arrow Flight SQL in Apache Doris for 10X faster data transfer\n\nApache DorisApril 16, 2024\n\nFor years, JDBC and ODBC have been commonly adopted norms for database\ninteraction. Now, as we gaze upon the vast expanse of the data realm, the rise\nof data science and data lake analytics brings bigger and bigger datasets.\nCorrespondingly, we need faster and faster data reading and transmission, so\nwe start to look for better answers than JDBC and ODBC. Thus, we include Arrow\nFlight SQL protocol into Apache Doris 2.1, which provides tens-fold speedups\nfor data transfer.\n\nTip\n\nA demo of loading data from Apache Doris to Python using Arrow Flight SQL.\n\n## High-speed data transfer based on Arrow Flight SQL\n\nAs a column-oriented data warehouse, Apache Doris arranges its query results\nin the form of data Blocks in a columnar format. Before version 2.1, the\nBlocks must be serialized into bytes in row-oriented formats before they can\nbe transferred to a target client via a MySQL client or JDBC/ODBC driver.\nMoreover, if the target client is a columnar database or a column-oriented\ndata science component like Pandas, the data should then be de-serialized. The\nserialization-deserialization process is a speed bump for data transmission.\n\nApache Doris 2.1 has a data transmission channel built on Arrow Flight SQL.\n(Apache Arrow is a software development platform designed for high data\nmovement efficiency across systems and languages, and the Arrow format aims\nfor high-performance, lossless data exchange.) It allows high-speed, large-\nscale data reading from Doris via SQL in various mainstream programming\nlanguages. For target clients that also support the Arrow format, the whole\nprocess will be free of serialization/deserialization, thus no performance\nloss. Another upside is, Arrow Flight can make full use of multi-node and\nmulti-core architecture and implement parallel data transfer, which is another\nenabler of high data throughput.\n\nFor example, if a Python client reads data from Apache Doris, Doris will first\nconvert the column-oriented Blocks to Arrow RecordBatch. Then in the Python\nclient, Arrow RecordBatch will be converted to Pandas DataFrame. Both\nconversions are fast because the Doris Blocks, Arrow RecordBatch, and Pandas\nDataFrame are all column-oriented.\n\nIn addition, Arrow Flight SQL provides a general JDBC driver to facilitate\nseamless communication between databases that supports the Arrow Flight SQL\nprotocol. This unlocks the the potential of Doris to be connected to a wider\necosystem and to be used in more cases.\n\n## Performance test\n\nThe \"tens-fold speedups\" conclusion is based on our benchmark tests. We tried\nreading data from Doris using PyMySQL, Pandas, and Arrow Flight SQL, and\njotted down the durations, respectively. The test data is the ClickBench\ndataset.\n\nResults on various data types are as follows:\n\nAs shown, Arrow Flight SQL outperforms PyMySQL and Pandas in all data types by\na factor ranging from 20 to several hundreds.\n\n## Usage\n\nWith support for Arrow Flight SQL, Apache Doris can leverage the Python ADBC\nDriver for fast data reading. I will showcase a few frequently executed\ndatabase operations using the Python ADBC Driver (version 3.9 or later),\nincluding DDL, DML, session variable setting, and show statements.\n\n### 01 Install library\n\nThe relevant library is already published on PyPI. It can be installed simply\nas follows:\n\n    \n    \n    pip install adbc_driver_manager pip install adbc_driver_flightsql\n\nImport the following module/library to interact with the installed library:\n\n    \n    \n    import adbc_driver_manager import adbc_driver_flightsql.dbapi as flight_sql\n\n### 02 Connect to Doris\n\nCreate a client for interacting with the Doris Arrow Flight SQL service.\nPrerequisites include: Doris frontend (FE) host, Arrow Flight port, and login\nusername/password.\n\nConfigure parameters for Doris frontend (FE) and backend (BE):\n\n  * In fe/conf/fe.conf, set arrow_flight_sql_port to an available port, such as 9090.\n\n  * In be/conf/be.conf, set arrow_flight_port to an available port, such as 9091.\n\nSuppose that the Arrow Flight SQL services for the Doris instance will run on\nports 9090 and 9091 for FE and BE respectively, and the Doris\nusername/password is \"user\" and \"pass\", the connection process would be:\n\n    \n    \n    conn = flight_sql.connect(uri=\"grpc://127.0.0.1:9090\", db_kwargs={ adbc_driver_manager.DatabaseOptions.USERNAME.value: \"user\", adbc_driver_manager.DatabaseOptions.PASSWORD.value: \"pass\", }) cursor = conn.cursor()\n\nOnce the connection is established, you can interact with Doris using SQL\nstatements through the returned cursor object. This allows you to perform\nvarious operations such as table creation, metadata retrieval, data import,\nand query execution.\n\n### 03 Create table and retrieve metadata\n\nPass the query to the cursor.execute() function, which creates tables and\nretrieves metadata.\n\n    \n    \n    cursor.execute(\"DROP DATABASE IF EXISTS arrow_flight_sql FORCE;\") print(cursor.fetchallarrow().to_pandas())\n    \n    cursor.execute(\"create database arrow_flight_sql;\") print(cursor.fetchallarrow().to_pandas())\n    \n    cursor.execute(\"show databases;\") print(cursor.fetchallarrow().to_pandas())\n    \n    cursor.execute(\"use arrow_flight_sql;\") print(cursor.fetchallarrow().to_pandas())\n    \n    cursor.execute(\"\"\"CREATE TABLE arrow_flight_sql_test ( k0 INT, k1 DOUBLE, K2 varchar(32) NULL DEFAULT \"\" COMMENT \"\", k3 DECIMAL(27,9) DEFAULT \"0\", k4 BIGINT NULL DEFAULT '10', k5 DATE, ) DISTRIBUTED BY HASH(k5) BUCKETS 5 PROPERTIES(\"replication_num\" = \"1\");\"\"\") print(cursor.fetchallarrow().to_pandas())\n    \n    cursor.execute(\"show create table arrow_flight_sql_test;\") print(cursor.fetchallarrow().to_pandas())\n\nIf the returned StatusResult is 0, that means the query is executed\nsuccessfully. (Such design is to ensure compatibility with JDBC.)\n\n    \n    \n    StatusResult 0 0\n    \n    StatusResult 0 0\n    \n    Database 0 __internal_schema 1 arrow_flight_sql .. ... 507 udf_auth_db\n    \n    [508 rows x 1 columns]\n    \n    StatusResult 0 0\n    \n    StatusResult 0 0 Table Create Table 0 arrow_flight_sql_test CREATE TABLE `arrow_flight_sql_test` (\\n `k0`...\n\n### 04 Ingest data\n\nExecute an INSERT INTO statement to load test data into the table created:\n\n    \n    \n    cursor.execute(\"\"\"INSERT INTO arrow_flight_sql_test VALUES ('0', 0.1, \"ID\", 0.0001, 9999999999, '2023-10-21'), ('1', 0.20, \"ID_1\", 1.00000001, 0, '2023-10-21'), ('2', 3.4, \"ID_1\", 3.1, 123456, '2023-10-22'), ('3', 4, \"ID\", 4, 4, '2023-10-22'), ('4', 122345.54321, \"ID\", 122345.54321, 5, '2023-10-22');\"\"\") print(cursor.fetchallarrow().to_pandas())\n\nIf you see the following returned result, the data ingestion is successful.\n\n    \n    \n    StatusResult 0 0\n\nIf the data size to ingest is huge, you can apply the Stream Load method using\npydoris.\n\n### 05 Execute queries\n\nPerform queries on the above table, such as aggregation, sorting, and session\nvariable setting.\n\n    \n    \n    cursor.execute(\"select * from arrow_flight_sql_test order by k0;\") print(cursor.fetchallarrow().to_pandas())\n    \n    cursor.execute(\"set exec_mem_limit=2000;\") print(cursor.fetchallarrow().to_pandas())\n    \n    cursor.execute(\"show variables like \\\"%exec_mem_limit%\\\";\") print(cursor.fetchallarrow().to_pandas())\n    \n    cursor.execute(\"select k5, sum(k1), count(1), avg(k3) from arrow_flight_sql_test group by k5;\") print(cursor.fetchallarrow().to_pandas())\n\nThe results are as follows:\n\n    \n    \n    k0 k1 K2 k3 k4 k5 0 0 0.10000 ID 0.000100000 9999999999 2023-10-21 1 1 0.20000 ID_1 1.000000010 0 2023-10-21 2 2 3.40000 ID_1 3.100000000 123456 2023-10-22 3 3 4.00000 ID 4.000000000 4 2023-10-22 4 4 122345.54321 ID 122345.543210000 5 2023-10-22\n    \n    [5 rows x 6 columns]\n    \n    StatusResult 0 0\n    \n    Variable_name Value Default_Value Changed 0 exec_mem_limit 2000 2147483648 1\n    \n    k5 Nullable(Float64)_1 Int64_2 Nullable(Decimal(38, 9))_3 0 2023-10-22 122352.94321 3 40784.214403333 1 2023-10-21 0.30000 2 0.500050005\n    \n    [2 rows x 5 columns]\n\n### 06 Complete code\n\n    \n    \n    # Doris Arrow Flight SQL Test\n    \n    # step 1, library is released on PyPI and can be easily installed. # pip install adbc_driver_manager # pip install adbc_driver_flightsql import adbc_driver_manager import adbc_driver_flightsql.dbapi as flight_sql\n    \n    # step 2, create a client that interacts with the Doris Arrow Flight SQL service. # Modify arrow_flight_sql_port in fe/conf/fe.conf to an available port, such as 9090. # Modify arrow_flight_port in be/conf/be.conf to an available port, such as 9091. conn = flight_sql.connect(uri=\"grpc://127.0.0.1:9090\", db_kwargs={ adbc_driver_manager.DatabaseOptions.USERNAME.value: \"root\", adbc_driver_manager.DatabaseOptions.PASSWORD.value: \"\", }) cursor = conn.cursor()\n    \n    # interacting with Doris via SQL using Cursor def execute(sql): print(\"\\n### execute query: ###\\n \" + sql) cursor.execute(sql) print(\"### result: ###\") print(cursor.fetchallarrow().to_pandas())\n    \n    # step3, execute DDL statements, create database/table, show stmt. execute(\"DROP DATABASE IF EXISTS arrow_flight_sql FORCE;\") execute(\"show databases;\") execute(\"create database arrow_flight_sql;\") execute(\"show databases;\") execute(\"use arrow_flight_sql;\") execute(\"\"\"CREATE TABLE arrow_flight_sql_test ( k0 INT, k1 DOUBLE, K2 varchar(32) NULL DEFAULT \"\" COMMENT \"\", k3 DECIMAL(27,9) DEFAULT \"0\", k4 BIGINT NULL DEFAULT '10', k5 DATE, ) DISTRIBUTED BY HASH(k5) BUCKETS 5 PROPERTIES(\"replication_num\" = \"1\");\"\"\") execute(\"show create table arrow_flight_sql_test;\")\n    \n    # step4, insert into execute(\"\"\"INSERT INTO arrow_flight_sql_test VALUES ('0', 0.1, \"ID\", 0.0001, 9999999999, '2023-10-21'), ('1', 0.20, \"ID_1\", 1.00000001, 0, '2023-10-21'), ('2', 3.4, \"ID_1\", 3.1, 123456, '2023-10-22'), ('3', 4, \"ID\", 4, 4, '2023-10-22'), ('4', 122345.54321, \"ID\", 122345.54321, 5, '2023-10-22');\"\"\")\n    \n    # step5, execute queries, aggregation, sort, set session variable execute(\"select * from arrow_flight_sql_test order by k0;\") execute(\"set exec_mem_limit=2000;\") execute(\"show variables like \\\"%exec_mem_limit%\\\";\") execute(\"select k5, sum(k1), count(1), avg(k3) from arrow_flight_sql_test group by k5;\")\n    \n    # step6, close cursor cursor.close()\n\n## Examples of data transmission at scale\n\n### 01 Python\n\nIn Python, after connecting to Doris using the ADBC Driver, you can use\nvarious ADBC APIs to load the Clickbench dataset from Doris into Python.\nHere's how:\n\n    \n    \n    #!/usr/bin/env python # -*- coding: utf-8 -*-\n    \n    import adbc_driver_manager import adbc_driver_flightsql.dbapi as flight_sql import pandas from datetime import datetime\n    \n    my_uri = \"grpc://0.0.0.0:`fe.conf_arrow_flight_port`\" my_db_kwargs = { adbc_driver_manager.DatabaseOptions.USERNAME.value: \"root\", adbc_driver_manager.DatabaseOptions.PASSWORD.value: \"\", } sql = \"select * from clickbench.hits limit 1000000;\"\n    \n    # PEP 249 (DB-API 2.0) API wrapper for the ADBC Driver Manager. def dbapi_adbc_execute_fetchallarrow(): conn = flight_sql.connect(uri=my_uri, db_kwargs=my_db_kwargs) cursor = conn.cursor() start_time = datetime.now() cursor.execute(sql) arrow_data = cursor.fetchallarrow() dataframe = arrow_data.to_pandas() print(\"\\n##################\\n dbapi_adbc_execute_fetchallarrow\" + \", cost:\" + str(datetime.now() - start_time) + \", bytes:\" + str(arrow_data.nbytes) + \", len(arrow_data):\" + str(len(arrow_data))) print(dataframe.info(memory_usage='deep')) print(dataframe)\n    \n    # ADBC reads data into pandas dataframe, which is faster than fetchallarrow first and then to_pandas. def dbapi_adbc_execute_fetch_df(): conn = flight_sql.connect(uri=my_uri, db_kwargs=my_db_kwargs) cursor = conn.cursor() start_time = datetime.now() cursor.execute(sql) dataframe = cursor.fetch_df() print(\"\\n##################\\n dbapi_adbc_execute_fetch_df\" + \", cost:\" + str(datetime.now() - start_time)) print(dataframe.info(memory_usage='deep')) print(dataframe)\n    \n    # Can read multiple partitions in parallel. def dbapi_adbc_execute_partitions(): conn = flight_sql.connect(uri=my_uri, db_kwargs=my_db_kwargs) cursor = conn.cursor() start_time = datetime.now() partitions, schema = cursor.adbc_execute_partitions(sql) cursor.adbc_read_partition(partitions[0]) arrow_data = cursor.fetchallarrow() dataframe = arrow_data.to_pandas() print(\"\\n##################\\n dbapi_adbc_execute_partitions\" + \", cost:\" + str(datetime.now() - start_time) + \", len(partitions):\" + str(len(partitions))) print(dataframe.info(memory_usage='deep')) print(dataframe)\n    \n    dbapi_adbc_execute_fetchallarrow() dbapi_adbc_execute_fetch_df() dbapi_adbc_execute_partitions()\n\nResults are as follows (omitting the repeated outputs). It only takes 3s to\nload a Clickbench dataset containing 1 million rows and 105 columns.\n\n    \n    \n    ################## dbapi_adbc_execute_fetchallarrow, cost:0:00:03.548080, bytes:784372793, len(arrow_data):1000000 <class 'pandas.core.frame.DataFrame'> RangeIndex: 1000000 entries, 0 to 999999 Columns: 105 entries, CounterID to CLID dtypes: int16(48), int32(19), int64(6), object(32) memory usage: 2.4 GB None CounterID EventDate UserID EventTime WatchID JavaEnable Title GoodEvent ... UTMCampaign UTMContent UTMTerm FromTag HasGCLID RefererHash URLHash CLID 0 245620 2013-07-09 2178958239546411410 2013-07-09 19:30:27 8302242799508478680 1 OWAProfessionov \u2014 \u041c\u043e\u0439 \u041a\u0440\u0443\u0433 (\u0421\u0412\u0410\u041e \u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d 1 ... 0 -7861356476484644683 -2933046165847566158 0 999999 1095 2013-07-03 4224919145474070397 2013-07-03 14:36:17 6301487284302774604 0 @\u0434\u043d\u0435\u0432\u043d\u0438\u043a\u0438 Sinatra (\u041b\u0410\u0414\u0410, \u0446\u0435\u043d\u0430 \u0434\u043b\u044f \u0434\u0435\u0442\u0430\u043b\u043b\u0438 \u043a\u0442\u043e ... 1 ... 0 -296158784638538920 1335027772388499430 0\n    \n    [1000000 rows x 105 columns]\n    \n    ################## dbapi_adbc_execute_fetch_df, cost:0:00:03.611664 ################## dbapi_adbc_execute_partitions, cost:0:00:03.483436, len(partitions):1 ################## low_level_api_execute_query, cost:0:00:03.523598, stream.address:139992182177600, rows:-1, bytes:784322926, len(arrow_data):1000000 ################## low_level_api_execute_partitions, cost:0:00:03.738128streams.size:3, 1, -1\n\n### 02 JDBC\n\nThe open-source JDBC driver for the Arrow Flight SQL protocol provides\ncompatibility with the standard JDBC API. It allows most BI tools to access\nDoris via JDBC and supports high-speed transfer of Apache Arrow data.\n\nUsage of this driver is similar to using that for the MySQL protocol. You just\nneed to replace jdbc:mysql in the connection URL with jdbc:arrow-flight-sql.\nThe returned result will be in the JDBC ResultSet data structure.\n\n    \n    \n    import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.Statement;\n    \n    Class.forName(\"org.apache.arrow.driver.jdbc.ArrowFlightJdbcDriver\"); String DB_URL = \"jdbc:arrow-flight-sql://0.0.0.0:9090?useServerPrepStmts=false\" + \"&cachePrepStmts=true&useSSL=false&useEncryption=false\"; String USER = \"root\"; String PASS = \"\";\n    \n    Connection conn = DriverManager.getConnection(DB_URL, USER, PASS); Statement stmt = conn.createStatement(); ResultSet resultSet = stmt.executeQuery(\"show tables;\"); while (resultSet.next()) { String col1 = resultSet.getString(1); System.out.println(col1); }\n    \n    resultSet.close(); stmt.close(); conn.close();\n\n### 03 JAVA\n\nSimilar to that with Python, you can directly create an ADBC client with JAVA\nto read data from Doris. Firstly, you need to obtain the FlightInfo. Then, you\nconnect to each endpoint to pull the data.\n\n    \n    \n    // method one AdbcStatement stmt = connection.createStatement() stmt.setSqlQuery(\"SELECT * FROM \" + tableName) // executeQuery, two steps: // 1. Execute Query and get returned FlightInfo; // 2. Create FlightInfoReader to sequentially traverse each Endpoint; QueryResult queryResult = stmt.executeQuery()\n    \n    // method two AdbcStatement stmt = connection.createStatement() stmt.setSqlQuery(\"SELECT * FROM \" + tableName) // Execute Query and parse each Endpoint in FlightInfo, and use the Location and Ticket to construct a PartitionDescriptor partitionResult = stmt.executePartitioned(); partitionResult.getPartitionDescriptors() //Create ArrowReader for each PartitionDescriptor to read data ArrowReader reader = connection2.readPartition(partitionResult.getPartitionDescriptors().get(0).getDescriptor()))\n\n### 04 Spark\n\nFor Spark users, apart from connecting to Flight SQL Server using JDBC and\nJAVA, you can apply the Spark-Flight-Connector, which enables Spark to act as\na client for reading and writing data from/to a Flight SQL Server. This is\nmade possible by the fast data conversion between the Arrow format and the\nBlock in Apache Doris, which is 10 times faster than the conversion between\nCSV and Block. Moreover, the Arrow data format provides more comprehensive and\nrobust support for complex data types such as Map and Array.\n\n## Hop on the trend train\n\nA number of enterprise users of Doris has tried loading data from Doris to\nPython, Spark, and Flink using Arrow Flight SQL and enjoyed much faster data\nreading speed. In the future, we plan to include the support for Arrow Flight\nSQL in data writing, too. By then, most systems built with mainstream\nprogramming languages will be able to read and write data from/to Apache Doris\nby an ADBC client. That's high-speed data interaction which opens up numerous\npossibilities. On our to-do list, we also envision leveraging Arrow Flight to\nimplement parallel data reading by multiple backends and facilitate federated\nqueries across Doris and Spark.\n\nDownload Apache Doris 2.1 and get a taste of 100 times faster data transfer\npowered by Arrow Flight SQL. If you need assistance, come find us in the\nApache Doris developer and user community.\n\nRecent posts\n\nAuto-increment columns in databases: a simple magic that makes a big\ndifferenceAnother big leap: Apache Doris 2.1.0 is releasedVariant in Apache\nDoris 2.1.0: a new data type 8 times faster than JSON for semi-structured data\nanalysisBreaking down data silos with a unified data warehouse: an Apache\nDoris-based CDP\n\nView all blogs\n\nJoin DiscussionOn This Page\n\n  * High-speed data transfer based on Arrow Flight SQL\n  * Performance test\n  * Usage\n\n    * 01 Install library\n    * 02 Connect to Doris\n    * 03 Create table and retrieve metadata\n    * 04 Ingest data\n    * 05 Execute queries\n    * 06 Complete code\n  * Examples of data transmission at scale\n\n    * 01 Python\n    * 02 JDBC\n    * 03 JAVA\n    * 04 Spark\n  * Hop on the trend train\n\nASF\n\n  * Foundation\n  * License\n  * Events\n  * Sponsorship\n  * Privacy\n  * Security\n  * Thanks\n\nResources\n\n  * Download\n  * Docs\n  * Blog\n  * Ecosystem\n  * Users\n  * Discussions\n\nCommunity\n\n  * How to contribute\n  * Source code\n  * Improvement proposal\n  * Doris team\n  * Roadmap\n\nJoin the community\n\nConnect on WeChat\n\nCopyright \u00a9 2024 The Apache Software Foundation,Licensed under the Apache\nLicense, Version 2.0. Apache, Doris, Apache Doris, the Apache feather logo and\nthe Apache Doris logo are trademarks of The Apache Software Foundation.\n\n", "frontpage": false}
