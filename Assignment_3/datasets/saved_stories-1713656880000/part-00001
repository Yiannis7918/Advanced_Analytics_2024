{"aid": "40099176", "title": "Multimodal Integration", "url": "https://en.wikipedia.org/wiki/Multisensory_integration", "domain": "wikipedia.org", "votes": 1, "user": "downboots", "posted_at": "2024-04-20 17:46:01", "comments": 0, "source_title": "Multisensory integration", "source_text": "Multisensory integration - Wikipedia\n\nJump to content\n\nSearch\n\n# Multisensory integration\n\n  * \u0627\u0644\u0639\u0631\u0628\u064a\u0629\n  * \u0420\u0443\u0441\u0441\u043a\u0438\u0439\n\nEdit links\n\nFrom Wikipedia, the free encyclopedia\n\nStudy of senses and nervous system\n\nMultisensory integration, also known as multimodal integration, is the study\nof how information from the different sensory modalities (such as sight,\nsound, touch, smell, self-motion, and taste) may be integrated by the nervous\nsystem.^[1] A coherent representation of objects combining modalities enables\nanimals to have meaningful perceptual experiences. Indeed, multisensory\nintegration is central to adaptive behavior because it allows animals to\nperceive a world of coherent perceptual entities.^[2] Multisensory integration\nalso deals with how different sensory modalities interact with one another and\nalter each other's processing.\n\n## General introduction[edit]\n\nMultimodal perception is how animals form coherent, valid, and robust\nperception by processing sensory stimuli from various modalities. Surrounded\nby multiple objects and receiving multiple sensory stimulations, the brain is\nfaced with the decision of how to categorize the stimuli resulting from\ndifferent objects or events in the physical world. The nervous system is thus\nresponsible for whether to integrate or segregate certain groups of signals.\nMultimodal perception has been widely studied in cognitive science, behavioral\nscience, and neuroscience.\n\n### Stimuli and sensory modalities[edit]\n\nThere are four attributes of stimulus: modality, intensity, location, and\nduration. The neocortex in the mammalian brain has parcellations that\nprimarily process sensory input from one modality. For example, primary visual\narea, V1, or primary somatosensory area, S1. These areas mostly deal with low-\nlevel stimulus features such as brightness, orientation, intensity, etc. These\nareas have extensive connections to each other as well as to higher\nassociation areas that further process the stimuli and are believed to\nintegrate sensory input from various modalities. However, multisensory effects\nhave been shown to occur in primary sensory areas as well.^[3]\n\n### Binding problem[edit]\n\nMain article: Binding problem\n\nThe relationship between the binding problem and multisensory perception can\nbe thought of as a question \u2013 the binding problem \u2013 and its potential solution\n\u2013 multisensory perception. The binding problem stemmed from unanswered\nquestions about how mammals (particularly higher primates) generate a unified,\ncoherent perception of their surroundings from the cacophony of\nelectromagnetic waves, chemical interactions, and pressure fluctuations that\nforms the physical basis of the world around us. It was investigated initially\nin the visual domain (colour, motion, depth, and form), then in the auditory\ndomain, and recently in the multisensory areas. It can be said therefore, that\nthe binding problem is central to multisensory perception.^[4]\n\nHowever, considerations of how unified conscious representations are formed\nare not the full focus of multisensory Integration research. It is obviously\nimportant for the senses to interact in order to maximize how efficiently\npeople interact with the environment. For perceptual experience and behavior\nto benefit from the simultaneous stimulation of multiple sensory modalities,\nintegration of the information from these modalities is necessary. Some of the\nmechanisms mediating this phenomenon and its subsequent effects on cognitive\nand behavioural processes will be examined hereafter. Perception is often\ndefined as one's conscious experience, and thereby combines inputs from all\nrelevant senses and prior knowledge. Perception is also defined and studied in\nterms of feature extraction, which is several hundred milliseconds away from\nconscious experience. Notwithstanding the existence of Gestalt psychology\nschools that advocate a holistic approach to the operation of the\nbrain,^[5]^[6] the physiological processes underlying the formation of\npercepts and conscious experience have been vastly understudied. Nevertheless,\nburgeoning neuroscience research continues to enrich our understanding of the\nmany details of the brain, including neural structures implicated in\nmultisensory integration such as the superior colliculus (SC)^[7] and various\ncortical structures such as the superior temporal gyrus (GT) and visual and\nauditory association areas. Although the structure and function of the SC are\nwell known, the cortex and the relationship between its constituent parts are\npresently the subject of much investigation. Concurrently, the recent impetus\non integration has enabled investigation into perceptual phenomena such as the\nventriloquism effect,^[8] rapid localization of stimuli and the McGurk\neffect;^[9] culminating in a more thorough understanding of the human brain\nand its functions.\n\n### History[edit]\n\nStudies of sensory processing in humans and other animals has traditionally\nbeen performed one sense at a time,^[10] and to the present day, numerous\nacademic societies and journals are largely restricted to considering sensory\nmodalities separately ('Vision Research', 'Hearing Research' etc.). However,\nthere is also a long and parallel history of multisensory research. An example\nis the Stratton's (1896) experiments on the somatosensory effects of wearing\nvision-distorting prism glasses.^[11]^[12] Multisensory interactions or\ncrossmodal effects in which the perception of a stimulus is influenced by the\npresence of another type of stimulus are referred since very early in the\npast. They were reviewed by Hartmann^[13] in a fundamental book where, among\nseveral references to different types of multisensory interactions, reference\nis made to the work of Urbantschitsch in 1888^[14] who reported on the\nimprovement of visual acuity by auditive stimuli in subjects with damaged\nbrains. This effect was also found later in individuals with undamaged brains\nby Krakov^[15] and Hartmann,^[16] as well as the fact that the visual acuity\ncould be improved by other type of stimuli.^[16] It is also noteworthy the\namount of work in the early 1930s on intersensory relations in the Soviet\nUnion, reviewed by London.^[17] A remarkable multisensory research is the\nextensive and pioneering work of Gonzalo ^[18] in the mid-20th century on the\ncharacterization of a multisensory syndrome in patients with parieto-occipital\ncortical lesions. In this syndrome, all the sensory functions are affected,\nand with symmetric bilaterality, in spite of being a unilateral lesion where\nthe primary areas were not involved. A feature of this syndrome is the great\npermeability to crossmodal effects between visual, tactile, auditive stimuli\nas well as muscular effort to improve the perception, also decreasing the\nreaction times. The improvement by crossmodal effect was found to be greater\nas the primary stimulus to be perceived was weaker, and as the cortical lesion\nwas greater (Vol 1 and 2 of reference^[18]). This author interpreted these\nphenomena under a dynamic physiological concept, and from a model based on\nfunctional gradients through the cortex and scaling laws of dynamical systems,\nthus highlighting the functional unity of the cortex. According to the\nfunctional cortical gradients, the specificity of the cortex would be\ndistributed in gradation, and the overlap of different specific gradients\nwould be related to multisensory interactions.^[19] Multisensory research has\nrecently gained enormous interest and popularity.\n\n### Example of spatial and structural congruence[edit]\n\nWhen we hear a car honk, we would determine which car triggers the honk by\nwhich car we see is the spatially closest to the honk. It's a spatially\ncongruent example by combining visual and auditory stimuli. On the other hand,\nthe sound and the pictures of a TV program would be integrated as structurally\ncongruent by combining visual and auditory stimuli. However, if the sound and\nthe pictures did not meaningfully fit, we would segregate the two stimuli.\nTherefore, spatial or structural congruence comes from not only combining the\nstimuli but is also determined by our understanding.\n\n## Theories and approaches[edit]\n\n### Visual dominance[edit]\n\nLiterature on spatial crossmodal biases suggests that visual modality often\ninfluences information from other senses.^[20] Some research indicates that\nvision dominates what we hear, when varying the degree of spatial congruency.\nThis is known as the ventriloquist effect.^[21] In cases of visual and haptic\nintegration, children younger than 8 years of age show visual dominance when\nrequired to identify object orientation. However, haptic dominance occurs when\nthe factor to identify is object size.^[22]^[23]\n\n### Modality appropriateness[edit]\n\nAccording to Welch and Warren (1980), the Modality Appropriateness Hypothesis\nstates that the influence of perception in each modality in multisensory\nintegration depends on that modality's appropriateness for the given task.\nThus, vision has a greater influence on integrated localization than hearing,\nand hearing and touch have a greater bearing on timing estimates than\nvision.^[24]^[25]\n\nMore recent studies refine this early qualitative account of multisensory\nintegration. Alais and Burr (2004), found that following progressive\ndegradation in the quality of a visual stimulus, participants' perception of\nspatial location was determined progressively more by a simultaneous auditory\ncue.^[26] However, they also progressively changed the temporal uncertainty of\nthe auditory cue; eventually concluding that it is the uncertainty of\nindividual modalities that determine to what extent information from each\nmodality is considered when forming a percept.^[26] This conclusion is similar\nin some respects to the 'inverse effectiveness rule'. The extent to which\nmultisensory integration occurs may vary according to the ambiguity of the\nrelevant stimuli. In support of this notion, a recent study shows that weak\nsenses such as olfaction can even modulate the perception of visual\ninformation as long as the reliability of visual signals is adequately\ncompromised.^[27]\n\n### Bayesian integration[edit]\n\nThe theory of Bayesian integration is based on the fact that the brain must\ndeal with a number of inputs, which vary in reliability.^[28] In dealing with\nthese inputs, it must construct a coherent representation of the world that\ncorresponds to reality. The Bayesian integration view is that the brain uses a\nform of Bayesian inference.^[29] This view has been backed up by computational\nmodeling of such a Bayesian inference from signals to coherent representation,\nwhich shows similar characteristics to integration in the brain.^[29]\n\n#### Cue combination vs. causal inference models[edit]\n\nWith the assumption of independence between various sources, the traditional\ncue combination model is successful in modality integration. However,\ndepending on the discrepancies between modalities, there might be different\nforms of stimuli fusion: integration, partial integration, and segregation. To\nfully understand the other two types, we have to use causal inference model\nwithout the assumption as cue combination model. This freedom gives us general\ncombination of any numbers of signals and modalities by using Bayes' rule to\nmake causal inference of sensory signals.^[30]\n\n#### The hierarchical vs. non-hierarchical models[edit]\n\nThe difference between two models is that hierarchical model can explicitly\nmake causal inference to predict certain stimulus while non-hierarchical model\ncan only predict joint probability of stimuli. However, hierarchical model is\nactually a special case of non-hierarchical model by setting joint prior as a\nweighted average of the prior to common and independent causes, each weighted\nby their prior probability. Based on the correspondence of these two models,\nwe can also say that hierarchical is a mixture modal of non-hierarchical\nmodel.\n\n#### Independence of likelihoods and priors[edit]\n\nFor Bayesian model, the prior and likelihood generally represent the\nstatistics of the environment and the sensory representations. The\nindependence of priors and likelihoods is not assured since the prior may vary\nwith likelihood only by the representations. However, the independence has\nbeen proved by Shams with series of parameter control in multi sensory\nperception experiment.^[31]\n\n### Shared intentionality[edit]\n\nThe shared intentionality approach proposes the holistic explanation of the\nneurophysiological processes underlying the formation of percepts and\nconscious experience. The psychological construct of shared intentionality was\nintroduced at the end of 20th century.^[32]^[33]^[34] Michael Tomasello\ndeveloped it to explain cognition beginning in the earlier developmental stage\nthrough unaware collaboration in mother-child dyads.^[35]^[36] Over the past\ntwenty years, knowledge of this notion has evolved through observing shared\nintentionality from different perspectives, e.g., psychophysiology,^[37]^[38]\nand neurobiology.^[39] According to Igor Val Danilov, shared intentionality\nenables the mother-child pair to share the essential sensory stimulus of the\nactual cognitive problem.^[40] The hypothesis of neurophysiological processes\noccurring during shared intentionality explains its integrative complexity\nfrom neuronal to interpersonal dynamics levels.^[41] This collaborative\ninteraction provides environmental learning of the immature organism, starting\nat the reflexes stage of development, for processing the organization,\nidentification, and interpretation of sensory information in developing\nperception.^[42] From this perspective, Shared intentionality contributes to\nthe formation of percepts and conscious experiences, solving the binding\nproblem, or at least complements one of the mechanisms noted above.^[43]\n\n## Principles[edit]\n\nThe contributions of Barry Stein, Alex Meredith, and their colleagues\n(e.g.\"The merging of the senses\" 1993,^[44]) are widely considered to be the\ngroundbreaking work in the modern field of multisensory integration. Through\ndetailed long-term study of the neurophysiology of the superior colliculus,\nthey distilled three general principles by which multisensory integration may\nbest be described.\n\n  * The spatial rule^[45]^[46] states that multisensory integration is more likely or stronger when the constituent unisensory stimuli arise from approximately the same location.\n  * The temporal rule^[46]^[47] states that multisensory integration is more likely or stronger when the constituent unisensory stimuli arise at approximately the same time.\n  * The principle of inverse effectiveness^[48]^[49] states that multisensory integration is more likely or stronger when the constituent unisensory stimuli evoke relatively weak responses when presented in isolation.\n\n### Perceptual and behavioral consequences[edit]\n\nA unimodal approach dominated scientific literature until the beginning of\nthis century. Although this enabled rapid progression of neural mapping, and\nan improved understanding of neural structures, the investigation of\nperception remained relatively stagnant, with a few exceptions. The recent\nrevitalized enthusiasm into perceptual research is indicative of a substantial\nshift away from reductionism and toward gestalt methodologies. Gestalt theory,\ndominant in the late 19th and early 20th centuries espoused two general\nprinciples: the 'principle of totality' in which conscious experience must be\nconsidered globally, and the 'principle of psychophysical isomorphism' which\nstates that perceptual phenomena are correlated with cerebral activity. Just\nthese ideas were already applied by Justo Gonzalo in his work of brain\ndynamics, where a sensory-cerebral correspondence is considered in the\nformulation of the \"development of the sensory field due to a psychophysical\nisomorphism\" (pag. 23 of the English translation of ref.^[19]). Both ideas\n'principle of totality' and 'psychophysical isomorphism' are particularly\nrelevant in the current climate and have driven researchers to investigate the\nbehavioural benefits of multisensory integration.\n\n### Decreasing sensory uncertainty[edit]\n\nIt has been widely acknowledged that uncertainty in sensory domains results in\nan increased dependence of multisensory integration.^[26] Hence, it follows\nthat cues from multiple modalities that are both temporally and spatially\nsynchronous are viewed neurally and perceptually as emanating from the same\nsource. The degree of synchrony that is required for this 'binding' to occur\nis currently being investigated in a variety of approaches.^[50] The\nintegrative function only occurs to a point beyond which the subject can\ndifferentiate them as two opposing stimuli. Concurrently, a significant\nintermediate conclusion can be drawn from the research thus far. Multisensory\nstimuli that are bound into a single percept, are also bound on the same\nreceptive fields of multisensory neurons in the SC and cortex.^[26]\n\n### Decreasing reaction time[edit]\n\nResponses to multiple simultaneous sensory stimuli can be faster than\nresponses to the same stimuli presented in isolation. Hershenson (1962)\npresented a light and tone simultaneously and separately, and asked human\nparticipants to respond as rapidly as possible to them. As the asynchrony\nbetween the onsets of both stimuli was varied, it was observed that for\ncertain degrees of asynchrony, reaction times were decreased.^[51] These\nlevels of asynchrony were quite small, perhaps reflecting the temporal window\nthat exists in multisensory neurons of the SC. Further studies have analysed\nthe reaction times of saccadic eye movements;^[52] and more recently\ncorrelated these findings to neural phenomena.^[53] In patients studied by\nGonzalo,^[18] with lesions in the parieto-occipital cortex, the decrease in\nthe reaction time to a given stimulus by means of intersensory facilitation\nwas shown to be very remarkable.\n\n#### Redundant target effects[edit]\n\nThe redundant target effect is the observation that people typically respond\nfaster to double targets (two targets presented simultaneously) than to either\nof the targets presented alone. This difference in latency is termed the\nredundancy gain (RG).^[54]\n\nIn a study done by Forster, Cavina-Pratesi, Aglioti, and Berlucchi (2001),\nnormal observers responded faster to simultaneous visual and tactile stimuli\nthan to single visual or tactile stimuli. RT to simultaneous visual and\ntactile stimuli was also faster than RT to simultaneous dual visual or tactile\nstimuli. The advantage for RT to combined visual-tactile stimuli over RT to\nthe other types of stimulation could be accounted for by intersensory neural\nfacilitation rather than by probability summation. These effects can be\nascribed to the convergence of tactile and visual inputs onto neural centers\nwhich contain flexible multisensory representations of body parts.^[55]\n\n### Multisensory illusions[edit]\n\n#### McGurk effect[edit]\n\nIt has been found that two converging bimodal stimuli can produce a perception\nthat is not only different in magnitude than the sum of its parts, but also\nquite different in quality. In a classic study labeled the McGurk effect,^[56]\na person's phoneme production was dubbed with a video of that person speaking\na different phoneme.^[57] The result was the perception of a third, different\nphoneme. McGurk and MacDonald (1976) explained that phonemes such as ba, da,\nka, ta, ga and pa can be divided into four groups, those that can be visually\nconfused, i.e. (da, ga, ka, ta) and (ba and pa), and those that can be audibly\nconfused. Hence, when ba \u2013 voice and ga lips are processed together, the\nvisual modality sees ga or da, and the auditory modality hears ba or da,\ncombining to form the percept da.^[56]\n\n#### Ventriloquism[edit]\n\nVentriloquism has been used as the evidence for the modality appropriateness\nhypothesis. The ventriloquism effect is the situation in which auditory\nlocation perception is shifted toward a visual cue. The original study\ndescribing this phenomenon was conducted by Howard and Templeton, (1966) after\nwhich several studies have replicated and built upon the conclusions they\nreached.^[58] In conditions in which the visual cue is unambiguous, visual\ncapture reliably occurs. Thus to test the influence of sound on perceived\nlocation, the visual stimulus must be progressively degraded.^[26]\nFurthermore, given that auditory stimuli are more attuned to temporal changes,\nrecent studies have tested the ability of temporal characteristics to\ninfluence the spatial location of visual stimuli. Some types of EVP \u2013\nelectronic voice phenomenon, mainly the ones using sound bubbles are\nconsidered a kind of modern ventriloquism technique and is played by the use\nof sophisticated software, computers and sound equipment.\n\n#### Double-flash illusion[edit]\n\nThe double flash illusion was reported as the first illusion to show that\nvisual stimuli can be qualitatively altered by audio stimuli.^[59] In the\nstandard paradigm participants are presented combinations of one to four\nflashes accompanied by zero to 4 beeps. They were then asked to say how many\nflashes they perceived. Participants perceived illusory flashes when there\nwere more beeps than flashes. fMRI studies have shown that there is crossmodal\nactivation in early, low level visual areas, which was qualitatively similar\nto the perception of a real flash. This suggests that the illusion reflects\nsubjective perception of the extra flash.^[60] Further, studies suggest that\ntiming of multisensory activation in unisensory cortexes is too fast to be\nmediated by a higher order integration suggesting feed forward or lateral\nconnections.^[61] One study has revealed the same effect but from vision to\naudition, as well as fission rather than fusion effects, although the level of\nthe auditory stimulus was reduced to make it less salient for those illusions\naffecting audition.^[62]\n\n#### Rubber hand illusion[edit]\n\nIn the rubber hand illusion (RHI),^[63] human participants view a dummy hand\nbeing stroked with a paintbrush, while they feel a series of identical\nbrushstrokes applied to their own hand, which is hidden from view. If this\nvisual and tactile information is applied synchronously, and if the visual\nappearance and position of the dummy hand is similar to one's own hand, then\npeople may feel that the touches on their own hand are coming from the dummy\nhand, and even that the dummy hand is, in some way, their own hand.^[63] This\nis an early form of body transfer illusion. The RHI is an illusion of vision,\ntouch, and posture (proprioception), but a similar illusion can also be\ninduced with touch and proprioception.^[64] It has also been found that the\nillusion may not require tactile stimulation at all, but can be completely\ninduced using mere vision of the rubber hand being in a congruent posture with\nthe hidden real hand.^[65] The very first report of this kind of illusion may\nhave been as early as 1937 (Tastevin, 1937).^[66]^[67]^[68]\n\n#### Body transfer illusion[edit]\n\nBody transfer illusion typically involves the use of virtual reality devices\nto induce the illusion in the subject that the body of another person or being\nis the subject's own body.\n\n## Neural mechanisms[edit]\n\n### Subcortical areas[edit]\n\n#### Superior colliculus[edit]\n\nSuperior colliculus\n\nThe superior colliculus (SC) or optic tectum (OT) is part of the tectum,\nlocated in the midbrain, superior to the brainstem and inferior to the\nthalamus. It contains seven layers of alternating white and grey matter, of\nwhich the superficial contain topographic maps of the visual field; and deeper\nlayers contain overlapping spatial maps of the visual, auditory and\nsomatosensory modalities.^[69] The structure receives afferents directly from\nthe retina, as well as from various regions of the cortex (primarily the\noccipital lobe), the spinal cord and the inferior colliculus. It sends\nefferents to the spinal cord, cerebellum, thalamus and occipital lobe via the\nlateral geniculate nucleus (LGN). The structure contains a high proportion of\nmultisensory neurons and plays a role in the motor control of orientation\nbehaviours of the eyes, ears and head.^[53]\n\nReceptive fields from somatosensory, visual and auditory modalities converge\nin the deeper layers to form a two-dimensional multisensory map of the\nexternal world. Here, objects straight ahead are represented caudally and\nobjects on the periphery are represented rosterally. Similarly, locations in\nsuperior sensory space are represented medially, and inferior locations are\nrepresented laterally.^[44]\n\nHowever, in contrast to simple convergence, the SC integrates information to\ncreate an output that differs from the sum of its inputs. Following a\nphenomenon labelled the 'spatial rule', neurons are excited if stimuli from\nmultiple modalities fall on the same or adjacent receptive fields, but are\ninhibited if the stimuli fall on disparate fields.^[70] Excited neurons may\nthen proceed to innervate various muscles and neural structures to orient an\nindividual's behaviour and attention toward the stimulus. Neurons in the SC\nalso adhere to the 'temporal rule', in which stimulation must occur within\nclose temporal proximity to excite neurons. However, due to the varying\nprocessing time between modalities and the relatively slower speed of sound to\nlight, it has been found the neurons may be optimally excited when stimulated\nsome time apart.^[71]\n\n#### Putamen[edit]\n\nSingle neurons in the macaque putamen have been shown to have visual and\nsomatosensory responses closely related to those in the polysensory zone of\nthe premotor cortex and area 7b in the parietal lobe.^[72]^[73]\n\n### Cortical areas[edit]\n\nMultisensory neurons exist in a large number of locations, often integrated\nwith unimodal neurons. They have recently been discovered in areas previously\nthought to be modality specific, such as the somatosensory cortex; as well as\nin clusters at the borders between the major cerebral lobes, such as the\noccipito-parietal space and the occipito-temporal space.^[74]^[53]^[75]\n\nHowever, in order to undergo such physiological changes, there must exist\ncontinuous connectivity between these multisensory structures. It is generally\nagreed that information flow within the cortex follows a hierarchical\nconfiguration.^[76] Hubel and Wiesel showed that receptive fields and thus the\nfunction of cortical structures, as one proceeds out from V1 along the visual\npathways, become increasingly complex and specialized.^[76] From this it was\npostulated that information flowed outwards in a feed-forward fashion; the\ncomplex end products eventually binding to form a percept. However, via fMRI\nand intracranial recording technologies, it has been observed that the\nactivation time of successive levels of the hierarchy does not correlate with\na feed-forward structure. That is, late activation has been observed in the\nstriate cortex, markedly after activation of the prefrontal cortex in response\nto the same stimulus.^[77]\n\nComplementing this, afferent nerve fibres have been found that project to\nearly visual areas such as the lingual gyrus from late in the dorsal (action)\nand ventral (perception) visual streams, as well as from the auditory\nassociation cortex.^[78] Feedback projections have also been observed in the\nopossum directly from the auditory association cortex to V1.^[76] This last\nobservation currently highlights a point of controversy within the\nneuroscientific community. Sadato et al. (2004) concluded, in line with\nBernstein et al. (2002), that the primary auditory cortex (A1) was\nfunctionally distinct from the auditory association cortex, in that it was\nvoid of any interaction with the visual modality. They hence concluded that A1\nwould not at all be effected by cross modal plasticity.^[79]^[80] This concurs\nwith Jones and Powell's (1970) contention that primary sensory areas are\nconnected only to other areas of the same modality.^[81]\n\nIn contrast, the dorsal auditory pathway, projecting from the temporal lobe is\nlargely concerned with processing spatial information, and contains receptive\nfields that are topographically organized. Fibers from this region project\ndirectly to neurons governing corresponding receptive fields in V1.^[76] The\nperceptual consequences of this have not yet been empirically acknowledged.\nHowever, it can be hypothesized that these projections may be the precursors\nof increased acuity and emphasis of visual stimuli in relevant areas of\nperceptual space. Consequently, this finding rejects Jones and Powell's (1970)\nhypothesis^[81] and thus is in conflict with Sadato et al.'s (2004)\nfindings.^[79] A resolution to this discrepancy includes the possibility that\nprimary sensory areas can not be classified as a single group, and thus may be\nfar more different from what was previously thought.\n\nThe multisensory syndrome with symmetric bilaterality, characterized by\nGonzalo and called by this author 'central syndrome of the cortex',^[18]^[19]\nwas originated from a unilateral parieto-occipital cortical lesion equidistant\nfrom the visual, tactile, and auditory projection areas (the middle of area\n19, the anterior part of area 18 and the most posterior of area 39, in\nBrodmann terminology) that was called 'central zone'. The gradation observed\nbetween syndromes led this author to propose a functional gradient scheme in\nwhich the specificity of the cortex is distributed with a continuous\nvariation,^[19] the overlap of the specific gradients would be high or maximum\nin that 'central zone'.\n\n#### Frontal lobe[edit]\n\nThis section provides insufficient context for those unfamiliar with the\nsubject. Please help improve the article by providing more context for the\nreader. (July 2013) (Learn how and when to remove this template message)  \n---  \n  \nArea F4 in macaques\n\nArea F5 in macaques^[82]^[83]\n\nPolysensory zone of premotor cortex (PZ) in macaques^[84]\n\n#### Occipital lobe[edit]\n\nThis section provides insufficient context for those unfamiliar with the\nsubject. Please help improve the article by providing more context for the\nreader. (July 2013) (Learn how and when to remove this template message)  \n---  \n  \nPrimary visual cortex (V1)^[85]\n\nLingual gyrus in humans\n\nLateral occipital complex (LOC), including lateral occipital tactile visual\narea (LOtv)^[86]\n\n#### Parietal lobe[edit]\n\nThis section provides insufficient context for those unfamiliar with the\nsubject. Please help improve the article by providing more context for the\nreader. (July 2013) (Learn how and when to remove this template message)  \n---  \n  \nVentral intraparietal sulcus (VIP) in macaques^[82]\n\nLateral intraparietal sulcus (LIP) in macaques^[82]\n\nArea 7b in macaques^[87]\n\nSecond somatosensory cortex (SII)^[88]\n\n#### Temporal lobe[edit]\n\nThis section provides insufficient context for those unfamiliar with the\nsubject. Please help improve the article by providing more context for the\nreader. (July 2013) (Learn how and when to remove this template message)  \n---  \n  \nPrimary auditory cortex (A1)\n\nSuperior temporal cortex (STG/STS/PT) Audio visual cross modal interactions\nare known to occur in the auditory association cortex which lies directly\ninferior to the Sylvian fissure in the temporal lobe.^[79] Plasticity was\nobserved in the superior temporal gyrus (STG) by Petitto et al. (2000).^[89]\nHere, it was found that the STG was more active during stimulation in native\ndeaf signers compared to hearing non signers. Concurrently, further research\nhas revealed differences in the activation of the Planum temporale (PT) in\nresponse to non linguistic lip movements between the hearing and deaf; as well\nas progressively increasing activation of the auditory association cortex as\npreviously deaf participants gain hearing experience via a cochlear\nimplant.^[79]\n\nAnterior ectosylvian sulus (AES) in cats^[90]^[91]^[92]\n\nRostral lateral suprasylvian sulcus (rLS) in cats^[91]\n\n### Cortical-subcortical interactions[edit]\n\nThe most significant interaction between these two systems (corticotectal\ninteractions) is the connection between the anterior ectosylvian sulcus (AES),\nwhich lies at the junction of the parietal, temporal and frontal lobes, and\nthe SC. The AES is divided into three unimodal regions with multisensory\nneurons at the junctions between these sections.^[93] (Jiang & Stein, 2003).\nNeurons from the unimodal regions project to the deep layers of the SC and\ninfluence the multiplicative integration effect. That is, although they can\nreceive inputs from all modalities as normal, the SC can not enhance or\ndepress the effect of multisensory stimulation without input from the\nAES.^[93]\n\nConcurrently, the multisensory neurons of the AES, although also integrally\nconnected to unimodal AES neurons, are not directly connected to the SC. This\npattern of division is reflected in other areas of the cortex, resulting in\nthe observation that cortical and tectal multisensory systems are somewhat\ndissociated.^[94] Stein, London, Wilkinson and Price (1996) analysed the\nperceived luminance of an LED in the context of spatially disparate auditory\ndistracters of various types. A significant finding was that a sound increased\nthe perceived brightness of the light, regardless of their relative spatial\nlocations, provided the light's image was projected onto the fovea.^[95] Here,\nthe apparent lack of the spatial rule, further differentiates cortical and\ntectal multisensory neurons. Little empirical evidence exists to justify this\ndichotomy. Nevertheless, cortical neurons governing perception, and a separate\nsub cortical system governing action (orientation behavior) is synonymous with\nthe perception action hypothesis of the visual stream.^[96] Further\ninvestigation into this field is necessary before any substantial claims can\nbe made.\n\n### Dual \"what\" and \"where\" multisensory routes[edit]\n\nResearch suggests the existence of two multisensory routes for \"what\" and\n\"where\". The \"what\" route identifying the identity of things involving area\nBrodmann area 9 in the right inferior frontal gyrus and right middle frontal\ngyrus, Brodmann area 13 and Brodmann area 45 in the right insula-inferior\nfrontal gyrus area, and Brodmann area 13 bilaterally in the insula. The\n\"where\" route detecting their spatial attributes involving the Brodmann area\n40 in the right and left inferior parietal lobule and the Brodmann area 7 in\nthe right precuneus-superior parietal lobule and Brodmann area 7 in the left\nsuperior parietal lobule.^[97]\n\n## Development of multisensory operations[edit]\n\n### Theories of development[edit]\n\nAll species equipped with multiple sensory systems, utilize them in an\nintegrative manner to achieve action and perception.^[44] However, in most\nspecies, especially higher mammals and humans, the ability to integrate\ndevelops in parallel with physical and cognitive maturity. Children until\ncertain ages do not show mature integration patterns.^[98]^[99] Classically,\ntwo opposing views that are principally modern manifestations of the\nnativist/empiricist dichotomy have been put forth. The integration\n(empiricist) view states that at birth, sensory modalities are not at all\nconnected. Hence, it is only through active exploration that plastic changes\ncan occur in the nervous system to initiate holistic perceptions and actions.\nConversely, the differentiation (nativist) perspective asserts that the young\nnervous system is highly interconnected; and that during development,\nmodalities are gradually differentiated as relevant connections are rehearsed\nand the irrelevant are discarded.^[100]\n\nUsing the SC as a model, the nature of this dichotomy can be analysed. In the\nnewborn cat, deep layers of the SC contain only neurons responding to the\nsomatosensory modality. Within a week, auditory neurons begin to occur, but it\nis not until two weeks after birth that the first multisensory neurons appear.\nFurther changes continue, with the arrival of visual neurons after three\nweeks, until the SC has achieved its fully mature structure after three to\nfour months. Concurrently in species of monkey, newborns are endowed with a\nsignificant complement of multisensory cells; however, along with cats there\nis no integration effect apparent until much later.^[53] This delay is thought\nto be the result of the relatively slower development of cortical structures\nincluding the AES; which as stated above, is essential for the existence of\nthe integration effect.^[93]\n\nFurthermore, it was found by Wallace (2004) that cats raised in a light\ndeprived environment had severely underdeveloped visual receptive fields in\ndeep layers of the SC.^[53] Although, receptive field size has been shown to\ndecrease with maturity, the above finding suggests that integration in the SC\nis a function of experience. Nevertheless, the existence of visual\nmultisensory neurons, despite a complete lack of visual experience, highlights\nthe apparent relevance of nativist viewpoints. Multisensory development in the\ncortex has been studied to a lesser extent, however a similar study to that\npresented above was performed on cats whose optic nerves had been severed.\nThese cats displayed a marked improvement in their ability to localize stimuli\nthrough audition; and consequently also showed increased neural connectivity\nbetween V1 and the auditory cortex.^[76] Such plasticity in early childhood\nallows for greater adaptability, and thus more normal development in other\nareas for those with a sensory deficit.\n\nIn contrast, following the initial formative period, the SC does not appear to\ndisplay any neural plasticity. Despite this, habituation and sensititisation\nover the long term is known to exist in orientation behaviors. This apparent\nplasticity in function has been attributed to the adaptability of the AES.\nThat is, although neurons in the SC have a fixed magnitude of output per unit\ninput, and essentially operate an all or nothing response, the level of neural\nfiring can be more finely tuned by variations in input by the AES.\n\nAlthough there is evidence for either perspective of the\nintegration/differentiation dichotomy, a significant body of evidence also\nexists for a combination of factors from either view. Thus, analogous to the\nbroader nativist/empiricist argument, it is apparent that rather than a\ndichotomy, there exists a continuum, such that the integration and\ndifferentiation hypotheses are extremes at either end.\n\n### Psychophysical development of integration[edit]\n\nNot much is known about the development of the ability to integrate multiple\nestimates such as vision and touch.^[98] Some multisensory abilities are\npresent from early infancy, but it is not until children are eight years or\nolder before they use multiple modalities to reduce sensory uncertainty.^[98]\n\nOne study demonstrated that cross-modal visual and auditory integration is\npresent from within 1 year of life.^[101] This study measured response time\nfor orientating towards a source. Infants who were 8\u201310 months old showed\nsignificantly decreased response times when the source was presented through\nboth visual and auditory information compared to a single modality. Younger\ninfants, however, showed no such change in response times to these different\nconditions. Indeed, the results of the study indicates that children\npotentially have the capacity to integrate sensory sources at any age.\nHowever, in certain cases, for example visual cues, intermodal integration is\navoided.^[98] Another study found that cross-modal integration of touch and\nvision for distinguishing size and orientation is available from at least 8\nyears of age.^[99] For pre-integration age groups, one sense dominates\ndepending on the characteristic discerned (see visual dominance).^[99]\n\nA study investigating sensory integration within a single modality (vision)\nfound that it cannot be established until age 12 and above.^[98] This\nparticular study assessed the integration of disparity and texture cues to\nresolve surface slant. Though younger age groups showed a somewhat better\nperformance when combining disparity and texture cues compared to using only\ndisparity or texture cues, this difference was not statistically\nsignificant.^[98] In adults, the sensory integration can be mandatory, meaning\nthat they no longer have access to the individual sensory sources.^[102]\n\nAcknowledging these variations, many hypotheses have been established to\nreflect why these observations are task-dependent. Given that different senses\ndevelop at different rates, it has been proposed that cross-modal integration\ndoes not appear until both modalities have reached maturity.^[99]^[103] The\nhuman body undergoes significant physical transformation throughout childhood.\nNot only is there growth in size and stature (affecting viewing height), but\nthere is also change in inter-ocular distance and eyeball length. Therefore,\nsensory signals need to be constantly re-evaluated to appreciate these various\nphysiological changes.^[99] Some support comes from animal studies that\nexplore the neurobiology behind integration. Adult monkeys have deep inter-\nneuronal connections within the superior colliculus providing strong,\naccelerated visuo-auditory integration.^[104] Young animals conversely, do not\nhave this enhancement until unimodal properties are fully\ndeveloped.^[105]^[106]\n\nAdditionally, to rationalize sensory dominance, Gori et al. (2008) advocates\nthat the brain utilises the most direct source of information during sensory\nimmaturity.^[99] In this case, orientation is primarily a visual\ncharacteristic. It can be derived directly from the object image that forms on\nthe retina, irrespective of other visual factors. In fact, data shows that a\nfunctional property of neurons within primate visual cortices' are their\ndiscernment to orientation.^[107] In contrast, haptic orientation judgements\nare recovered through collaborated patterned stimulations, evidently an\nindirect source susceptible to interference. Likewise, when size is concerned\nhaptic information coming from positions of the fingers is more immediate.\nVisual-size perceptions, alternatively, have to be computed using parameters\nsuch as slant and distance. Considering this, sensory dominance is a useful\ninstinct to assist with calibration. During sensory immaturity, the more\nsimple and robust information source could be used to tweak the accuracy of\nthe alternate source.^[99] Follow-up work by Gori et al. (2012) showed that,\nat all ages, vision-size perceptions are near perfect when viewing objects\nwithin the haptic workspace (i.e. at arm's reach).^[108] However, systematic\nerrors in perception appeared when the object was positioned beyond this\nzone.^[109] Children younger than 14 years tend to underestimate object size,\nwhereas adults overestimated. However, if the object was returned to the\nhaptic workspace, those visual biases disappeared.^[108] These results support\nthe hypothesis that haptic information may educate visual perceptions. If\nsources are used for cross-calibration they cannot, therefore, be combined\n(integrated). Maintaining access to individual estimates is a trade-off for\nextra plasticity over accuracy, which could be beneficial in retrospect to the\ndeveloping body.^[99]^[103]\n\nAlternatively, Ernst (2008) advocates that efficient integration initially\nrelies upon establishing correspondence \u2013 which sensory signals belong\ntogether.^[103] Indeed, studies have shown that visuo-haptic integration fails\nin adults when there is a perceived spatial separation, suggesting sensory\ninformation is coming from different targets.^[110] Furthermore, if the\nseparation can be explained, for example viewing an object through a mirror,\nintegration is re-established and can even be optimal.^[111]^[112] Ernst\n(2008) suggests that adults can obtain this knowledge from previous\nexperiences to quickly determine which sensory sources depict the same target,\nbut young children could be deficient in this area.^[103] Once there is a\nsufficient bank of experiences, confidence to correctly integrate sensory\nsignals can then be introduced in their behaviour.\n\nLastly, Nardini et al. (2010) recently hypothesised that young children have\noptimized their sensory appreciation for speed over accuracy.^[98] When\ninformation is presented in two forms, children may derive an estimate from\nthe fastest available source, subsequently ignoring the alternate, even if it\ncontains redundant information. Nardini et al. (2010) provides evidence that\nchildren's (aged 6 years) response latencies are significantly lower when\nstimuli are presented in multi-cue over single-cue conditions.^[98]\nConversely, adults showed no change between these conditions. Indeed, adults\ndisplay mandatory fusion of signals, therefore they can only ever aim for\nmaximum accuracy.^[98]^[102] However, the overall mean latencies for children\nwere not faster than adults, which suggests that speed optimization merely\nenable them to keep up with the mature pace. Considering the haste of real-\nworld events, this strategy may prove necessary to counteract the general\nslower processing of children and maintain effective vision-action\ncoupling.^[113]^[114]^[115] Ultimately the developing sensory system may\npreferentially adapt for different goals \u2013 speed and detecting sensory\nconflicts \u2013 those typical of objective learning.\n\nThe late development of efficient integration has also been investigated from\ncomputational point of view.^[116] Daee et al. (2014) showed that having one\ndominant sensory source at early age, rather than integrating all sources,\nfacilitates the overall development of cross-modal integrations.\n\n### Applications[edit]\n\n#### Prosthesis[edit]\n\nProsthetics designers should carefully consider the nature of dimensionality\nalteration of sensorimotor signaling from and to the CNS when designing\nprosthetic devices. As reported in literatures, neural signaling from the CNS\nto the motors is organized in a way that the dimensionalities of the signals\nare gradually increased as you approach the muscles, also called muscle\nsynergies. In the same principal, but in opposite ordering, on the other hand,\nsignals dimensionalities from the sensory receptors are gradually integrated,\nalso called sensory synergies, as they approaches the CNS. This bow tie like\nsignaling formation enables the CNS to process abstract yet valuable\ninformation only. Such as process will decrease complexity of the data, handle\nthe noises and guarantee to the CNS the optimum energy consumption. Although\nthe current commercially available prosthetic devices mainly focusing in\nimplementing the motor side by simply uses EMG sensors to switch between\ndifferent activation states of the prosthesis. Very limited works have\nproposed a system to involve by integrating the sensory side. The integration\nof tactile sense and proprioception is regarded as essential for implementing\nthe ability to perceive environmental input.^[117]\n\n#### Visual rehabilitation[edit]\n\nMultisensory integration has also been shown to ameliorate visual hemianopia.\nThrough the repeated presentation of multisensory stimuli in the blind\nhemifield, the ability to respond to purely visual stimuli gradually returns\nto that hemifield in a central to peripheral manner. These benefits persist\neven after the explicit multisensory training ceases.^[118]\n\n## See also[edit]\n\n  * Body transfer illusion\n  * Sensory processing disorder\n  * Synesthesia\n\n## References[edit]\n\n  1. ^ Stein, BE.; Stanford, TR.; Rowland, BA. (Dec 2009). \"The neural basis of multisensory integration in the midbrain: its organization and maturation\". Hear Res. 258 (1\u20132): 4\u201315. doi:10.1016/j.heares.2009.03.012. PMC 2787841. PMID 19345256.\n  2. ^ Lewkowicz DJ, Ghazanfar AA (November 2009). \"The emergence of multisensory systems through perceptual narrowing\" (PDF). Trends Cogn. Sci. (Regul. Ed.). 13 (11): 470\u20138. CiteSeerX 10.1.1.554.4323. doi:10.1016/j.tics.2009.08.004. PMID 19748305. S2CID 14289579.\n  3. ^ Lemus L, Hern\u00e1ndez A, Luna R, Zainos A, Romo R (July 2010). \"Do sensory cortices process more than one sensory modality during perceptual judgements?\". Neuron. 67 (2): 335\u201348. doi:10.1016/j.neuron.2010.06.015. PMID 20670839. S2CID 16043442.\n  4. ^ Zmigrod, S.; Hommel, B. (Jan 2010). \"Temporal dynamics of unimodal and multimodal feature binding\" (PDF). Atten Percept Psychophys. 72 (1): 142\u201352. doi:10.3758/APP.72.1.142. PMID 20045885. S2CID 7055915.\n  5. ^ Wagemans, J.; Elder, JH.; Kubovy, M.; Palmer, SE.; Peterson, MA.; Singh, M.; von der Heydt, R. (Nov 2012). \"A century of Gestalt psychology in visual perception: I. Perceptual grouping and figure-ground organization\". Psychol Bull. 138 (6): 1172\u2013217. CiteSeerX 10.1.1.452.8394. doi:10.1037/a0029333. PMC 3482144. PMID 22845751.\n  6. ^ Wagemans, J.; Feldman, J.; Gepshtein, S.; Kimchi, R.; Pomerantz, JR.; van der Helm, PA.; van Leeuwen, C. (Nov 2012). \"A century of Gestalt psychology in visual perception: II. Conceptual and theoretical foundations\". Psychol Bull. 138 (6): 1218\u201352. doi:10.1037/a0029334. PMC 3728284. PMID 22845750.\n  7. ^ Stein, BE.; Rowland, BA. (2011). \"Organization and plasticity in multisensory integration\". Enhancing Performance for Action and Perception - Multisensory Integration, Neuroplasticity and Neuroprosthetics, Part I. Progress in Brain Research. Vol. 191. pp. 145\u201363. doi:10.1016/B978-0-444-53752-2.00007-2. ISBN 9780444537522. PMC 3245961. PMID 21741550. {{cite book}}: |journal= ignored (help)\n  8. ^ Recanzone, GH. (Dec 2009). \"Interactions of auditory and visual stimuli in space and time\". Hear Res. 258 (1\u20132): 89\u201399. doi:10.1016/j.heares.2009.04.009. PMC 2787663. PMID 19393306.\n  9. ^ Smith, E.; Duede, S.; Hanrahan, S.; Davis, T.; House, P.; Greger, B. (2013). \"Seeing is believing: neural representations of visual stimuli in human auditory cortex correlate with illusory auditory perceptions\". PLOS ONE. 8 (9): e73148. Bibcode:2013PLoSO...873148S. doi:10.1371/journal.pone.0073148. PMC 3762867. PMID 24023823.\n  10. ^ Fodor, Jerry A. (1983). Modularity of mind: an essay on faculty psychology. Cambridge, Mass: MIT Press. ISBN 978-0-262-06084-4. OCLC 551957787.\n  11. ^ Stratton, George M. (1896). \"Some preliminary experiments on vision without inversion of the retinal image\". Psychological Review. 3 (6): 611\u2013617. doi:10.1037/h0072918.\n  12. ^ Stratton, George M. (1897). \"Vision without inversion of the retinal image\". Psychological Review. 4 (4): 341\u2013360, 463\u2013481. doi:10.1037/h0075482.\n  13. ^ Hartmann, G.M. (1935). Gestalt Psychology. New York: The Ronald Press.\n  14. ^ Urbantschitsch, V. (1888). \"\u00dcber den Einfluss einer Sinneserregung auf die \u00fcbrigen Sinnesempfindungen\". Pfl\u00fcgers Archiv. 42: 154\u2013182. doi:10.1007/bf01669354. S2CID 42136599.\n  15. ^ Kravkov, S.V. (1930). \"\u00dcber die Abh\u00e4ngigkeit der Sehsch\u00e4rfe vom Schallreiz\". Arch. Ophthalmol. 124 (2): 334\u2013338. doi:10.1007/bf01853661. S2CID 30040170.\n  16. ^ Jump up to: ^a ^b Hartmann, G.W. (1933). \"Changes in Visual Acuity through Simultaneous Stimulation of Other Sense Organs\". J. Exp. Psychol. 16 (3): 393\u2013407. doi:10.1037/h0074549.\n  17. ^ London, I.D. (1954). \"Research of sensory interaction in the Soviet Union\". Psychol. Bull. 51 (6): 531\u2013568. doi:10.1037/h0056730. PMID 13215683.\n  18. ^ Jump up to: ^a ^b ^c ^d Gonzalo, J. (1945, 1950, 1952, 2010, 2023). Din\u00e1mica Cerebral, Open Access. Edici\u00f3n facs\u00edmil 2010 del Vol. 1 1945, Vol. 2 1950 (Madrid: Inst. S. Ram\u00f3n y Cajal, CSIC), Suplemento I 1952 (Trab. Inst. Cajal Invest. Biol.) y 1a ed. Suplemento II 2010. Red Tem\u00e1tica en Tecnolog\u00edas de Computaci\u00f3n Artificial/Natural (RTNAC) y Universidad de Santiago de Compostela (USC). ISBN 978-84-9887-458-7. Brain Dynamics, English edition 2023 (Vols. 1 and 2, Supplements I and II), I. Gonzalo-Fonrodona (ed.) Editorial CSIC, Open Access.\n  19. ^ Jump up to: ^a ^b ^c ^d Gonzalo, J. (1952). \"Las funciones cerebrales humanas seg\u00fan nuevos datos y bases fisiol\u00f3gicas. Una introducci\u00f3n a los estudios de Din\u00e1mica Cerebral\". Trabajos del Inst. Cajal de Investigaciones Biol\u00f3gicas, XLIV: pp. 95\u2013157. It is the Supplement I of Brain Dynamics, English edition 2023 (Vols. 1 and 2, Supplements I and II), I. Gonzalo-Fonrodona (ed.) Editorial CSIC, Open Access.\n  20. ^ Witten, IB.; Knudsen, EI. (Nov 2005). \"Why seeing is believing: merging auditory and visual worlds\". Neuron. 48 (3): 489\u201396. doi:10.1016/j.neuron.2005.10.020. PMID 16269365. S2CID 17244783.\n  21. ^ Shams, L.; Beierholm, UR. (Sep 2010). \"Causal inference in perception\". Trends Cogn Sci. 14 (9): 425\u201332. doi:10.1016/j.tics.2010.07.001. PMID 20705502. S2CID 7750709.\n  22. ^ Gori, M.; Del Viva, M.; Sandini, G.; Burr, DC. (May 2008). \"Young children do not integrate visual and haptic form information\" (PDF). Curr Biol. 18 (9): 694\u20138. doi:10.1016/j.cub.2008.04.036. PMID 18450446. S2CID 13899031.\n  23. ^ Gori, M.; Sandini, G.; Burr, D. (2012). \"Development of visuo-auditory integration in space and time\". Front Integr Neurosci. 6: 77. doi:10.3389/fnint.2012.00077. PMC 3443931. PMID 23060759.\n  24. ^ Welch RB, Warren DH (November 1980). \"Immediate perceptual response to intersensory discrepancy\". Psychol Bull. 88 (3): 638\u201367. doi:10.1037/0033-2909.88.3.638. PMID 7003641.\n  25. ^ Lederman, Susan J.; Klatzky, Roberta L. (2004). \"Multisensory Texture Perception\". In Calvert, Gemma A.; Spence, Charles; Stein, Barry E. (eds.). The Handbook of Multisensory Processing. Cambridge, MA: MIT Press. pp. 107\u2013122. ISBN 978-0-262-03321-3.\n  26. ^ Jump up to: ^a ^b ^c ^d ^e Alais D, Burr D (February 2004). \"The ventriloquist effect results from near-optimal bimodal integration\". Curr. Biol. 14 (3): 257\u201362. CiteSeerX 10.1.1.220.4159. doi:10.1016/j.cub.2004.01.029. PMID 14761661. S2CID 3125842.\n  27. ^ Kuang, S.; Zhang, T. (2014). \"Smelling directions: Olfaction modulates ambiguous visual motion perception\". Scientific Reports. 4: 5796. Bibcode:2014NatSR...4E5796K. doi:10.1038/srep05796. PMC 4107342. PMID 25052162.\n  28. ^ Deneve S, Pouget A (2004). \"Bayesian multisensory integration and cross-modal spatial links\" (PDF). J. Physiol. Paris. 98 (1\u20133): 249\u201358. CiteSeerX 10.1.1.133.7694. doi:10.1016/j.jphysparis.2004.03.011. PMID 15477036. S2CID 9831111.\n  29. ^ Jump up to: ^a ^b Pouget A, Deneve S, Duhamel JR (September 2002). \"A computational perspective on the neural basis of multisensory spatial representations\". Nature Reviews Neuroscience. 3 (9): 741\u20137. doi:10.1038/nrn914. PMID 12209122. S2CID 1035721.\n  30. ^ Vilares, I.; Kording, K. (Apr 2011). \"Bayesian models: the structure of the world, uncertainty, behavior, and the brain\". Annals of the New York Academy of Sciences. 1224 (1): 22\u201339. Bibcode:2011NYASA1224...22V. doi:10.1111/j.1749-6632.2011.05965.x. PMC 3079291. PMID 21486294.\n  31. ^ Beierholm, UR.; Quartz, SR.; Shams, L. (2009). \"Bayesian priors are encoded independently from likelihoods in human multisensory perception\". J Vis. 9 (5): 23.1\u20139. doi:10.1167/9.5.23. PMID 19757901.\n  32. ^ Gilbert M. (1989). On social facts. London: Routledge.\n  33. ^ Searle JR. (1992). The rediscovery of the mind. London: MIT Press.\n  34. ^ Tuomela R. (1995). The importance of us. SUP. Stanford, CA: Stanford University Press.\n  35. ^ Tomasello, M. (1999). The Cultural Origins of Human Cognition. Cambridge, Massachusetts: Harvard University Press.\n  36. ^ Tomasello, M. (2019). Becoming Human: A Theory of Ontogeny. Cambridge, Massachusetts: Harvard University Press.\n  37. ^ McClung, J. S., Plac\u00ec, S., Bangerter, A., Cl\u00e9ment, F., & Bshary, R. (2017). \"The language of cooperation: shared intentionality drives variation in helping as a function of group membership.\" Proceedings of the Royal Society B: Biological Sciences, 284(1863), 20171682. http://dx.doi.org/10.1098/rspb.2017.1682.\n  38. ^ Shteynberg, G., & Galinsky, A. D. (2011). \"Implicit coordination: Sharing goals with similar others intensifies goal pursuit.\" Journal of Experimental Social Psychology, 47(6), 1291-1294., https://doi.org/10.1016/j.jesp. 2011.04.012.\n  39. ^ Fishburn, F. A., Murty, V. P., Hlutkowsky, C. O., MacGillivray, C. E., Bemis, L. M., Murphy, M. E., ... & Perlman, S. B. (2018). \"Putting our heads together: interpersonal neural synchronization as a biological mechanism for shared intentionality.\" Social cognitive and affective neuroscience, 13(8), 841-849.\n  40. ^ Val Danilov I. (2023). \"Theoretical Grounds of Shared Intentionality for Neuroscience in Developing Bioengineering Systems.\" OBM Neurobiology 2023; 7(1): 156; doi:10.21926/obm.neurobiol.2301156\n  41. ^ Val Danilov, I. & Mihailova S. (2021). \"Neuronal Coherence Agent for Shared Intentionality: A Hypothesis of Neurobiological Processes Occurring during Social Interaction.\" OBM Neurobiology 2021;5(4):26; doi:10.21926/obm.neurobiol.2104113\n  42. ^ Val Danilov I. (2023).\"Low-Frequency Oscillations for Nonlocal Neuronal Coupling in Shared Intentionality Before and After Birth: Toward the Origin of Perception.\" OBM Neurobiology 2023; 7(4): 192; doi:10.21926/obm.neurobiol.2304192. https://www.lidsen.com/journals/neurobiology/neurobiology-07-04-192\n  43. ^ Val Danilov I. (2023). \"Shared Intentionality Modulation at the Cell Level: Low-Frequency Oscillations for Temporal Coordination in Bioengineering Systems.\" OBM Neurobiology 2023; 7(4): 185; doi:10.21926/obm.neurobiol.2304185. https://www.lidsen.com/journals/neurobiology/neurobiology-07-04-185\n  44. ^ Jump up to: ^a ^b ^c Stein, Barry; Meredith, M. Alex (1993). The merging of the senses. Cambridge, Mass: MIT Press. ISBN 978-0-262-19331-3. OCLC 25869284.\n  45. ^ Meredith, MA.; Stein, BE. (Feb 1986). \"Spatial factors determine the activity of multisensory neurons in cat superior colliculus\". Brain Res. 365 (2): 350\u20134. doi:10.1016/0006-8993(86)91648-3. PMID 3947999. S2CID 12807282.\n  46. ^ Jump up to: ^a ^b King AJ, Palmer AR (1985). \"Integration of visual and auditory information in bimodal neurones in the guinea-pig superior colliculus\". Exp Brain Res. 60 (3): 492\u2013500. doi:10.1007/bf00236934. PMID 4076371. S2CID 25796198.\n  47. ^ Meredith, MA.; Nemitz, JW.; Stein, BE. (Oct 1987). \"Determinants of multisensory integration in superior colliculus neurons. I. Temporal factors\". J Neurosci. 7 (10): 3215\u201329. doi:10.1523/JNEUROSCI.07-10-03215.1987. PMC 6569162. PMID 3668625.\n  48. ^ Meredith MA, Stein BE (July 1983). \"Interactions among converging sensory inputs in the superior colliculus\". Science. 221 (4608): 389\u201391. Bibcode:1983Sci...221..389M. doi:10.1126/science.6867718. PMID 6867718.\n  49. ^ Meredith, MA.; Stein, BE. (Sep 1986). \"Visual, auditory, and somatosensory convergence on cells in superior colliculus results in multisensory integration\". J Neurophysiol. 56 (3): 640\u201362. doi:10.1152/jn.1986.56.3.640. PMID 3537225.\n  50. ^ Cervantes Constantino, F.; S\u00e1nchez-Costa, T.; Cipriani, G.A.; Carboni, A. (2023). \"Visuospatial attention revamps cortical processing of sound amid audiovisual uncertainty\". Psychophysiology. 60 (10): e14329. doi:10.1111/psyp.14329. PMID 37166096. S2CID 258617930.\n  51. ^ Hershenson M (March 1962). \"Reaction time as a measure of intersensory facilitation\". J Exp Psychol. 63 (3): 289\u201393. doi:10.1037/h0039516. PMID 13906889.\n  52. ^ Hughes, HC.; Reuter-Lorenz, PA.; Nozawa, G.; Fendrich, R. (Feb 1994). \"Visual-auditory interactions in sensorimotor processing: saccades versus manual responses\". J Exp Psychol Hum Percept Perform. 20 (1): 131\u201353. doi:10.1037/0096-1523.20.1.131. PMID 8133219.\n  53. ^ Jump up to: ^a ^b ^c ^d ^e Wallace, Mark T. (2004). \"The development of multisensory processes\". Cognitive Processing. 5 (2): 69\u201383. doi:10.1007/s10339-004-0017-z. ISSN 1612-4782. S2CID 16710851.\n  54. ^ Ridgway N, Milders M, Sahraie A (May 2008). \"Redundant target effect and the processing of colour and luminance\". Exp Brain Res. 187 (1): 153\u201360. doi:10.1007/s00221-008-1293-0. PMID 18264703. S2CID 23092762.\n  55. ^ Forster B, Cavina-Pratesi C, Aglioti SM, Berlucchi G (April 2002). \"Redundant target effect and intersensory facilitation from visual-tactile interactions in simple reaction time\". Exp Brain Res. 143 (4): 480\u20137. doi:10.1007/s00221-002-1017-9. PMID 11914794. S2CID 115844.\n  56. ^ Jump up to: ^a ^b McGurk H, MacDonald J (1976). \"Hearing lips and seeing voices\". Nature. 264 (5588): 746\u20138. Bibcode:1976Natur.264..746M. doi:10.1038/264746a0. PMID 1012311. S2CID 4171157.\n  57. ^ Nath, AR.; Beauchamp, MS. (Jan 2012). \"A neural basis for interindividual differences in the McGurk effect, a multisensory speech illusion\". NeuroImage. 59 (1): 781\u20137. doi:10.1016/j.neuroimage.2011.07.024. PMC 3196040. PMID 21787869.\n  58. ^ Hairston WD, Wallace MT, Vaughan JW, Stein BE, Norris JL, Schirillo JA (January 2003). \"Visual localization ability influences cross-modal bias\". J Cogn Neurosci. 15 (1): 20\u20139. doi:10.1162/089892903321107792. PMID 12590840. S2CID 13636325.\n  59. ^ Shams L, Kamitani Y, Shimojo S (December 2000). \"Illusions. What you see is what you hear\". Nature. 408 (6814): 788. Bibcode:2000Natur.408..788S. doi:10.1038/35048669. PMID 11130706. S2CID 205012107.\n  60. ^ Watkins S, Shams L, Josephs O, Rees G (August 2007). \"Activity in human V1 follows multisensory perception\". NeuroImage. 37 (2): 572\u20138. doi:10.1016/j.neuroimage.2007.05.027. PMID 17604652. S2CID 17477883.\n  61. ^ Shams L, Iwaki S, Chawla A, Bhattacharya J (April 2005). \"Early modulation of visual cortex by sound: an MEG study\". Neurosci. Lett. 378 (2): 76\u201381. doi:10.1016/j.neulet.2004.12.035. PMID 15774261. S2CID 4675944.\n  62. ^ Andersen TS, Tiippana K, Sams M (November 2004). \"Factors influencing audiovisual fission and fusion illusions\". Brain Res Cogn Brain Res. 21 (3): 301\u2013308. doi:10.1016/j.cogbrainres.2004.06.004. PMID 15511646.\n  63. ^ Jump up to: ^a ^b Botvinick M, Cohen J (February 1998). \"Rubber hands 'feel' touch that eyes see\" (PDF). Nature. 391 (6669): 756. Bibcode:1998Natur.391..756B. doi:10.1038/35784. PMID 9486643. S2CID 205024516.\n  64. ^ Ehrsson HH, Holmes NP, Passingham RE (November 2005). \"Touching a rubber hand: feeling of body ownership is associated with activity in multisensory brain areas\". J. Neurosci. 25 (45): 10564\u201373. doi:10.1523/JNEUROSCI.0800-05.2005. PMC 1395356. PMID 16280594.\n  65. ^ Samad M, Chung A, Shams L (February 2015). \"Perception of Body Ownership is Driven by Bayesian Sensory Inference\". PLOS ONE. 10 (2): e0117178. Bibcode:2015PLoSO..1017178S. doi:10.1371/journal.pone.0117178. PMC 4320053. PMID 25658822.\n  66. ^ Holmes NP, Crozier G, Spence C (June 2004). \"When mirrors lie: \"visual capture\" of arm position impairs reaching performance\" (PDF). Cogn Affect Behav Neurosci. 4 (2): 193\u2013200. doi:10.3758/CABN.4.2.193. PMC 1314973. PMID 15460925.\n  67. ^ J. Tastevin (Feb 1937). \"En partant de l'exp\u00e9rience d'Aristote: Les d\u00e9placements artificiels des parties du corps ne sont pas suivis par le sentiment de ces parties ni par les sensations qu'on peut y produire\" [Starting from Aristotle's experiment: The artificial displacements of parts of the body are not followed by feeling in these parts or by the sensations which can be produced there]. L'Encephale [fr] (in French). 32 (2): 57\u201384. (English abstract)\n  68. ^ J. Tastevin (Mar 1937). \"En partant de l'exp\u00e9rience d'Aristote\". L'Encephale (in French). 32 (3): 140\u2013158.\n  69. ^ Bergman, Ronald A.; Afifi, Adel K. (2005). Functional neuroanatomy: text and atlas. New York: McGraw-Hill. ISBN 978-0-07-140812-7. OCLC 475017241.\n  70. ^ Giard MH, Peronnet F (September 1999). \"Auditory-visual integration during multimodal object recognition in humans: a behavioral and electrophysiological study\". J Cogn Neurosci. 11 (5): 473\u201390. doi:10.1162/089892999563544. PMID 10511637. S2CID 5735865.\n  71. ^ Miller LM, D'Esposito M (June 2005). \"Perceptual fusion and stimulus coincidence in the cross-modal integration of speech\". J. Neurosci. 25 (25): 5884\u201393. doi:10.1523/JNEUROSCI.0896-05.2005. PMC 6724802. PMID 15976077.\n  72. ^ Graziano MS, Gross CG (1993). \"A bimodal map of space: somatosensory receptive fields in the macaque putamen with corresponding visual receptive fields\" (PDF). Exp Brain Res. 97 (1): 96\u2013109. doi:10.1007/BF00228820. PMID 8131835. S2CID 9387557.\n  73. ^ Gentile, G.; Petkova, VI.; Ehrsson, HH. (Feb 2011). \"Integration of visual and tactile signals from the hand in the human brain: an FMRI study\". J Neurophysiol. 105 (2): 910\u201322. doi:10.1152/jn.00840.2010. PMC 3059180. PMID 21148091.\n  74. ^ Thesen, T., Vibell, J., Calvert, G.A., & Osterbauer, R. (2004). Neuroimaging of multisensory processing in vision, audition, touch and olfaction. Cognitive Processing, 5, 84-93.\n  75. ^ Wallace MT, Ramachandran R, Stein BE (February 2004). \"A revised view of sensory cortical parcellation\". Proc. Natl. Acad. Sci. U.S.A. 101 (7): 2167\u201372. Bibcode:2004PNAS..101.2167W. doi:10.1073/pnas.0305697101. PMC 357070. PMID 14766982.\n  76. ^ Jump up to: ^a ^b ^c ^d ^e Clavagnier S, Falchier A, Kennedy H (June 2004). \"Long-distance feedback projections to area V1: implications for multisensory integration, spatial awareness, and visual consciousness\" (PDF). Cogn Affect Behav Neurosci. 4 (2): 117\u201326. doi:10.3758/CABN.4.2.117. PMID 15460918. S2CID 6907281.\n  77. ^ Foxe JJ, Simpson GV (January 2002). \"Flow of activation from V1 to frontal cortex in humans. A framework for defining \"early\" visual processing\". Exp Brain Res. 142 (1): 139\u201350. doi:10.1007/s00221-001-0906-7. PMID 11797091. S2CID 25506401.\n  78. ^ Macaluso E, Frith CD, Driver J (August 2000). \"Modulation of human visual cortex by crossmodal spatial attention\". Science. 289 (5482): 1206\u20138. Bibcode:2000Sci...289.1206M. CiteSeerX 10.1.1.420.5403. doi:10.1126/science.289.5482.1206. PMID 10947990.\n  79. ^ Jump up to: ^a ^b ^c ^d Sadato N, Yamada H, Okada T, et al. (December 2004). \"Age-dependent plasticity in the superior temporal sulcus in deaf humans: a functional MRI study\". BMC Neurosci. 5: 56. doi:10.1186/1471-2202-5-56. PMC 539237. PMID 15588277.\n  80. ^ Bernstein LE, Auer ET, Moore JK, Ponton CW, Don M, Singh M (March 2002). \"Visual speech perception without primary auditory cortex activation\". NeuroReport. 13 (3): 311\u20135. doi:10.1097/00001756-200203040-00013. PMID 11930129. S2CID 44484836.\n  81. ^ Jump up to: ^a ^b Jones EG, Powell TP (1970). \"An anatomical study of converging sensory pathways within the cerebral cortex of the monkey\". Brain. 93 (4): 793\u2013820. doi:10.1093/brain/93.4.793. PMID 4992433.\n  82. ^ Jump up to: ^a ^b ^c Grefkes, C.; Fink, GR. (Jul 2005). \"The functional organization of the intraparietal sulcus in humans and monkeys\". J Anat. 207 (1): 3\u201317. doi:10.1111/j.1469-7580.2005.00426.x. PMC 1571496. PMID 16011542.\n  83. ^ Murata, A.; Fadiga, L.; Fogassi, L.; Gallese, V.; Raos, V.; Rizzolatti, G. (Oct 1997). \"Object representation in the ventral premotor cortex (area F5) of the monkey\". J Neurophysiol. 78 (4): 2226\u201330. doi:10.1152/jn.1997.78.4.2226. PMID 9325390.\n  84. ^ Smiley, JF.; Falchier, A. (Dec 2009). \"Multisensory connections of monkey auditory cerebral cortex\". Hear Res. 258 (1\u20132): 37\u201346. doi:10.1016/j.heares.2009.06.019. PMC 2788085. PMID 19619628.\n  85. ^ Sharma, J.; Dragoi, V.; Tenenbaum, JB.; Miller, EK.; Sur, M. (Jun 2003). \"V1 neurons signal acquisition of an internal representation of stimulus location\". Science. 300 (5626): 1758\u201363. Bibcode:2003Sci...300.1758S. doi:10.1126/science.1081721. PMID 12805552. S2CID 14716502.\n  86. ^ Lacey, S.; Tal, N.; Amedi, A.; Sathian, K. (May 2009). \"A putative model of multisensory object representation\". Brain Topogr. 21 (3\u20134): 269\u201374. doi:10.1007/s10548-009-0087-4. PMC 3156680. PMID 19330441.\n  87. ^ Neal, JW.; Pearson, RC.; Powell, TP. (Jul 1990). \"The ipsilateral cortico-cortical connections of area 7b, PF, in the parietal and temporal lobes of the monkey\". Brain Res. 524 (1): 119\u201332. doi:10.1016/0006-8993(90)90500-B. PMID 1698108. S2CID 24535669.\n  88. ^ Eickhoff, SB.; Schleicher, A.; Zilles, K.; Amunts, K. (Feb 2006). \"The human parietal operculum. I. Cytoarchitectonic mapping of subdivisions\". Cereb Cortex. 16 (2): 254\u201367. doi:10.1093/cercor/bhi105. PMID 15888607.\n  89. ^ Petitto LA, Zatorre RJ, Gauna K, Nikelski EJ, Dostie D, Evans AC (December 2000). \"Speech-like cerebral activity in profoundly deaf people processing signed languages: implications for the neural basis of human language\". Proc. Natl. Acad. Sci. U.S.A. 97 (25): 13961\u20136. doi:10.1073/pnas.97.25.13961. PMC 17683. PMID 11106400.\n  90. ^ Meredith, MA.; Clemo, HR. (Nov 1989). \"Auditory cortical projection from the anterior ectosylvian sulcus (Field AES) to the superior colliculus in the cat: an anatomical and electrophysiological study\". J Comp Neurol. 289 (4): 687\u2013707. doi:10.1002/cne.902890412. PMID 2592605. S2CID 221577963.\n  91. ^ Jump up to: ^a ^b Jiang, W.; Wallace, MT.; Jiang, H.; Vaughan, JW.; Stein, BE. (Feb 2001). \"Two cortical areas mediate multisensory integration in superior colliculus neurons\". J Neurophysiol. 85 (2): 506\u201322. doi:10.1152/jn.2001.85.2.506. PMID 11160489. S2CID 2499047.\n  92. ^ Wallace, MT.; Carriere, BN.; Perrault, TJ.; Vaughan, JW.; Stein, BE. (Nov 2006). \"The development of cortical multisensory integration\". Journal of Neuroscience. 26 (46): 11844\u20139. doi:10.1523/JNEUROSCI.3295-06.2006. PMC 6674880. PMID 17108157.\n  93. ^ Jump up to: ^a ^b ^c Jiang W, Stein BE (October 2003). \"Cortex controls multisensory depression in superior colliculus\". J. Neurophysiol. 90 (4): 2123\u201335. doi:10.1152/jn.00369.2003. PMID 14534263.\n  94. ^ Wallace MT, Meredith MA, Stein BE (June 1993). \"Converging influences from visual, auditory, and somatosensory cortices onto output neurons of the superior colliculus\". J. Neurophysiol. 69 (6): 1797\u2013809. doi:10.1152/jn.1993.69.6.1797. PMID 8350124.\n  95. ^ Stein, Barry E.; London, Nancy; Wilkinson, Lee K.; Price, Donald D. (1996). \"Enhancement of Perceived Visual Intensity by Auditory Stimuli: A Psychophysical Analysis\". Journal of Cognitive Neuroscience. 8 (6): 497\u2013506. doi:10.1162/jocn.1996.8.6.497. PMID 23961981. S2CID 43705477.\n  96. ^ Goodale MA, Milner AD (January 1992). \"Separate visual pathways for perception and action\" (PDF). Trends Neurosci. 15 (1): 20\u20135. CiteSeerX 10.1.1.207.6873. doi:10.1016/0166-2236(92)90344-8. PMID 1374953. S2CID 793980.\n  97. ^ Renier LA, Anurova I, De Volder AG, Carlson S, VanMeter J, Rauschecker JP (September 2009). \"Multisensory integration of sounds and vibrotactile stimuli in processing streams for \"what\" and \"where\"\". J. Neurosci. 29 (35): 10950\u201360. doi:10.1523/JNEUROSCI.0910-09.2009. PMC 3343457. PMID 19726653.\n  98. ^ Jump up to: ^a ^b ^c ^d ^e ^f ^g ^h ^i Nardini, M; Bedford, R; Mareschal, D (Sep 28, 2010). \"Fusion of visual cues is not mandatory in children\". Proceedings of the National Academy of Sciences of the United States of America. 107 (39): 17041\u20136. Bibcode:2010PNAS..10717041N. doi:10.1073/pnas.1001699107. PMC 2947870. PMID 20837526.\n  99. ^ Jump up to: ^a ^b ^c ^d ^e ^f ^g ^h Gori, M; Del Viva, M; Sandini, G; Burr, DC (May 6, 2008). \"Young children do not integrate visual and haptic form information\" (PDF). Current Biology. 18 (9): 694\u20138. doi:10.1016/j.cub.2008.04.036. PMID 18450446. S2CID 13899031.\n  100. ^ Lewkowicz, D; Kraebel, K (2004). Gemma Calvert; Charles Spence; Barry E Stein (eds.). The value of multisensory redundancy in the development of intersensory perception. Cambridge, Mass: MIT Press, cop. pp. 655\u201378. ISBN 9780262033213. OCLC 803222288. {{cite book}}: |work= ignored (help)\n  101. ^ Neil, PA; Chee-Ruiter, C; Scheier, C; Lewkowicz, DJ; Shimojo, S (Sep 2006). \"Development of multisensory spatial integration and perception in humans\". Developmental Science. 9 (5): 454\u201364. doi:10.1111/j.1467-7687.2006.00512.x. PMID 16911447. S2CID 25690976.\n  102. ^ Jump up to: ^a ^b Hillis, JM; Ernst, MO; Banks, MS; Landy, MS (Nov 22, 2002). \"Combining sensory information: mandatory fusion within, but not between, senses\". Science. 298 (5598): 1627\u201330. Bibcode:2002Sci...298.1627H. CiteSeerX 10.1.1.278.6134. doi:10.1126/science.1075396. PMID 12446912. S2CID 15607270.\n  103. ^ Jump up to: ^a ^b ^c ^d Ernst, MO (Jun 24, 2008). \"Multisensory integration: a late bloomer\". Current Biology. 18 (12): R519\u201321. doi:10.1016/j.cub.2008.05.002. PMID 18579094. S2CID 130911.\n  104. ^ Stein, BE; Meredith, MA; Wallace, MT (1993). Chapter 8 the visually responsive neuron and beyond: Multisensory integration in cat and monkey. Progress in Brain Research. Vol. 95. pp. 79\u201390. doi:10.1016/s0079-6123(08)60359-3. ISBN 9780444894922. PMID 8493355.\n  105. ^ Stein, BE; Labos, E; Kruger, L (Jul 1973). \"Sequence of changes in properties of neurons of superior colliculus of the kitten during maturation\". Journal of Neurophysiology. 36 (4): 667\u201379. doi:10.1152/jn.1973.36.4.667. PMID 4713313.\n  106. ^ Wallace, MT; Stein, BE (Nov 15, 2001). \"Sensory and multisensory responses in the newborn monkey superior colliculus\". The Journal of Neuroscience. 21 (22): 8886\u201394. doi:10.1523/JNEUROSCI.21-22-08886.2001. PMC 6762279. PMID 11698600.\n  107. ^ Tootell, RB; Hadjikhani, NK; Vanduffel, W; Liu, AK; Mendola, JD; Sereno, MI; Dale, AM (Feb 3, 1998). \"Functional analysis of primary visual cortex (V1) in humans\". Proceedings of the National Academy of Sciences of the United States of America. 95 (3): 811\u20137. Bibcode:1998PNAS...95..811T. doi:10.1073/pnas.95.3.811. PMC 33802. PMID 9448245.\n  108. ^ Jump up to: ^a ^b Gori, M; Giuliana, L; Sandini, G; Burr, D (Nov 2012). \"Visual size perception and haptic calibration during development\". Developmental Science. 15 (6): 854\u201362. doi:10.1111/j.1467-7687.2012.2012.01183.x. PMID 23106739.\n  109. ^ Granrud, CE; Schmechel, TT (Nov 2006). \"Development of size constancy in children: a test of the proximal mode sensitivity hypothesis\". Perception & Psychophysics. 68 (8): 1372\u201381. doi:10.3758/bf03193736. PMID 17378423.\n  110. ^ Gepshtein, S; Burge, J; Ernst, MO; Banks, MS (Dec 28, 2005). \"The combination of vision and touch depends on spatial proximity\". Journal of Vision. 5 (11): 1013\u201323. doi:10.1167/5.11.7. PMC 2632311. PMID 16441199.\n  111. ^ Helbig, HB; Ernst, MO (Jun 2007). \"Optimal integration of shape information from vision and touch\". Experimental Brain Research. 179 (4): 595\u2013606. doi:10.1007/s00221-006-0814-y. PMID 17225091. S2CID 12049308.\n  112. ^ Helbig, HB; Ernst, MO (2007). \"Knowledge about a common source can promote visual- haptic integration\". Perception. 36 (10): 1523\u201333. doi:10.1068/p5851. PMID 18265835. S2CID 14884284.\n  113. ^ Kail, RV; Ferrer, E (Nov\u2013Dec 2007). \"Processing speed in childhood and adolescence: longitudinal models for examining developmental change\". Child Development. 78 (6): 1760\u201370. doi:10.1111/j.1467-8624.2007.01088.x. PMID 17988319.\n  114. ^ Kail, R (May 1991). \"Developmental change in speed of processing during childhood and adolescence\". Psychological Bulletin. 109 (3): 490\u2013501. doi:10.1037/0033-2909.109.3.490. PMID 2062981.\n  115. ^ Ballard, DH; Hayhoe, MM; Pook, PK; Rao, RP (Dec 1997). \"Deictic codes for the embodiment of cognition\". The Behavioral and Brain Sciences. 20 (4): 723\u201342, discussion 743\u201367. CiteSeerX 10.1.1.49.3813. doi:10.1017/s0140525x97001611. PMID 10097009. S2CID 1961389.\n  116. ^ Daee, Pedram; Mirian, Maryam S.; Ahmadabadi, Majid Nili (2014). \"Reward Maximization Justifies the Transition from Sensory Selection at Childhood to Sensory Integration at Adulthood\". PLOS ONE. 9 (7): e103143. Bibcode:2014PLoSO...9j3143D. doi:10.1371/journal.pone.0103143. PMC 4110011. PMID 25058591.\n  117. ^ Rincon-Gonzalez L, WarrenJ P (2011). \"Haptic interaction of touch and proprioception: implications for neuroprosthetics\". IEEE Trans. Neural Syst. Rehabil. Eng. 19 (5): 490\u2013500. doi:10.1109/tnsre.2011.2166808. PMID 21984518. S2CID 20575638.\n  118. ^ Jiang, Huai; Stein, Barry E.; McHaffie, John G. (2015-05-29). \"Multisensory training reverses midbrain lesion-induced changes and ameliorates haemianopia\". Nature Communications. 6: 7263. Bibcode:2015NatCo...6.7263J. doi:10.1038/ncomms8263. ISSN 2041-1723. PMC 6193257. PMID 26021613.\n\n## Further reading[edit]\n\n  * Kujala, T.; Alho, K.; Huotilainen, M.; Ilmoniemi, RJ.; Lehtokoski, A.; Leinonen, A.; Rinne, T.; et, al. (Mar 1997). \"Electrophysiological evidence for cross-modal plasticity in humans with early- and late-onset blindness\". Psychophysiology. 34 (2): 213\u20136. doi:10.1111/j.1469-8986.1997.tb02134.x. PMID 9090272.\n  * Pascual-Leone, A.; Theoret, H.; et, al. (2006). Morton A Heller; Soledad Ballesteros (eds.). The Role of Visual Cortex in Tactile Processing: A Metamodal Brain. Mahwah, N.J.: Lawrence Erlbaum Associates. ISBN 9780805847260. OCLC 6124743. {{cite book}}: |work= ignored (help)\n  * Wallace, Mark; Murray, Micah Middelmann (2012). The Neural Bases of Multisensory Processes (Frontiers in Neuroscience). Boca Raton: CRC Press. ISBN 978-1-4398-1217-4. OCLC 707710852.\n\n## External links[edit]\n\n  * Hearing Research special edition 2009 \"Multisensory integration in auditory and auditory-related areas of cortex\"\n\nRetrieved from\n\"https://en.wikipedia.org/w/index.php?title=Multisensory_integration&oldid=1215177450\"\n\nCategories:\n\n  * Perception\n  * Sensory systems\n  * Neurology\n\nHidden categories:\n\n  * CS1 errors: periodical ignored\n  * CS1 French-language sources (fr)\n  * Articles with short description\n  * Short description is different from Wikidata\n  * Wikipedia articles needing context from July 2013\n  * All Wikipedia articles needing context\n  * All pages needing cleanup\n\n  * This page was last edited on 23 March 2024, at 16:12 (UTC).\n  * Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n\n  * Privacy policy\n  * About Wikipedia\n  * Disclaimers\n  * Contact Wikipedia\n  * Code of Conduct\n  * Developers\n  * Statistics\n  * Cookie statement\n  * Mobile view\n  * Edit preview settings\n\n", "frontpage": false}
