{"aid": "40009743", "title": "Fivefold Slower Compared to Go? Optimizing Rust's Protobuf Decoding Performance", "url": "https://www.greptime.com/blogs/2024-04-09-rust-protobuf-performance", "domain": "greptime.com", "votes": 12, "user": "signa11", "posted_at": "2024-04-12 05:52:06", "comments": 0, "source_title": "Fivefold Slower Compared to Go? Optimizing Rust's Protobuf Decoding Performance", "source_text": "Fivefold Slower Compared to Go? Optimizing Rust's Protobuf Decoding Performance | Greptime\n\n\u2715\n\nJoin us at KubeCon + CloudNativeCon North America, Nov 7-9 in Chicago! Swing\nby Booth L26 for exclusive SWAG \ud83c\udf81\n\n\u2715\n\nSkip to content\n\nOn this page\n\nShare on\n\nTweet\n\n### Subscribe to our newsletter\n\nGet the latest dates and news about GreptimeDB.\n\nEngineering\n\n\u2022\n\nApril 09, 2024\n\n# Fivefold Slower Compared to Go? Optimizing Rust's Protobuf Decoding\nPerformance\n\nWhen optimizing the write performance of GreptimeDB v0.7, we found that the\ntime spent on parsing Protobuf data with the Prometheus protocol was nearly\nfive times longer than that of similar products implemented in Go. This led us\nto consider optimizing the overhead of the protocol layer. This article\nintroduces several methods that the GreptimeDB team tried to optimize the\noverhead of Protobuf deserialization.\n\nLeiSoftware Engineer\n\n## Background\n\nWhen optimizing the write performance of GreptimeDB v0.7, we discovered\nthrough flame graphs that the CPU time spent parsing Prometheus write requests\naccounted for about 12% of the total. In comparison, the CPU time spent on\nprotocol parsing by VictoriaMetrics, which is implemented in Go, is only\naround 5%. This forced us to start considering optimizing the overhead of the\nprotocol conversion layer.\n\nTo simplify the discussion, all the test code is stored in the GitHub\nrepository https://github.com/v0y4g3r/prom-write-request-bench.\n\nbash\n\n    \n    \n    git clone https://github.com/v0y4g3r/prom-write-request-bench cd prom-write-request-bench export PROJECT_ROOT=$(pwd)\n\n## Optimization Steps\n\n### Step 1: Reproduce\n\nFirst, let's set up the baseline using a minimal reproducible benchmark.\nCorresponding branch:\n\nbash\n\n    \n    \n    git checkout step1/reproduce\n\nRust-related benchmark code(benches/prom_decode.rs):\n\nrust\n\n    \n    \n    fn bench_decode_prom_request(c: &mut Criterion) { let mut d = std::path::PathBuf::from(env!(\"CARGO_MANIFEST_DIR\")); d.push(\"assets\"); d.push(\"1709380533560664458.data\"); let data = Bytes::from(std::fs::read(d).unwrap()); let mut request_pooled = WriteRequest::default(); c.benchmark_group(\"decode\") .bench_function(\"write_request\", |b| { b.iter(|| { let mut request = WriteRequest::default(); let data = data.clone(); request.merge(data).unwrap(); }); }); }\n\nRun the benchmark command multiple times:\n\nbash\n\n    \n    \n    cargo bench -- decode/write_request\n\nTo receive the baseline result:\n\ntext\n\n    \n    \n    decode/write_request time: [7.3174 ms 7.3274 ms 7.3380 ms] change: [+128.55% +129.11% +129.65%] (p = 0.00 < 0.05)\n\nPull the VictoriaMetrics code in the current directory to set up a Go\nperformance testing environment:\n\nbash\n\n    \n    \n    git clone https://github.com/VictoriaMetrics/VictoriaMetrics cd VictoriaMetrics cat <<EOF > ./lib/prompb/prom_decode_bench_test.go package prompb import ( \"io/ioutil\" \"testing\" ) func BenchmarkDecodeWriteRequest(b *testing.B) { data, _ := ioutil.ReadFile(\"${PROJECT_ROOT}/assets/1709380533560664458.data\") wr := &WriteRequest{} for n := 0; n < b.N; n++ { b.StartTimer() wr.Reset() err := wr.UnmarshalProtobuf(data) if err != nil { panic(\"failed to unmarshall\") } b.StopTimer() } } EOF go test github.com/VictoriaMetrics/VictoriaMetrics/lib/prompb --bench BenchmarkDecodeWriteRequest\n\n> The data file path points to the 1709380533560664458.data file located in\n> the assets directory of the prom-write-request-bench repository.\n\nThe outputs are as follows:\n\nbash\n\n    \n    \n    goos: linux goarch: amd64 pkg: github.com/VictoriaMetrics/VictoriaMetrics/lib/prompb cpu: AMD Ryzen 7 7735HS with Radeon Graphics BenchmarkDecodeWriteRequest-16 961 1196101 ns/op PASS ok github.com/VictoriaMetrics/VictoriaMetrics/lib/prompb 1.328s\n\nYou can see that Rust takes about 7.3ms to parse a Prometheus write request\nwith 10,000 timelines, while the Go version by VictoriaMetrics only takes\n1.2ms, which is only 1/6 of the Rust version's time.\n\nNote\n\nThough you may notice that despite all the efforts below, the Go version still\noutperforms Rust version. That's the tradeoff between code maintainability and\nperformance. If we replace all Bytes with &[u8], then the Rust version can\nreach the same performance. It is also worth noting that VictoriaMetrics team\nmade great effort in optimizing Protobuf performance using techniques like\nobject pooling and they even created easyproto to replace the official\nProtobuf implementation.\n\nHere, you might have quickly spotted the issue: In the Go version, each\ndeserialization uses the same WriteRequest structure, merely performing a\nreset before deserialization to avoid data contamination. In contrast, Rust\nuses a new structure for each deserialization.\n\nThis is one of the optimizations VictoriaMetrics has made for write\nperformance VictoriaMetrics heavily leverages the pooling technique (as in\nsync.Pool) in its write path to reduce the cost of garbage collection. If the\nGo version builds a new structure for each deserialization like Rust, then the\ntime consumed would increase to about 10ms, worse than the Rust result\nmentioned above.\n\nSo, can a similar pooling technique be used in the Rust version? We can\nconduct a simple experiment: pooled_write_request.\n\nThe logic of the test is similar to the Go version:\n\nrust\n\n    \n    \n    let mut request_pooled = WriteRequest::default(); c.bench_function(\"pooled_write_request\", |b| { b.iter(|| { let data = data.clone(); request_pooled.clear(); request_pooled.merge(data).unwrap(); }); });\n\nExecute:\n\nbash\n\n    \n    \n    cargo bench -- decode/pooled_write_request\n\nThe result is as follows:\n\ntext\n\n    \n    \n    decode/pooled_write_request time: [7.1445 ms 7.1645 ms 7.1883 ms]\n\nIt appears that there isn't a significant performance improvement. So, why is\nthat?\n\nPerformance Review So Far:\n\n  * Rust baseline time: 7.3ms\n  * Go parsing time: 1.2ms\n  * Rust current time: 7.1ms\n\n### Step 2: RepeatedField\n\nCorresponding branch:\n\nbash\n\n    \n    \n    git checkout step2/repeated_field\n\nTo answer the question above, let's take a look at the data structure of\nPrometheus's WriteRequest:\n\nThe WriteRequest contains a vector of TimeSeries, and each TimeSeries in turn\nholds vectors of Label, Sample, and Exemplar. If we only reuse the outermost\nWriteRequest, every time we clear it, the vectors for Labels, Samples, and\nExemplars will also be cleared, and thus we fail to reuse the inner struct.\n\nWhat about Go? Let's take a look at the Reset method of WriteRequest:\n\ngo\n\n    \n    \n    wr.Timeseries = tss[:0]\n\nRather than setting the TimeSeries field to nil, the Go version sets it to an\nempty slice. This means the original elements within the slice are still\nretained and not garbage collected (only the len field is set to 0), hence\nGo's object reuse mechanism can effectively avoid repeated memory allocations.\n\nCould Rust adopt a similar mechanism?\n\nHere we come to a mechanism called RepeatedField from another popular Protobuf\nlibrary in the Rust ecosystem, rust-protobuf v2.x.\n\nIt's designed to avoid the drop overhead from Vec::clear by manually\nmaintaining the vec and len fields. When clearing, it only sets len to 0\nwithout calling clear from vec, thereby ensuring the elements inside the vec\nand the vecs within those elements are not dropped.\n\nThen, the question arises: How can we integrate the RepeatedField mechanism\ninto PROST? Obviously, PROST does not have a similar configuration option, so\nwe need to manually expand the code generated by PROST's procedural macros.\n\nDuring this process, we found that some fields are currently not needed in the\nwriting process so that we can skip them.\n\nbash\n\n    \n    \n    cargo bench -- decode/pooled_write_request\n\ntext\n\n    \n    \n    decode/pooled_write_request time: [2.6941 ms 2.7004 ms 2.7068 ms] change: [-66.969% -66.417% -65.965%] (p = 0.00 < 0.05) Performance has improved.\n\nWow! By using the RepeatedField mechanism, we successfully reduced the\nprocessing time to about 36% of the original.\n\nBut can this time be further reduced, and what other things can we learn from\nGo's code?\n\n> It's worth mentioning that, since RepeatedField is not as convenient to use\n> as Vec, version 3.x of rust-protobuf has removed it. However, the author\n> also mentioned that there might be an option to add RepeatedField back in\n> the future.\n\nPerformance Review So Far:\n\n  * Rust baseline time: 7.3ms\n  * Go parsing time: 1.2ms\n  * Rust current time: 2.7ms\n\n### Step 3: String or Bytes?\n\nCorresponding branch:\n\nbash\n\n    \n    \n    git checkout step3/bytes\n\nIn Go, a string is just a simple wrapper around []byte, and deserializing a\nstring field can be done by simply assigning the original buffer's pointer and\nlength to the string field. However, Rust's PROST, when deserializing String\ntype fields, needs to copy the data from the original buffer into the String,\nensuring that the lifecycle of the deserialized structure is independent of\nthe original buffer. However, this introduces an additional overhead of data\ncopying.\n\nSo could we change the Label fields to Bytes instead of String? I recall that\nthere's a Config::bytes option in PROST_build. In this PR for PROST, support\nwas added to generate fields of bytes type as Bytes instead of the default\nVec<u8>, thus enabling zero-copy parsing.\n\nWe could similarly change the types of Label's name and value fields to Bytes.\nThe advantage of this is that it eliminates the need for copying, but the\nproblem is also clear: where Label is needed to use, Bytes must still be\nconverted to String. In this conversion step, we could choose to use\nString::from_utf8_unchecked to skip the string valid check to further\nimproving performance.\n\nOf course, if a GreptimeDB instance is exposed to the public internet, such an\noperation is clearly unsafe. Therefore, in #3435, we mentioned the need to add\na strict mode to verify the legality of the strings.\n\nAfter modifying the types of Label::name and Label::value, we run the test\nagain:\n\nbash\n\n    \n    \n    cargo bench -- decode/pooled_write_request\n\nHere comes the result:\n\ntext\n\n    \n    \n    decode/pooled_write_request time: [3.4295 ms 3.4315 ms 3.4336 ms] change: [+26.763% +27.076% +27.383%] (p = 0.00 < 0.05) Performance has regressed.\n\nWait. Why did the performance get even worse? Let's generate a flame graph to\nbetter understand the underlying issues.\n\nIt's apparent that the majority of CPU time is being spent on copy_to_bytes.\nFrom the code in PROST for parsing Bytes fields, we can see the following:\n\nrust\n\n    \n    \n    pub fn merge<A, B>( wire_type: WireType, value: &mut A, buf: &mut B, _ctx: DecodeContext, ) -> Result<(), DecodeError> where A: BytesAdapter, B: Buf, { check_wire_type(WireType::LengthDelimited, wire_type)?; let len = decode_varint(buf)?; if len > buf.remaining() as u64 { return Err(DecodeError::new(\"buffer underflow\")); } let len = len as usize; //... value.replace_with(buf.copy_to_bytes(len)); Ok(()) }\n\nWhen the type of the value variable is Bytes, the value.replace_with call will\ninvoke copy_to_bytes again.\n\nrust\n\n    \n    \n    impl sealed::BytesAdapter for Bytes { fn replace_with<B>(&mut self, mut buf: B) where B: Buf, { *self = buf.copy_to_bytes(buf.remaining()); } }\n\nCould we eliminate one copy operation? Although Bytes::copy_to_bytes doesn't\ninvolve actual data copying but rather pointer operations, its overhead is\nstill considerable.\n\nPerformance Review So Far:\n\n  * Rust baseline time: 7.3ms\n  * Go parsing time: 1.2ms\n  * Rust current time: 3.4ms\n\n### Step 4: Eliminate One Copy\n\nCorresponding branch:\n\nbash\n\n    \n    \n    git checkout step4/bytes-eliminate-one-copy\n\nSince we parse Prometheus's WriteRequest from Bytes, we can directly\nspecialize the generic parameter B: Buf to Bytes. This way,\nPROST::encoding::bytes::merge becomes the following merge_bytes method:\n\nrust\n\n    \n    \n    #[inline(always)] fn copy_to_bytes(data: &mut Bytes, len: usize) -> Bytes { if len == data.remaining() { std::mem::replace(data, Bytes::new()) } else { let ret = data.slice(0..len); data.advance(len); ret } } pub fn merge_bytes(value: &mut Bytes, buf: &mut Bytes) -> Result<(), DecodeError> { let len = decode_varint(buf)?; if len > buf.remaining() as u64 { return Err(DecodeError::new(format!( \"buffer underflow, len: {}, remaining: {}\", len, buf.remaining() ))); } *value = copy_to_bytes(buf, len as usize); Ok(()) }\n\nAfter making the replacement, run the benchmark again:\n\nbash\n\n    \n    \n    cargo bench -- decode/pooled_write_request\n\nThe results are as follows:\n\ntext\n\n    \n    \n    decode/pooled_write_request time: [2.7597 ms 2.7630 ms 2.7670 ms] change: [-19.582% -19.483% -19.360%] (p = 0.00 < 0.05) Performance has improved.\n\nWe can see that there's improvement, but not much. It seems we've only\nreturned to the performance level we had just achieved. So, can we go even\nfurther?\n\nPerformance Review So Far:\n\n  * Rust baseline time: 7.3ms\n  * Go parsing time: 1.2ms\n  * Rust current time: 2.76ms\n\n### Step 5: Why is Bytes::slice So Slow?\n\nCorresponding branch:\n\nbash\n\n    \n    \n    git checkout step5/bench-bytes-slice\n\nThe primary reason is that PROST's field trait bound is BytesAdapter, while\nthe trait bound for the deserialized Bytes is Buf. Although Bytes implements\nboth traits, if you want to assign one type to another, you need to go through\nthe copy_to_bytes process twice to convert it. In the merge method, because\nthe actual type of Buf is unknown, it first needs to convert Buf into Bytes\nusing Buf::copy_to_bytes. Then, it passes Bytes to BytesAdapter::replace_with,\nwhere it again uses <<Bytes as Buf>>::copy_to_bytes to convert Buf into Bytes.\nFinally, we get the specific type that implements BytesAdapter: Bytes.\n\nFrom the perspective of PROST, Bytes::copy_to_bytes does not involve copying\ndata, so it can be considered a zero-copy operation. However, the overhead of\nthis zero-copy operation is not that low.\n\nLet's do a simple test to verify:\n\nrust\n\n    \n    \n    c.benchmark_group(\"slice\").bench_function(\"bytes\", |b| { let mut data = data.clone(); b.iter(|| { let mut bytes = data.clone(); for _ in 0..10000 { bytes = black_box(bytes.slice(0..1)); } }); });\n\ngo\n\n    \n    \n    func BenchmarkBytesSlice(b *testing.B) { data, _ := ioutil.ReadFile(\"<any binary file>\") for n := 0; n < b.N; n++ { b.StartTimer() bytes := data for i :=0; i < 10000; i++ { bytes = bytes[:1] } b.StopTimer() } }\n\nThe execution time in Go is 2.93 microseconds, whereas in Rust, it is 103.31\nmicroseconds:\n\ntext\n\n    \n    \n    goos: linux goarch: amd64 pkg: github.com/VictoriaMetrics/VictoriaMetrics/lib/prompb cpu: AMD Ryzen 7 7735HS with Radeon Graphics BenchmarkBytesSlice-16 497607 2930 ns/op PASS ok github.com/VictoriaMetrics/VictoriaMetrics/lib/prompb 6.771s\n\ntext\n\n    \n    \n    slice/bytes time: [103.23 \u03bcs 103.31 \u03bcs 103.40 \u03bcs] change: [+7.6697% +7.8029% +7.9374%] (p = 0.00 < 0.05)\n\nIt can be observed that the slice operation in Rust is two orders of magnitude\nslower than in Go.\n\nGo's slice only includes three fields: ptr, cap, and len. Its slice operation\ninvolves only modifications to these three variables.\n\nIn Rust, to ensure memory safety, the output of deserialization (WriteRequest)\nmust be independent with the lifecycle of the input data (Bytes). To avoid\ndata copying, Bytes employs a reference counting mechanism.\n\nAs illustrated below, two Bytes instances, A and B, fundamentally point to the\nsame underlying memory area. However, each also has a data pointer pointing to\na structure that holds the reference count information. The original memory\narray is only dropped when the reference count reaches zero. While this\napproach avoids copying, it also incurs some overhead.\n\n  * The slice operation for Bytes instances created via From<Vec<u8>> is based on reference counting. Each slicing requires copying the original buffer's pointer, length, reference count, and the pointer and length of the slice's return value, etc. Compared to Go's garbage collection, which is based on reachability analysis, the efficiency is undoubtedly much lower.\n\n  * Since Bytes supports multiple implementations, certain methods (such as clone) rely on vtable for dynamic dispatch.\n\n  * To ensure the safety of slice operations, Bytes manually inserts bounds checks in many places.\n\n### Step6: A little bit unsafe\n\nCorresponding branch:\n\nbash\n\n    \n    \n    git checkout step6/optimize-slice\n\nIs there a way to optimize the overhead further?\n\nA distinctive feature of the write interface in GreptimeDB is that once a\nWriteRequest is parsed, it will be immediately transformed into GreptimeDB's\nown data structures instead of directly using the WriteRequest. This means the\nlifespan of the deserialized input Bytes is always longer than that of the\nparsed structure.\n\nTherefore, we can make some hacky modifications to the slice operation,\ndirectly assembling the returned Bytes using the original array's pointer and\nlength. In this way, as long as the original A instance remains alive, all\nBytes instances sliced from it will point to valid memory.\n\nWe replace the data.slice(..len) operation with the following split_to method:\n\nrust\n\n    \n    \n    pub fn split_to(buf: &mut Bytes, end: usize) -> Bytes { let len = buf.len(); assert!( end <= len, \"range end out of bounds: {:?} <= {:?}\", end, len, ); if end == 0 { return Bytes::new(); } let ptr = buf.as_ptr(); let x = unsafe { slice::from_raw_parts(ptr, end) }; // `Bytes::drop` does nothing when it's built via `from_static`. Bytes::from_static(x) } // benchmark c.bench_function(\"split_to\", |b| { let data = data.clone(); b.iter(|| { let mut bytes = data.clone(); for _ in 0..10000 { bytes = black_box(unsafe { split_to(&bytes, 1) }); } }); })\n\nLet's benchmark it again to see the result:\n\ntext\n\n    \n    \n    slice/bytes time: [103.23 \u03bcs 103.31 \u03bcs 103.40 \u03bcs] change: [+7.6697% +7.8029% +7.9374%] (p = 0.00 < 0.05) slice/split_to time: [24.061 \u03bcs 24.089 \u03bcs 24.114 \u03bcs] change: [+0.2058% +0.4198% +0.6371%] (p = 0.00 < 0.05)\n\nTime consumption dropped considerably from 103us to 24us. Now, what about the\noverall overhead of deserialization?\n\ntext\n\n    \n    \n    decode/pooled_write_request time: [1.6169 ms 1.6181 ms 1.6193 ms] change: [-37.960% -37.887% -37.815%] (p = 0.00 < 0.05) Performance has improved.\n\nFinally, we have managed to reduce the time it takes to parse a single\nWriteRequest to about 1.6ms, which is only 33.3% slower than Go's 1.2ms!\n\nOf course, there is still room for optimization. If we completely abandon\nBytes and use Rust's slices (&[u8]), we can achieve performance close to Go's\n(considering only the overhead of slicing):\n\nrust\n\n    \n    \n    c.bench_function(\"slice\", |b| { let data = data.clone(); let mut slice = data.as_ref(); b.iter(move || { for _ in 0..10000 { slice = black_box(&slice[..1]); } }); });\n\nThe corresponding result is as follows:\n\ntext\n\n    \n    \n    slice/slice time: [4.6192 \u03bcs 4.7333 \u03bcs 4.8739 \u03bcs] change: [+6.1294% +9.8655% +13.739%] (p = 0.00 < 0.05) Performance has regressed.\n\nHowever, since this part of the overhead already constitutes a very low\nproportion of the entire write pathway, further optimization would not\nsignificantly affect the overall throughput.\n\nIf you are interested, you can also try to refactor the deserialization code\nusing slices, and we'd be happy if you shared your experience with us.\n\nPerformance Review So Far:\n\n  * Rust baseline duration: 7.3ms\n  * Go parsing duration: 1.2ms\n  * Rust current duration: 1.62ms\n\n## Summary\n\nIn this article, we tried various means to optimize the overhead of\ndeserializing Protobuf-encoded WriteRequest data.\n\nFirst, we utilized pooling techniques to avoid repeated memory allocation and\ndeallocation, directly reducing the time consumption to about 36% of the\nbaseline. Then, to leverage zero-copy features, we replaced the Label's String\nfields with Bytes type, but found that performance actually decreased. Flame\ngraphs revealed that PROST introduced some extra overhead to allow Bytes to\nconvert between the BytesAdapter and Buf traits. By specializing the type, we\nmanaged to eliminate these overheads. Additionally, we noticed in the flame\ngraphs that some extra overheads are introduced by Bytes::slice itself to\nensure memory safety. Considering our use case, we hacked the slice\nimplementation, eventually reducing the time consumption to about 20% of the\nbaseline.\n\nOverall, Rust imposes quite a few restrictions when directly manipulating byte\narrays to ensure memory safety. Using Bytes can circumvent lifetime issues via\nreference counting, but at the cost of low efficiency. On the other hand,\nusing &[u8] forces one to deal with the contagion of lifetimes.\n\nIn this article, a compromise approach was adopted, bypassing the reference\ncounting mechanism of Bytes through unsafe methods, manually ensuring the\ninput buffer remains valid for the entire lifecycle of the output. It's worth\nnoting that this isn't a universally applicable optimization method, but it's\nworth trying when the cost is part of a hot code path.\n\nFurthermore, \"zero-cost abstraction\" is one of the key design philosophies of\nthe Rust language. However, not all abstractions are zero-cost. In this\narticle, we saw the overhead of conversion between PROST's BytesAdapter and\nBuf traits, and the dynamic dispatch cost introduced by Bytes to accommodate\ndifferent underlying data sources, etc. This reminds us to pay more attention\nto the underlying implementation of critical code paths and guarantee high\nperformance through continuous profiling.\n\nBesides optimizing deserialization, we also made other efforts in the write\npath for GreptimeDB v0.7. Initially, the WriteRequest had to be fully parsed\nbefore converting to GreptimeDB's RowInsertRequest. Now, we eliminate the\nintermediate structure, directly converting the TimeSeries structure into\ntable-dimension write data during the deserialization of WriteRequest. In this\nway, it can reduce the traversal of all timelines (#3425, #3478), while also\nlowering the memory overhead of the intermediate structure. Moreover, the\ndefault HashMap in Rust based on SipHash did not perform ideally for\nconstructing table-dimension write data. By switching to a HashMap based on\naHash, we achieved nearly a 40% performance improvement in table lookup.\n\nPerformance optimization is inherently systematic, marked by the meticulous\naccumulation of improvements in even the smallest details that cumulatively\nyield substantial gains. The GreptimeDB team is steadfastly committed to this\nongoing journey, striving to push the boundaries of efficiency and excellence.\n\n#### About Greptime\n\nWe help industries that generate large amounts of time-series data, such as\nConnected Vehicles (CV), IoT, and Observability, to efficiently uncover the\nhidden value of data in real-time.\n\nVisit the latest v0.7 from any device to get started and get the most out of\nyour data.\n\n  * GreptimeDB, written in Rust, is a distributed, open-source, time-series database designed for scalability, efficiency, and powerful analytics.\n  * GreptimeCloud offers a fully managed DBaaS that integrates well with observability and IoT sectors.\n  * GreptimeAI is a tailored observability solution for LLM applications.\n\nIf anything above draws your attention, don't hesitate to star us on GitHub or\njoin GreptimeDB Community on Slack. Also, you can go to our contribution page\nto find some interesting issues to start with.\n\nShare on\n\nTweet\n\n## Join our community\n\nGet the latest updates and discuss with other users.\n\n### Join our developer community\n\nGreptimeDB is open source. Follow us on Twitter, star our GitHub repo, and\njoin our developer community on Slack!\n\nTwitter\n\nGitHub\n\nSlack\n\n### Subscribe to our newsletter\n\nGet the latest dates and news about GreptimeDB.\n\nEnglish\n\n\u7b80\u4f53\u4e2d\u6587\n\n#### Products\n\n  * GreptimeDB\n  * GreptimeCloud\n  * GreptimeDB Operator\n\n#### Resources\n\n  * Documentation\n  * Blog\n  * GreptimePlay\n  * Download\n  * Docker Repository\n\n#### Developer\n\n  * Open Source\n  * Roadmap\n  * Contributing\n  * Changelog\n  * Slack Community\n\n#### Company\n\n  * About Greptime\n  * Careers\n  * Brand\n  * Contact\n  * Privacy\n  * Terms\n\n\u00a9Copyright 2024 Greptime Inc. All Rights Reserved\n\n", "frontpage": true}
