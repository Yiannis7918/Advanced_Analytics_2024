{"aid": "40030890", "title": "The Data Center Is the New Compute Unit Nvidia's Vision for System-Level Scaling", "url": "https://www.fabricatedknowledge.com/p/the-data-center-is-the-new-compute", "domain": "fabricatedknowledge.com", "votes": 1, "user": "_____k", "posted_at": "2024-04-14 13:24:28", "comments": 0, "source_title": "The Data Center is the New Compute Unit: Nvidia's Vision for System-Level Scaling", "source_text": "The Data Center is the New Compute Unit: Nvidia's Vision for System-Level\nScaling\n\n# Fabricated Knowledge\n\nShare this post\n\n#### The Data Center is the New Compute Unit: Nvidia's Vision for System-Level\nScaling\n\nwww.fabricatedknowledge.com\n\n# The Data Center is the New Compute Unit: Nvidia's Vision for System-Level\nScaling\n\n### Copper, Cooling, and Compute Density: The Pillars of Nvidia's Data Center\nVision\n\nDoug O'Laughlin\n\nApr 04, 2024\n\n\u2219 Paid\n\n109\n\nShare this post\n\n#### The Data Center is the New Compute Unit: Nvidia's Vision for System-Level\nScaling\n\nwww.fabricatedknowledge.com\n\n9\n\nShare\n\nNvidia CEO Jensen Huang has repeatedly emphasized that the data center is the\nnew unit of compute. While this concept seemed straightforward at first, the\ndeeper implications became clearer after Nvidia's presentations at GTC and OFC\n2024.\n\nI only recently grasped exactly what is happening, and a simple reframing of\nthe underlying principles that drove Moore\u2019s Law makes the entire picture\nclearer. In this new paradigm, the rack itself is similar to a chip, and now,\nif we frame the rack as the new chip, we have a whole new vector to scale\nperformance and power. Let\u2019s talk Moore\u2019s Law from the perspective of the data\ncenter.\n\n####\n\nMoore\u2019s Law as a Fractal\n\nIt all starts with Moore\u2019s Law. There is a profound beauty in semiconductors,\nas the same problem that is happening at the chip scale is the same problem\nthat is happening at the data center level. Moore\u2019s Law is a fractal, and the\nprinciples that apply to nanometers apply to racks.\n\nThe first profound principle we have to talk about is miniaturization. Moore\u2019s\nLaw was built on the simple observation that shrinking transistors would take\nless power and give you more performance as the electrons physically don\u2019t\nhave to travel as far. That is why Moore\u2019s Law was about halfing the physical\nspace of a transistor for decades. But in recent years, we have reached the\neconomic limits of scaling much smaller chips, so we have hit some asymptotes.\nThis is the wildly heralded end of Moore\u2019s Law. Making chips that much smaller\nhas become harder.\n\nBut the things that apply at the bottom (moving bits closer) apply to things\nat the top. Moving electrons further takes time and more energy, and the\ncloser all of the data, power, and logic are, the less energy is wasted by\ndistance. The problem is still the same at a nanometer scale as at a rack\nscale, and moving cables and logic closer leads to system performance gains.\nThis problem applies to all networks. There are economies of scale by moving\nthings closer as long as there aren\u2019t geographic costs.\n\nSo, what is the answer? Let\u2019s not just move the electrons closer within the\nchip; let\u2019s move them closer within the rack. And that\u2019s exactly what Nvidia\nis doing.\n\n####\n\nMoore\u2019s Law Shifts to STCO\n\nThe pursuit of better chips will continue, but I think that Nvidia is\nintelligently pursuing better systems and will likely get multiple generations\nof chip shrink improvements from the intelligent design of the system. What is\nso attractive is that a generation or more of improvement is available with\ntoday\u2019s current technology.\n\nWhile others will figure it out, Nvidia will benefit from boldly going first\nand achieving the total system vision before anyone else. Moreover, Nvidia is\njust at the leading edge of an old concept.\n\nThis concept is called System Technology Co-Optimization, and while many\ntalked about this being a potential new vector of progress, I don\u2019t think\nanyone expected Nvidia to pop up with an opinionated and compelling version of\none in 2024. What\u2019s more, if you squint, you can see how STCO fits neatly\nwithin the decades of long computing history. This is no different from the\nmove from ICs to VLSI and SoCs to Nvidias System of Chips.\n\nLet\u2019s have a brief history lesson. Initially, the transistor was created, and\nthen the integrated circuit combined multiple transistors to make electronic\ncomponents. Then LSI or VLSI focused on making thousands of transistors work\ntogether and was the beginning of the Microprocessor. Next was the observation\nthat you could put multiple systems of semiconductors onto a single chip or\nSystem On Chip (SoC). We have recently been scaling out of the chip and onto\nthe package, a la chiplets, heterogeneous compute, and advanced packaging like\nCoWoS.\n\nBut I think Nvidia is taking the scaling game outside the chip to a System of\nChips. I\u2019m sure someone will eventually make a much more compelling acronym,\nbut I think there\u2019s a real chance the 2020s and 2030s are about scaling out\nthese larger systems than silicon. And it's all beautiful, consistent with\nwhat came before it.\n\nIf the data center is the new unit of compute, it\u2019s time to apply Moore\u2019s Law\nand hardware vendors' tricks to system-level optimizations. Nvidia has already\nshown us its hand, and Andy Bechtolsheim pretty much gave the entire scaling\nroadmap in a presentation at OFC.\n\n####\n\nThe Data Center as a Giant Chip\n\nImagine the data center as a giant chip. It is just a scaled-out advanced\npackage of the transistors of memory and logic for your problem. Instead of a\nmulti-chip SoC, each GPU board is a \u201ctile\u201d in your chip. They are connected\nwith copper and moved together as closely as possible, so there is more\nperformance and speed.\n\nDoes moving things off the package make sense, aka out of the rack? If\nperformance and bandwidth are your objective, it doesn\u2019t make much sense as\nthat slows the entire chips\u2019s performance massively. The key bottleneck is\nmoving the data back and forth to train the model and keeping the model in\nmemory, so keeping the data together makes sense. That is precisely why we are\ntrying to scale HBM, dies on the same package as an accelerator.\n\nSo, in the case of a data center as a chip, you\u2019d try to package everything\ntogether as closely as possible for as cheap as possible. There are a few\ndifferent packaging options, with the closest being chip-to-chip packaging,\nthen HBM on the package, NVLink over passive copper, and scaling out to\nInfiniband or Ethernet.\n\nAnd wouldn\u2019t you know it, Nvidia has been pursuing this problem using this\nexact lens. The goal is to scale out chip-to-chip interconnect, HBM, NVLink,\nand Infiniband. They even have this handy graph that puts the entire debate\ninto perspective. As expensive as HBM is, it\u2019s virtually free for purchased\nbandwidth. If bandwidth is the problem, it makes it difficult to scale up the\ncheapest bandwidth before relying on other scaling layers.\n\nThe closer it is to logic, the better it is for price and performance\n\nThis slide was shown many times at OFC and is my conclusion for where we will\ntry to scale up as much as possible. It makes sense to scale the domain where\nthe cost of bandwidth is the cheapest before considering other domains.\n\nIn the case of Nvidia, the point is to scale up as much memory in HBM3 before\nwe consider NVLink and then try to keep as much computing within NVLink before\neven considering scaling up to the network.\n\nPut differently, connecting 1 million accelerators over ethernet is wasteful,\nbut connecting 1 million accelerators over passive copper in a short-reach\ninterconnected node is economical and brilliant. Nvidia is pursuing the most\nscaling possible over passive copper before needing to use optics. This will\nbe the lowest cost and highest performance solution.\n\nThe copper backplane in the data center rack is effectively the new advanced\npackaging in the system-level Moore\u2019s Law race. The new way to shrink the rack\nis to put as much silicon in the most economical package, connected over the\ncheapest and most power-efficient interconnect as closely as possible. This is\nthe design ethos of the GB200 NV72L.\n\nImagine each GPU rack a chiplet and each copper cable CoWoS packaging\n\nThis is the new Moore\u2019s Law; you\u2019re looking at the new compute unit. The goal\nhere is to increase the power of this rack and move as many chips into a\nsingle rack as possible. It is the obvious cheapest and most power-efficient\nway to scale. Jensen referenced this at GTC and talked about how the new GB200\nrack takes 1/4th of the power and uses less space to train the same GPT 1.8T\nmodel.\n\nLess space, less power, more performance\u2014that\u2019s Moore\u2019s Law in all but name.\nWelcome to the new system scaling era, so let\u2019s discuss how we scale from\nhere. Andy Bechtolsheim\u2019s talks at OFC opened my eyes because we have at least\na generation of scaling from here based on our current technology.\n\n####\n\nLiquid Cooling and Copper in the Data Center\n\nBefore summarizing Andy\u2019s talk, I want to explain why we haven\u2019t done this\nbefore. The critical change that makes this all possible is liquid cooling.\nBecause of the shift to liquid cooling, we can cool another doubling of power\nin a rack. The 120 kW GB200 NVL rack is doubling over the current solution,\nand next generation, I would expect another doubling of power.\n\nIn some respects, this is a new power scaling envelope, and we will likely\nscale as quickly as possible until we hit the asymptote beyond liquid cooling.\nIn some ways, this is a clever permutation of Dennard\u2019s scaling.\n\nThe logical and obvious goal is to push the cooling to reach the maximum power\nwe can miniaturize in a data center rack. Andy talked about exactly how much\nfurther we could go. Andy believes we can put about 300+ kW in a single rack\nand cool it with liquid cooling. The power density level will be equivalent to\nat least a generation of process shrink.\n\nIt would look like this: We push HBM density as much as possible in a single\nchip, with 64 stacks of 16-hi HBM, or almost 500+ gigabytes of HBM. Andy even\ntalked about using XDDR to further scale out of the package and scale memory.\n\nAnd what\u2019s more, Panel-level packaging means that we can likely scale out more\nsilicon die area. With larger packages and substrates, we can now use advanced\npackaging to scale out to 10x+ larger than current CoWoS packages. The goal\nwould be to put as much silicon area as thermally possible in a single liquid-\ncooled rack.\n\nSo now imagine we can put 4-10x more silicon area in a single package and have\nthe means to cool at least 2x more power. So assume some power savings; the\ngoal is to put as much silicon into a rack, cool it as best as possible, and\nthen interconnect it with passive copper. Only after passive copper, all\nwithin the NVLink domain, can we talk about optics and DSPs. The goal is to\nscale up before you need to pay for networking, and Nvidia will push this\nsystem scaling while simultaneously pursuing silicon scaling. This is a\ncompelling scale-up and roadmap.\n\nThis system solution will be an order of magnitude cheaper than optics and\nprobably the best price-to-performance you can buy. Nvidia is creating another\nlayer of networking moat around its GPUs. If they pull off LPO first, this\nwill be just another layer of networking advantage. This is pretty much why\nCXL is dead: It makes zero sense to put any compute or memory over the\nnetwork; it costs too much and adds complexity to straightforward scaling.\n\nI conclude that Copper will reign supreme at the rack scale level and can push\nMoore\u2019s Law scaling further. AI networking aims to scale copper networking as\nhard as possible before we have to use Optics. Andy puts it this way:\n\nNow that liquid cooling has opened a relatively easy frontier of scaling,\nthere will be a race to push that scaling. Nvidia is already using this level\nof integration as a vast new vector of differentiation. They are looking for\nbetter solutions and ways to buy the most flops at the cheapest power\nconstraint. It\u2019s time to think about Moore\u2019s Law beyond the narrow confines of\nthe chip and at the system.\n\nThe new Moore\u2019s Law is about pushing the most compute into a rack. Also,\nlooking at Nvidia\u2019s networking moat as InfiniBand versus Ethernet is\ncompletely missing the entire point. I think the NVLink domain over passive\ncopper is the new benchmark of success, and it will make a lot of sense to buy\nGB200 NV72 racks instead of just B200s.\n\nIt\u2019s a new era in system design using larger substrates, denser memory, and\npassive copper to keep the information as close together as possible. This was\nalready being pursued at the chip level; now, it\u2019s happening at the rack\nlevel.\n\nIn this world, the leaves of the fat tree architecture become even denser. The\nfat leaves in this architecture will try to consume as much computing and\nmemory as possible before scaling out to the network is even needed. Nvidia is\ncleverly trying to eat the network from the bottom up.\n\nMeanwhile, Broadcom is pursuing the scale out from the top of the rack down,\nbut given the cost and ability to scale on copper, I think the energy and\nperformance of the scale-up from the leaves make a lot more sense. The tightly\nintegrated mainframe solution Nvidia offers will be the best in performance.\nAnd nowhere in this conversation is AMD, which will be trying to scale as a\ncomponent in a network using open consortiums.\n\nThe strategy of scaling out racks is clever and completely orthogonal to the\nprevious ways we scaled chips. The hyperscalers, while likely aware of the\nbenefits, probably didn\u2019t foresee this roadmap entirely as defined as Nvidia\nhas played. I think it\u2019s time to start thinking about scaling Systems of\nChips, and Nvidia, as usual, has already thought out and deployed the first\nedition of that future.\n\nShare\n\nFor more competitive thoughts and specific aspects from rack scaling, I will\npost thoughts behind the paywall. I believe that the hyper scaler\u2019s\ninfrastructure edge is one of the most overestimated competitive advantages in\nthe market. For more thoughts, read behind the paywall. The hyperscalers are\nthe most at risk in this data center scale-out regime.\n\n## Continue reading this post for free, courtesy of Doug OLaughlin.\n\nOr upgrade your subscription. Upgrade to paid\n\n\u00a9 2024 Doug OLaughlin\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n", "frontpage": false}
