{"aid": "40023366", "title": "Notes on AI Bias (2019)", "url": "https://www.ben-evans.com/benedictevans/2019/4/15/notes-on-ai-bias", "domain": "ben-evans.com", "votes": 1, "user": "Tomte", "posted_at": "2024-04-13 14:28:09", "comments": 0, "source_title": "Notes on AI Bias", "source_text": "Notes on AI Bias \u2014 Benedict Evans\n\nBenedict Evans\n\nBenedict Evans\n\n#\n\nNotes on AI Bias\n\nMachine learning is the new centre of tech, and like all big new things there\nare issues. \u2018AI bias\u2019 is much-discussed right now: machine learning finds\npatterns but sometimes it finds the wrong one, and it can be hard to tell.\n\n> \u201cRaw data is both an oxymoron and a bad idea; to the contrary, data should\n> be cooked, with care.\u201d\n>\n> \\- Geoffrey Bowker\n\nUntil about 2013, If you wanted to make a software system that could, say,\nrecognise a cat in a photo, you would write logical steps. You\u2019d make\nsomething that looked for edges in an image, and an eye detector, and a\ntexture analyser for fur, and try to count legs, and so on, and you\u2019d bolt\nthem all together... and it would never really work. Conceptually, this is\nrather like trying to make a mechanical horse - it\u2019s possible in theory, but\nin practice the complexity is too great for us to be able to describe. You end\nup with hundreds or thousands of hand-written rules without getting a working\nmodel.\n\nWith machine learning, we don\u2019t use hand-written rules to recognise X or Y.\nInstead, we take a thousand examples of X and a thousand examples of Y, and we\nget the computer to build a model based on statistical analysis of those\nexamples. Then we can give that model a new data point and it says, with a\ngiven degree of accuracy, whether it fits example set X or example set Y.\nMachine learning uses data to generate a model, rather than a human being\nwriting the model. This produces startlingly good results, particularly for\nrecognition or pattern-finding problems, and this is the reason why the whole\ntech industry is being remade around machine learning.\n\nHowever, there\u2019s a catch. In the real world, your thousand (or hundred\nthousand, or million) examples of X and Y also contain A, B, J, L, O, R, and\nP. Those may not be evenly distributed, and they may be prominent enough that\nthe system pays more attention to L and R than it does to X.\n\nWhat does that mean in practice? My favorite example is the tendency of image\nrecognition systems to look at a photo of a grassy hill and say \u2018sheep\u2019. Most\nof the pictures that are examples of \u2018sheep\u2019 were taken on grassy hills,\nbecause that\u2019s where sheep tend to live, and the grass is a lot more prominent\nin the images than the little white fluffy things, so that\u2019s where the systems\nplace the most weight.\n\nA more serious example came up recently with a project to look for skin cancer\nin photographs. It turns out that dermatologists often put rulers in photos of\nskin cancer, for scale, but that the example photos of healthy skin do not\ncontain rulers. To the system, the rulers (or rather, the pixels that we see\nas a ruler) were just differences between the example sets, and sometimes more\nprominent than the small blotches on the skin. So, the system that was built\nto detect skin cancer was, sometimes, detecting rulers instead.\n\nA central thing to understand here is that the system has no semantic\nunderstanding of what it\u2019s looking at. We look at a grid of pixels and\ntranslate that into sheep, or skin, or rulers, but the system just sees a\nstring of numbers. It isn\u2019t seeing 3D space, or objects, or texture, or sheep.\nIt\u2019s just seeing patterns in data.\n\nMeanwhile, the challenge in trying to diagnose issues like this is that the\nmodel your machine learning system has generated (the neural network) contains\nthousands or hundreds of thousands of nodes. There is no straightforward way\nto look inside the model and see how it\u2019s making the decision - if you could,\nthen the process would be simple enough that you wouldn\u2019t have needed ML in\nthe first place and you could have just written the rules yourself. People\nworry that ML is a \u2018black box\u2019. (As I explain later, however, this issue is\noften hugely overstated.)\n\nThis, hugely simplified, is the \u2018AI bias\u2019 or \u2018machine learning bias\u2019 problem:\na system for finding patterns in data might find the wrong patterns, and you\nmight not realise. This is a fundamental characteristic of the technology, and\nit is very well-understood by everyone working on this in academia and at\nlarge tech companies (data people do understand sample basis, yes), but its\nconsequences are complex, and our potential resolutions to those consequences\nare also complex.\n\nFirst, the consequences.\n\n## AI bias scenarios\n\nThe most obvious and immediately concerning place that this issue can be\nmanifested is in human diversity. It was recently reported that Amazon had\ntried building a machine learning system to screen resum\u00e9s for recruitment.\nSince Amazon\u2019s current employee base skews male, the examples of \u2018successful\nhires\u2019 also, mechanistically, skewed male and so, therefore, did this system\u2019s\nselection of resum\u00e9s. Amazon spotted this and the system was never put into\nproduction.\n\nThe most important part of this example is that the system reportedly\nmanifested this skew even if the gender was not explicitly marked on the\nresum\u00e9s. The system was seeing patterns in the sample set of \u2018successful\nemployees\u2019 in other things - for example, women might use different words to\ndescribe accomplishments, or have played different sports at school. Of\ncourse, the system doesn\u2019t know what ice hockey is, nor what people are, nor\nwhat \u2018success\u2019 is - it was just doing statistical analysis of the text. But\nthe patterns that it was seeing were not necessarily things that a human being\nwould have noticed, and with some things (vocabulary describing success,\nperhaps, is something we now know can vary between genders) a human might have\nstruggled to see them even if they were looking for them.\n\nIt gets worse. A machine learning system that is very good at spotting skin\ncancer on pale skin might be worse at spotting skin cancer on darker coloured\nskin, or vice versa, not perhaps because of bias in the sample but because you\nmight need to construct the model differently to begin with to pick out\ndifferent characteristics. Machine learning systems are not interchangeable,\neven in a narrow application like image recognition. You have to tune the\nstructure of the system, sometimes just by trial and error, to be good at\nspotting the particular features in the data that you\u2019re interested in, until\nyou get to the desired degree of accuracy. But you might not realise that the\nsystem is 98% accurate for one group but only 91% accurate for another group\n(even if that accuracy still surpasses human analysis).\n\nSo far I\u2019ve mostly used examples around people and their characteristics, and\nnaturally this is where a lot of the conversation around this tends to focus.\nBut it\u2019s important to understand that bias around people is a subset of the\nissue: we will use ML for lots of things and sample bias will be part of the\nconsideration in all of those. And equally, even if you are working with\npeople, the bias in the data might not be around people.\n\nTo understand this systematically, it\u2019s useful to go back to the skin cancer\nexample from earlier, and consider three hypothetical ways it might break:\n\n  1. You don\u2019t have an even distribution of people: your photos of skin with different tones is unbalanced, so your system gives false positives or false negatives based on skin pigmentation.\n\n  2. Your data contains a prominent and unevenly distributed non-human characteristic with no diagnostic value, and the system trains on that - the ruler in the photo of skin cancer, or the grass in the photo of a flock of sheep. In this case it alters its result if the pixels that we see as a \u2018ruler\u2019 (but that it does not) are present.\n\n  3. Your data contains some other characteristic that a human cannot see even if they look for it.\n\nWhat does \u2018even if they look for it\u2019 mean? Well, we know a priori, or ought to\nknow, that the data might be skewed around different human groups, and can at\nleast plan to look for this (to put this the other way around, there are all\nsorts of social reasons why you might expect your data to come with bias\naround human groups). And if we look at the photo with the ruler, we can see\nthe ruler - we just ignored it, because we knew it was irrelevant and we\nforgot that the system did not know anything.\n\nBut what if all of your photos of unhealthy skin are taken in an office with\nincandescent light and your photos of healthy skin are taken under fluorescent\nlight? What if you updated the operating system on your smartphone between\ntaking the healthy photos and the unhealthy photos, and Apple or Google made\nsome small change to the noise reduction algorithm? This might be totally\ninvisible to a human, no matter how hard they look, but the machine learning\nsystem will see it instantly and use it. It doesn\u2019t know anything.\n\nNext, so far we\u2019ve been talking about correlations that are false, but there\nmay also be patterns in the data that are entirely accurate and correct\npredictors, but that you don\u2019t want to use, for ethical, legal or product-\nbased reasons. In some jurisdictions, for example, you are not allowed to give\nbetter car insurance rates to women even though women might tend to be safer\ndrivers. One could easily imagine a system that looks at the historical data\nand learns to associate \u2018female\u2019 first names with lower risk, so you would\nremove the first names from the data - but, as with the Amazon example above,\nthere might be other factors that reveal the gender to the system (though of\ncourse it would have no concept of gender, or indeed cars), and you might not\nrealise this until the regulator did an ex ante statistical analysis of the\nquotes you\u2019ve given and fined you.\n\nFinally, this is sometimes talked about as though we will only use these\nsystems for things that involve people and social interactions and assumptions\nin some way. We won\u2019t. If you make gas turbines, you would be very interested\nin applying machine learning to the telemetry coming from dozens or hundreds\nof sensors on your product (audio, vibration, temperature, or any other sensor\ngenerates data that is repurposed for a machine learning model with great\nease). Hypothetically, you might say \u2018here is data from a thousand turbines\nthat were about to fail and here is data from a thousand turbines that were\nworking fine - build a model to tell the difference\u2019. Now, suppose that 75% of\nthe bad turbines use a Siemens sensor and only 12% of the good turbines use\none (and suppose this has no connection to the failure). The system will build\na model to spot turbines with Siemens sensors. Oops.\n\n## AI bias management\n\nWhat do we do about this? You can divide thinking in the field into three\nareas:\n\n  1. Methodological rigour in the collection and management of the training data\n\n  2. Technical tools to analyse and diagnose the behavior of the model.\n\n  3. Training, education and caution in the deployment of ML in products.\n\nThere\u2019s a joke in Moli\u00e8re's Bourgeois Gentilhomme about a man who is taught\nthat literature is divided into \u2018poetry\u2019 and \u2018prose\u2019, and is delighted to\ndiscover that he\u2019s been speaking prose his whole life without realising.\nStatisticians might feel the same way today - they\u2019ve been working on\n\u2018artificial intelligence\u2019 and \u2018sample bias\u2019 for their whole careers without\nrealising. Looking for and worrying about sample bias is not a new problem -\nwe just have to be very systematic about it. As mentioned above, in some ways\nthis might actually, mechanistically, be easier when looking at issues around\npeople, since we know a priori that we might have bias against different human\ngroups where we might not realise a priori that we might have bias against\nSiemens.\n\nThe part that\u2019s new, of course, is that the people aren\u2019t doing the\nstatistical analysis directly anymore - it\u2019s being done by machines, that\ngenerate models of great complexity and size that are not straightforward to\nanalyse. This question of transparency is one of the main areas of concern\naround bias - the fear is not just that it\u2019s biased but that there is no way\nto tell, and that this is somehow fundamentally new and different from other\nforms of organization or automation, where there are (supposedly) clear\nlogical steps that you can audit.\n\nThere are two problems with this: we probably can audit ML systems in some\nways, and it\u2019s not really any easier to audit any other system.\n\nFirst, one part of current machine learning research is around finding tools\nand methods to work out what features are most prominent in a machine learning\nsystem. Meanwhile, machine learning (in its current manifestation) is a very\nnew field and the science is changing fast - one should not assume that what\nis not practical today will not become practical soon. This OpenAI project is\nan interesting example of exactly this.\n\nSecond, the idea that you can audit and understand decision-making in existing\nsystems or organisations is true in theory but flawed in practice. It is not\nat all easy to audit how a decision is taken in a large organisation. There\nmay well be a formal decision process, but that\u2019s not how the people actually\ninteract, and the people themselves often do not have a clearly logical and\nsystematic way of making their individual decisions. As my colleague Vijay\nPande argued here, people are black boxes too - combine thousands of people in\nmany overlapping companies and institutions and the problem compounds. We know\nex post that the Space Shuttle was going to disintegrate on re-entry, and\ndifferent people inside NASA had information that made them think something\nbad might happen, but the system overall did not know that. Meanwhile, NASA\nhad been through exactly this auditing process when it lost the previous space\nshuttle, and yet it lost another one for very similar reasons. It\u2019s easy to\nsay that organizations and human systems follow clear logical rules that you\ncan audit, understand and change - experience suggests otherwise. This is the\nGosplan fallacy.\n\nIn this context, I often compare machine learning to databases, and especially\nrelational databases - a new fundamental technology that changed what was\npossible in computer science and changed the broader world, that became a\ncommodity that was part of everything, and that we now use all the time\nwithout noticing. But databases had problems too, and the problems had the\nsame character: the system could be built on bad assumptions, or bad data, it\nwould be hard to tell, and the people using it would do what the system told\nthem without questioning it. There are lots of old jokes about the tax office\nmisspelling your name, and it being easier to change your name than persuade\nthem to fix the spelling. Is this best thought of as a technical problem\ninherent to SQL, an execution failure by Oracle, or an institutional failure\nby a large bureaucracy? And how easy would it be to work out the exact process\nwhereby a system was deployed with no capability to fix typos, or know that it\nhad done this before people started complaining?\n\nAt an even simpler level, one can see this issue in the phenomena of people\ndriving their cars into rivers because of an out-of-date SatNav. Yes, the maps\nshould be kept up to date. But, how far is it TomTom\u2019s fault that your car is\nfloating out to sea?\n\nAll of this is to say that ML bias will cause problems, in roughly the same\nkinds of ways as problems in the past, and will be resolvable and\ndiscoverable, or not, to roughly the same degree as they were in the past.\nHence, the scenario for AI bias causing harm that is easiest to imagine is\nprobably not one that comes from leading researchers at a major institution.\nRather, it is a third tier technology contractor or software vendor that bolts\ntogether something out of open source components, libraries and tools that it\ndoesn\u2019t really understand and then sells it to an unsophisticated buyer that\nsees \u2018AI\u2019 on the sticker and doesn\u2019t ask the right questions, gives it to\nminimum-wage employees and tells them to do whatever the \u2018AI\u2019 says. This is\nwhat happened with databases. This is not, particularly, an AI problem, or\neven a \u2018software\u2019 problem. It\u2019s a \u2018human\u2019 problem.\n\n## Conclusion\n\n> \u201cMachine Learning can do anything you could train a dog to do - but you\u2019re\n> never totally sure what you trained the dog to do.\u201d\n\nI often think that the term \u2018artificial intelligence\u2019 is deeply unhelpful in\nconversations like this. It creates the largely false impression that we have\nactually created, well, intelligence - that we are somehow on a path to HAL\n9000 or Skynet - towards something that actually understands. We aren\u2019t. These\nare just machines, and it\u2019s much more useful to compare them to, say, a\nwashing machine. A washing machine is much better than a human at washing\nclothes, but if you put dishes in a washing machine instead of clothes and\npress start, it will wash them. They\u2019ll even get clean. But this won\u2019t be the\nresult you were looking for, and it won\u2019t be because the system is biased\nagainst dishes. A washing machine doesn\u2019t know what clothes or dishes are -\nit\u2019s just a piece of automation, and it is not conceptually very different\nfrom any previous wave of automation.\n\nThat is, just as for cars, or aircraft, or databases, these systems can be\nboth extremely powerful and extremely limited, and depend entirely on how\nthey\u2019re used by people, and on how well or badly intentioned and how educated\nor ignorant people are of how these systems work.\n\nHence, it is completely false to say that \u2018AI is maths, so it cannot be\nbiased\u2019. But it is equally false to say that ML is \u2018inherently biased\u2019. ML\nfinds patterns in data - what patterns depends on the data, and the data is up\nto us, and what we do with it is up to us. Machine learning is much better at\ndoing certain things than people, just as a dog is much better at finding\ndrugs than people, but you wouldn\u2019t convict someone on a dog\u2019s evidence. And\ndogs are much more intelligent than any machine learning.\n\nPolicy, Artificial IntelligenceBenedict Evans15 April 2019\n\n## Subscribe\n\nWhat mattered in tech this week?\n\nOnce a week, I send an email newsletter to over 150,000 people - what happened\nin tech that actually mattered, and what it means. I pick out the changes and\nideas you don\u2019t want to miss in all the noise, and give them context and\nanalysis.\n\nSubscribe\n\n\u00a9 Benedict Evans\n\nHours\n\nNewsletter\n\n2024\n\nWhat mattered in tech this week?\n\nOnce a week, I send a newsletter to 175,000 people - what happened in tech\nthat actually mattered, and what it means. I pick out the changes and ideas\nyou don\u2019t want to miss in all the noise, and give them context and analysis.\n\nSUBSCRIBE\n\n\u00a9 BENEDICT EVANS\n\nThis site uses cookies to function, and for anonymous analytics. You can read\nthe privacy policy here.\n\n", "frontpage": false}
