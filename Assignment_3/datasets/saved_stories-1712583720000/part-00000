{"aid": "39967199", "title": "Building a Managed Postgres Service in Rust", "url": "https://tembo.io/blog/managed-postgres-rust", "domain": "tembo.io", "votes": 8, "user": "samaysharma", "posted_at": "2024-04-08 07:40:40", "comments": 0, "source_title": "Building a Managed Postgres Service in Rust: Part 1 | Tembo", "source_text": "Building a Managed Postgres Service in Rust: Part 1 | Tembo\n\n# Building a Managed Postgres Service in Rust: Part 1\n\nMar 27, 2024 \u2022 8 min read\n\n# Adam Hendel\n\nFounding Engineer\n\nTembo was founded in December 2022 with a mission to make the best Postgres\nservice to deploy extensions. That mission has evolved since then to provide\noptimized Postgres instances for your workload, but let\u2019s go back to the\nstart. At the very minimum, we wanted our users to be able to come to Tembo\nCloud, create a new Postgres instance, install Postgres extensions into that\ninstance, and connect to that Postgres instance over the public internet.\n\nWith this early vision in place, we started to sketch out an architecture for\nour product. Drawing inspiration from other cloud platforms, we quickly\ndecided that we would architect the platform with two high-level components\u2014a\ncontrol-plane and a data-plane. Two early assumptions were that 1) there could\nbe many data-planes per control-plane, and 2) the control-plane may not always\nbe able to reach the data plane (for example if the data-plane\u2019s network is\nprivate, dedicated for one customer). So, these two components must stay\nhighly decoupled. Making the decision to decouple the infrastructure enabled\nus to move quickly in the control-plane without risk of impacting the managed\ninfrastructure in the data-plane.\n\n## The Control Plane\n\nThe initial version of the control-plane consisted of just a web UI and a HTTP\nserver. The HTTP server serves the purpose of handling Create, Read, Update,\nand Delete (CRUD) requests for Tembo instances. The server\u2019s GET endpoints do\nthings like list all the instances that belong to an organization or list all\nthe attributes and configurations for a specific instance. PATCH routes handle\nupdating instances with operations like restarting, installing an extension,\nor changing a configuration.\n\nAll state is persisted in a Postgres instance dedicated to the control-plane.\nThis includes all metadata related to instances in the platform such as their\ninfrastructure requirements (cpu / mem / storage), Postgres extensions\ninstalled, and any custom configurations applied to the instance.\n\nTembo\u2019s clients, the web UI (hosted at cloud.tembo.io), the Tembo Terraform\nprovider, and the Tembo CLI all communicate with the control-plane through an\nHTTP interface.\n\n## A Finite State Machine\n\nManaging the lifecycle of Postgres, infrastructure, and its related services\nis complex. For example, many operations in Postgres require restarts (e.g.\nchanging shared_preload_libraries) and can be highly disruptive. Other changes\nshould be multi-step; for example, a user may be required to suspend or pause\nan instance before deleting it. Simple HTTP request validation was not enough\nto model the state changes, and transitions would not only happen on requests.\nOthers have implemented state machines to help facilitate the management of\ndatabases, which originally led us down this path.\n\nTransitions are events that move an instance from one state to another. These\ncan be thought of as events. For example, when instances are first created\nthey are in the \u201cSubmitted\u201d state. When that instance receives a \u201cCreated\u201d\nevent back from the data-plane, it can transition to \u201cUp\u201d. Likewise, \u201cUp\u201d\ninstances can move to a \u201cConfiguring\u201d state when they receive \u201cUpdate\u201d events,\nand do not transition back to \u201cUp\u201d until their configuration is reported as\ncomplete via an \u201cUpdated\u201d event. Error events can transition an instance into\nan error state and instances can recover out of an error state when recovered.\n\nThe early iterations of the platform were more restrictive on state\ntransitions. We modeled our state transition, as shown below, before\nimplementing the first version of our state machine. As we\u2019ve added more\nfunctionality, it has significantly evolved and become less restrictive. For\nexample, instances can now be deleted and configuration changes can be\nrequested from any state. The Stopping and Stopped states were temporarily\ndisabled during the migration from our own Postgres operator to the CNPG\nOperator. Read more on the Operator decision in our Tembo Operator blog post.\nThe ability to suspend or stop a Tembo instance is also being added soon.\n\n## FSM in Rust\n\nThe majority of the Tembo platform is written in Rust, and the state machine\nis no different. All possible states of a Tembo instance are represented as\nvariants of a single Enum. At any moment in time, a Tembo instance is in\nexactly one state. This state is persisted in a Postgres instance dedicated to\nthe control-plane.\n\n    \n    \n    pub enum State { Configuring, Deleted, Deleting, Error, Restarting, Starting, Stopped, Stopping, Submitted Up, }\n\nThe transitions between states can be arbitrarily complex. For example,\ninstallation of certain extensions requires Postgres to be restarted, while\nothers do not. Therefore, an Update event on an instance in the Up state could\nresult in an instance transitioning to either a Configuring state or the\nRestarting state. Invalid state transitions result in an error and no\ntransition in state.\n\nBelow is a subset of the transition definitions implemented into the Tembo\nfinite state machine that were created with these complexities in mind. Any\nexisting State can attempt to transition to the next state by calling next()\non itself, along with the event type and the instance itself. The transition\nis defined as the pair of the current state and the Event type. As mentioned\nearlier, arbitrary transition logic can be applied on the transitions but it\nis not required. For example, (State::Up, Event::Restarted) => Ok(State::Up)\ndefines the transition from Submitted to Up, via a Restarting event.\n\n    \n    \n    impl State { pub fn next( self, event: Event, instance: &Instance, ) -> Result<State, StateError> { match (self, event) { (State::Up, Event::Restarted) => Ok(State::Up), (State::Up, Event::Update) => { let desired = Some(instance.clone().desired_spec); let actual = instance.clone().actual_spec; match restart::restart_expected(&desired, &actual).await { true => Ok(State::Restarting), false => Ok(State::Configuring), } } (State::Up, Event::Updated) => Ok(State::Up), (State::Restarting, Event::Restarted) => Ok(State::Restarting), ... other state transitions ... (State::Deleted, _) => Err(StateError::IllegalTransition( format!(\"instance {} is deleted\", instance.instance_id).to_owned(), )), // undefined states (s, e) => Err(StateError::UndefinedTransition(format!( \"Invalid state-event combination: state: {s:#?} => event:{e:#?}\" ))), } } }\n\nWith a finite state machine in place, all CRUD requests pass through basic\nHTTP request validation and state transitions are validated before any\ndatabase change is applied. All valid state change requests are emitted as a\nCRUD event to the data-plane.\n\n## Buffer tasks between the planes\n\nThe task duration for processing events in the data-plane is quite variable.\nFor example, creating a new instance could take seconds to minutes depending\non whether an image is cached on a node or whether Kubernetes needs to add a\nnew node to the cluster in order to deploy the instance. Once an instance is\ncreated, events like installing an extension could happen within seconds for a\nsmall extension. Changing a configuration could happen in milliseconds for a\nsimple change like changing the search_path. In the case of a\nshared_preload_libraries change, it could take several seconds to over a\nminute since Postgres will also need to be restarted.\n\nImplementing a queue for these tasks allows us to buffer these requests, and\nallows the data-plane to determine when and how often tasks are retried.\nFurther, implementing a queue means that complete outages in either the\ncontrol-plane or the data-plane do not immediately cause system failures\n(though the queues will begin to build up).\n\nAs a Postgres company, like many others (Dagster and Crunchy Data as a couple\nexamples) we implemented the queue on Postgres. It was initially built as a\nRust crate to fit with our tech stack, but we quickly realized that we could\nshare the implementation with all Postgres users if it lived as an extension,\nso we released it as PGMQ.\n\n## Conducting events in the data-plane\n\nAfter the control-plane sends CRUD events to PGMQ, Conductor, the service that\nhandles all execution of orders from the control-plane and into the data-\nplane, processes them. Conductor has a critical job but a narrow set of\nresponsibilities. It reads the Spec for an instance from the queue, then\napplies the updated resources by communicating with the Kubernetes API. It may\ncreate new Tembo instances or otherwise can update, restart, or delete\ninstances. All of the operations applied by Conductor are ultimately applied\nto Tembo instances via the Tembo Operator.\n\nRead more about our Kubernetes footprint in the Tembo Operator Blog Post from\nOctober 2023.\n\n## Closing the loop\n\nThe control-plane receives state events from the data-plane via the queue.\nThese state events are then persisted to the control-plane\u2019s Postgres\ndatabase. Additionally, further reconciliation between the user\u2019s desired\nstate of an instance and the actual state of the instance happens here. For\ncertain transitions, when the desired state does not equal the actual state,\nevents are resent to the data-plane. In other cases, errors are logged and\nalerts are triggered.\n\nWe continue to iterate on our platform. The Kubernetes operator, Conductor and\nrest of the services that make up the data-plane are open source and can be\nexplored on Github.\n\n## Moving forward\n\nEnjoying learning about the decisions and thought processes behind building a\nmanaged Postgres service using Rust? There\u2019s more to come - stay tuned for the\nnext blog post in this series.\n\nWe are hiring for a variety of platform and product roles. If making decisions\nand solving challenges like these sounds interesting to you, reach out to us\nat careers@tembo.io to apply. If you have suggestions on how we could improve\nour platform, reach out and talk to us on Slack.\n\n### What's next?\n\nTry Tembo Cloud for free\n\nStar on Github\n\nSubscribe on Youtube\n\nView our RSS feed\n\nFollow on X\n\nNext post\n\n#### Advanced PostgreSQL Metrics and Insights on Tembo Cloud with pganalyze\n\nApr 3, 2024\n\npostgres rust engineering\n\n## On this page\n\nThe Control PlaneA Finite State MachineFSM in RustBuffer tasks between the\nplanesConducting events in the data-planeClosing the loopMoving forward\n\n## Share this article\n\n###### Company\n\nDocs Blog Pricing Product Cloud Trunk Roadmap Changelog\n\n###### Connect\n\nGithub Twitter LinkedIn Youtube Tembo Slack Trunk Slack\n\n###### Resources\n\nCareers\n\nPrivacy policy Terms of service\n\nSOC2 Type 1 compliant\n\n", "frontpage": true}
