{"aid": "40025501", "title": "DSPy on Databricks", "url": "https://www.databricks.com/blog/dspy-databricks", "domain": "databricks.com", "votes": 1, "user": "jamesblonde", "posted_at": "2024-04-13 19:48:25", "comments": 0, "source_title": "DSPy on Databricks | Databricks", "source_text": "DSPy on Databricks | Databricks\n\nSkip to main content\n\n# DSPy on Databricks\n\nby Arnav Singhvi, Michael Carbin and Matei Zaharia\n\nApril 8, 2024 in Mosaic AI Research\n\nShare this post\n\nLarge language models (LLMs) have generated interest in effective human-AI\ninteraction through optimizing prompting techniques. \u201cPrompt engineering\u201d is a\ngrowing methodology for tailoring model outputs, while advanced techniques\nlike Retrieval Augmented Generation (RAG) enhance LLMs\u2019 generative\ncapabilities by fetching and responding with relevant information.\n\nDSPy, developed by the Stanford NLP Group, has emerged as a framework for\nbuilding compound AI systems through \u201cprogramming, not prompting, foundation\nmodels.\u201d DSPy now supports integrations with Databricks developer endpoints\nfor Model Serving and Vector Search.\n\n## Engineering Compound AI\n\nThese prompting techniques signal a shift towards complex \u201cprompting\npipelines\u201d where AI developers incorporate LLMs, retrieval models (RMs), and\nother components while developing compound AI systems.\n\n## Programming not Prompting: DSPy\n\nDSPy optimizes AI-driven systems performance by composing LLM calls alongside\nother computational tools towards downstream task metrics. Unlike traditional\n\u201cprompt engineering,\u201d DSPy automates prompt tuning by translating user-defined\nnatural language signatures into complete instructions and few-shot examples.\nMirroring end-to-end pipeline optimization as in PyTorch, DSPy enables users\nto define and compose AI systems layer by layer while optimizing for the\ndesired objective.\n\n    \n    \n    class RAG(dspy.Module): def __init__(self, num_passages=3): super().__init__() # declare three modules: the retriever, a query generator, and an answer generator self.retrieve = retriever_model self.generate_answer = dspy.Predict(\"context, query -> answer\") def forward(self, query): retrieved_context = self.retrieve(query) context, context_ids = retrieved_context.docs, retrieved_context.doc_ids prediction = self.generate_answer(context=context, query=query) return dspy.Prediction(answer=prediction.answer)\n\nPrograms in DSPy have two main methods:\n\n  1. Initialization: Users can define the components of their prompting pipelines as DSPy layers. For instance, to account for the steps involved in RAG, we define a retrieval layer and a generation layer.\n\n    1. We define a retrieval layer `dspy.Retrieve` which uses the user-configured RM to retrieve a set of relevant passages/documents for an inputted search query.\n    2. We then initialize our generation layer, for which we use the `dspy.Predict` module, which internally prepares the prompt for generation. To configure this generation layer, we define our RAG task in a natural language signature format, specified by a set of input fields (\u201ccontext, query\u201d) and the expected output field (\u201canswer\u201d). This module then internally formats the prompt to match this defined formatting, and then returns the generation from the user-configured LM.\n  2. Forward: Akin to PyTorch forward passes, the DSPy program forward function allows for user composition of the prompting pipeline logic. By using the layers we initialized, we set up the computational flow of RAG by retrieving a set of passages given a query and then using these passages as context alongside the query to generate an answer, outputting the expected output in a DSPy dictionary object.\n\nLet\u2019s take a look at RAG in action using the DSPy program and DBRX\u2019s\ngeneration.\n\nFor this example, we use a sample question from the HotPotQA Dataset which\nincludes questions that require multiple steps to deduce the correct answer.\n\n    \n    \n    query = \"The Wings entered a new era, following the retirement of which Canadian retired professional ice hockey player and current general manager of the Tampa Bay Lightning of the National Hockey League (NHL)?\" answer = \"Steve Yzerman\"\n\nLet\u2019s first configure our LM and RM in DSPy. DSPy offers a variety of language\nand retrieval model integrations, and users can set these parameters to ensure\nany DSPy defined program runs through these configurations.\n\n    \n    \n    dspy.settings.configure(lm=lm, rm=retriever_model)\n\nLet\u2019s now declare our defined DSPy RAG program and simply pass in the question\nas the input.\n\n    \n    \n    rag = RAG() rag(query=query)\n\nDuring the retrieval step, the query is passed to the self.retrieve layer\nwhich outputs the top-3 relevant passages, which are internally formatted as\nbelow:\n\n    \n    \n    [1] \u00abSteve Yzerman | Stephen Gregory \"Steve\" Yzerman ( ; born May 9, 1965) is a Canadian retired professional ice hockey player and current general manager of the Tampa Bay Lightning of the National Hockey League (NHL). He is ...\u00bb [2] \u00ab2006\u201307 Detroit Red Wings season | The 2006\u201307 Detroit Red Wings season was the ...\u00bb [3] \u00abList of Tampa Bay Lightning general managers | The Tampa Bay Lightning are ...\u00bb\n\nWith these retrieved passages, we can pass this alongside our query into the\ndspy.Predict module self.generate_answer, matching the natural language\nsignature input fields \u201ccontext, query\u201d. This internally applies some basic\nformatting and phrasing, and enables you to direct the model with your exact\ntask description without prompt engineering the LM.\n\nOnce the formatting is declared, the input fields \u201ccontext\u201d and \u201cquery\u201d are\npopulated and the final prompt is sent to DBRX:\n\n    \n    \n    Given the fields `context`, `query`, produce the fields `answer`. --- Follow the following format. Context: ${context} Query: ${query} Reasoning: Let's think step by step in order to ${produce the answer}. We ... Answer: ${answer} --- Context: [1] \u00abSteve Yzerman | Stephen Gregory \"Steve\" Yzerman ( ; born May 9, 1965) is a Canadian retired professional ice hockey player and current general manager of the Tampa Bay Lightning of the National Hockey League (NHL). He is ...\u00bb [2] \u00ab2006\u201307 Detroit Red Wings season | The 2006\u201307 Detroit Red Wings season was the ...\u00bb [3] \u00abList of Tampa Bay Lightning general managers | The Tampa Bay Lightning are ...\u00bb Query: The Wings entered a new era, following the retirement of which Canadian retired professional ice hockey player and current general manager of the Tampa Bay Lightning of the National Hockey League (NHL)? Answer:\n\nDBRX generates an answer which is populated in the Answer: field, and we can\nobserve this prompt-generation through calling:\n\n    \n    \n    lm.inspect_history(n=1)\n\nThis outputs the last prompt-generation from the LM with the generated answer\n\u201cSteve Yzerman\u201d, which is the correct answer!\n\n    \n    \n    Given the fields `context`, `query`, produce the fields `answer`. --- Follow the following format. Context: ${context} Query: ${query} Reasoning: Let's think step by step in order to ${produce the answer}. We ... Answer: ${answer} --- Context: [1] \u00abSteve Yzerman | Stephen Gregory \"Steve\" Yzerman ( ; born May 9, 1965) is a Canadian retired professional ice hockey player and current general manager of the Tampa Bay Lightning of the National Hockey League (NHL). He is ...\u00bb [2] \u00ab2006\u201307 Detroit Red Wings season | The 2006\u201307 Detroit Red Wings season was the ...\u00bb [3] \u00abList of Tampa Bay Lightning general managers | The Tampa Bay Lightning are ...\u00bb Query: The Wings entered a new era, following the retirement of which Canadian retired professional ice hockey player and current general manager of the Tampa Bay Lightning of the National Hockey League (NHL)? Answer: Steve Yzerman.\n\nDSPy has been widely used across various language model tasks such as fine-\ntuning, in-context learning, information extraction, self-refinement, and\nnumerous others. This automated approach outperforms standard few-shot\nprompting with human-written demonstrations by up to 46% for GPT-3.5 and 65%\nfor Llama2-13b-chat on natural language tasks like multi-hop RAG and math\nbenchmarks like GSM8K.\n\n## DSPy on Databricks\n\nDSPy now supports integrations with Databricks developer endpoints for Model\nServing and Vector Search. Users can configure Databricks-hosted foundation\nmodel APIs under the OpenAI SDK through dspy.Databricks. This ensures users\ncan evaluate their end-to-end DSPy pipelines on Databricks-hosted models.\nCurrently, this supports models on the Model Serving Endpoints: chat (DBRX\nInstruct, Mixtral-8x7B Instruct, Llama 2 70B Chat), completion (MPT 7B\nInstruct) and embedding (BGE Large (En)) models.\n\n### Chat Models\n\n    \n    \n    lm = dspy.Databricks(model='databricks-dbrx-instruct', model_type='chat', api_key = {Databricks API key}, api_base = {Databricks Model Endpoint url}) lm(prompt)\n\n### Completion Models\n\n    \n    \n    lm = dspy.Databricks(model=\"databricks-mpt-7b-instruct\", ...) lm(prompt)\n\n### Embedding Models\n\n    \n    \n    lm = dspy.Databricks(model=\"databricks-bge-large-en\", model_type='embeddings', ...) lm(prompt)\n\n### Retriever Models/Vector Search\n\nAdditionally, users can configure retriever models through Databricks Vector\nSearch. Following the creation of a Vector Search index and endpoint, users\ncan specify the corresponding RM parameters through dspy.DatabricksRM:\n\n    \n    \n    retriever_model = DatabricksRM(databricks_index_name = index_name, databricks_endpoint = workspace_base_url, databricks_token = databricks_api_token, columns= [\"id\", \"text\", \"metadata\", \"text_vector\"], k=3, ...)\n\nUsers can configure this globally by setting the LM and RM to corresponding\nDatabricks endpoints and running DSPy programs.\n\n    \n    \n    dspy.settings.configure(lm=llm, rm=retriever_model)\n\nWith this integration, users can build and evaluate end-to-end DSPy\napplications, such as RAG, using Databricks endpoints!\n\nCheck out the official DSPy GitHub repository, documentation and Discord to\nlearn more about how to transform generative AI tasks into versatile DSPy\npipelines with Databricks!\n\n## Related posts\n\nJanuary 22, 2024\n\n#### Building and Customizing GenAI with Databricks: LLMs and Beyond\n\n  *     * Why Databricks\n    * Discover\n\n      * For Executives\n      * For Startups\n      * Lakehouse Architecture\n      * DatabricksIQ\n      * Mosaic Research\n    * Customers\n\n      * Featured\n      * See All\n    * Partners\n\n      * Cloud Providers\n      * Technology Partners\n      * Data Partners\n      * Built on Databricks\n      * Consulting & System Integrators\n      * C&SI Partner Program\n      * Partner Solutions\n    * Discover\n\n      * For Executives\n      * For Startups\n      * Lakehouse Architecture\n      * DatabricksIQ\n      * Mosaic Research\n    * Customers\n\n      * Featured\n      * See All\n    * Partners\n\n      * Cloud Providers\n      * Technology Partners\n      * Data Partners\n      * Built on Databricks\n      * Consulting & System Integrators\n      * C&SI Partner Program\n      * Partner Solutions\n  *     * Product\n    * Databricks Platform\n\n      * Platform Overview\n      * Sharing\n      * Governance\n      * Artificial Intelligence\n      * DBRX\n      * Data Management\n      * Data Warehousing\n      * Real-Time Analytics\n      * Data Engineering\n      * Data Science\n    * Pricing\n\n      * Pricing Overview\n      * Pricing Calculator\n    * Open Source\n    * Integrations and Data\n\n      * Marketplace\n      * IDE Integrations\n      * Partner Connect\n    * Databricks Platform\n\n      * Platform Overview\n      * Sharing\n      * Governance\n      * Artificial Intelligence\n      * DBRX\n      * Data Management\n      * Data Warehousing\n      * Real-Time Analytics\n      * Data Engineering\n      * Data Science\n    * Pricing\n\n      * Pricing Overview\n      * Pricing Calculator\n    * Integrations and Data\n\n      * Marketplace\n      * IDE Integrations\n      * Partner Connect\n  *     * Solutions\n    * Databricks For Industries\n\n      * Communications\n      * Financial Services\n      * Healthcare and Life Sciences\n      * Manufacturing\n      * Media and Entertainment\n      * Public Sector\n      * Retail\n      * View All\n    * Cross Industry Solutions\n\n      * Customer Data Platform\n      * Cyber Security\n    * Data Migration\n    * Professional Services\n    * Solution Accelerators\n    * Databricks For Industries\n\n      * Communications\n      * Financial Services\n      * Healthcare and Life Sciences\n      * Manufacturing\n      * Media and Entertainment\n      * Public Sector\n      * Retail\n      * View All\n    * Cross Industry Solutions\n\n      * Customer Data Platform\n      * Cyber Security\n  *     * Resources\n    * Documentation\n    * Customer Support\n    * Community\n    * Training and Certification\n\n      * Learning Overview\n      * Training Overview\n      * Certification\n      * University Alliance\n      * Databricks Academy Login\n    * Events\n\n      * Data + AI Summit\n      * Data + AI World Tour\n      * Data Intelligence Days\n      * Full Calendar\n    * Blog and Podcasts\n\n      * Databricks Blog\n      * Databricks Mosaic Research Blog\n      * Data Brew Podcast\n      * Champions of Data & AI Podcast\n    * Training and Certification\n\n      * Learning Overview\n      * Training Overview\n      * Certification\n      * University Alliance\n      * Databricks Academy Login\n    * Events\n\n      * Data + AI Summit\n      * Data + AI World Tour\n      * Data Intelligence Days\n      * Full Calendar\n    * Blog and Podcasts\n\n      * Databricks Blog\n      * Databricks Mosaic Research Blog\n      * Data Brew Podcast\n      * Champions of Data & AI Podcast\n  *     * About\n    * Company\n\n      * Who We Are\n      * Our Team\n      * Databricks Ventures\n    * Careers\n\n      * Open Jobs\n      * Working at Databricks\n    * Press\n\n      * Awards and Recognition\n      * Newsroom\n    * Security and Trust\n    * Company\n\n      * Who We Are\n      * Our Team\n      * Databricks Ventures\n    * Careers\n\n      * Open Jobs\n      * Working at Databricks\n    * Press\n\n      * Awards and Recognition\n      * Newsroom\n\nDatabricks Inc. 160 Spear Street, 13th Floor San Francisco, CA 94105\n1-866-330-0121\n\nSee Careers at Databricks\n\n\u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark and the\nSpark logo are trademarks of the Apache Software Foundation.\n\n  * Privacy Notice\n  * |Terms of Use\n  * |Your Privacy Choices\n  * |Your California Privacy Rights\n\n## We Care About Your Privacy\n\nBy clicking \u201cAccept All Cookies\u201d, you agree to the storing of cookies on your\ndevice to enhance site navigation, analyze site usage, and assist in our\nmarketing efforts.\n\n## Privacy Preference Center\n\n  * ### Your Privacy\n\n  * ### Strictly Necessary Cookies\n\n  * ### Performance Cookies\n\n  * ### Functional Cookies\n\n  * ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your\nbrowser, mostly in the form of cookies. This information might be about you,\nyour preferences or your device and is mostly used to make the site work as\nyou expect it to. The information does not usually directly identify you, but\nit can give you a more personalized web experience. Because we respect your\nright to privacy, you can choose not to allow some types of cookies. Click on\nthe different category headings to find out more and change our default\nsettings. However, blocking some types of cookies may impact your experience\nof the site and the services we are able to offer. For more information, see\nour Cookie Notice\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched\noff in our systems. They are usually only set in response to actions made by\nyou which amount to a request for services, such as setting your privacy\npreferences, logging in or filling in forms. You can set your browser to block\nor alert you about these cookies, but some parts of the site will not then\nwork.\n\n#### Performance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure\nand improve the performance of our site. They help us to know which pages are\nthe most and least popular and see how visitors move around the site.\n\n#### Functional Cookies\n\nThese cookies enable the website to provide enhanced functionality and\npersonalization. They may be set by us or by third party providers whose\nservices we have added to our pages. If you do not allow these cookies then\nsome or all of these services may not function properly.\n\n#### Targeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They\nmay be used by those companies to build a profile of your interests and show\nyou relevant advertisements on other sites. If you do not allow these cookies,\nyou will experience less targeted advertising.\n\n### Cookie List\n\nConsent Leg.Interest\n\nlabel\n\nlabel\n\nlabel\n\nlabel\n\n", "frontpage": false}
