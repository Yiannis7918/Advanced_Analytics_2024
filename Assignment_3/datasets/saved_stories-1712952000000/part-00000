{"aid": "40012962", "title": "Pragmatic Unicode (2012)", "url": "https://nedbatchelder.com/text/unipain.html", "domain": "nedbatchelder.com", "votes": 2, "user": "Tomte", "posted_at": "2024-04-12 13:57:39", "comments": 0, "source_title": "Pragmatic Unicode", "source_text": "Pragmatic Unicode | Ned Batchelder\n\n# Pragmatic Unicode\n\nCreated 10 March 2012\n\nThis is a presentation I gave at PyCon 2012. You can read the slides and text\non this page, or open the actual presentation in your browser, or watch the\nvideo:\n\nAlso, clicking the slide images will jump into the full presentation at that\npoint. The Symbola font is included, but will have to be downloaded before\nsome of the special symbols will appear.\n\nHi, I\u2019m Ned Batchelder. I\u2019ve been writing in Python for over ten years, which\nmeans at least a half-dozen times, I\u2019ve made the same Unicode mistakes that\neveryone else has.\n\nIf you\u2019re like most Python programmers, you\u2019ve done it too: you\u2019ve built a\nnice application, and everything seemed to be going fine. Then one day an\naccented character appeared out of nowhere, and your program started belching\nUnicodeErrors.\n\nYou kind of knew what to do with those, so you added an encode or a decode\nwhere the error was raised, but the UnicodeError happened somewhere else. You\nwent to the new place, and added a decode, maybe an encode. After playing\nwhack-a-mole like this for a while, the problem seemed to be fixed.\n\nThen a few days later, another accent appeared in another place, and you had\nto play a little bit more whack-a-mole until the problem finally stopped.\n\nSo now you have a program that works, but you\u2019re annoyed and uncomfortable, it\ntook too long, you know it isn\u2019t \u201cright,\u201d and you hate yourself. And the main\nthing you know about Unicode is that you don\u2019t like Unicode.\n\nYou don\u2019t want to know about weirdo character sets, you just want to be able\nto write a program that doesn\u2019t make you feel bad.\n\nYou don\u2019t have to play whack-a-mole. Unicode isn\u2019t simple, but it isn\u2019t\ndifficult either. With knowledge and discipline, you can deal with Unicode\neasily and with grace.\n\nI\u2019ll teach you five Facts of Life, and give you three Pro Tips that will solve\nyour Unicode problems. We\u2019re going to cover the basics of Unicode, and how\nboth Python 2 and Python 3 work. They are different, but the strategies you\u2019ll\nuse are basically the same.\n\n# The World & Unicode\n\nWe\u2019ll start with the basics of Unicode.\n\nThe first Fact of Life: everything in a computer is bytes. Files on disk are a\nseries of bytes, and network connections only transmit bytes. Almost without\nexception, all the data going into or out of any program you write, is bytes.\n\nThe problem with bytes is that by themselves they are meaningless, we need\nconventions to give them meaning.\n\nTo represent text, we\u2019ve been using the ASCII code for nearly 50 years. Every\nbyte is assigned one of 95 symbols. When I send you a byte 65, you know that I\nmean an upper-case A, but only because we\u2019ve agreed beforehand on what each\nbyte represents.\n\nISO Latin 1, or 8859-1, is ASCII extended with 96 more symbols.\n\nWindows added 27 more symbols to produce CP1252. This is pretty much the best\nyou can do to represent text as single bytes, because there\u2019s not much room\nleft to add more symbols.\n\nWith character sets like these, we can represent at most 256 characters. But\nFact of Life #2 is that there are way more than 256 symbols in the world\u2019s\ntext. A single byte simply can\u2019t represent text world-wide. During your\ndarkest whack-a-mole moments, you may have wished that everyone spoke English,\nbut it simply isn\u2019t so. People need lots of symbols to communicate.\n\nFact of Life #1 and Fact of Life #2 together create a fundamental conflict\nbetween the structure of our computing devices, and the needs of the world\u2019s\npeople.\n\nThere have been a number of attempts to resolve this conflict. Single-byte\ncharacter codes like ASCII map bytes to characters. Each one pretends that\nFact of Life #2 doesn\u2019t exist.\n\nThere are many single-byte codes, and they don\u2019t solve the problem. Each is\nonly good for representing one small slice of human language. They can\u2019t solve\nthe global text problem.\n\nPeople tried creating double-byte character sets, but they were still\nfragmented, serving different subsets of people. There were multiple standards\nin place, and they still weren\u2019t large enough to deal with all the symbols\nneeded.\n\nUnicode was designed to deal decisively with the issues with older character\ncodes. Unicode assigns integers, known as code points, to characters. It has\nroom for 1.1 million code points, and only 110,000 are already assigned, so\nthere\u2019s plenty of room for future growth.\n\nUnicode\u2019s goal is to have everything. It starts with ASCII, and includes\nthousands of symbols, including the famous Snowman, covers all the writing\nsystems of the world, and is constantly being expanded. For example, the\nlatest update gave us the symbol PILE OF POO.\n\nHere is a string of six exotic Unicode characters. Unicode code points are\nwritten as 4-, 5-, or 6-digits of hex with a U+ prefix. Every character has an\nunambiguous full name which is always in uppercase ASCII.\n\nThis string is designed to look like the word \u201cPython\u201d, but doesn\u2019t use any\nASCII characters at all.\n\nSo Unicode makes room for all of the characters we could ever need, but we\nstill have Fact of Life #1 to deal with: computers need bytes. We need a way\nto represent Unicode code points as bytes in order to store or transmit them.\n\nThe Unicode standard defines a number of ways to represent code points as\nbytes. These are called encodings.\n\nUTF-8 is easily the most popular encoding for storage and transmission of\nUnicode. It uses a variable number of bytes for each code point. The higher\nthe code point value, the more bytes it needs in UTF-8. ASCII characters are\none byte each, using the same values as ASCII, so ASCII is a subset of UTF-8.\n\nHere we show our exotic string as UTF-8. The ASCII characters H and i are\nsingle bytes, and other characters use two or three bytes depending on their\ncode point value. Some Unicode code points require four bytes, but we aren\u2019t\nusing any of those here.\n\n# Python 2\n\nOK, enough theory, let\u2019s talk about Python 2. In the slides, Python 2 samples\nhave a big 2 in the upper-right corner, Python 3 samples will have a big 3.\n\nIn Python 2, there are two different string data types. A plain-old string\nliteral gives you a \u201cstr\u201d object, which stores bytes. If you use a \u201cu\u201d prefix,\nyou get a \u201cunicode\u201d object, which stores code points. In a unicode string\nliteral, you can use backslash-u to insert any Unicode code point.\n\nNotice that the word \u201cstring\u201d is problematic. Both \u201cstr\u201d and \u201cunicode\u201d are\nkinds of strings, and it\u2019s tempting to call either or both of them \u201cstring,\u201d\nbut it\u2019s better to use more specific terms to keep things straight.\n\nByte strings and unicode strings each have a method to convert it to the other\ntype of string. Unicode strings have a .encode() method that produces bytes,\nand byte strings have a .decode() method that produces unicode. Each takes an\nargument, which is the name of the encoding to use for the operation.\n\nWe can define a Unicode string named my_unicode, and see that it has 9\ncharacters. We can encode it to UTF-8 to create the my_utf8 byte string, which\nhas 19 bytes. As you\u2019d expect, reversing the operation by decoding the UTF-8\nstring produces the original Unicode string.\n\nUnfortunately, encoding and decoding can produce errors if the data isn\u2019t\nappropriate for the specified encoding. Here we try to encode our exotic\nUnicode string to ASCII. It fails because ASCII can only represent charaters\nin the range 0 to 127, and our Unicode string has code points outside that\nrange.\n\nThe UnicodeEncodeError that\u2019s raised indicates the encoding being used, in the\nform of the \u201ccodec\u201d (short for coder/decoder), and the actual position of the\ncharacter that caused the problem.\n\nDecoding can also produce errors. Here we try to decode our UTF-8 string as\nASCII and get a UnicodeDecodeError because again, ASCII can only accept values\nup to 127, and our UTF-8 string has bytes outside that range.\n\nEven UTF-8 can\u2019t decode any sequence of bytes. Next we try to decode some\nrandom junk, and it also produces a UnicodeDecodeError. Actually, one of\nUTF-8\u2019s advantages is that there are invalid sequences of bytes, which helps\nto build robust systems: mistakes in data won\u2019t be accepted as if they were\nvalid.\n\nWhen encoding or decoding, you can specify what should happen when the codec\ncan\u2019t handle the data. An optional second argument to encode or decode\nspecifies the policy. The default value is \u201cstrict\u201d, which means raise an\nerror, as we\u2019ve seen.\n\nA value of \u201creplace\u201d means, give me a standard replacement character. When\nencoding, the replacement character is a question mark, so any code point that\ncan\u2019t be encoded using the specified encoding will simply produce a \u201c?\u201d.\n\nOther error handlers are more useful. \u201cxmlcharrefreplace\u201d produces an HTML/XML\ncharacter entity reference, so that \\u01B4 becomes \u201c&#436;\u201d (hex 01B4 is\ndecimal 436.) This is very useful if you need to output unicode for an HTML\nfile.\n\nNotice that different error policies are used for different reasons. \u201cReplace\u201d\nis a defensive mechanism against data that cannot be interpreted, and loses\ninformation. \u201cXmlcharrefreplace\u201d preserves all the original information, and\nis used when outputting data where XML escapes are acceptable.\n\nYou can also specify error handling when decoding. \u201cIgnore\u201d will drop bytes\nthat can\u2019t decode properly. \u201cReplace\u201d will insert a Unicode U+FFFD,\n\u201cREPLACEMENT CHARACTER\u201d for problem bytes. Notice that since the decoder can\u2019t\ndecode the data, it doesn\u2019t know how many Unicode characters were intended.\nDecoding our UTF-8 bytes as ASCII produces 16 replacement characters, one for\neach byte that couldn\u2019t be decoded, while those bytes were meant to only\nproduce 6 Unicode characters.\n\nPython 2 tries to be helpful when working with unicode and byte strings. If\nyou try to perform a string operation that combines a unicode string with a\nbyte string, Python 2 will automatically decode the byte string to produce a\nsecond unicode string, then will complete the operation with the two unicode\nstrings.\n\nFor example, we try to concatenate a unicode \u201cHello \u201d with a byte string\n\u201cworld\u201d. The result is a unicode \u201cHello world\u201d. On our behalf, Python 2 is\ndecoding the byte string \u201cworld\u201d using the ASCII codec. The encoding used for\nthese implicit decodings is the value of sys.getdefaultencoding().\n\nThe implicit encoding is ASCII because it\u2019s the only safe guess: ASCII is so\nwidely accepted, and is a subset of so many encodings, that it\u2019s unlikely to\nproduce false positives.\n\nOf course, these implicit decodings are not immune to decoding errors. If you\ntry to combine a byte string with a unicode string and the byte string can\u2019t\nbe decoded as ASCII, then the operation will raise a UnicodeDecodeError.\n\nThis is the source of those painful UnicodeErrors. Your code inadvertently\nmixes unicode strings and byte strings, and as long as the data is all ASCII,\nthe implicit conversions silently succeed. Once a non-ASCII character finds\nits way into your program, an implicit decode will fail, causing a\nUnicodeDecodeError.\n\nPython 2\u2019s philosophy was that unicode strings and byte strings are confusing,\nand it tried to ease your burden by automatically converting between them,\njust as it does for ints and floats. But the conversion from int to float\ncan\u2019t fail, while byte string to unicode string can.\n\nPython 2 silently glosses over byte to unicode conversions, making it much\neasier to write code that deals with ASCII. The price you pay is that it will\nfail with non-ASCII data.\n\nThere are lots of ways to combine two strings, and all of them will decode\nbytes to unicode, so you have to watch out for them.\n\nHere we use an ASCII format string, with unicode data. The format string will\nbe decoded to unicode, then the formatting performed, resulting in a unicode\nstring.\n\nNext we switch the two: A unicode format string and a byte string again\ncombine to produce a unicode string, because the byte string data is decoded\nas ASCII.\n\nEven just attempting to print a unicode string will cause an implicit\nencoding: output is always bytes, so the unicode string has to be encoded into\nbytes before it can be printed.\n\nThe next one is truly confusing: we ask to encode a byte string to UTF-8, and\nget an error about not being about to decode as ASCII! The problem here is\nthat byte strings can\u2019t be encoded: remember encode is how you turn unicode\ninto bytes. So to perform the encoding you want, Python 2 needs a unicode\nstring, which it tries to get by implicitly decoding your bytes as ASCII.\n\nSo you asked to encode to UTF-8, and you get an error about decoding ASCII. It\npays to look carefully at the error, it has clues about what operation is\nbeing attempted, and how it failed.\n\nLastly, we encode an ASCII string to UTF-8, which is silly, encode should be\nused on unicode string. To make it work, Python performs the same implicit\ndecode to get a unicode string we can encode, but since the string is ASCII,\nit succeeds, and then goes on to encode it as UTF-8, producing the original\nbyte string, since ASCII is a subset of UTF-8.\n\nThis is the most important Fact of Life: bytes and unicode are both important,\nand you need to deal with both of them. You can\u2019t pretend that everything is\nbytes, or everything is unicode. You need to use each for their purpose, and\nexplicitly convert between them as needed.\n\n# Python 3\n\nWe\u2019ve seen the source of Unicode pain in Python 2, now let\u2019s take a look at\nPython 3. The biggest change from Python 2 to Python 3 is their treatment of\nUnicode.\n\nJust as in Python 2, Python 3 has two string types, one for unicode and one\nfor bytes, but they are named differently.\n\nNow the \u201cstr\u201d type that you get from a plain string literal stores unicode,\nand the \u201cbytes\u201d types stores bytes. You can create a bytes literal with a b\nprefix.\n\nSo \u201cstr\u201d in Python 2 is now called \u201cbytes,\u201d and \u201cunicode\u201d in Python 2 is now\ncalled \u201cstr\u201d. This makes more sense than the Python 2 names, since Unicode is\nhow you want all text stored, and byte strings are only for when you are\ndealing with bytes.\n\nThe biggest change in the Unicode support in Python 3 is that there is no\nautomatic decoding of byte strings. If you try to combine a byte string with a\nunicode string, you will get an error all the time, regardless of the data\ninvolved!\n\nAll of those operations I showed where Python 2 silently converted byte\nstrings to unicode strings to complete an operation, every one of them is an\nerror in Python 3.\n\nIn addition, Python 2 considers a unicode string and a byte string equal if\nthey contain the same ASCII bytes, and Python 3 won\u2019t. A consequence of this\nis that unicode dictionary keys can\u2019t be found with byte strings, and vice-\nversa, as they can be in Python 2.\n\nThis drastically changes the nature of Unicode pain in Python 3. In Python 2,\nmixing unicode and bytes succeeds so long as you only use ASCII data. In\nPython 3, it fails immediately regardless of the data.\n\nSo Python 2\u2019s pain is deferred: you think your program is correct, and find\nout later that it fails with exotic characters.\n\nWith Python 3, your code fails immediately, so even if you are only handling\nASCII, you have to explicitly deal with the difference between bytes and\nunicode.\n\nPython 3 is strict about the difference between bytes and unicode. You are\nforced to be clear in your code which you are dealing with. This has been\ncontroversial, and can cause you pain.\n\nBecause of this new strictness, Python 3 has changed how you read files.\nPython has always had two modes for reading files: binary and text. In Python\n2, it only affected the line endings, and on Unix platforms, even that was a\nno-op.\n\nIn Python 3, the two modes produce different results. When you open a file in\ntext mode, either with \u201cr\u201d, or by defaulting the mode entirely, the data read\nfrom the file is implicitly decoded into Unicode, and you get str objects.\n\nIf you open a file in binary mode, by supplying \u201crb\u201d as the mode, then the\ndata read from the file is bytes, with no processing done on them.\n\nThe implicit conversion from bytes to unicode uses the encoding returned from\nlocale.getpreferredencoding(), and it may not give you the results you expect.\nFor example, when we read hi_utf8.txt, it\u2019s being decoded using the locale\u2019s\npreferred encoding, which since I created these samples on Windows, is\n\u201ccp1252\u201d. Like ISO 8859-1, CP-1252 is a one-byte character code that will\naccept any byte value, so it will never raise a UnicodeDecodeError. That also\nmeans that it will happily decode data that isn\u2019t actually CP-1252, and\nproduce garbage.\n\nTo get the file read properly, you should specify an encoding to use. The\nopen() function now has an optional encoding parameter.\n\n# Pain relief\n\nOK, so how do we deal with all this pain? The good news is that the rules to\nremember are simple, and they\u2019re the same for Python 2 and Python 3.\n\nAs we saw with Fact of Life #1, the data coming into and going out of your\nprogram must be bytes. But you don\u2019t need to deal with bytes on the inside of\nyour program. The best strategy is to decode incoming bytes as soon as\npossible, producing unicode. You use unicode throughout your program, and then\nwhen outputting data, encode it to bytes as late as possible.\n\nThis creates a Unicode sandwich: bytes on the outside, Unicode on the inside.\n\nKeep in mind that sometimes, a library you\u2019re using may do some of these\nconversions for you. The library may present you with Unicode input, or will\naccept Unicode for output, and the library will take care of the edge\nconversion to and from bytes. For example, Django provides Unicode, as does\nthe json module.\n\nThe second rule is, you have to know what kind of data you are dealing with.\nAt any point in your program, you need to know whether you have a byte string\nor a unicode string. This shouldn\u2019t be a matter of guessing, it should be by\ndesign.\n\nIn addition, if you have a byte string, you should know what encoding it is if\nyou ever intend to deal with it as text.\n\nWhen debugging your code, you can\u2019t simply print a value to see what it is.\nYou need to look at the type, and you may need to look at the repr of the\nvalue in order to get to the bottom of what data you have.\n\nI said you have to understand what encoding your byte strings are. Here\u2019s Fact\nof Life #4: You can\u2019t determine the encoding of a byte string by examining it.\nYou need to know through other means. For example, many protocols include ways\nto specify the encoding. Here we have examples from HTTP, HTML, XML, and\nPython source files. You may also know the encoding by prior arrangement, for\nexample, the spec for a data source may specify the encoding.\n\nThere are ways to guess at the encoding of the bytes, but they are just\nguesses. The only way to be sure of the encoding is to find it out some other\nway.\n\nHere\u2019s an example of our exotic Unicode string, encoded as UTF-8, and then\nmistakenly decoded in a variety of encodings. As you can see, decoding with an\nincorrect encoding might succeed, but produce the wrong characters. Your\nprogram can\u2019t tell it\u2019s decoding wrong, only when people try to read the text\nwill you know something has gone wrong.\n\nThis is a good demonstration of Fact of Life #4: the same stream of bytes is\ndecodable using a number of different encodings. The bytes themselves don\u2019t\nindicate what encoding they use.\n\nBTW, there\u2019s a term for this garbage display, from the Japanese who have been\ndealing with this for years and years: Mojibake.\n\nUnfortunately, because the encoding for bytes has to be communicated\nseparately from the bytes themselves, sometimes the specified encoding is\nwrong. For example, you may pull an HTML page from a web server, and the HTTP\nheader claims the page is 8859-1, but in fact, it is encoded with UTF-8.\n\nIn some cases, the encoding mismatch will succeed and cause mojibake. Other\ntimes, the encoding is invalid for the bytes, and will cause a UnicodeError of\nsome sort.\n\nIt should go without saying: you should explicitly test your Unicode support.\nTo do this, you need challenging Unicode data to pump through your code. If\nyou are an English-only speaker, you may have a problem doing this, because\nlots of non-ASCII data is hard to read. Luckily, the variety of Unicode code\npoints mean you can construct complex Unicode strings that are still readable\nby English speakers.\n\nHere\u2019s an example of overly-accented text, readable pseudo-ASCII text, and\nupside-down text. One good source of these sorts of strings are various web\nsites that offer strings like this for teenagers to paste into social\nnetworking sites.\n\nDepending on your application, you may need to dig deeper into the other\ncomplexities in the Unicode world. There are many details that I haven\u2019t\ncovered here, and they can be very involved. I call this Fact of Life #51\u20442\nbecause you may not have to deal with any of this.\n\nTo review, these are the five unavoidable Facts of Life:\n\n  1. All input and output of your program is bytes.\n  2. The world needs more than 256 symbols to communicate text.\n  3. Your program has to deal with both bytes and Unicode.\n  4. A stream of bytes can\u2019t tell you its encoding.\n  5. Encoding specifications can be wrong.\n\nThese are the three Pro Tips to keep in mind as you build your software to\nkeep your code Unicode-clean:\n\n  1. Unicode sandwich: keep all text in your program as Unicode, and convert as close to the edges as possible.\n  2. Know what your strings are: you should be able to explain which of your strings are Unicode, which are bytes, and for your byte strings, what encoding they use.\n  3. Test your Unicode support. Use exotic strings throughout your test suites to be sure you\u2019re covering all the cases.\n\nIf you follow these tips, you\u2019ll write good solid code that deals well with\nUnicode, and won\u2019t fall over no matter how wild the Unicode it encounters.\n\nOther resources you might find helpful:\n\nJoel Spolsky wrote The Absolute Minimum Every Software Developer Absolutely,\nPositively Must Know About Unicode and Character Sets (No Excuses!), which\ncovers how Unicode works and why. It has no Python-specific information, but\nis better written than this talk!\n\nIf you need to deal with the semantics of arbitrary Unicode characters, the\nunicodedata module in the Python standard library has functions that can help.\n\nFor testing Unicode, the various \u201cfancy\u201d text generators for use on social\nnetworks work great.\n\n  *     * about\n    * site\n    * email\n    * mastodon\n    * bluesky\n    * twitter\n    * irc\n    * slack\n    * discord\n    * sponsor\n    * tidelift\n    * github\n    * linkedin\n    * r\u00e9sum\u00e9\n\nDarkLight Something wrong?\n\n\u00a9 Copyright 2012, Ned Batchelder\n\n", "frontpage": false}
