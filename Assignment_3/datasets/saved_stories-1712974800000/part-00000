{"aid": "40017198", "title": "Fighting Alert Fatigue", "url": "https://www.checklyhq.com/blog/alert-fatigue/", "domain": "checklyhq.com", "votes": 1, "user": "serverlessmom", "posted_at": "2024-04-12 20:18:09", "comments": 0, "source_title": "How to Fight Alert Fatigue with Synthetic Monitoring", "source_text": "How to Fight Alert Fatigue with Synthetic Monitoring\n\n/Blog/Alert Fatigue\n\nApril 12, 2024\n\n(Updated: April 11, 2024)\n\n# How to Fight Alert Fatigue with Synthetic Monitoring: 7 Best Practices\n\nNo\u010dnica Mellifera\n\nDeveloper Advocate\n\nPlaywright\n\nShare on social\n\n## What is alert fatigue?\n\nIt\u2019s 1am, and something has gone very wrong. The head of sales is in the\nincident response channel because our top customer is reporting a system-wide\noutage. Everyone\u2019s running around trying to figure it out. As you look at\nservice maps and traces, you get a sinking feeling. Earlier the previous\nevening, you got an alert that user-access-service was running out of memory.\nYou\u2019d gotten the same alert a few times a day for the last two months, and\neach time by the time you accessed the service either the process eating\nmemory had died or garbage collection had kicked off. It was a false alarm\nevery time. It was on your list to investigate, but for the time being every\ntime you saw user-access-service on an alert you just swiped it away and went\nabout your day. Now you were realizing that this time it wasn\u2019t a false alarm,\nand you were going to have some very weird numbers for time-to-detect and\ntime-to-resolve at the post-mortem for this incident.\n\nThe scenario above is one we can all relate to: an alert that could be safely\nignored for days or weeks suddenly showed up as the first indicator of real\nproblems. No one decided to let our service fail, and we meant to improve the\nsituation at some point, but it was allowed to continue until it caused, or at\nleast failed to prevent, an outage and an SLA breach. If your team practices\nblameless post-mortems, the cause won\u2019t be \u2018laziness\u2019 or \u2018failure to\ninvestigate alerts\u2019 but a more subtle, insidious and universal failure: alert\nfatigue.\n\nAs this article will explore, alert fatigue is the limiting factor on the\nscale of monitoring and alerting. Alert fatigue must be monitored as closely\nas infrastructure cost and team happiness if you want to ensure your team will\ntake the right actions to prevent an outage.\n\nAlert fatigue is a hidden cost of observability that significantly affects the\njob satisfaction and well-being of Site Reliability Engineers (SREs),\nOperations Engineers, and Developers, especially those on an on-call rotation.\n\n## How Alert Fatigue Happens\n\nAlert fatigue didn\u2019t start with APM, but the prevalence of advanced\nobservability tools have made the issue more widespread. Observability allows\nteams to monitor their systems in real-time, anticipate issues, and respond\nswiftly to ensure uninterrupted service. Almost all observability tools offer\nintegrations with notification infrastructure to let you tell the right people\nwhen a problem gets serious, Checkly is no exception with detailed\nnotification settings and alert thresholds. When we turn our thresholds for\nalerting too low, however, the result is alert fatigue. If most alerts can be\nsafely ignored, humans will stop examining each alert closely and instead\nignore all alerts until something notifies them at a higher level. In the\nscenario offered above, this higher alert level was a user talking to their\nAE, and the sales team getting in touch with Operations.\n\nOne contributing factor to alert fatigue is the prevalence of alerts delivered\nto mobile devices and engineers receiving alerts outside their working hours.\nIn my experience, when alerts are delivered to your phone but investigating\nthat alert requires opening your laptop, it's more likely that a few false\nalarms will quickly lead an engineer to start dismissing almost all alerts\nout-of-hand unless the alert type is unusual.\n\n### Alert Fatigue is Universal\n\nWhen researching this article, I ran across some research from the National\nInstitutes of Health (NIH) describing how it was the number and repetition of\nalerts, not the underlying workload of the recipient, that caused alert\nfatigue. In this case, it wasn\u2019t Operations engineers getting alerts on memory\nleaks, but doctors getting alerts on patient drug interactions, but the\nprinciple remains the same: even when the baseline workload is manageable,\nrepetitive alerts causes people to stop accepting new information and acting\nappropriately.\n\nIn the study mentioned above, the critical factor for an alert to be ignored\nby a doctor was the number of times the same patient had produced an alert.\nThere\u2019s clearly a cognitive bias for \u2018oh, this source always alerts.\u2019 That\nmakes it more likely that you\u2019ll ignore crucial alerts.\n\n### Alert fatigue causes more than increased time-to-resolution\n\nImagine being bombarded with hundreds of alerts daily, with a significant\nportion being false alarms or non-critical. Over time, this constant barrage\ncan desensitize even the most dedicated professionals, leading to slower\nresponse times, missed critical alerts, and, ultimately, a decrease in system\nreliability. This situation not only threatens the stability of services but\nalso erodes the job satisfaction of those tasked with maintaining them.\nAcknowledging and addressing alert fatigue is essential in safeguarding not\nonly our systems, but also the people who keep them running.\n\n## The Risks of Alert Fatigue\n\nWe all know the risks of not getting an alert: missed outages, broken SLA\u2019s,\nand eroded trust with users. But when we turn up alerting too high, what are\nthe risks we run there?\n\n### Missed Alerts\n\nThe first and most obvious consequence of alert fatigue is the number of times\nalerts are simply missed. Any time we get more than a few alerts an hour\nduring working hours, or more than a few times a day when not working and on\ncall, it\u2019s likely that some alerts won\u2019t be read or fully interpreted. Every\nunecessary alert increases the chance that the most critical alert will be\nignored. This isn\u2019t a human failure or the result of laziness, but rather the\nnatural outcome of burying critical signals in a lot of noise.\n\n### Burnout\n\nTech workers are, by and large, quite generous with their time and\navailability. Whether it\u2019s a weekend hackathon or a \u2018quick question\u2019 on Slack,\nwe tend not to strictly enforce our time boundaries. Teams like Checkly have\ngreat work-life balance, insisting that work only happen during our scheduled\nhours, so when we get a late-night ping we can be quite certain it\u2019s an\nemergency.\n\nWhen we ping our team members too often, both trust and work-life balance\nquickly get broken. Every time I look at my phone and see a \u2018critical\u2019 alert\nthat turns out to be nothing, my weekend gets slightly worse. If alerts are\ncoming several times every day when I shouldn\u2019t be working at all, I start\nbrowsing LinkedIn to look for my next role.\n\nIf we let alert fatigue get out of hand, we increase the risk of losing our\nbest team members.\n\n## 7 best practices to fight alert fatigue with Synthetic Monitoring\n\nSynthetic monitoring is still sometimes referred to as a \u2018site pinger\u2019 that\nrepeatedly checks your site. How can such a high-frequency service check be\npart of the solution for alert fatigue? There are three key themes to\nsynthetics monitoring best practies:\n\n  1. Ensure the flows being tested are critical to actual users.\n  2. Give reliable alerts with retries and alert thresholds.\n  3. Work from a shared repository that makes tests easier to maintain.\n\nSynthetic monitoring, when executed with best practices, serves as a critical\ntool for ensuring that alerts are meaningful and actionable, especially in\nscenarios where immediate attention is required. Here's how synthetic\nmonitoring can be optimized to combat alert fatigue:\n\n### 1\\. Don\u2019t hard-code waiting time\n\nIf you\u2019re using an older testing framework, you may be used to directly coding\nwait statements in seconds to wait for elements to load or respond. This has\ntwo major drawbacks: it increases the minimum time for a test to execute, and\ncan cause too-rigid tests that fail too frequently.\n\nWe\u2019ve all interacted with a test suite before where \u201cthose 40 tests always\nfail\u201d and we want to avoid that situation as much as possible.\n\nIf you use a modern testing framework like playwright, you don\u2019t need to add\nmanual wait times, since playwright will automatically wait until an element\nis available. Of course, if slow loading is the problem you\u2019re most worried\nabout, you can always set a threshold for the maximum time allowed for the\ntest.\n\nBy storing this threshold in the test configuration rather than buried in\nindividual lines of code (often with manual waits you have to add up several\nlines to calculate the maximum test time), you enable others to easily audit\nand tweak these test expectations. This is further empowered if you implement\nMonitoring as Code, as described in point six below.\n\n### 2\\. Focus on Crucial User Flows\n\nSynthetic monitoring should prioritize testing the most critical user flows.\nThis user-centric approach ensures that the monitoring efforts are directly\ntied to the user experience, making each alert an indicator of potential user\nimpact.\n\nAs a practice, try writing test names in terms of user expectations. This\nisn\u2019t a full \u2018user story\u2019 format but rather just describing what the user is\ntrying to do for example: user_login_and_check_profile rather than\nfrontend_test_37. If you find that you can\u2019t summarize what your test is\nchecking, or you need to describe your test in terms of the systems its\nchecking (for example recent_browsing_dropdown), it\u2019s possible that you aren\u2019t\nchecking a critical user flow.\n\n### 3\\. Implement Smart Retries\n\nYou don\u2019t always want to get an alert every single time an element can\u2019t be\nfound on a page, or an image takes more than 7 seconds to load. Incorporating\nsmart retries into your synthetic monitoring strategy helps differentiate\nbetween transient glitches and persistent issues. By automatically retrying a\ntest before firing an alert, you can filter out temporary network hiccups or\nshort-lived server issues, ensuring that only verified, consistent problems\ntrigger notifications.\n\nCheckly offers three retry strategies for most checks (except Heartbeat\nchecks) and check groups, allowing customization based on the specific needs\nof your monitoring setup:\n\nFixed sets a single flat interval between retries, Linear increases the\nincrement arithmetically, and Exponential multiplies the interval with each\nretry, e.g., 5 seconds, then 25 seconds, then 125 seconds. If your test is\nkicking off a heavy duty compute process, exponential can help ensure that\nit\u2019s not your own retries that are causing a slowdown.\n\nWith Checkly you can also set a cap on the total time spent in retries to\navoid excessive attempts. This duration includes the execution time of the\ncheck itself. For instance, if the maximum is set to two minutes and the check\ntakes 1.5 minutes to run, only 30 seconds are available for retry attempts.\n\n### 4\\. Label test steps\n\nGoing back to the research cited in the introduction, it\u2019s repetitive alerts\nthat trigger fatigue. If you can offer as much information to the engineer as\npossible in the report on a failed check. To this end, break down more complex\ntests into well-labeled steps. In Playwright you can create steps as a top\nlevel organization of the pieces in your test.\n\nThese steps can then be viewed in the generated HTML report, providing a high-\nlevel overview of the test's flow. This not only makes it easier for someone\nnew to the test to understand what's happening but also aids in debugging by\npinpointing which step failed.\n\nAdding steps, along with point six about embracing monitoring-as-code also has\nthe knock-on effect of making your entire set of tests more maintainable. When\nyou go back to work on a test 6 months after you first wrote it, you\u2019ll be\nglad you included steps to show your intent with each chunk.\n\n### 5\\. Use visual tools for faster interpretation\n\nUtilizing visual regression testing and screen captures can provide deeper\ninsights into the issues detected. Visual regression testing allows you to\nspot unintended changes in the UI, while screen captures can offer immediate\nvisual context to the problem at hand. When our engineers are using mobile\ndevices or otherwise aren\u2019t using their main PC to investigate, a picture can\nreally be worth a thousand words. Take an example like the one below:\n\nA visual regression test letting the engineer compare the current screenshot\nto the \u2018golden\u2019 version.\n\nThis visual regression test triggered an alert, but a quick view of the\nchanges identifies there\u2019s no missing components or broken UI, it\u2019s just that\nsomeone swapped a visual asset without updating the test. With this view, the\nengineer can quickly decide if they need to investigate right away or if the\nalert can be safely ignored. The result is a faster time to understanding and\nbetter experience for your on-call team.\n\n### 6\\. Embrace Monitoring as Code (MaC)\n\nFundamentally, alert fatigue is often a sign of tech debt within our testing\nsystem. In the intial scenario offered in this article, we knew that a daily\nalert shouldn\u2019t be firing and resolved to fix it \u2018at some point\u2019 but it was\nallowed to continue for some time. The more you make your tests easy to\nmaintain and modify, the more relevant your alerts, and the less the risk of\nalert fatigue.\n\nSadly, in a tech world that adopted source control for application code a\ndecade ago, still many monitors and alerts are managed only through a browser\ninterface, often with no clear change history or signing of edits. To create\ntruly maintainable tests, you must break this old system and create a shared\nrepository of your tests and their configuration.\n\nAdopting a Monitoring as Code (MaC) approach for synthetic monitoring ensures\nthat your monitoring setup is as version-controlled, maintainable as your\napplication code. With a tool like the Checkly CLI you can write tests\nlocally, merge them to a shared repository, and then deploy the updated tests\nto Checkly. While writing your tests, you can also run them through the\ncheckly system without having to edit existing monitors.\n\nMaC facilitates easier maintenance, scalability, and collaboration across\nteams, significantly reducing the likelihood of outdated or irrelevant alerts.\n\n### 7\\. Set your monitoring cadence and alert thresholds based on your SLA\n\nWe\u2019ve talked above about setting the \u2018right\u2019 thresholds for alerts and\nretries, but how do you decide the right configuration? More over, how do you\ndecide how often to run tests in the first place? To make sure you\u2019re doing\nenough monitoring without drowning your SRE\u2019s in alerts, set your monitoring\ncadence based on your SLA. Here\u2019s a guide I wrote last month to help calculate\nthe right monitoring cadence.\n\nFurther, think about your own history of incidents and their causes and\nresolutions to decide if parallel or round-robin scheduling is right for your\nteam. With parallel scheduling, you\u2019ll run checks simultaneously from multiple\ngeographies and fine single-region failures much faster. With round-robin you\ncan run fewer total checks by running each test from a single location.\n\n## Conclusion: synthetics can lessen alert fatigue\n\nSynthetic monitoring, when tailored to focus on user-centric flows, maintained\nthrough Monitoring as Code, and enhanced with smart retries and reliable\ntests, becomes an invaluable asset in reducing alert fatigue. By ensuring that\nalerts are meaningful, actionable, and indicative of real user-impacting\nissues, engineers can trust the system to wake them only for matters that\nrequire immediate attention. With correctly configured synthetics monitors, we\ncan expect fewer alerts, and the alerts we do get should show that something\nimportant is broken for users.\n\nIf you\u2019d like to discuss alert fatigue, what you\u2019ve experienced and how you\ntry to solve it, connect with us at the Checkly Slack and meet other engineers\ndedicated to the best possible Synthetics experience.\n\nNo\u010dnica Mellifera\n\nDeveloper Advocate\n\nNo\u010dnica Mellifera (She/Her) was a developer for seven years before moving into\ndeveloper relations. She specializes in containerized workloads, serverless,\nand public cloud engineering. No\u010dnica has long been an advocate for open\nstandards, and has given talks and workshops on OpenTelemetry and Kubernetes\narchitecture. She enjoys making music that friends have described as\n\"unlistenable.\" In her spare time she cares for her two children and hundreds\nof houseplants.\n\nShare on social\n\n## Related Articles\n\n### Join us April 25th - Using Visual Regression checks to Make Sure You Never\nMiss a Problem on Production\n\nApril 8, 2024\n\nProduct\n\nMonitoring as codeSynthetic monitoringAPI monitoringAlertingPrivate\nlocationsIntegrationsDashboardsLive Checkly dashboardChangelogPricingStatus\n\nCompany\n\nAboutCareersBlogSecurityTerms of usePrivacy\n\nMonitoring as code\n\nCheckly CLIPulumi ProviderTerraform Provider\n\nConnect\n\nContact UsSupportSlack CommunityTwitter @ChecklyHQYouTubeLinkedInPublic\nroadmap\n\nLearn\n\nDocsCheckly GuidesPlaywright Tips\n\nHow we compare\n\nAlternative to DatadogAlternative to New RelicAlternative to\nDynatraceAlternative to Pingdom\n\nArticles\n\nWhat is Synthetic Monitoring?What is API monitoring?What is Playwright?A guide\nto Monitoring as CodeWhy Monitoring as Code?Cypress vs Selenium vs Playwright\nspeed comparison\n\nCopyright \u00a9 2024 Checkly Inc. All rights reserved.\n\n", "frontpage": false}
