{"aid": "40013071", "title": "HP's PA-RISC/PA-7200 CPU (Byte Magazine, 1994)", "url": "https://web.archive.org/web/19980115044046/http://www.byte.com/art/9408/sec11/art3.htm", "domain": "archive.org", "votes": 2, "user": "bell-cot", "posted_at": "2024-04-12 14:06:28", "comments": 2, "source_title": "August 1994 / Core Technologies / A Different Kind of RISC", "source_text": "August 1994 / Core Technologies / A Different Kind of RISC\n\nThe Wayback Machine - http://byte.com:80/art/9408/sec11/art3.htm\n\n# A Different Kind of RISC\n\nAugust 1994 / Core Technologies / A Different Kind of RISC\n\n> ## HP's elegant new implementation of its PA-RISC architecture delivers\n> world-class performance\n>\n> Dick Pountain\n>\n> When people argue about RISC architectures nowadays, Hewlett-Packard's PA-\n> RISC is unlikely to figure prominently in the discussion. PA-RISC chips have\n> a lower profile than the PowerPC, Mips, or DEC Alpha chips, because HP has\n> so far kept them almost to itself. The company doesn't sell its PA-RISC\n> chips on the open merchant market; instead, it sells only to partners in its\n> PRO (Precision RISC Organization). HP has also been relatively slow in\n> licensing second sources.\n>\n> The irony of this situation is that the recently announced PA-RISC 7200\n> (HP's ninth implementation of the architecture) is likely to hold the\n> ``fastest RISC in town'' title for the immediate future, at least until the\n> PowerPC 620 and Mips T5 come on stream next year. This becomes even more\n> impressive when you realize that the 7200's superscalar design is far less\n> aggressive than that of its competitors. Nevertheless, it is expected to top\n> 175 SPECint92 and 250 SPECfp92, just bettering the Alpha 21064A's 170\n> SPECint92 rating. But raw SPECmarks are perhaps less appropriate than usual\n> for measuring the 7200, because HP has clearly stated that its aim is to\n> optimize the PA-RISC architecture for the real-world applications that its\n> workstation customers run--mainly scientific and commercial transaction\n> processing on huge data sets--rather than for the best benchmark figures.\n>\n> A splendid sentiment, and one that can't be dismissed as mere manufacturer's\n> hype because the technical details support it. In the 7200 implementation,\n> HP's design team has concentrated on an artful cache design and a fast new\n> memory bus, rather than on the multiple instruction issue and fancy branch\n> prediction that the competition focuses on. Combined, the new design and\n> faster bus will tend to accelerate large programs and data sets that don't\n> fit in the cache.\n>\n> Inside the 7200\n>\n> Fabricated in HP's new three-metal 0.55-micron CMOS process, the 7200 is\n> designed to run at up to 140 MHz. Its 540-pin ceramic PGA (Pin Grid Array)\n> package is truly gigantic. This pin count reflects the fact that like its\n> predecessors, the 7200 supports external data and instruction caches with\n> separate 64-bit interfaces. It also includes a 64-bit interface to the new\n> high-bandwidth Runway bus. The chip's RISC core operates at an unusual 4.4 V\n> but the I/O circuitry works at 3.3 V; power dissipation is expected to be up\n> to 29 W at 140 MHz.\n>\n> By current standards, the 7200 is only a modestly superscalar design. It can\n> issue two operations per cycle to its two integer units and one FPU. The\n> instructions are classified into three groups; integer, load/store, and\n> floating point. You can pair any two from different groups or two from the\n> integer group. Branches are considered to be special integer operations that\n> may be paired with their predecessor but not their successor. Branch\n> instructions employ static branch prediction.\n>\n> The 7200's five-stage execution pipeline is designed to minimize the stall\n> penalties caused by data, control, and fetch dependencies between\n> instructions; you incur only a one-cycle penalty for a mispredicted branch,\n> for immediately using a floating-point result, and for store/load or\n> load/use combinations. Unlike in previous PA-RISC chips, store/store incurs\n> no penalty, as the off-chip SRAM (static RAM) cache now cycles at full\n> processor frequency.\n>\n> To keep the pipeline flowing as smoothly as possible, instructions with data\n> dependencies and resource conflicts should not be paired. The 7200 uses\n> hardware checking for dependencies, but to save time, it performs some of\n> this work as the instructions are loaded from memory into the instruction\n> cache. Six extra predecode bits are stored with each pair of instructions in\n> the cache to encode this information. On their own, these predecode bits\n> don't completely specify whether the instructions can be paired, but they\n> enable the final checks made in the pipeline to be fast enough so that\n> instruction decode/issue is never prolonged beyond one cycle. The predecode\n> bits add about 10 percent to the SRAM overhead.\n>\n> As with its PA-RISC predecessors, the 7200 uses off-chip caching; however,\n> its main innovation is an on-chip assist cache that makes the caching system\n> much more efficient. The 7200 also separates its instruction and data caches\n> (up to 1 MB each) in place of the single unified cache that the 7100 uses.\n> These caches have to be built from the fastest SRAM and must be able to\n> cycle at full processor speed, which means a 6-nanosecond access time at\n> speeds of greater than 120 MHz. Because such memory is expensive (and hard\n> to source), it increases system costs.\n>\n> Cache Assistant\n>\n> The 7200's assist cache is a 2-KB on-chip memory that holds 64 32-byte cache\n> lines and is fully associative, storing the full address of the last 64\n> memory accesses. Full associativity requires a lot of lookup logic and is\n> too expensive for all but the smallest of caches. In contrast, both off-chip\n> caches are direct-mapped, which means that many main memory locations map to\n> the same cache line. Direct mapping is inexpensive and fast, because the\n> logic need only inspect one line to look for a hit. But it suffers badly\n> from ``thrashing'' if your program continually accesses several different\n> addresses that all happen to map to the same cache index, which can happen\n> easily in vector calculations.\n>\n> For example, in the following vector calculation\n>\n> FOR i := 0 TO n\n>\n> DO A[i] := B[i] + C[i] + D[i]\n>\n> it is possible for elements A[i], B[i], C[i], and D[i] to map to the same\n> physical cache location. A direct-mapped cache will thrash by reloading the\n> same line as each element is accessed, with a devastating performance\n> penalty of four cache misses per iteration of the loop. Larger cache size\n> can't help this problem but greater associativity can.\n>\n> The assist cache sits between main memory and the off-chip primary data\n> cache. Lines from memory move through the assist cache in FIFO (first-\n> in/first-out) order into the data cache; in effect, acting as an overflow\n> queue for the primary cache. The assist cache would eliminate the thrashing\n> described above because each line can move into the assist cache without\n> displacing the others. Both the primary and assist caches respond in a\n> single cycle, and they behave like a single logical cache whose\n> associativity varies dynamically with the data. The assist cache might hold\n> 64 lines that map to the same primary cache line, or 64 different primary\n> cache lines, or anything in between. When a processing unit requests data\n> from the cache, 65 entries (i.e., 64 assist cache entries plus one main\n> cache entry) get searched for a match. This work needs to be done inside one\n> cycle, and HP had to use the fastest self-timed logic for the assist cache's\n> lookup circuitry. In effect, the assist cache combines the high\n> associativity of an on-chip cache with the large size of an off-chip cache.\n> HP is so pleased with the result that it's patenting the assist cache.\n>\n> Another twist is a new ``spatial locality only'' hint bit you can\n> incorporate into the encoding of load/store instructions. The hint bit tells\n> the assist cache that the data will be used only once, and that when the\n> line needs to be replaced, it should write the data straight back to main\n> memory (bypassing the off-chip cache). This enables efficient processing of\n> long sequences of contiguous data without polluting the primary cache's\n> temporally local data (i.e., variables that are being used repeatedly).\n>\n> The 7200 uses simple but effective prefetch strategies for both instructions\n> and data, which can often hide the penalties caused by cache misses and\n> memory latency. When the instruction cache misses, it fetches not just the\n> missing line but the next line, too. When such a prefetched line is accessed\n> for the first time, the next line is fetched again, even if another prefetch\n> is still in progress--up to four prefetches can be outstanding. This results\n> in significant speed ups on long linear code sequences, but you can turn it\n> off for programs with short routines and many branches.\n>\n> Data is prefetched explicitly (i.e., by instructing a load to register zero)\n> or automatically whenever an instruction that modifies a base register\n> address is executed. For example, the load-word-indexed instruction LDWX,m\n> R1(R2),R3 loads R3 from the address held in R2 and then post-increments R2\n> by adding R1 to it. If this instruction causes a data-cache miss then the\n> 7200 is smart enough to prefetch from R2+R1 (rather than from R2+1) after it\n> fills the missing line; it takes note of the ``stride'' of the indexed load.\n>\n> The Runway Bus\n>\n> To make full use of its efficient caches, the 7200 needed a high-bandwidth\n> data path into memory--hence, the new Runway bus. This proprietary\n> synchronous 64-bit bus runs at 120 MHz; however, it supports 1-to-1, 3-to-2,\n> and 4-to-3 ratios between its own clock speed and the CPU's so that the CPU\n> can be run faster. It employs a distributed arbitration scheme where each\n> device attached to the bus contains its own arbiter logic, and arbitration\n> proceeds in parallel with data transfer along separate wires.\n>\n> The Runway bus uses a split transaction protocol in which up to six\n> transactions can be pending at once, so the bus is available even while\n> waiting for memory to deliver. Each transaction is labeled with an\n> identification code--carried via yet another set of signal wires--so each\n> device can sort out its own return data from the stream. The Runway bus\n> multiplexes address and data at the cost of one address cycle for every four\n> data cycles, making for a total sustainable bandwidth of 786 MBps. That's an\n> impressive figure, not only three times faster than HP's own previous\n> processor bus but faster than Sun Microsystems' advanced XDbus and pushing\n> up into supercomputer territory.\n>\n> More to the point, it's sufficient to support four 7200 chips in an SMP\n> (symmetric multiprocessing) system without becoming a bottleneck. The bus\n> interface supports a snooping cache coherency protocol, and to minimize the\n> penalties for snooping on processor-to-cache bandwidth, the interface\n> maintains deep coherency queues (up to 10 transactions for the main cache\n> and three for the translation look-aside buffer, or TLB).\n>\n> By building the bus interface onto the PA-RISC 7200 chip, HP will be able to\n> build multiprocessor systems with a minimum of glue logic. In doing so, the\n> company will keep the price and performance of its SMP workstations and\n> servers highly competitive.\n>\n> Illustration: The PA-RISC 7200 Unique in a number of ways, the PA-RISC\n> architecture is best exemplified by its use of off-chip primary instruction\n> and data caches. It integrates 1.3 million transistors onto a 210-mm\n> superscript 2 die.\n>\n> Dick Pountain is a BYTE contributing editor based in London. You can reach\n> him on the Internet or BIX at dickp@bix.com.\n\nCopyright \u00a9 1994-1998 BYTE\n\n", "frontpage": false}
