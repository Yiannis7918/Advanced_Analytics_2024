{"aid": "40014936", "title": "Living human retinal cells imaged with AI assisted adaptive optics tomography", "url": "https://www.nature.com/articles/s43856-024-00483-1", "domain": "nature.com", "votes": 1, "user": "bookofjoe", "posted_at": "2024-04-12 16:45:08", "comments": 0, "source_title": "Revealing speckle obscured living human retinal cells with artificial intelligence assisted adaptive optics optical coherence tomography", "source_text": "Revealing speckle obscured living human retinal cells with artificial intelligence assisted adaptive optics optical coherence tomography | Communications Medicine\n\nSkip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n\n## Search\n\nAdvanced search\n\n### Quick links\n\n    * Explore articles by subject\n    * Find a job\n    * Guide to authors\n    * Editorial policies\n\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n\n  * Sign up for alerts\n  * RSS feed\n\nRevealing speckle obscured living human retinal cells with artificial\nintelligence assisted adaptive optics optical coherence tomography\n\nDownload PDF\n\nDownload PDF\n\n  * Article\n  * Open access\n  * Published: 10 April 2024\n\n# Revealing speckle obscured living human retinal cells with artificial\nintelligence assisted adaptive optics optical coherence tomography\n\n  * Vineeta Das ORCID: orcid.org/0000-0001-9114-1455^1,\n  * Furu Zhang^1,\n  * Andrew J. Bower ORCID: orcid.org/0000-0003-1645-5950^1,\n  * Joanne Li ORCID: orcid.org/0000-0003-2845-2490^1,\n  * Tao Liu ORCID: orcid.org/0000-0001-9864-3896^1,\n  * Nancy Aguilera ORCID: orcid.org/0000-0003-0863-596X^1,\n  * Bruno Alvisio^1,\n  * Zhuolin Liu ORCID: orcid.org/0000-0001-8019-2054^2,\n  * Daniel X. Hammer ORCID: orcid.org/0000-0003-0532-1914^2 &\n  * ...\n  * Johnny Tam ORCID: orcid.org/0000-0003-2300-0567^1\n\nCommunications Medicine volume 4, Article number: 68 (2024) Cite this article\n\n  * 223 Accesses\n\n  * 95 Altmetric\n\n  * Metrics details\n\n## Abstract\n\n### Background\n\nIn vivo imaging of the human retina using adaptive optics optical coherence\ntomography (AO-OCT) has transformed medical imaging by enabling visualization\nof 3D retinal structures at cellular-scale resolution, including the retinal\npigment epithelial (RPE) cells, which are essential for maintaining visual\nfunction. However, because noise inherent to the imaging process (e.g.,\nspeckle) makes it difficult to visualize RPE cells from a single volume\nacquisition, a large number of 3D volumes are typically averaged to improve\ncontrast, substantially increasing the acquisition duration and reducing the\noverall imaging throughput.\n\n### Methods\n\nHere, we introduce parallel discriminator generative adversarial network\n(P-GAN), an artificial intelligence (AI) method designed to recover speckle-\nobscured cellular features from a single AO-OCT volume, circumventing the need\nfor acquiring a large number of volumes for averaging. The combination of two\nparallel discriminators in P-GAN provides additional feedback to the generator\nto more faithfully recover both local and global cellular structures. Imaging\ndata from 8 eyes of 7 participants were used in this study.\n\n### Results\n\nWe show that P-GAN not only improves RPE cell contrast by 3.5-fold, but also\nimproves the end-to-end time required to visualize RPE cells by 99-fold,\nthereby enabling large-scale imaging of cells in the living human eye. RPE\ncell spacing measured across a large set of AI recovered images from 3\nparticipants were in agreement with expected normative ranges.\n\n### Conclusions\n\nThe results demonstrate the potential of AI assisted imaging in overcoming a\nkey limitation of RPE imaging and making it more accessible in a routine\nclinical setting.\n\n## Plain language summary\n\nThe retinal pigment epithelium (RPE) is a single layer of cells within the eye\nthat is crucial for vision. These cells are unhealthy in many eye diseases,\nand this can result in vision problems, including blindness. Imaging RPE cells\nin living human eyes is time consuming and difficult with the current\ntechnology. Our method substantially speeds up the process of RPE imaging by\nincorporating artificial intelligence. This enables larger areas of the eye to\nbe imaged more efficiently. Our method could potentially be used in the future\nduring routine eye tests. This could lead to earlier detection and treatment\nof eye diseases, and the prevention of some causes of blindness.\n\n### Similar content being viewed by others\n\n### A novel deep learning conditional generative adversarial network for\nproducing angiography images from retinal fundus photographs\n\nArticle Open access 09 December 2020\n\nAlireza Tavakkoli, Sharif Amit Kamran, ... Stewart Lee Zuckerbrod\n\n### Ultra-wide field and new wide field composite retinal image registration\nwith AI-enabled pipeline and 3D distortion correction algorithm\n\nArticle Open access 19 December 2023\n\nFritz Gerald P. Kalaw, Melina Cavichini, ... William R. Freeman\n\n### Adaptive optics: principles and applications in ophthalmology\n\nArticle 30 November 2020\n\nEngin Akyol, Ahmed M. Hagag, ... Andrew J. Lotery\n\n## Introduction\n\nHigh-resolution in vivo ophthalmic imaging enables visualization and\nquantification of cells^1, offering the possibility of revealing the status of\nindividual cells in health and disease. For many optical imaging instruments,\nnoise inherent in the imaging processes reduces contrast. The most direct way\nto suppress noise is the incoherent averaging of a large number of\nvolumes^2,3,4. However, this lengthens the overall acquisition time, not only\ndue to the additional volumes required, but also, because of the possibility\nfor artifacts or registration errors across the sequentially acquired volumes\ndue to constant involuntary eye movements that translate and distort the\ncellular visualization obtained from the microscopic imaging field of view\n(FOV) (~0.5 mm \u00d7 0.5 mm) that is commonly used in adaptive optics (AO) retinal\nimaging^1,5,6.\n\nAdaptive optics optical coherence tomography (AO-OCT) is an emerging\nophthalmic imaging tool that relies on the detection of interfered light to\nenable 3D visualization of the retina at single cell level resolution,\ndirectly in the living human eye^1,7,8,9. However, AO-OCT volumes are\ninherently susceptible to speckle noise contamination, which arises due to the\ninterference between light scattered from multiple points within the cells^10.\nThis high contrast, complex intensity distribution of speckle noise can mask\ncells and limit the visibility of cellular structures. In particular, the\nretinal pigment epithelial (RPE) cells, which are essential for maintaining\nvisual function^11 have low intrinsic contrast compared to speckle noise and\ntherefore are challenging to image directly^12. To overcome the low intrinsic\ncontrast, a large number of AO-OCT volumes need to be averaged (e.g., 120\nvolumes) in order to visualize the cells^13. These volumes, obtained by\nrepeatedly imaging the same retinal patch, must be acquired at a sufficiently\nspaced time interval (about 5 s) to allow for the speckle to decorrelate\nacross volumes^14. Not only does this strategy substantially increase the\ntotal acquisition time, but it also introduces the potential for eye motion\nartifacts and patient fatigue, both of which can degrade image quality. For\nsome applications, hardware modifications have been proposed to allow for the\nrepeated volume acquisitions to be obtained more quickly, based on shortening\nthe speckle decorrelation time^15,16 or by producing uncorrelated speckle\npatterns via frequency^17, angular^18, or polarization compounding^19.\nHowever, the capability to visualize cellular details from only a single\nacquisition alone, rather than from the multiple acquisitions that are still\nneeded even with these hardware modifications, could substantially reduce the\ntime needed to visualize the RPE cells and would be a transformative step\ntowards making AO-OCT a more efficient clinical imaging tool.\n\nData-driven artificial intelligence (AI) methods have offered promising\nsolutions to generative modeling tasks such as denoising of OCT images^20,21,\nhigh-resolution reconstruction of OCT angiograms^22,23, and data augmentation\nin AO images^24. Here, we explore the potential of AI for recovering the\ncomplete cellular structure from only a single noisy AO-OCT acquisition. The\ndeep learning-based generative adversarial network (GAN) provides a powerful\nframework for synthesizing realistic-looking images from random noise through\na competition between discriminator and generator networks^25,26, and has been\nsuccessfully applied for image enhancement applications in which a low signal-\nto-noise ratio (SNR) image is used as input to generate a high SNR\ncounterpart^21,26,27,28. Following earlier applications of GAN for improving\nimage quality and training stability^29,30,31, conditional GAN (C-GAN)^30 was\nintroduced to better control the quality of the synthesized images by\nsupplying the discriminator with image and ground truth pairs instead of\nimages alone, as used in the original GAN. However, supervised training of\nC-GAN with paired images can sometimes be restrictive. Thus, CycleGAN^31\novercame this requirement by using multiple generator and discriminator\nnetworks along with specialized loss functions from unpaired images. Although\nthese strategies have greatly improved image synthesis and style transfer,\ncontrol over image characteristics have mostly focused on global image\nfeatures with little or no control over fine local object details, such as\nindividual cells within images. In the case of AO-OCT RPE images which are\nmasked by an overwhelming amount of speckle noise, it is difficult to\nvisualize cellular structure in a single, unaveraged image, making the cell\nrecovery process extremely challenging.\n\nBuilding upon the various GANs that have been developed, we hypothesize that\nthe generator can better recover cellular structure if the discriminator is\nenhanced to specifically evaluate similarities in local structural details\nbetween the generated and the ground truth averaged images. We describe\ndeveloping and evaluating a custom GAN framework that contains a generator, a\nSiamese twin discriminator, as well as a convolutional neural network (CNN)\ndiscriminator to recover the RPE cellular structures from single, unaveraged,\nand noisy AO-OCT images. As both the discriminators work in parallel towards\nproviding strong feedback to the generator network to synthesize perceptually\nsimilar images to the ground truth (averaged images), we call the proposed\nnetwork parallel discriminator GAN (P-GAN). We show that after training, the\ngenerator can be applied to recover the cellular morphology from only a single\nspeckled RPE image. This, in turn, enables wide-scale visualization of the RPE\nmosaic from AO-OCT images acquired across multiple contiguous retinal\nlocations, as the overall time required for RPE visualization at a single\nlocation is substantially reduced by eliminating the need for multiple volume\nacquisition and averaging. The incorporation of AI into the overall image\nacquisition strategy has the potential to transform the current state-of-the-\nart ophthalmic imaging with an estimated improvement of 99-fold in the overall\nthroughput.\n\n## Methods\n\n### Adaptive optics imaging\n\nParticipants with no history or signs of ocular disease were recruited for\nthis study between the years 2019 to 2022. All participants underwent a\ncomprehensive ophthalmic assessment. In total, eight eyes from seven healthy\nparticipants (aged: 29.1 \u00b1 9.1 years) from the National Eye Institute Eye\nClinic (National Institutes of Health, Bethesda, Maryland, USA) were imaged\nusing a custom-built AO-OCT retinal imager^13. Eyes were dilated with 2.5%\nphenylephrine hydrochloride (Akorn Inc.) and 1% tropicamide (Sandoz, A\nNovartis Division). This study was approved by the Institutional Review Board\nof the National Institutes of Health (NCT02317328). Research procedures\nadhered to the tenets of the Declaration of Helsinki. Written, informed\nconsent was obtained from all participants after the nature of the research\nand possible consequences of the study were explained.\n\n### Experimental design\n\n#### Data for training and validating AI models\n\nAO-OCT volumes from five eyes of five participants were acquired at a rate of\n147 kHz (300 \u00d7 300 pixels at a rate of 1.6 volumes per second) from up to four\nretinal locations ranging from 0\u20133 mm temporal to the fovea with a FOV of 1.5\ndegrees. At each location, 120 speckled volumes were acquired. Following image\nacquisition, volumes were digitally flattened based on the outer retinal\nlayers, corrected for eye motion after manual selection of reference frames\nfor registration, and then averaged to generate ground truth averaged RPE en\nface images^13 (Fig. 1a and Supplementary Fig. 1). Due to a scanner artifact\n(distortion arising from the turnaround of the scanner from one line to the\nnext), 50 pixels from the left and right sides of the image were cropped off\nto yield a final image of 300 \u00d7 200 pixels. The reference frames (speckled\nimages) and the ground truth averaged images are used as training data for the\nAI model in a supervised manner.\n\nFig. 1: Overview of artificial intelligence (AI) enhanced retinal pigment\nepithelial (RPE) cell imaging strategy.\n\na Adaptive optics optical coherence tomography (AO-OCT) imaging based on\nmultiple acquisitions (120 volumes) at sufficiently spaced time intervals\n(about 5 s)^13,14. The en face images of the RPE cells (obtained by segmenting\nthe 2D image of the RPE layer from the 3D volume) have a speckled appearance\nin a single acquisition but can be averaged across multiple acquisitions to\nreveal the individual RPE cells appearing as the dark cell centers with bright\ncell surroundings in the averaged en face image. b AO-OCT imaging enhanced\nwith AI can recover the cellular features from only a single speckled AO-OCT\nacquisition, thereby eliminating the need for multiple volume acquisition for\naveraging and substantially reducing the imaging duration. c Parallel\ndiscriminator generative adversarial network (P-GAN), the proposed AI model,\nis comprised of three networks: a generator (G) to recover the cellular\nstructures from the speckled images of the RPE cells, a twin discriminator\n(D1) with two identical twin convolutional neural networks (CNNs) to perform a\none-to-one feature comparison of the recovered images from the generator and\nthe averaged (ground truth) images yielding a similarity score, and a CNN\ndiscriminator (D2) that assigns a label of fake/real to the recovered images.\nThe adversarial learning of the three networks facilitates the faithful\nrecovery of both local structural details as well as the global mosaic of the\nRPE cells. Matched speckled and averaged image pairs are used to train P-GAN.\nDetails about the network architecture are presented in Supplementary Fig. 2.\nAfter training, the trained generator can be deployed to reveal the cellular\ndetails from speckled images obtained from a single AO-OCT acquisition. d\nApplying the trained generator to predict the cellular structures of the RPE\ncells of two participants (S1 and S2) from the corresponding speckled images.\nThe ground truth averaged images (average of 120 acquired AO-OCT volumes) are\nshown for comparison. Scale bar: 25 \u03bcm.\n\nFull size image\n\n#### Experimental data for RPE assessment from the recovered images\n\nFor the large-scale assessment of the RPE cells, speckled volumes from 63\noverlapping locations spanning a 1 mm \u00d7 3 mm region of the retina from the\nfovea extending in the temporal direction were acquired from three\nparticipants (Supplementary Table 1). To ensure that the algorithm performance\nwas assessed on never-seen images, three eyes that were not used in the\ntraining and validation of the AI model were selected for this purpose. To\nfacilitate image acquisition and to allow for brief breaks in between\nacquisitions, a total of 10 volumes were acquired at each location, from which\nthe one with the least distortion (subjectively determined minimal motion\nartifacts and no eye blinks) was selected as input to the P-GAN for cellular\nrecovery. To validate the accuracy of RPE recovery on the experimental data,\nadditional volumes (120 volumes) were acquired from four retinal locations of\nthe three participants. The ground truth averaged images were created by\naveraging 120 speckled volumes for objective image recovery comparison.\n\n#### Model details\n\nThe proposed framework (P-GAN) contains a generator (G), a siamese twin\ndiscriminator (D1), as well as a CNN discriminator (D2) (Fig. 1c and\nSupplementary Fig. 2). G takes the specked image as input and creates an image\nof the RPE using a series of CNN-based encoder and decoder network components\n(Supplementary Fig. 2 and P-GAN network architecture in Supplementary\nMethods). D1 is designed to use Siamese twin neural network^32, which has a\nspecialized architecture (Supplementary Fig. 2) to naturally rank similarity\nbetween the generator created and averaged RPE images in a representative\nfeature space using L1 norm^33. We found through experimentation that fusing\nfeatures from two intermediate layers with the last convolutional layer of the\ntwin network with appropriate weights ensured better cellular recovery\n(Supplementary Tables 2, 3). We introduced a weighted feature fusion (WFF)\nblock that concatenated features from three layers of the twin network to\nestimate the similarity (Supplementary Fig. 2 and P-GAN network architecture\nin Supplementary Methods). Additionally, D2 also helped the cell recovery\nprocess by determining if the images recovered by G were closer to the\nstatistical distribution of the averaged ground truth images. G, D1, and D2\nwere simultaneously trained using two adversarial and content loss functions\n(Objective loss functions in Supplementary Methods).\n\nThe dataset to train the model was created from the acquired volumes from five\nparticipants. The training dataset was augmented by leveraging the natural eye\nmotion of the participants during imaging by selecting multiple (up to 50)\nreference (speckled) frames to create a set of ground truth averaged images\nthat were each slightly shifted with respect to each other (Supplementary Fig.\n3). In addition, due to the combination of simultaneously occurring eye motion\nand point-scanning nature of image acquisition, each of the averaged images\nalso contained unique intravolume distortions, which served as a further means\nfor natural data augmentation. This resulted in a total of 5968 image patches\n(150 \u00d7 150 pixels) extracted from the speckled and averaged image pairs used\nfor training. P-GAN was trained for 100 epochs with a batch size of 8 using an\nAdam optimizer with a learning rate of 0.0002, and exponential decay rates of\n0.5 for the first moment and 0.9 for the second moment. Four NVIDIA TITAN V\ngraphical processing units (GPU) were used to accelerate the training process.\nAfter training is complete, the discriminators were no longer needed, and the\ngenerator could be used to recover the RPE structure from the speckled images\n(Fig. 1d).\n\nTo evaluate the performance, a leave-one-participant-out validation protocol\nwas used. A total of forty paired images at different retinal locations from\nthe five participants were used for validation of the method. Four objective\nimage quality assessment metrics (Validation metrics in Supplementary\nMethods): perceptual image error assessment through pairwise preference\n(PieAPP)^34, learned perceptual image patch similarity (LPIPS)^35, deep image\nstructure and texture similarity (DISTS)^36, and Fr\u00e9chet Inception Distance\n(FID)^37 were used to validate the performance.\n\n#### Quantification of cellular spacing and contrast\n\nCell spacing and contrast were quantified to assess the efficacy of P-GAN for\nRPE recovery. Cell spacing was estimated using the circumferentially averaged\npower spectrum^38 of each image region of interest (200 \u00d7 200 pixels). The\npeak spatial frequency of the spectrum (i.e., the RPE fundamental frequency)\nwas an estimate of cell spacing. To convert from pixels to \u03bcm, a paraxial ray\ntrace on a three-surfaced simplified model eye^39 was used after replacing the\naxial length, corneal curvature, and anterior chamber depth with measurements\nof these values obtained from each participant (IOL Master, Carl Zeiss\nMeditec)^40.\n\nVoronoi neighborhoods were generated from the manually identified cell centers\non selected images to analyze the packing properties of the RPE cells. At\nleast two expert graders sequentially marked each image and then interactively\nre-reviewed images until full consensus on the markings were achieved. The\ncellular contrast of the P-GAN-created images and the averaged images were\ncompared using a peak distinctiveness measure, defined as the height of the\npeak in the circumferentially averaged power spectrum computed as the\ndifference between the log power spectral density (PSD) between the peak and\nthe local minima to the left of the peak.\n\n### Reporting summary\n\nFurther information on research design is available in the Nature Portfolio\nReporting Summary linked to this article.\n\n## Results\n\n### P-GAN enables visualization of cellular structure from a single speckled\nimage\n\nThe overall goal was to learn a mapping between the single speckled and\naveraged images (Fig. 1b) using a paired training dataset. Inspired by the\nability of traditional GAN networks to recover aspects of the cellular\nstructure (Supplementary Fig. 4), we sought to further improve upon these\nnetworks with P-GAN. In our network architecture (Supplementary Fig. 2), the\ntwin and the CNN discriminators were designed to ensure that the generator\nfaithfully recovered both the local structural details of the individual cells\nas well as the overall global mosaic of the RPE cells. In addition, we\nincorporated a WFF strategy to the twin discriminator that concatenated\nfeatures from different layers of the twin CNN with appropriate weights,\nfacilitating effective comparisons and learning of the complex cellular\nstructures and global patterns of the images.\n\nP-GAN was successful in recovering the retinal cellular structure from the\nspeckled images (Fig. 1d and Supplementary Movie 1). Toggling between the\naveraged RPE images (obtained by averaging 120 acquired AO-OCT volumes) and\nthe P-GAN recovered images showed similarity in the cellular structure\n(Supplementary Movie 2). Qualitatively, P-GAN showed better cell recovery\ncapability than other competitive deep learning networks (U-Net^41, GAN^25,\nPix2Pix^30, CycleGAN^31, medical image translation using GAN (MedGAN)^42, and\nuncertainty guided progressive GAN (UP-GAN)^43) (additional details about\nnetwork architectures and training are shown in Other network architectures\nsection in Supplementary Methods and Supplementary Table 4, respectively) with\nclearer visualization of the dark cell centers and bright cell surroundings of\nthe RPE cells (e.g., magenta arrows in Supplementary Fig. 4 and Supplementary\nMovie 3), possibly due to the twin discriminator\u2019s similarity assessment.\nNotably, CycleGAN was able to generate some cells that were perceptually\nsimilar to the averaged images, but in certain areas, undesirable artifacts\nwere introduced (e.g., the yellow circle in Supplementary Fig. 4).\n\nQuantitative comparison between P-GAN and the off-the-shelf networks\n(U-Net^41, GAN^25, Pix2Pix^30, CycleGAN^31, MedGAN^42, and UP-GAN^43) using\nobjective performance metrics (PieAPP^34, LPIPS^35, DISTS^36, and FID^37)\nfurther corroborated our findings on the performance of P-GAN (Supplementary\nTable 5). There was an average reduction of at least 16.8% in PieAPP and 7.3%\nin LPIPS for P-GAN compared to the other networks, indicating improved\nperceptual similarity of P-GAN recovered images with the averaged images.\nLikewise, P-GAN also achieved the best DISTS and FID scores among all\nnetworks, demonstrating better structural and textural correlations between\nthe recovered and the ground truth averaged images. Overall, these results\nindicated that P-GAN outperformed existing AI-based methods and could be used\nto successfully recover cellular structure from speckled images.\n\n### Twin discriminator improves cell recovery performance\n\nOur preliminary explorations of the off-the-shelf GAN frameworks showed that\nthese methods have the potential for recovering cellular structure and\ncontrast but alone are insufficient to recover the fine local cellular details\nin extremely noisy conditions (Supplementary Fig. 4). To further reveal and\nvalidate the contribution of the twin discriminator, we trained a series of\nintermediate models and observed the cell recovery outcomes. We began by\ntraining a conventional GAN, comprising of the generator, G, and the CNN\ndiscriminator, D2. Although GAN (G + D2) showed promising RPE visualization\n(Fig. 2c) relative to the speckled images (Fig. 2a), the individual cells were\nhard to discern in certain areas (yellow and orange arrows in Fig. 2c). To\nimprove the cellular visualization, we replaced D2 with the twin\ndiscriminator, D1. Indeed, a 7.7% reduction in DISTS was observed with clear\nimprovements in the visualization of some of the cells (orange arrows in Fig.\n2c, d).\n\nFig. 2: Effect of parallel discriminator generative adversarial network\n(P-GAN) components on the recovery of retinal pigment epithelial (RPE) cells.\n\na Single speckled image compared to images of the RPE obtained via b average\nof 120 volumes (ground truth), c generator with the convolutional neural\nnetwork (CNN) discriminator (G + D2), d generator with the twin discriminator\n(G + D1), e generator with CNN and twin discriminators without the weighted\nfeature fusion (WFF) module (G + D2 + D1-WFF), and f P-GAN. The yellow and\norange arrows indicate cells that are better visualized using P-GAN compared\nto the intermediate models. g\u2013i Comparison of the recovery performance using\ndeep image structure and texture similarity (DISTS), perceptual image error\nassessment through pairwise preference (PieAPP), and learned perceptual image\npatch similarity (LPIPS) metrics. The bar graphs indicate the average values\nof the metrics across sample size, n = 5 healthy participants (shown in\ncircles) for different methods. The error bars denote the standard deviation.\nScale bar: 50 \u03bcm.\n\nFull size image\n\nHaving shown the outcomes of training D1 and D2 independently with G, we\nshowed that combining both D1 and D2 with G (P-GAN) boosted the performance\neven further, evident in the improved values (lower scores implying better\nperceptual similarity) of the perceptual measures (Fig. 2g\u2013i). For this\ncombination of D1 and D2, we replaced the WFF block, which concatenated\nfeatures from different layers of the twin CNN with appropriate weights, with\nglobal average pooling of the last convolutional layer (G + D2 + D1-WFF).\nWithout the WFF, the model did not adequately extract powerful discriminative\nfeatures for similarity assessment and hence resulted in poor cell recovery\nperformance. This was observed both qualitatively (yellow and orange arrows in\nFig. 2e, f) as well as quantitatively with the higher objective scores\n(indicating low perceptual similarity with ground truth averaged images) for G\n+ D2 + D1-WFF compared to P-GAN (Fig. 2g\u2013i).\n\nTaken together, this established that the CNN discriminator (D2) helped to\nensure that recovered images were closer to the statistical distribution of\nthe averaged images, while the twin discriminator (D1), working in conjunction\nwith D2, ensured structural similarity of local cellular details between the\nrecovered and the averaged images. The adversarial learning of G with D1 and\nD2 ensured that the recovered images not only have global similarity to the\naveraged images but also share nearly identical local features.\n\nFinally, experimentation using different weighting configurations in WFF\nrevealed that the fusion of the intermediate layers with weights of 0.2 with\nthe last convolutional layer proved complementary in extracting shape and\ntexture information for improved performance (Supplementary Tables 2, 3).\nThese ablation experiments indicated that the global perceptual closeness\n(offered by D2) and the local feature similarity (offered by D1 and WFF) were\nboth important for faithful cell recovery.\n\n### Leveraging eye motion for data augmentation\n\nGiven the relatively recent demonstration of RPE imaging using AO-OCT in\n2016^12, and the long durations needed to generate these images, currently,\nthere are no publicly available datasets for image analysis. Therefore, we\nacquired a small dataset using our custom-built AO-OCT imager^13 consisting of\nseventeen retinal locations obtained by imaging up to four different retinal\nlocations for each of the five participants (Supplementary Table 1). To obtain\nthis dataset, a total of 84 h was needed (~2 h for image acquisition followed\nby 82 hours of data processing which included conversion of raw data to 3D\nvolumes and correction for eye motion-induced artifacts). After performing\ntraditional augmentation (horizontal flipping), this resulted in an initial\ndataset of only 136 speckled and averaged image pairs. However, considering\nthat this and all other existing AO-OCT datasets that we are aware of are\ninsufficient in size compared to the training datasets available for other\nimaging modalities^44,45, it was not surprising that P-GAN trained on this\ninitial dataset yielded very low objective perceptual similarity (indicated by\nthe high scores of DISTS, PieAPP, LPIPS, and FID in Supplementary Table 6)\nbetween the recovered and the averaged images.\n\nTo overcome this limitation, we leveraged the natural eye motion of the\nparticipants to augment the initial training dataset. The involuntary\nfixational eye movements, which are typically faster than the imaging speed of\nour AO-OCT system (1.6 volumes/s), resulted in two types of motion-induced\nartifacts. First, due to bulk tissue motion, a displacement of up to hundreds\nof cells between acquired volumes could be observed. This enabled us to create\naveraged images of different retinal locations containing slightly different\ncells within each image. Second, due to the point-scanning nature of the AO-\nOCT system compounded by the presence of continually occurring eye motion,\neach volume contained unique intra-frame distortions. The unique pattern of\nthe shifts in the volumes was desirable for creating slightly different\naveraged images, without losing the fidelity of the cellular information\n(Supplementary Fig. 3). By selecting a large number of distinct reference\nvolumes onto which the remaining volumes were registered, we were able to\ncreate a dataset containing 2984 image pairs (22-fold augmentation compared to\nthe initial limited dataset) which was further augmented by an additional\nfactor of two using horizontal flipping, resulting in a final training dataset\nof 5996 image pairs for P-GAN (also described in Data for training and\nvalidating AI models in Methods). Using the augmented dataset for training\nP-GAN yielded high perceptual similarity of the recovered and the ground truth\naveraged images which was further corroborated by improved quantitative\nmetrics (Supplementary Table 6). By leveraging eye motion for data\naugmentation, we were able to obtain a sufficiently large training dataset\nfrom a recently introduced imaging technology to enable P-GAN to generalize\nwell for never-seen experimental data (Supplementary Table 1 and Experimental\ndata for RPE assessment from the recovered images in Methods).\n\n### Objective assessment of the cellular contrast offered by AI\n\nIn addition to the structural and perceptual similarity that we demonstrated\nbetween P-GAN recovered and averaged images, here, we objectively assessed the\ndegree to which cellular contrast was enhanced by P-GAN compared to averaged\nimages and other AI methods. As expected, examination of the 2D power spectra\nof the images revealed a bright ring in the power spectra (indicative of the\nfundamental spatial frequency present within the healthy RPE mosaic arising\nfrom the regularly repeating pattern of individual RPE cells) for the\nrecovered and averaged images (insets in Fig. 3b\u2013i).\n\nFig. 3: Using power spectra analysis to estimate the cellular contrast\nachieved using artificial intelligence (AI).\n\na Example specked image acquired from participant S1. Recovered images using b\nU-Net, c generative adversarial network (GAN), d Pix2Pix, e CycleGAN, f\nmedical image translation using GAN (MedGAN), g uncertainty guided progressive\nGAN (UP-GAN), h parallel discriminator GAN (P-GAN). i Ground truth averaged\nimage (obtained by averaging 120 adaptive optics optical coherence tomography\n(AO-OCT) volumes). Insets in (a\u2013i) show the corresponding 2D power spectra of\nthe images. A bright ring representing the fundamental spatial frequency of\nthe retinal pigment epithelial (RPE) cells can be observed in U-Net, GAN,\nPix2Pix, CycleGAN, MedGAN, UP-GAN, P-GAN, and averaged images power spectrum\ncorresponds to the cell spacing. j Circumferentially averaged power spectral\ndensity (PSD) for each of the images. A visible peak corresponding to the RPE\ncell spacing was observed for U-Net, GAN, Pix2Pix, CycleGAN, MedGAN, UP-GAN,\nP-GAN, and averaged images. The vertical line indicates the approximate\nlocation of the fundamental spatial frequency associated with the RPE cell\nspacing. The height of the peak (defined as peak distinctiveness (PD))\nindicates the RPE cellular contrast measured as the difference in the log PSD\nbetween the peak and the local minima to the left of the peak (inset in (j)).\nScale bar: 50 \u03bcm.\n\nFull size image\n\nInterestingly, although this ring was not readily apparent on the speckled\nsingle image (inset in Fig. 3a), it was present in all the recovered images,\nreinforcing our observation of the potential of AI to decipher the true\npattern of the RPE mosaic from the speckled images. Furthermore, the radius of\nthe ring, representative of the approximate cell spacing (computed from the\npeak frequency of the circumferentially averaged PSD) (Quantification of cell\nspacing and contrast in Methods), showed consistency among the different\nmethods (shown by the black vertical line along the peak of the\ncircumferentially averaged PSD in Fig. 3j and Table 1), indicating high\nfidelity of recovered cells in comparison to the averaged images.\n\nTable 1 Comparison of cellular contrast and cell spacing error across the\ndifferent networks\n\nFull size table\n\nThe height of the local peak of the circumferentially averaged power spectra\n(which we defined as peak distinctiveness) provided an opportunity to\nobjectively quantify the degree to which cellular contrast was enhanced. Among\nthe different AI methods, the peak distinctiveness achieved by P-GAN was\nclosest to the averaged images with a minimal absolute error of 0.08 compared\nto ~0.16 for the other methods (Table 1), which agrees with our earlier\nresults indicating the improved performance of P-GAN. In particular, P-GAN\nachieved a contrast enhancement of 3.54-fold over the speckled images (0.46\nfor P-GAN compared with 0.13 for the speckled images). These observations\ndemonstrate P-GAN\u2019s effectiveness in boosting cellular contrast in addition to\nstructural and perceptual similarity.\n\n### AI enables efficient visualization of the RPE mosaic across retinal\nlocations\n\nHaving demonstrated the efficacy and reliability of P-GAN on test data, we\nwanted to evaluate the performance of P-GAN on experimental data from never-\nseen human eyes across an experimental dataset (Supplementary Table 1), which\nto the best of our knowledge, covered the largest extent of AO-OCT imaged RPE\ncells reported (63 overlapping locations per eye). This feat was made possible\nusing the AI-enhanced AO-OCT approach developed and validated in this paper.\nUsing the P-GAN approach, in our hands, it took 30 min of time (including time\nneeded for rest breaks) to acquire single volume acquisitions from 63 separate\nretinal locations compared to only 4 non-overlapping locations imaged with\nnearly the same duration using the repeated averaging process (15.8-fold\nincrease in number of locations). Scaling up the averaging approach from 4 to\n63 locations would have required nearly 6 h to acquire the same amount of RPE\ndata (note that this does not include any data processing time), which is not\nreadily achievable in clinical practice. This fundamental limitation explains\nwhy AO-OCT RPE imaging is currently performed only on a small number of\nretinal locations^12,13.\n\nLeveraging P-GAN\u2019s ability to successfully recover cellular structures from\nnever-seen experimental data, we stitched together overlapping recovered RPE\nimages to construct montages of the RPE mosaic (Fig. 4 and Supplementary Fig.\n5). To further validate the accuracy of the recovered RPE images, we also\ncreated ground truth averaged images by acquiring 120 volumes from four of\nthese locations per eye (12 locations total) (Experimental data for RPE\nassessment from the recovered images in Methods). The AI-enhanced and averaged\nimages for the experimental data at the 12 locations were similar in\nappearance (Supplementary Fig. 6). Objective assessment using PieAPP, DISTS,\nLPIPS, and FID also showed good agreement with the averaged images (shown by\ncomparable objective scores for experimental data in Supplementary Table 7 and\ntest data in Supplementary Table 5) at these locations, confirming our\nprevious results and illustrating the reliability of performing RPE recovery\nfor other non-seen locations as well (P-GAN was trained using images obtained\nfrom up to 4 retinal locations across all participants). The cell spacing\nestimated using the circumferentially averaged PSD between the recovered and\nthe averaged images (Supplementary Fig. 7 and Supplementary Table 8) at the 12\nlocations showed an error of 0.6 \u00b1 1.1 \u03bcm (mean \u00b1 SD). We further compared the\nRPE cell spacing from the montages of the recovered RPE from the three\nparticipants (S2, S6, and S7) with the previously published in vivo studies\n(obtained using different imaging modalities) and histological values (Fig.\n5)^12,46,47,48,49,50,51. Considering the range of values in Fig. 5, the metric\nexhibited inter-participant variability, with cell spacing varying up to 0.5\n\u03bcm across participants at any given retinal location. Nevertheless, overall\nour measurements were within the expected range compared to the published\nnormative data^12,46,47,48,49,50,51. Finally, peak distinctiveness computed at\n12 retinal locations of the montages demonstrated similar or better\nperformance of P-GAN compared to the averaged images in improving the cellular\ncontrast (Supplementary Table 8).\n\nFig. 4: Parallel discriminator generative adversarial network (P-GAN) enabled\nwide-scale visualization of the retinal pigment epithelial (RPE) cellular\nmosaic.\n\nThe image shows the visualization of the RPE mosaic using the P-GAN recovered\nimages (this montage was manually constructed from up to 63 overlapping\nrecovered RPE images from the left eye of participant S2). The white squares\n(a\u2013e) indicate regions that are further magnified for better visualization at\nretinal locations a 0.3 mm, b 0.8 mm, c 1.3 mm, d 1.7 mm, and e 2.4 mm\ntemporal to the fovea, respectively. Additional examples of montages from two\nadditional participants are shown in Supplementary Fig. 5.\n\nFull size image\n\nFig. 5: Comparison of cell spacing of the parallel discriminator generative\nadversarial network (P-GAN) recovered images with previously published data\nacross retinal locations (eccentricities) temporal to the fovea.\n\nSymbols in black indicate cell spacing estimated from P-GAN recovered images\nfor three participants (S2, S6, and S7) at different retinal locations. For\ncomparison, data in gray denote the mean and standard deviation values from\npreviously published studies (adaptive optics infrared autofluorescence (AO-\nIRAF)^48, adaptive optics optical coherence tomography (AO-OCT)^12, adaptive\noptics with short-wavelength autofluorescence (AO-SWAF)^49, and\nhistology^46,51).\n\nFull size image\n\nVoronoi analysis performed on P-GAN and averaged images at 12 locations\n(Supplementary Fig. 8) resulted in similar shapes and sizes of the Voronoi\nneighborhoods. Cell spacing computed from the Voronoi analysis (Supplementary\nTable 9) fell within the expected ranges and showed an average error of 0.5 \u00b1\n0.9 \u03bcm. These experimental results demonstrate the possibility of using AI to\ntransform the way in which AO-OCT is used to visualize and quantitatively\nassess the contiguous RPE mosaic across different retinal locations directly\nin the living human eye.\n\n## Discussion\n\nWe demonstrated that P-GAN can effectively recover the cellular structure from\nspeckle-obscured AO-OCT images of the RPE. The key feature of our approach is\nthat cellular contrast can be improved using only a single speckled\nacquisition, completely bypassing the need for sequential volume averaging\ncurrently being used for AO-OCT RPE imaging^12,13. This is an important step\ntowards more routine clinical application of AO-OCT imaging for probing the\nhealth of the retinal tissue at the cellular level, especially for the task of\nmorphometric measurements of cell structure across different retinal locations\n(Figs. 4, 5 and Supplementary Fig. 5).\n\nThe success of cellular recovery using P-GAN can be attributed to the Siamese\nnetwork-inspired twin discriminator that provided local structural cues of\nfeature similarity (between the recovered and the ground truth averaged\nimages) to the generator. The improvement of P-GAN over U-Net, traditional\nGAN, Pix2Pix, CycleGAN, MedGAN, and UPGAN (Supplementary Fig. 4 and\nSupplementary Table 5) was unsurprising given that these other networks were\nnot intended to handle highly speckled noisy environments in which the\ncellular structures were not readily apparent. Our ablation studies indicated\nthe synergistic improvement realized through the WFF combination to the twin\ndiscriminator (D1) for the recovery of the fine local structural details and\nthe traditional CNN-based discriminator (D2) for global feature recovery (Fig.\n2). In terms of computational complexity, it should also be noted that the\nnetwork architecture of P-GAN has much fewer number of parameters (8.8-fold)\ncompared to CycleGAN (Supplementary Table 10).\n\nSubstantial time saving was realized using our P-GAN-inspired approach,\nallowing us to cover more than 15-fold more imaging locations in nearly the\nsame amount of imaging time. Without accounting for the possibility of\nparticipant fatigue, we estimate that it would have required at least 6 h to\nacquire the same amount of RPE data (12-fold reduction using P-GAN),\nillustrating how the integration of AI into the overall image acquisition\npipeline can enable novel experimental design of imaging sequences (so as not\nto relegate AI to only the post-processing regime). On top of the time spent\non image acquisition, it must be noted that data handling after image\nacquisition is an order of magnitude more costly than the image acquisition\nitself. In our current AO-OCT imager^13, which acquires streams of raw data at\na rate of 640 MB/s (expected to increase substantially with technological\nadvancements), a typical scanning session quickly adds up to terabytes of data\nfor a single participant because of the requirements of averaging\n(Supplementary Table 11). With P-GAN enabling recovery of the cellular\nfeatures from a single acquired AO-OCT volume, a 12-fold reduction (2.8 TB for\naveraging compared to 0.23 TB using P-GAN) in the size of the raw data was\nachieved. Post-processing of this data to correct for eye motion and other\nartifacts requires intense computational resources and the processing time for\na typical scanning session (four locations with averaging) is on the order of\none day or more. The post-processing for 63 locations imaged with repeated\naveraging would have taken an estimated 13 days as opposed to only 2.7 h using\nthe strategy of cellular recovery from a single acquired volume using P-GAN\n(116-fold reduction). Overall, considering both the image acquisition time as\nwell as post-processing time, the generation of a 63 location montage was\nachieved with a substantial time savings of 99-fold.\n\nThis paper contributes to the growing trend of using AI for improving spatial\nor temporal resolution and enhancing SNR in the fields of biomedical\nimaging^20,52,53 and biological microscopy^54,55,56, especially in the area of\nspeckle noise. Unlike other sources of noise, speckle noise is particularly\ntroublesome to handle due to its complex nature, non-Gaussian distribution,\nand multiplicative nature^57 (as opposed to additive noise). Consequently,\nalthough promising, traditional approaches suitable for the removal of more\nclassical types of noise did not perform as well as P-GAN. In the case of RPE\nimaging, there was no visible evidence of cellular structure in single volumes\ndue to the overwhelming presence of speckle noise. Given our demonstrated\nsuccess in applying P-GAN to this problem, we anticipate the possibility of\napplying AI to other applications affected by speckle noise in which averaging\nof sequentially acquired volumes is essential, such as AO-OCT imaging of the\ntransparent inner retinal cells (e.g., ganglion cells)^58 and optical\ncoherence tomography angiography (OCTA)^59,60,61,62.\n\nFuture application of our approach to diseased eyes will first require\nconsensus on image interpretation of diseased RPE which can have substantial\ndifferences in contrast, appearance, and size when compared to healthy RPE\ncells^63. Also, images of diseased RPE cells will need to be captured in an\nappropriately sized training dataset. As it is generally more challenging to\nobtain high-quality images from patients with disease, due in part to the\nlimited amount of clinic time that may be available for assessment, we are\nhopeful that future improvements using AI-assisted AO imaging will be\ntransformative. Nonetheless, establishing a larger normative database of\nhealthy RPE images is a critical step for comparison with diseased eyes.\n\nIn conclusion, we introduced an AI-assisted strategy to enhance the\nvisualization of the cellular details from a single speckle-obscured AO-OCT\nimage that can potentially transform the way in which imaging data is\nacquired. Not only does this strategy enable the wide-scale visualization and\nnoninvasive assessment of cellular structure in the living human eye, but\nalso, it substantially reduces the time and burden of data handling associated\nwith obtaining data. These advances help to make AO imaging more accessible\nfor routine clinical application and are critical steps towards clarifying our\nunderstanding of the structure, function, and pathophysiology of blinding\nretinal diseases.\n\n## Data availability\n\nDatasets used for training and validation are not publicly available due to\ntheir containing information that could compromise the privacy of research\nparticipants. Requests to access the training and validation datasets should\nbe directed to the corresponding author. It may be possible to make data\navailable as part of a future academic collaboration through institutional\ncollaboration agreements and additional IRB approval.\n\n## Code availability\n\nA TensorFlow implementation of P-GAN is publicly available in the Zenodo\nrepository (https://doi.org/10.5281/zenodo.10455740).\n\n## References\n\n  1. Miller, D. T. & Kurokawa, K. Cellular-scale imaging of transparent retinal structures and processes using adaptive optics optical coherence tomography. Ann. Rev. Vis. Sci. 6, 115 (2020).\n\nArticle Google Scholar\n\n  2. F\u00f6ldesy, P. et al. Ensemble averaging laser speckle contrast imaging: statistical model of improvement as function of static scatterers. Opt. Express 29, 29366\u201329377 (2021).\n\nArticle PubMed Google Scholar\n\n  3. Leineweber, M. et al. Averaging improves strain images of the biceps brachii using quasi-static ultrasound elastography. Br. J. Radiol. 87, 20130624 (2014).\n\n  4. Miller, D. T. et al. Coherence gating and adaptive optics in the eye. In Proc. Coherence Domain Optical Methods and Optical Coherence Tomography in Biomedicine VII. (SPIE, 2003).\n\n  5. Burns, S. A. et al. Adaptive optics imaging of the human retina. Prog. Retin. Eye Res. 68, 1\u201330 (2019).\n\nArticle PubMed Google Scholar\n\n  6. Roorda, A. & Duncan, J. L. Adaptive optics ophthalmoscopy. Ann. Rev. Vis. Sci. 1, 19 (2015).\n\nArticle Google Scholar\n\n  7. Hampson, K. M. et al. Adaptive optics for high-resolution imaging. Nat. Rev. Methods Primers 1, 1\u201326 (2021).\n\nArticle Google Scholar\n\n  8. Jonnal, R. S. et al. A review of adaptive optics optical coherence tomography: technical advances, scientific applications, and the future. Invest. Ophthalmol. Vis. Sci. 57, OCT51\u2013OCT68 (2016).\n\nArticle PubMed PubMed Central Google Scholar\n\n  9. Pircher, M. & Zawadzki, R. J. Review of adaptive optics OCT (AO-OCT): principles and applications for retinal imaging. Biomed. Opt. Express 8, 2536\u20132562 (2017).\n\nArticle PubMed PubMed Central Google Scholar\n\n  10. Schmitt, J. M., Xiang, S., and & Yung, K. M. Speckle in optical coherence tomography. J. Biomed. Opt. 4, 95\u2013105 (1999).\n\nArticle CAS PubMed Google Scholar\n\n  11. Strauss, O. The retinal pigment epithelium in visual function. Physiol. Rev. 85, 845\u2013881 (2005).\n\nArticle CAS PubMed Google Scholar\n\n  12. Liu, Z., Kocaoglu, O. P., and & Miller, D. T. 3D imaging of retinal pigment epithelial cells in the living human retina. Invest. Ophthalmol. Vis. Sci. 57, OCT533\u2013OCT543 (2016).\n\nArticle PubMed PubMed Central Google Scholar\n\n  13. Bower, A. J. et al. Integrating adaptive optics-SLO and OCT for multimodal visualization of the human retinal pigment epithelial mosaic. Biomed. Opt. Express 12, 1449\u20131466 (2021).\n\nArticle PubMed PubMed Central Google Scholar\n\n  14. Liu, Z. et al. In vivo measurement of organelle motility in human retinal pigment epithelial cells. Biomed. Opt. Express 10, 4142\u20134158 (2019).\n\nArticle PubMed PubMed Central Google Scholar\n\n  15. Liba, O. et al. Speckle-modulating optical coherence tomography in living mice and humans. Nat. Commun. 8, 1\u201313 (2017).\n\nGoogle Scholar\n\n  16. Zhang, P. et al. Aperture phase modulation with adaptive optics: a novel approach for speckle reduction and structure extraction in optical coherence tomography. Biomed. Opt. Express 10, 552\u2013570 (2019).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  17. Pircher, M. et al. Speckle reduction in optical coherence tomography by frequency compounding. J. Biomed. Opt. 8, 565\u2013569 (2003).\n\nArticle PubMed Google Scholar\n\n  18. Desjardins, A. et al. Angle-resolved optical coherence tomography with sequential angular selectivity for speckle reduction. Opt. Express 15, 6200\u20136209 (2007).\n\nArticle CAS PubMed Google Scholar\n\n  19. G\u00f6tzinger, E. et al. Speckle noise reduction in high speed polarization sensitive spectral domain optical coherence tomography. Opt. Express 19, 14568\u201314584 (2011).\n\nArticle PubMed Google Scholar\n\n  20. Das, V., Dandapat, S., and & Bora, P. K. Unsupervised super-resolution of OCT images using generative adversarial network for improved age-related macular degeneration diagnosis. IEEE Sens. J. 20, 8746\u20138756 (2020).\n\nArticle Google Scholar\n\n  21. Huang, Y. et al. Simultaneous denoising and super-resolution of optical coherence tomography images based on generative adversarial network. Opt. Express 27, 12289\u201312307 (2019).\n\nArticle PubMed Google Scholar\n\n  22. Gao, M. et al. Reconstruction of high-resolution 6\u00d7 6-mm OCT angiograms using deep learning. Biomed. Opt. Express 11, 3585\u20133600 (2020).\n\nArticle PubMed PubMed Central Google Scholar\n\n  23. Gao, M. et al. An open-source deep learning network for reconstruction of high-resolution oct angiograms of retinal intermediate and deep capillary plexuses. Transl. Vis. Sci. Technol. 10, 13\u201313 (2021).\n\nArticle PubMed PubMed Central Google Scholar\n\n  24. Liu, J. et al. Active cell appearance model induced generative adversarial networks for annotation-efficient cell segmentation and identification on adaptive optics retinal images. IEEE Trans. Med. Imag. 40, 2820\u20132831 (2021).\n\nArticle Google Scholar\n\n  25. Goodfellow, I. et al. Generative adversarial networks. Commun. ACM 63, 139\u2013144 (2020).\n\nArticle Google Scholar\n\n  26. Ledig, C. et al. Photo-realistic single image super-resolution using a generative adversarial network. In Proc. IEEE Conference on Computer Vision and Pattern Recognition 4681\u20134690 (IEEE, 2017).\n\n  27. Wang, X. et al. Esrgan: Enhanced super-resolution generative adversarial networks. In Proc. European Conference on Computer Vision (ECCV) Workshops 63\u201379 (Springer, 2018).\n\n  28. Wang, Y. et al. 3D conditional generative adversarial networks for high-quality PET image estimation at low dose. Neuroimage 174, 550\u2013562 (2018).\n\nArticle PubMed Google Scholar\n\n  29. Arjovsky, M., Chintala, S & Bottou, L. Wasserstein generative adversarial networks. In Int. Conference on Machine Learning 214\u2013223 (JMLR.org, 2017).\n\n  30. Isola, P. et al. Image-to-image translation with conditional adversarial networks. In Proc. IEEE Conference on Computer Vision and Pattern Recognition 1125\u20131134 (IEEE, 2017).\n\n  31. Zhu, J.-Y. et al. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proc. IEEE International Conference on Computer Vision 1113-2232 (IEEE, 2017).\n\n  32. Koch, G., Zemel, R. & Salakhutdinov, R. Siamese neural networks for one-shot image recognition. In ICML Deep Learning Workshop (2015).\n\n  33. Chopra, S., Hadsell, R. & LeCun, Y. Learning a similarity metric discriminatively, with application to face verification. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) 539\u2013546 (IEEE, 2005).\n\n  34. Prashnani, E. et al. Pieapp: perceptual image-error assessment through pairwise preference. In Proc. IEEE Conference on Computer Vision and Pattern Recognition 1808\u20131817 (IEEE, 2018).\n\n  35. Zhang, R. et al. The unreasonable effectiveness of deep features as a perceptual metric. In Proc. IEEE conference on Computer Vision and Pattern Recognition 586\u2013595 (IEEE, 2018).\n\n  36. Ding, K. et al. Image quality assessment: Unifying structure and texture similarity. IEEE Trans. Pattern Anal. Mach. Intell. 44, 2567\u20132581 (2020).\n\nGoogle Scholar\n\n  37. Heusel, M. et al. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems (2017).\n\n  38. Cooper, R. F., Aguirre, G. K. & Morgan, J. I. Fully automated estimation of spacing and density for retinal mosaics. Transl. Vis. Sci. Technol. 8, 26\u201326 (2019).\n\nArticle PubMed PubMed Central Google Scholar\n\n  39. Bennett, A. & Rabbetts, R. Proposals for new reduced and schematic eyes. Ophthalmic Physiol. Opt. 9, 228\u2013230 (1989).\n\nArticle CAS PubMed Google Scholar\n\n  40. Liu, J. et al. Automated photoreceptor cell identification on nonconfocal adaptive optics images using multiscale circular voting. Invest. Ophthalmol. Vis. Sci. 58, 4477\u20134489 (2017).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  41. Ronneberger, O., Fischer, P. & Brox, T. U-net: convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention 234\u2013241 (2015).\n\n  42. Armanious, K. et al. MedGAN: medical image translation using GANs. Comput. Med. Imaging Graph. 79, 101684 (2020).\n\nArticle PubMed Google Scholar\n\n  43. Upadhyay, U. et al. Uncertainty-guided progressive GANs for medical image translation. In 24th International Conference on Medical Image Computing and Computer Assisted Intervention 614\u2013624 (Springer, 2021).\n\n  44. Snoek, L. et al. The Amsterdam open MRI collection, a set of multimodal MRI datasets for individual difference analyses. Sci. Data 8, 1\u201323 (2021).\n\nArticle Google Scholar\n\n  45. Wang, X. et al. Chestx-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proc. IEEE Conference on Computer Vision and Pattern Recognition 2097-2106 (IEEE, 2017).\n\n  46. Ach, T. et al. Quantitative autofluorescence and cell density maps of the human retinal pigment epithelium. Invest.Ophthalmol. Vis. Sci. 55, 4832\u20134841 (2014).\n\nArticle PubMed PubMed Central Google Scholar\n\n  47. Gao, H. & Hollyfield, J. Aging of the human retina. Differential loss of neurons and retinal pigment epithelial cells. Invest. Ophthalmol. Vis. Sci. 33, 1\u201317 (1992).\n\nCAS PubMed Google Scholar\n\n  48. Liu, T. et al. Noninvasive near infrared autofluorescence imaging of retinal pigment epithelial cells in the human retina using adaptive optics. Biomed. Opt. Express 8, 4348\u20134360 (2017).\n\nArticle PubMed PubMed Central Google Scholar\n\n  49. Morgan, J. I. et al. In vivo autofluorescence imaging of the human and macaque retinal pigment epithelial cell mosaic. Invest. Ophthalmol. Vis. Sci. 50, 1350\u20131359 (2009).\n\nArticle PubMed Google Scholar\n\n  50. Panda-Jonas, S., Jonas, J. B., and & Jakobczyk-Zmija, M. Retinal pigment epithelial cell count, distribution, and correlations in normal human eyes. Am.J. Ophthalmol. 121, 181\u2013189 (1996).\n\nArticle CAS PubMed Google Scholar\n\n  51. Watzke, R. C., Soldevilla, J. D., and & Trune, D. R. Morphometric analysis of human retinal pigment epithelium: correlation with age and location. Curr. Eye Res. 12, 133\u2013142 (1993).\n\nArticle CAS PubMed Google Scholar\n\n  52. Liu, J. et al. Graded image generation using stratified CycleGAN. In 23rd International Conference on Medical Image Computing and Computer Assisted Intervention 760\u2013769 (Springer-Verlag, 2020) .\n\n  53. Liu, J. et al. Artificial intelligence-based image enhancement in pet imaging: noise reduction and resolution enhancement. PET Clin. 16, 553\u2013576 (2021).\n\nArticle PubMed PubMed Central Google Scholar\n\n  54. Chen, J. et al. Three-dimensional residual channel attention networks denoise and sharpen fluorescence microscopy image volumes. Nat. Methods 18, 678\u2013687 (2021).\n\nArticle CAS PubMed Google Scholar\n\n  55. Wang, H. et al. Deep learning enables cross-modality super-resolution in fluorescence microscopy. Nat. Methods 16, 103\u2013110 (2019).\n\nArticle CAS PubMed Google Scholar\n\n  56. Qiao, C. et al. Rationalized deep learning super-resolution microscopy for sustained live imaging of rapid subcellular processes. Nat. Biotechnol. 41, 367\u2013377 (2022).\n\n  57. Goodman, J. W. Statistical properties of laser speckle patterns. In Laser speckle and related phenomena, (ed. Dainty, J. C.) (Springer, 1975).\n\n  58. Liu, Z. et al. Imaging and quantifying ganglion cells and other transparent neurons in the living human retina. Proc. Natl Acad. Sci. USA 114, 12803\u201312808 (2017).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  59. De Carlo, T. E. et al. A review of optical coherence tomography angiography (OCTA). Int.J. Retina Vitreous 1, 1\u201315 (2015).\n\nArticle Google Scholar\n\n  60. Jia, Y. et al. Split-spectrum amplitude-decorrelation angiography with optical coherence tomography. Opt. Express 20, 4710\u20134725 (2012).\n\nArticle PubMed PubMed Central Google Scholar\n\n  61. Kurokawa, K., Liu, Z., and & Miller, D. T. Adaptive optics optical coherence tomography angiography for morphometric analysis of choriocapillaris. Biomed. Opt. Express 8, 1803\u20131822 (2017).\n\nArticle PubMed PubMed Central Google Scholar\n\n  62. Migacz, J. V. et al. Megahertz-rate optical coherence tomography angiography improves the contrast of the choriocapillaris and choroid in human retinal imaging. Biomed. Opt. Express 10, 50\u201365 (2019).\n\nArticle PubMed Google Scholar\n\n  63. Aguilera, N. et al. Widespread subclinical cellular changes revealed across a neural-epithelial-vascular complex in choroideremia using adaptive optics. Commun. Biol. 5, 893 (2022).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\nDownload references\n\n## Acknowledgements\n\nThe authors would like to thank A. Dubra for assistance with adaptive optics\ninstrumentation, H. Shroff for helpful discussions about artificial\nintelligence, and W. Zein, L.A. Huryn, C. Cukras, D. Claus, S. Yin, C.\nAppleman, J. Suy, G. Babilonia-Ayukawa, M. Arango, and D. Cunningham for\nassistance with clinical procedures. This work utilized the computational\nresources of the NIH HPC Biowulf cluster (http://hpc.nih.gov). The Office of\nData Science Strategy, National Institutes of Health provided a seed grant\nenabling us to train deep learning models using cloud-based computational\nresources, and the Office of the Scientific Information Officer (OSIO)\nBioinformatics Core, National Eye Institute, National Institutes of Health\nprovided computational and bioinformatics support related to adaptive optics\noptical coherence tomography data processing. This work was also supported by\nthe Intramural Research Program of the National Institutes of Health, the\nNational Eye Institute. The mention of commercial products, their sources, or\ntheir use in connection with material reported herein is not to be construed\nas either an actual or implied endorsement of such products by the U.S.\nDepartment of Health and Human Services.\n\n## Author information\n\n### Authors and Affiliations\n\n  1. National Eye Institute, National Institutes of Health, Bethesda, MD, 20892, USA\n\nVineeta Das, Furu Zhang, Andrew J. Bower, Joanne Li, Tao Liu, Nancy Aguilera,\nBruno Alvisio & Johnny Tam\n\n  2. Center for Devices and Radiological Health, U.S. Food and Drug Administration, Silver Spring, MD, 20993, USA\n\nZhuolin Liu & Daniel X. Hammer\n\nAuthors\n\n  1. Vineeta Das\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  2. Furu Zhang\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  3. Andrew J. Bower\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  4. Joanne Li\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  5. Tao Liu\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  6. Nancy Aguilera\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  7. Bruno Alvisio\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  8. Zhuolin Liu\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  9. Daniel X. Hammer\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  10. Johnny Tam\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n### Contributions\n\nV.D., A.J.B., Z.L., and J.T. conceived and designed the experiments. V.D.,\nF.Z., A.J.B., J.L., T.L., and N.A. collected the data. V.D., F.Z., and A.J.B.\nanalyzed the data. V.D., F.Z., A.J.B., T.L, B.A., Z.L., D.X.H, and J.T.\ncontributed to materials and analysis tools. V.D. and J.T. prepared the\nmanuscript with input from all authors.\n\n### Corresponding author\n\nCorrespondence to Johnny Tam.\n\n## Ethics declarations\n\n### Competing interests\n\nThe authors declare no competing interests.\n\n## Peer review\n\n### Peer review information\n\nCommunications Medicine thanks the anonymous reviewers for their contribution\nto the peer review of this work.\n\n## Additional information\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional affiliations.\n\n## Supplementary information\n\n### Supplementary Information\n\n### Description of Additional Supplementary Files\n\n### Supplementary Movie 1\n\n### Supplementary Movie 2\n\n### Supplementary Movie 3\n\n### Supplementary Data 1\n\n### Reporting Summary\n\n## Rights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0\nInternational License, which permits use, sharing, adaptation, distribution\nand reproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article\u2019s\nCreative Commons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article\u2019s Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this licence, visit\nhttp://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\n## About this article\n\n### Cite this article\n\nDas, V., Zhang, F., Bower, A.J. et al. Revealing speckle obscured living human\nretinal cells with artificial intelligence assisted adaptive optics optical\ncoherence tomography. Commun Med 4, 68 (2024).\nhttps://doi.org/10.1038/s43856-024-00483-1\n\nDownload citation\n\n  * Received: 18 April 2023\n\n  * Accepted: 13 March 2024\n\n  * Published: 10 April 2024\n\n  * DOI: https://doi.org/10.1038/s43856-024-00483-1\n\n### Share this article\n\nAnyone you share the following link with will be able to read this content:\n\nSorry, a shareable link is not currently available for this article.\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\n### Subjects\n\n  * Interference microscopy\n  * Optical imaging\n  * Retina\n  * Three-dimensional imaging\n\nDownload PDF\n\nAdvertisement\n\nCommunications Medicine (Commun Med) ISSN 2730-664X (online)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n  * Cancel contracts here\n\n\u00a9 2024 Springer Nature Limited\n\nSign up for the Nature Briefing newsletter \u2014 what matters in science, free to\nyour inbox daily.\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing\n\n", "frontpage": false}
