{"aid": "39960590", "title": "What's hard about end-to-end encryption in the browser?", "url": "https://tender.run/blog/web-apps-trust-and-integrity", "domain": "tender.run", "votes": 1, "user": "simonpure", "posted_at": "2024-04-07 13:23:38", "comments": 0, "source_title": "What's hard about end-to-end encryption in the browser?", "source_text": "What's hard about end-to-end encryption in the browser? | Tender\n\nSkip to content\n\nTender\n\nAppearance\n\n\u2190 Back to blog\n\n# What's hard about end-to-end encryption in the browser?\n\nWe're Tender - an inbox for your personal finances. We're building a secure,\nprivate personal finance tracker that doesn't sacrifice the conveniences of a\nmodern web-based service.\n\nOne of the key features we're thinking about is browser-based end-to-end\nencryption (e2ee) to add an extra layer of security. The idea behind e2ee is\nthat encryption happens on the user's device, using keys that only the user\nhas access to.\n\nIn the past, implementing this kind of encryption was complex because there\nweren't readily available web APIs for client-side encryption. However, modern\nbrowsers have been shipping crypto primitives for some years now. Even then,\nthe encryption scheme is only part of the puzzle. The code to perform said\nencryption has to come from the server to begin with.\n\nSo the model breaks down here - if the server is compromised, then the whole\nthing is still compromised.\n\n## Building trust\n\nFrom the angle of minimizing our attack surface, we can try to reduce the\nscope of what needs to be trusted from \"the entire server\" to a smaller\nsurface that can be secured more easily.\n\nTo do this, we can try to do what other \"app store\" distribution channels have\ndone forever: we can sign the application and distribute public keys so that\nusers can verify the application came from us. That way, even if an attacker\ngets a hold of our server, they can't send users a compromised copy of the app\nwithout tripping some alarms.\n\n## Implementing signing on the web\n\nSigning our web app involves two main steps:\n\n  * signing the app (or with subresource integrity, at least signing the root index.html file for our single-page app)\n  * distributing the public key to verify the app\n\nWhile there are no out-of-the-box solutions for these steps, there have been\nexciting developments in the last few years.\n\nLet's explore at some approaches.\n\n## Verification via Browser Extensions\n\n### Mylar\n\nMy first encounter with this problem came from the Mylar paper from 2014. The\napproach roughly boils down to signing the index.html contents with a key\nthat's verified via chrome extension.\n\nThe user has to download this separate chrome extension which also has to be\ntrusted. There's then a cute mechanism to deliver the public keys via a\nsection of the TLS cert, i.e. \"if you trust my TLS cert, then you can trust my\napp.\"\n\nThe clever part here is combining the public key distribution with an existing\nmechanism - the TLS cert.\n\n### Meta Code Verify\n\nMore recently, Facebook Meta has taken up a similar approach with Code Verify\n(2/5 stars on the chrome web store) which works in a similar fashion to secure\nmessenger.com and instagram web.\n\nIn Meta's scheme, the hashes for the app are separately hosted by Cloudflare -\nthe idea being that it'd be hard for an attacker to compromise both Meta and\nCloudflare at the same time.\n\n### WebSign by Cyph\n\nWebSign had a pretty interesting take on the problem. Instead of relying on a\nbrowser extension, WebSign hands verification duties off to a secure\n\"bootloader\" of sorts via a service worker, since service workers can\nintercept requests and do some of the things a chrome extension can.\n\nOn first load of the application, WebSign installs this bootloader, which is\nmade permanent by abusing the now defunct HTTP Public Key Pinning (HPKP)\nfeature. tl; dr: WebSign would deliver the bootloader over TLS and\nperiodically destroy/remake its TLS cert. This effectively prevents the\nbrowser from fetching new versions of the loader code, since a new version's\nTLS cert wouldn't match the cert you previously received, which has been\npinned by HPKP. Kind of neat!\n\nThis really hinges on the bootloader being simple and free of bugs. HPKP would\nlater get removed from browsers for this very reason - developers could\ntrivially brick their app by accident with no way of fixing it.\n\nAs an aside, this approach reminds me of a security measure Apple takes with\niCloud Keychains; in that scheme, server code is signed with a private key\nthat then gets destroyed by a literal blender to prevent upgrade attacks.\n\n### Delivery via a big url blob\n\nOne final curiosity I found that's worth mentioning is caution.js. The idea is\nroughly the same as WebSign - the index.html page is signed, blah, blah, blah.\nThe cute part here is the delivery mechanism. The user downloads this\n\"bootloader\" equivalent into a bookmarklet (literally a browser bookmark to a\n\"url\" with some js in it javascript:function abc()...) which can't be tampered\nwith by the server.\n\nPretty cool - though good luck teaching users about bookmarklets.\n\n## Establishing Trust\n\nIn the end, these approaches are all pretty much the same. WebSign and\ncaution.js establish trust on first use (TOFU) by making sure subsequent\nversions of the app all come from the original source. Mylar and Meta's code\nverify don't do TOFU but instead distribute keys separately from the app.\n\nUnfortunately, it seems like these mechanisms for trusting web apps are still\npretty early and there's no blessed path for one to easily implement.\n\nOn the web standards side, there was a Webpackaging proposal a few years back\nthat tried to tackle this problem, but it doesn't look like it's gone\nanywhere. Notably, the Safari team opposed the addition in 2019 and Chrome\nremoved experimental support last February.\n\n## Stephen on March 15, 2024\n\nPrivacy Policy \u00b7 Terms of Service \u00b7 Security Overview\n\nCopyright \u00a9 2024 Tender\n\n", "frontpage": false}
