{"aid": "40087392", "title": "Optimizing the Deployment of Tiny Transformers on Low-Power MCUs", "url": "https://arxiv.org/abs/2404.02945", "domain": "arxiv.org", "votes": 1, "user": "PaulHoule", "posted_at": "2024-04-19 14:38:23", "comments": 0, "source_title": "Optimizing the Deployment of Tiny Transformers on Low-Power MCUs", "source_text": "[2404.02945] Optimizing the Deployment of Tiny Transformers on Low-Power MCUs\n\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, University of\nOxford, and all contributors. Donate\n\n> cs > arXiv:2404.02945\n\n# Computer Science > Machine Learning\n\narXiv:2404.02945 (cs)\n\n[Submitted on 3 Apr 2024]\n\n# Title:Optimizing the Deployment of Tiny Transformers on Low-Power MCUs\n\nAuthors:Victor J.B. Jung, Alessio Burrello, Moritz Scherer, Francesco Conti,\nLuca Benini\n\nView a PDF of the paper titled Optimizing the Deployment of Tiny Transformers\non Low-Power MCUs, by Victor J.B. Jung and 4 other authors\n\nView PDF HTML (experimental)\n\n> Abstract:Transformer networks are rapidly becoming SotA in many fields, such\n> as NLP and CV. Similarly to CNN, there is a strong push for deploying\n> Transformer models at the extreme edge, ultimately fitting the tiny power\n> budget and memory footprint of MCUs. However, the early approaches in this\n> direction are mostly ad-hoc, platform, and model-specific. This work aims to\n> enable and optimize the flexible, multi-platform deployment of encoder Tiny\n> Transformers on commercial MCUs. We propose a complete framework to perform\n> end-to-end deployment of Transformer models onto single and multi-core MCUs.\n> Our framework provides an optimized library of kernels to maximize data\n> reuse and avoid unnecessary data marshaling operations into the crucial\n> attention block. A novel MHSA inference schedule, named Fused-Weight Self-\n> Attention, is introduced, fusing the linear projection weights offline to\n> further reduce the number of operations and parameters. Furthermore, to\n> mitigate the memory peak reached by the computation of the attention map, we\n> present a Depth-First Tiling scheme for MHSA. We evaluate our framework on\n> three different MCU classes exploiting ARM and RISC-V ISA, namely the\n> STM32H7, the STM32L4, and GAP9 (RV32IMC-XpulpV2). We reach an average of\n> 4.79x and 2.0x lower latency compared to SotA libraries CMSIS-NN (ARM) and\n> PULP-NN (RISC-V), respectively. Moreover, we show that our MHSA depth-first\n> tiling scheme reduces the memory peak by up to 6.19x, while the fused-weight\n> attention can reduce the runtime by 1.53x, and number of parameters by 25%.\n> We report significant improvements across several Tiny Transformers: for\n> instance, when executing a transformer block for the task of radar-based\n> hand-gesture recognition on GAP9, we achieve a latency of 0.14ms and energy\n> consumption of 4.92 micro-joules, 2.32x lower than the SotA PULP-NN library\n> on the same platform.\n\nComments:| Pre-print manuscript submitted for review to the IEEE Transactions\non Computers  \n---|---  \nSubjects:| Machine Learning (cs.LG); Artificial Intelligence (cs.AI);\nDistributed, Parallel, and Cluster Computing (cs.DC); Performance (cs.PF)  \nCite as:| arXiv:2404.02945 [cs.LG]  \n(or arXiv:2404.02945v1 [cs.LG] for this version)  \nhttps://doi.org/10.48550/arXiv.2404.02945arXiv-issued DOI via DataCite  \n  \n## Submission history\n\nFrom: Victor Jung [view email] [v1] Wed, 3 Apr 2024 14:14:08 UTC (2,076 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Optimizing the Deployment of Tiny Transformers\non Low-Power MCUs, by Victor J.B. Jung and 4 other authors\n\n  * View PDF\n  * HTML (experimental)\n  * TeX Source\n  * Other Formats\n\nview license\n\nCurrent browse context:\n\ncs.LG\n\n< prev | next >\n\nnew | recent | 2404\n\nChange to browse by:\n\ncs cs.AI cs.DC cs.PF\n\n### References & Citations\n\n  * NASA ADS\n  * Google Scholar\n  * Semantic Scholar\n\na export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer (What is the Explorer?)\n\nLitmaps (What is Litmaps?)\n\nscite Smart Citations (What are Smart Citations?)\n\n# Code, Data and Media Associated with this Article\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\nDagsHub (What is DagsHub?)\n\nGotit.pub (What is GotitPub?)\n\nPapers with Code (What is Papers with Code?)\n\nScienceCast (What is ScienceCast?)\n\n# Demos\n\nReplicate (What is Replicate?)\n\nHugging Face Spaces (What is Spaces?)\n\nTXYZ.AI (What is TXYZ.AI?)\n\n# Recommenders and Search Tools\n\nInfluence Flower (What are Influence Flowers?)\n\nConnected Papers (What is Connected Papers?)\n\nCORE Recommender (What is CORE?)\n\nIArxiv Recommender (What is IArxiv?)\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new\narXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and\naccepted our values of openness, community, excellence, and user data privacy.\narXiv is committed to these values and only works with partners that adhere to\nthem.\n\nHave an idea for a project that will add value for arXiv's community? Learn\nmore about arXivLabs.\n\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n\n  * About\n  * Help\n\n  * Contact\n  * Subscribe\n\n  * Copyright\n  * Privacy Policy\n\n  * Web Accessibility Assistance\n  * arXiv Operational Status Get status notifications via email or slack\n\n", "frontpage": false}
