{"aid": "39965152", "title": "Making Peace with LLM Non-Determinism", "url": "https://barryzhang.substack.com/p/making-peace-with-llm-non-determinism", "domain": "barryzhang.substack.com", "votes": 1, "user": "gnahzby", "posted_at": "2024-04-08 00:28:42", "comments": 0, "source_title": "Making Peace with LLM Non-determinism", "source_text": "Making Peace with LLM Non-determinism\n\n# The Finest Tuners\n\nShare this post\n\n#### Making Peace with LLM Non-determinism\n\nbarryzhang.substack.com\n\n#### Discover more from The Finest Tuners\n\nContinue reading\n\nSign in\n\n# Making Peace with LLM Non-determinism\n\n### Digging into MoE and GPU cycles to realize non-determinism is not new,\nlanguage is.\n\nBarry Z\n\nand\n\nJulian Richey\n\nApr 07, 2024\n\nShare this post\n\n#### Making Peace with LLM Non-determinism\n\nbarryzhang.substack.com\n\nShare\n\nNon-determinism in LLMs has always bothered me. LLMs consistently surprise me\nwith out-of-nowhere outputs. This then introduces a string of anxiety about\nnot being able to reliably reproduce user journey, steer the outputs, set up\neffective unittest/monitoring, or frankly guarantee any product behavior at\n100%.\n\nIs my blog name a 7 or an 8?\n\nAfter hours of reading and eventually inviting my chip designer friend Julian\nto help me out, my understanding of the challenge evolved from non-determinism\nto language itself.\n\nThis post is about that journey.\n\n##\n\nMisconception about sampling:\n\nMy instinct for sampling was that \u201cThe greedier the sampling\n(Temp=0|top-p=0|top-k=1) the more deterministic it is'', and this is not\nwrong. As demonstrated by someone else\u2019s experiments below, greedier sampling\ndoes constrain the number of possibilities in the output space.\n\nNumber of different answers at different T and Top P for 3 prompts (source)\n\nHowever, while sampling parameters influence the perceived diversity of\ngenerated text, sampling itself is often pseudo-random and we could achieve\nthe \u201csame input, same output\u201d effect with seeding. Furthermore, with proper\nseeding, the choice of hyperparameters turns into a deterministic optimization\nproblem.\n\nThe seed choice introduces variances that matter in some cases (like RL.) One\nhas to decide whether their use cases warrants some understanding or even\noptimization over. In most cases, just trust whoever set it to 42 :-)\n\n##\n\nSeeding is all you need, unless you are sparse MoE\n\nSeeding it is then.\n\n  * Randomness in the sampling process? Seed it! (Pytorch, Jax, tensorflow).\n\n  * Have custom operators and code components? Seed it! (numpy, python)\n\n  * Using the GPT-4 API? Seed it! (cookbook)\n\nNot quite. Many (including the cookbook) will show that seeded GPT-4 is still\nnot deterministic. We found this blog post theorizing that the GPT-4 API\nlikely runs batched inference, and its Mixture of Expert architecture\nenforces/encourages balance among experts within a batch for efficiency. This\nmakes the routing of an input sequence dependent on its position and other\nsequences in the batch, and determinism only exists for similar batches and\nnot similar inputs. (from Soft MoE paper and Google\u2019s expert choice routing)\n\nThis is satisfying enough as batch_size of 1 should still give us determinism,\nbut there were mentions of hardware non-determinism that I wanted to\nunderstand more.\n\n> \u201cHey Julian, what could cause non-determinism at the hardware level?\u201d\n\n##\n\nHardware non-determinism\n\nWe went down such a deep rabbit hole that Julian might dive into the hardware\nside in another post, but here\u2019s what I distilled:\n\nAside from seed, there are other framework-level controls that further\nguarantee GPU-level determinism (for example, deterministic operation for\npytorch and tensorflow.) These controls are quite costly:\n\n2021 Nvidia slide on hardware determinism\n\nThat \u201c0-10% deceleration\u201d from enabling determinism gave us some clues:\nHardware non-determinism often comes from intentional operation\nimplementations that optimize for performance.\n\nWhy are performances and determinism at odds? Deterministic outputs across\nruns often require the consistent operation order because floating point\noperations aren\u2019t associative. For operation orders to be consistent,\nsynchronization over memory access is required, and it is one of the \u201cmost\nexpensive operations\u201d for GPUs.\n\nNon-associative floating point additions\n\nThe problem of operation orders gets worse when inference is performed by a\nnetwork of (potentially different) devices because now we have:\n\n  * Hardware Variation: frameworks tailor algorithms to each device, so maintaining operation orders across device architectures (e.g. AMD and NVDA) is challenging\n\n  * Device parallelization: the scale of sharded inference compounds the chance of running into one instance of non-deterministic behavior.\n\nWe found these to be the main reasons, but there were other sources like the\nchip\u2019s orientation relative to the sky that we...refuse to comment on.\n\nWe deduced that most ML use cases have prioritized performance over absolute\ndeterminism, but this may change if determinism becomes more important. -\nGroq\u2019s marketing of deterministic processor is a prime example. Hardware non-\ndeterminism in machine learning has always been around, so what makes LLMs\nspecial?\n\n##\n\nIf non-determinism isn\u2019t new, what is?\n\nLanguage.\n\nWe concluded that what made working with LLMs hard for me is not from these\nresidual non-determinism, but from language itself.\n\nLanguage is inherently ambiguous and high-cardinality, and having language as\nboth inputs and outputs makes any perturbation much more nebulous. We\ngenerally expect the magnitude of changes in a model's outputs to be\nproportional to that of its inputs, but language models often break this\nexpectation. A whole paragraph of prompt changes might not steer your stubborn\nmodel one bit, but the tiniest differences in punctuation or capitalization\ncould shift the entire output distribution. This then propagates to the next\ntoken, next LLM, next LLM-system/agent, ...\n\nWhat can we do? Here are some things that have helped me:\n\n  * Make better models: Lower perplexity and more steerable models will likely address some of these issues. Remember using GPT-4 or Claude-2 Opus for the first time?\n\n  * Reduce unnecessary non-determinism: The output space of LLM is incredibly large, and we should control perturbations that don\u2019t contribute to our goal.\n\n  * Reduce ambiguity:\n\n    * Pre-processing: We could reduce some of the ambiguity outright by consolidating the inputs (e.g. pre-process for typos and synonyms.) This should be immediately effective but likely doesn\u2019t scale well.\n\n    * Caching: Alternatively, we could cache the outputs and route similar inputs towards them. Some might argue for either low cache hit rate or producing overly stiff response, but I believe this could be done quite gracefully, like perplexity\u2019s discover page turning previous queries into content.\n\n  * Reduce cardinality:\n\n    * Guided decoding: GD can artificially constrain the sampling to only a subset of tokens (making LLMs more useful for, say, classification or NER.)\n\n    * Semantic Eval: Semantic metrics (e.g. BERTscore), though limited in precision to capture linguistic nuances, can help group sufficiently semantically similar outputs.\n\n    * Define clear axes for evaluation: Focusing evals on axes greatly reduces the multi-faceted trade-offs of language outputs and reduces cardinality for eval.\n\n  * Embrace language and all its random glory\n\n    * Natively Robust UX: Without having to strictly define a formal system, language-models provide one of the most robust UX I\u2019ve worked with. It has clear affordance, encourages exploration, and is fairly robust to bad inputs.\n\n    * Exploration: Non-determinism in language models provide us with an effective way to explore. Especially when we ground the results with deterministic scaffold like code-verification (e.g. Alpha Geometry), we can generate and verify ideas at very low marginal cost.\n\n### Subscribe to The Finest Tuners\n\nBy Barry Z \u00b7 Launched 7 months ago\n\nShare this post\n\n#### Making Peace with LLM Non-determinism\n\nbarryzhang.substack.com\n\nShare\n\nA guest post by| Julian RicheyASIC Design Engineer at Amazon| Subscribe to\nJulian  \n---|---  \n  \nComments\n\nOur Humble Attempt at \u201cHow Much Data Is Needed to Fine-Tune\u201d\n\nEfficacy of OpenAI Fine-tuning API, Cost/Latency Considerations, Experiencing\nCatastrophic Forgetting, and...Getting Roasted by a Fine-tuned GPT-3.5\n\nSep 22, 2023 \u2022\n\nBarry Z\n\n,\n\nDaniel Chang\n\n,\n\nEmma Qian\n\n, and\n\nMichael Agaby\n\n23\n\nShare this post\n\n#### Our Humble Attempt at \u201cHow Much Data Is Needed to Fine-Tune\u201d\n\nbarryzhang.substack.com\n\n4\n\nReady for more?\n\n\u00a9 2024 Barry Zhang\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great writing\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
