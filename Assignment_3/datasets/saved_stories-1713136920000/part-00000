{"aid": "40032651", "title": "Deep Dream and Mechanistic Interpretability", "url": "https://swe-to-mle.pages.dev/posts/deepdream-and-mechanistic-interpretability/", "domain": "swe-to-mle.pages.dev", "votes": 1, "user": "peluche_", "posted_at": "2024-04-14 17:19:45", "comments": 0, "source_title": "Deepdream and Mechanistic Interpretability", "source_text": "Deepdream and Mechanistic Interpretability - SWE to ML Engineer\n\nSWE to ML Engineer\n\nPosts Tags Categories About Hire Me Grimoire\n\nSWE to ML Engineer\n\nPostsTagsCategoriesAboutHire MeGrimoire\n\n## Contents\n\n# Deepdream and Mechanistic Interpretability\n\npeluche included in bestiary\n\n2023-10-25 748 words 4 minutes\n\nContents\n\nA Beholder awakens. Its myriad eyes, each a facet of mechanistic insight, gaze\nupon the intricate layers of information, revealing hidden patterns in the\ndreams of code. In the tapestry of deepdream, the Beholder becomes the\nguardian of interpretability, its central eye illuminating the enigmatic\nconnections woven within the digital labyrinth.\n\nBeauty is in the eye of the Beholder\n\n## The Quest\n\nProduce deepdreams from an image classifier. Try to identify specific features\nin the network, and alter them to blind the network.\n\n## Deepdream\n\nDeepdream is fairly similar to what we used to fool an image classifier.\nInstead of backpropagating to the original image to minimize the loss for a\nmalicious label. We backpropagate to the original image with the intention to\nmaximize some activation layer in the middle of the network. This is called\ngradient ascent.\n\n### Hook into the image classifier\n\nFirst we need to hook into the classifier to get access to the activation\nvalues of the network\n\n    \n    \n    def hook(layer, k, mem=None): if mem is None: mem = {} def f(module, input, output): mem[k] = output layer.register_forward_hook(f) return mem def hooked(model): m = copy.deepcopy(model).to(device) mem = {} for layer in range(37): mem = hook(m.features[layer], layer, mem=mem) return mem, m  \n  \n---  \n  \n### Dream\n\nBy performing gradient ascent on any layer we amplify the input pixels that\nwould make this layer more active.\n\n    \n    \n    def shallowdream(start, layer=35, channel=None, m=vgg_hooked, mem=vgg_mem, learning_rate=0.01, epochs=30): start = copy.deepcopy(start.detach()) # move to device dream = start.to(device).requires_grad_() m = m.to(device) for _ in tqdm(range(epochs)): m(dream) loss = mem[layer].norm() if channel is None else mem[layer][:, channel, :, :].norm() dream.grad = None loss.backward() dream.data = torch.clip((dream + dream.grad * learning_rate), 0., 1.).data # jumping through hoops to please pytorch return drea  \n  \n---  \n  \nNow we just have to choose a layer and let the model dream\n\nDreaming from the sky on all layers\n\nIn this example we can see that earlier layers produce simple features, and\nthe deeper we probe into the network the more complex patterns emerge.\n\nTo me some of the layers seem to have meaning (but it might just be an\nillusion spell):\n\n  * 25 looks like buildings, human cronstructions\n  * 30 like mountains\n  * and 27 like creatures\n\nSome special layers?\n\nWe can also choose a given layer and just dream deeper and deeper\n\nA dog's nightmare\n\nAnd a few more samples for different starting points\n\nDog on different layersVangogh on different layersVoid on different layers\n\n## Mechanistic interpretability\n\nOur secondary goal is to identify channels in each layers that are particulary\nketering to Kelpie dogs.\n\n### Identify channels\n\nWe can feed a bunch of pictures of kelpies and look at the channels that are\nthe most activated and shared between all kelpies.\n\n    \n    \n    def save_activations(start, m=vgg_hooked, mem=vgg_mem): # move to device dream = start.to(device).requires_grad_() m = m.to(device) # run the model m(dream) # make a copy of the activations activations = {k: copy.deepcopy(output.detach()) for k, output in mem.items()} return activations # compute the top n channels with the highest norm def topn(activation, n, threshold): channels = activation.shape[1] top = sorted(zip(activation.view(channels, -1).norm(dim=1), range(channels)), reverse=True)[:n] return [idx for norm, idx in top if norm > threshold] def topn_activations(activations, n=10, threshold=0): return {k: topn(activation, n=n, threshold=threshold) for k, activation in activations.items()} def count_topn(all_topn): counts = defaultdict(Counter) for topn in all_topn: for layer, top in topn.items(): counts[layer].update(top) return counts all_activations = [save_activations(kelpie) for kelpie in kelpies] all_topn = [topn_activations(activations) for activations in all_activations] counts = count_topn(all_topn)  \n  \n---  \n  \nTake a look at the features:\n\nChannel shared by all Kelpies\n\n### Blind the network\n\nNow lets disable the channels we identified and see how the classifier\nbehaves.\n\n    \n    \n    def blinder(counts, model, min_layer=20, most_common=5, threshold=4): # nuke a channel def nuke(layer, channel): def f(module, input, output): output[:, channel, :, :] = 0. layer.register_forward_hook(f) m = copy.deepcopy(model).to(device) for layer, count in counts.items(): # lower layers are basic features like edges, so it doesn't make sense to nuke them if layer < min_layer: continue for channel, occurences in count.most_common(most_common): if occurences < threshold: break nuke(m.features[layer], channel) return m vgg_blind = blinder(counts, vgg) @torch.no_grad() def evaluate_blinder(img): res = [] models = [ ('VGG19', vgg.eval()), ('blinded', vgg_blind.eval()), ] for name, model in models: label, confidence = classify(img, model) if len(label) > 20: label = label[:20] + '...' res.append(f'{name:8}: {label:23} ({confidence*100:.2f}%)') return '\\n'.join(res)  \n  \n---  \n  \nClassifying Kelpies after brain surgeryClassifying controls after brain\nsurgery\n\nThe third picture still register as kelpie, but everything else is gone, and\nthe control still match. I\u2019m ok with that, even a blind chicken finds a grain\nonce in a while ;)\n\n## The code\n\nYou can get the code at https://github.com/peluche/deepdream\n\nUpdated on 2023-10-25\n\ndeepdream, mechanistic interpretability, VGG19, CNN, image classifier, imagenet-1k, beholderBack | Home\n\nFooling an Image Classifier Neural Style Transfer\n\n2023 - 2024 peluche\n\n", "frontpage": false}
