{"aid": "40027173", "title": "Chinchilla Debunked", "url": "https://arxiv.org/abs/2404.06395", "domain": "arxiv.org", "votes": 2, "user": "andai", "posted_at": "2024-04-13 23:32:01", "comments": 0, "source_title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies", "source_text": "[2404.06395] MiniCPM: Unveiling the Potential of Small Language Models with\nScalable Training Strategies\n\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, supporters\nworldwide, including the CERN Library, and all contributors. Donate\n\n> cs > arXiv:2404.06395\n\n# Computer Science > Computation and Language\n\narXiv:2404.06395 (cs)\n\n[Submitted on 9 Apr 2024]\n\n# Title:MiniCPM: Unveiling the Potential of Small Language Models with\nScalable Training Strategies\n\nAuthors:Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi\nZheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai,\nKaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai,\nZhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu,\nMaosong Sun\n\nView a PDF of the paper titled MiniCPM: Unveiling the Potential of Small\nLanguage Models with Scalable Training Strategies, by Shengding Hu and 24\nother authors\n\nView PDF HTML (experimental)\n\n> Abstract:The burgeoning interest in developing Large Language Models (LLMs)\n> with up to trillion parameters has been met with concerns regarding resource\n> efficiency and practical expense, particularly given the immense cost of\n> experimentation. This scenario underscores the importance of exploring the\n> potential of Small Language Models (SLMs) as a resource-efficient\n> alternative. In this context, we introduce MiniCPM, specifically the 1.2B\n> and 2.4B non-embedding parameter variants, not only excel in their\n> respective categories but also demonstrate capabilities on par with 7B-13B\n> LLMs. While focusing on SLMs, our approach exhibits scalability in both\n> model and data dimensions for future LLM research. Regarding model scaling,\n> we employ extensive model wind tunnel experiments for stable and optimal\n> scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning\n> rate scheduler (LRS), conducive to continuous training and domain\n> adaptation. We present an in-depth analysis of the intriguing training\n> dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to\n> efficiently study data-model scaling law without extensive retraining\n> experiments on both axes of model and data, from which we derive the much\n> higher compute optimal data-model ratio than Chinchilla Optimal.\n> Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-\n> MoE and MiniCPM-128K, whose excellent performance further cementing\n> MiniCPM's foundation in diverse SLM applications. MiniCPM models are\n> available publicly at this https URL .\n\nComments:| 17 pages paper, 7 pages Appendix  \n---|---  \nSubjects:| Computation and Language (cs.CL); Machine Learning (cs.LG)  \nCite as:| arXiv:2404.06395 [cs.CL]  \n(or arXiv:2404.06395v1 [cs.CL] for this version)  \nhttps://doi.org/10.48550/arXiv.2404.06395arXiv-issued DOI via DataCite  \n  \n## Submission history\n\nFrom: Shengding Hu [view email] [v1] Tue, 9 Apr 2024 15:36:50 UTC (17,017 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MiniCPM: Unveiling the Potential of Small\nLanguage Models with Scalable Training Strategies, by Shengding Hu and 24\nother authors\n\n  * View PDF\n  * HTML (experimental)\n  * TeX Source\n  * Other Formats\n\nview license\n\nCurrent browse context:\n\ncs.CL\n\n< prev | next >\n\nnew | recent | 2404\n\nChange to browse by:\n\ncs cs.LG\n\n### References & Citations\n\n  * NASA ADS\n  * Google Scholar\n  * Semantic Scholar\n\na export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer (What is the Explorer?)\n\nLitmaps (What is Litmaps?)\n\nscite Smart Citations (What are Smart Citations?)\n\n# Code, Data and Media Associated with this Article\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\nDagsHub (What is DagsHub?)\n\nGotit.pub (What is GotitPub?)\n\nPapers with Code (What is Papers with Code?)\n\nScienceCast (What is ScienceCast?)\n\n# Demos\n\nReplicate (What is Replicate?)\n\nHugging Face Spaces (What is Spaces?)\n\nTXYZ.AI (What is TXYZ.AI?)\n\n# Recommenders and Search Tools\n\nInfluence Flower (What are Influence Flowers?)\n\nConnected Papers (What is Connected Papers?)\n\nCORE Recommender (What is CORE?)\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new\narXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and\naccepted our values of openness, community, excellence, and user data privacy.\narXiv is committed to these values and only works with partners that adhere to\nthem.\n\nHave an idea for a project that will add value for arXiv's community? Learn\nmore about arXivLabs.\n\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n\n  * About\n  * Help\n\n  * Contact\n  * Subscribe\n\n  * Copyright\n  * Privacy Policy\n\n  * Web Accessibility Assistance\n  * arXiv Operational Status Get status notifications via email or slack\n\n", "frontpage": false}
