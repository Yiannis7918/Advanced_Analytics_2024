{"aid": "40078464", "title": "Blauhaunt \u2013 tool for filtering and visualizing logon events", "url": "https://github.com/cgosec/Blauhaunt", "domain": "github.com/cgosec", "votes": 1, "user": "aa_is_op", "posted_at": "2024-04-18 17:13:05", "comments": 0, "source_title": "GitHub - cgosec/Blauhaunt: A tool collection for filtering and visualizing logon events. Designed to help answering the \"Cotton Eye Joe\" question (Where did you come from where did you go) in Security Incidents and Threat Hunts", "source_text": "GitHub - cgosec/Blauhaunt: A tool collection for filtering and visualizing\nlogon events. Designed to help answering the \"Cotton Eye Joe\" question (Where\ndid you come from where did you go) in Security Incidents and Threat Hunts\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\ncgosec / Blauhaunt Public\n\n  * Notifications\n  * Fork 7\n  * Star 98\n\nA tool collection for filtering and visualizing logon events. Designed to help\nanswering the \"Cotton Eye Joe\" question (Where did you come from where did you\ngo) in Security Incidents and Threat Hunts\n\n### License\n\nMIT license\n\n98 stars 7 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# cgosec/Blauhaunt\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n2 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\ncgosecMerge remote-tracking branch 'origin/main'Apr 16, 2024b14c3e7 \u00b7 Apr 16,\n2024Apr 16, 2024\n\n## History\n\n176 Commits  \n  \n### app\n\n|\n\n### app\n\n| Merge remote-tracking branch 'origin/main'| Apr 16, 2024  \n  \n### parser\n\n|\n\n### parser\n\n| Update Defender365_Query.md| Apr 1, 2024  \n  \n### test_data\n\n|\n\n### test_data\n\n| minor fix| Jan 5, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit| Oct 13, 2023  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md| Apr 1, 2024  \n  \n## Repository files navigation\n\n# Blauhaunt\n\nA tool collection for filtering and visualizing logon events. Designed to help\nanswering the \"Cotton Eye Joe\" question (Where did you come from where did you\ngo) in Security Incidents and Threat Hunts.\n\nThis tool is designed for experienced DFIR specialists. You may have little to\nnone usage from it without experience in Threat Hunting\n\n## Table of Contents\n\n  * Get started\n  * Integration in investigation\n  * Architecture\n  * PowerShell Script\n  * Velociraptor Artifact\n  * Defender 365 KUSTO Query\n  * Acknowledgements\n\n### Interactive User Graph\n\n### Heatmap of User activities\n\n### Timeline\n\n## Get started\n\nRunning Blauhaunt is as simple as that:\n\nopen https://cgosec.github.io/Blauhaunt/app/ since there is no backend no data\nwill leave your local system. (third party libraries integrated and I do not\ntake any responsibilities of their communication behavior. Imports are in the\nindex.html file on top)\n\nrun a cmd or bash or what ever you like...\n\nThen:\n\n    \n    \n    git clone https://github.com/cgosec/Blauhaunt cd Blauhaunt/app python -m http.server\n\nNow you can navigate to http://localhost:8000/ in your browser and start blau\nhaunting the baddies.\n\nSome random test data is in the directory test_data to get started. However\nthis is just randomly generated and nothing to start investigate with.\n\n## Integrate into Velociraptor\n\nYou can use Velociraptors reverse proxy capability to host Blauhaunt directly\nwithin your instance. Blauhaunt is Velo Aware. If You do so, Blauhaunt will\nget the Data automaticall from Velociraptor and you do not have to upload\ndata.\n\nYou need to start a Hunt with the Velo Artifact. You can use the Monitoring\nArtifact too to get real time data form Velo.\n\n### Velo Settings:\n\nsee: Velo Docs\n\nhint the url is absolute. I did not test yet, if you can just reference the\nhosted instance elsewhere...\n\nThats basically all you have to do.... :)\n\nbig big thanks to Mike Cohen who helped me with the workflow for CSRF-Tokens\nand the not documented REST-API of Velo.\n\n### Upload Data\n\nKlick \"Upload Data\" (surprising isn't it :-P)\n\nUpload the json export of the velo artifact or the result(s) of the powershell\nscript here. Do not upload the client_info.json or anything in here!\n\nThis is optional and just needed for having system tags and their os info.\nUpload your client_info.json extract here. This is just an export of the\nVelociraptor clients() function. Just use this query:\n\n    \n    \n    SELECT * FROM clients()\n\nand export the json\n\nThis is optional too. Upload a mapping for having IP-Addresses resolved to\ntheir hostnames. You need to have a file where there is one col for Hostnames\nand a col for IP-Addresses. If a System has multiple IP-Addresses you can have\nthem in this one col separated by an arbitrarily symbol e.G. \"/\".\n\nExample:\n\nHostname| IP-Addresses| MaybeSomeNotNeededStuff  \n---|---|---  \nSystem_A| 10.10.10.100| bonjour  \nSystem_B| 10.10.10.100 / 10.10.20.100| hello  \nSystem_C| 10.10.10.100| hola  \n  \nOnce a proper file is selected a delimiter (if non is specified a comma is\nexpected). And click Load Map.\n\n  1. Choose the name of the col where the hostname is in\n  2. OPTIONAL: Specify if there are any entries you want to exclude from parsing e.g. lines having an \"UNKNOWN\" in the Hostname Ip Mapping.\n  3. Choose the name of the col where the IP-Address is in\n  4. Specify the delimiter for multiple IP-Addresses in this line When everything is correct click\n\nWhen done click\n\nIf everything was processed as intended you should now see the number of total\nnodes and edges\n\n### Filtering\n\nClick to open the sidebar.\n\nThe Filter Sidepar shows up\n\nMOST FILTERS HAVE TOOLTIPS SO I WILL NOT EXPLAIN EVERY FILTER IN DETAIL\n\nFilter for a time span for activities.\n\nThe Daily times filter specifies from what time we are interested in the\nevents. This is useful if nightly user logons are not common in your\nenvironment. This is regardless of the date - that means in your timespan only\nevents that occurred during that hourly timespan are in the set. (Works over\nnight like in the example picture too)\n\nHighlighted: You can permanently highlight edges by holding CTRL and clicking\non them. This also works for every element where temporary highlighting is\nactice - just hold CRL and click on the element to highlight edges\npermanently. (Elements are e.g. Timeline on the left; Stats on mouse over;\nwhen clicking the destination host)\n\nToSelf: By default events where source and destination are the same node are\nnot displayed. If you want to display them active it by clicking.\n\nFiltering for EventIDs is a good idea to reduce the data. There is no\ndifference in choosing all or none.\n\nLogon Types are only relevant for 4624 or 4625 events. I assume you know them\nalready when you are using this tool.\n\nFiltering for Tags only is available when client infos are uploaded. Those are\nyour tags specified in Velociraptor for the Systems. It does not have an\neffect if all on none are chosen. Those apply only for the source not for the\ndestination system.\n\n### Source: System or User\n\nUsually I am rather focused on system -> system activity in favour of\nidentifying the initial access. Since there are a lot of situations you want\nto focus on user behavior you can choose what your source should be: System or\nUser.\n\n### Render Graph, Timeline or Heatmap\n\nWhen your filters are set you need to press render to display the results.\n\n#### Graph\n\nThe default Graph calculates the position of systems according to their\nactivitie median time (Y-Axis) and their total number of connections (X-Axis).\n\nY-Axis: Calculated Activitie time early-top to latest-down\n\nX-Axis: The more centered a system is, the more connections have this system\neither as source or destination. Left to right is randomly distributed. (The\nmore outside the less active a system has been)\n\nSize: The Size of the nodes indicates their outgoing activities\n\nThe Graph is calculated every time before rendering. Position and size is\nalways relative according to the filters set.\n\nWhen clicking on a Node you can get further systems information. (Some need\nthe clients() output like OS or Tags.\n\nIPs can be more than one. When data is loaded every Event that has the\nhostname and an IP in it, will create a list that is presented here. (Multiple\nentries can be e.g. because of NAT-Devices or Multiple Network Adapters / IP\nChanges)\n\nWhen clicking on an edge you get further information about the connection. You\ncan open up a list of Timestamps that shows you when this event has occured.\n\n#### Timeline\n\nThe Timeline is the timeline...\n\n#### Heatmap\n\nThe heatmap gives you a quick overview of the usual day by day behavior of\nusers. You can click on a day to quickly switch to the graph of the day and\nthe users connections.\n\nThe color indicator is not per user but in total. It takes account of your\nfilters.\n\nIf you want to change from one view to another: choose the view you need and\nthen click render. 'Be careful with Timeline! Few nodes and edges can still\nhave a huge timeline!* Checking the Stats is a good idea before rendering a\ntimeline.\n\n### Graph Style\n\nYou can choose between some variations...\n\n### Tag vizualisation\n\nYou can choose a color for a Tag. The number to specify indicates the\npriorities when multiple Tags match. The highest number wins.\n\n### Exports\n\nYou can Export:\n\n  * Timeline as CSV\n  * Graph as PNG / JPEG\n  * GraphJSON (from the library cytoscape)\n\n### Stats\n\nStats give you a good indication for what to filter out or to pivot for when\nstarting the investigation. Stats take account of your filters.\n\nSystem Stats:\n\n  * To Systems = Number of Systems connected to followed by (Sum of connections to systems in total)\n  * From Systems = Number of Systems that connected to this System followed by (Sum of connections to this systems in total)\n  * Users out = Number of Users that were observed connection to other systems from this System\n  * Users in = Number of Users that were observed connecting to this System\n\nUser Stats:\n\n  * To Systems = Number of Systems the User connected to followed by (Sum of connections in total)\n\n## Integration in investigation\n\nI recommend using Blauhaunt with Velociraptor since it is the fastest way to\nget data from multiple systems. The Blauhaunt import format for event data and\nclient info is the one that can be exported from Velo. The\nblauhaunt_script.ps1 works well if you prefer working with e.g. KAPE triage\ndata.\n\nBlauhaunt really gets useful if you have multiple systems to identify your\nnext pivot system or sus users. Blauhaunt standalone will not magically bring\nyou to the compromised systems and users. But if you have hundreds of systems\nto check it really speeds up your game.\n\n### Example workflow\n\n#### Known compromised system\n\n(e.g. from a Velo hunt) -> Check in Blauhaunt what users connected to this\nsystem -> sus user -> sus systems -> further sus users -> the story goes on.\nYou have good chances identifying the systems where deeper forensics will\nspeed you up in your hunt. If you e.g. identify compromised users on that\nsystem again you can go back to Blauhaunt and repeat the game.\n\n#### No idea where to start\n\nWith several filters Blauhaunt gives you statistical and visual possibilities\nidentifying unusual connections. You can e.g. check for user activities\noccurring at night. Or simply see a logon fire coming form a system where an\nattacker is enumerating the AD-Infrastructure.\n\n#### Lucky shot\n\nIf you are really lucky and have a noisy attacker + solid administration in\nthe network, Blauhaunt can potentially deliver you an optical attack map with\nthe timeline of compromised systems along the y-axis in the center.\n\n## Architecture\n\nBlauhaunt is designed to run entirely without a backend system. I suggest\nsimply starting a python http server on the local system from a shell in the\ndirectory where the index.html is in with this command:\n\n    \n    \n    python -m http.server\n\nif you are using linux likely you have to type python3 instead of python - but\nif you are using this tool you should be technical skilled enough to figure\nthat out yourself ;)\n\nSome day I will create a backend in Django with an API to get realtime data to\ndisplay for better threat hunting\n\n### Default Layout\n\nThe layout of the graph is calculated according to the set filters. The icon\nsize of a node is calculated by its activities within the set filters. The\nx-axis position of a node is calculated by its outgoing connections. Nodes\nhaving many outgoing connections are rather in the center of the graph. Nodes\nwith fewer outgoing connections are at the left and the right of the graph.\nThe y-axis is calculated by the first quatile of the nodes activity time.\n\nTo not have too many nodes at the same spot there is some movement when there\nare too many on the same spot.\n\nThe other layouts are defaults from the cytoscape universe that can be chosen\nas well.\n\n### Displays\n\ndescription comming soon\n\n### General Data Schema\n\nThere are three types of data - only the event data is mandatory\n\n#### Event Data\n\nThis is the input Schema for the Event data that is needed by Blauhaunt to\nprocess it:\n\n    \n    \n    { \"LogonTimes\":[ \"2023-07-28T20:30:19Z\", \"2023-07-27T21:12:12Z\", \"2023-07-27T21:10:49Z\" ], \"UserName\":\"Dumdidum\", \"SID\":\"-\", \"Destination\":\"Desti-LAPTOP\", \"Description\":\"using explicit credentials\", \"Distinction\": \"SomeCustomFieldToDistionctEdgesAndFilterFor\" \"EventID\":4648, \"LogonType\":\"-\", \"SourceIP\":\"-\", \"SourceHostname\":\"Sourci-LAPTOP\", \"LogonCount\":3 }\n\nTo correctly process the files each dataset starting with { and ending with }\nmust be in a new line\n\n#### Client Info\n\n    \n    \n    { \"os_info\": { \"hostname\": \"Desti-LAPTOP\" \"release\": \"Windows 10\" }, \"labels\": [ \"Touched\", \"C2\", \"CredDumped\" ] }\n\nTo correctly process the files each dataset starting with { and ending with }\nmust be in a new line\n\n#### Host IP Mapping\n\nCan be any CSV File. Delimiter can be specified and cols for Hostname and IP\ncan be choosen\n\n## PowerShell Script (deprectated - use the quick velo instead)\n\nblauhaunt_script.ps1 If you face any issues with execution policy the easiest\nthing to do is to spawn a powershell with execution policy bypass like this:\n\n    \n    \n    PowerShell.exe -ExecutionPolicy Bypass powershell\n\nTo get information about usage and parameters use Get-Help\n\n    \n    \n    Get-Help blauhaunt_script.ps1 -Detailed\n\n### Usage\n\nDepending on the size, StartDate and EndDate this can take quiet some time so\nbe a little patient\n\n## Velociraptor Artifact\n\nThis speeds up collecting the relevant data on scale. I recommend creating a\nnotebook (template may be provided soon here too) where all the results are\nlisted. You can simply take the json export from this artefact to import it\ninto Blauhaunt\n\nThe client_info import is designed to work directly with the client_info from\nVelociraptor too. You can simply export the json file and upload it into\nBlauhaunt.\n\n### Usage\n\nIf you want to parse event logs collected from a system offline using\nvelociraptor, you can do so like this:\n\n    \n    \n    .\\velociraptor*.exe artifacts --definitions Blauhaunt\\parser\\velociraptor\\ collect --format=jsonl Custom.Windows.EventLogs.Blauhaunt --args Security='C:\\my\\awesome\\storage\\path\\Security.evtx' --args System='C:\\my\\awesome\\storage\\path\\System.evtx' --args LocalSessionManager='C:\\my\\awesome\\storage\\path\\Microsoft-Windows-TerminalServices-LocalSessionManager%4Operational.evtx' --args RemoteConnectionManager='C:\\my\\awesome\\storage\\path\\Microsoft-Windows-TerminalServices-RemoteConnectionManager%4Operational.evtx' --args RDPClientOperational='C:\\my\\awesome\\storage\\path\\Microsoft-Windows-TerminalServices-RDPClient%4Operational.evtx'\n\nIf you dislike typing long paths, feel free to use the provided quick script:\n\n    \n    \n    .\\quick_velo.ps1 -EventLogDirectory C:\\my\\awesome\\storage\\path\n\n## Defender\n\nYou can import Data from Defender365 into Blauhaunt by using this Hunting\nQuery:\n\nDefender 365 Query\n\nrun the query, export the csv and direktly load it into Blauhaunt...\n\n## Acknowledgements\n\n  * SEC Consult This work was massively motivated by my work in and with the SEC Defence team\n  * Velociraptor is the game changer making it possible to collect the data to display at scale (tested with > 8000 systems already!)\n  * Cytoscape.js is the library making the interactive graph visualisation possible\n  * LogonTracer inspired the layout and part of the techstack of this project\n  * CyberChef inspired the idea of creating a version of Blauhaunt running without backend system all browser based\n\n(The icon is intentionally shitty - this is how I actually look while\nhunting... just the look in the face not the big arms though :-P )\n\n## About\n\nA tool collection for filtering and visualizing logon events. Designed to help\nanswering the \"Cotton Eye Joe\" question (Where did you come from where did you\ngo) in Security Incidents and Threat Hunts\n\n### Topics\n\nsecurity graph analysis incident-response forensics dfir investigation cyber-\ncrime velociraptor\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\n### Stars\n\n98 stars\n\n### Watchers\n\n2 watching\n\n### Forks\n\n7 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 2\n\n  * cgosec\n  * sec-hbaer\n\n## Languages\n\n  * JavaScript 65.5%\n  * HTML 23.4%\n  * PowerShell 8.5%\n  * CSS 2.6%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
