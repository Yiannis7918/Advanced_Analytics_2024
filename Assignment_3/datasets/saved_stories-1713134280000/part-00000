{"aid": "40032344", "title": "1B rows challenge in PostgreSQL and ClickHouse", "url": "https://ftisiot.net/posts/1brows/", "domain": "ftisiot.net", "votes": 1, "user": "edweis", "posted_at": "2024-04-14 16:36:12", "comments": 0, "source_title": "1 billion rows challenge in PostgreSQL and ClickHouse", "source_text": "1 billion rows challenge in PostgreSQL and ClickHouse | ftisiot ideas about tech, food and life\n\n1 billion rows challenge in PostgreSQL and ClickHouse\n\nSorting 1 billion rows challenge with PostgreSQL and ClickHouse\n\nPosts\n\n# 1 billion rows challenge in PostgreSQL and ClickHouse\n\nJanuary 3, 2024 - 6 minutes read - 1170 words\n\nLast week the good old Gunnar Morling launched an interesting challenge about\nordering 1 billion rows in Java. Like my ex colleague and friend Robin Moffat,\nI\u2019m not at all a Java expert, and while Robin used DuckDB to solve the\nchallenge, I did the same with PostgreSQL and ClickHouse.\n\n> Alert: the following is NOT a benchmark! The test is done with default\n> installations of both databases and NO optimization. The blog only shows the\n> technical viability of a solution.\n\n\ud83d\udc49 Need a FREE PostgreSQL database?\ud83d\udc48 \ud83e\udd80 Check Aiven\u2019s FREE plans! \ud83e\udd80\n\n\u26a1\ufe0f Need to optimize your SQL query with AI? \u26a1\ufe0f \ud83d\udc27 Check EverSQL! \ud83d\udc27\n\n## Generate the data\n\nI used pretty much the same steps as Robin to generate the data\n\n  * Forked the repository and cloned locally\n  * Installed Java21 to generate the data\n\n    \n    \n    sdk install java 21.0.1-zulu sdk use java 21.0.1-zulu\n\n  * Built the data generator\n\n    \n    \n    ./mvnw clean verify\n\n  * Generated some rows\n\n    \n    \n    ./create_measurements.sh 1000000000\n\nThe above generates a file named measurements.txt with the 1 billion rows.\n\n## PostgreSQL\n\n### Setup local PostgreSQL\n\nHow do I load a local file into a PostgreSQL database? Well, it depends where\nthe PostgreSQL database is. For my test I could have used Aiven but decided\nfor a local installation on my Mac.\n\nI installed PostgreSQL 16 with\n\n    \n    \n    brew install postgres@16\n\nI can check how to start PostgreSQL locally with:\n\n    \n    \n    brew info postgresql@16\n\nIt tells me to execute:\n\n    \n    \n    LC_ALL=\"C\" /usr/local/opt/postgresql@16/bin/postgres -D /usr/local/var/postgresql@16\n\nOnce run we should see the welcoming message LOG: database system is ready to\naccept connection\n\n## Load the data in PostgreSQL\n\nWith PostgreSQL running with all the default values (read more here) we are\nready to upload the data.\n\nWe can connect to our PostgreSQL database with\n\n    \n    \n    psql postgres\n\nNext step is to create a table containing our data with\n\n    \n    \n    CREATE UNLOGGED TABLE TEST(CITY TEXT, TEMPERATURE FLOAT);\n\nWe are using the UNLOGGED parameter to speed up the copy of the data, however\nwe are assuming the risk of not writing any WAL log in the process... probably\nnot the smartest idea if this is sensible data we\u2019ll want to reuse later. We\ncan load the data with the COPY command:\n\n    \n    \n    \\copy TEST(CITY, TEMPERATURE) FROM 'measurements.txt' DELIMITER ';' CSV;\n\n### PostgreSQL Results\n\nFollowing the same reasoning as Robin, I was able to get to a query like\n\n    \n    \n    WITH AGG AS( SELECT city, MIN(temperature) min_measure, cast(AVG(temperature) AS DECIMAL(8,1)) mean_measure, MAX(temperature) max_measure FROM test GROUP BY city LIMIT 5) SELECT STRING_AGG(CITY || '=' || CONCAT(min_measure,'/', mean_measure,'/', max_measure),', ' ORDER BY CITY) FROM AGG ;\n\nThe main difference, compared to DuckDB is on the usage of the STRING_AGG\nfunction that allows me to directly create the string ordered by CITY with all\nthe metrics.\n\nNote: If I use EverSQL to optimise the query, it\u2019ll provide a suggestion to\nadd an index on city and temperature:\n\n    \n    \n    CREATE INDEX test_idx_city_temperature ON \"test\" (\"city\",\"temperature\");\n\n### PostgreSQL Timing\n\nTo get the timing I created a file called test.sql with the entire set of\ncommands:\n\n    \n    \n    \\timing \\o /tmp/output -- Load the data DROP TABLE TEST; CREATE UNLOGGED TABLE TEST(CITY TEXT, TEMPERATURE FLOAT); COPY TEST(CITY, TEMPERATURE) FROM '<PATH_TO_FILE>/measurements.txt' DELIMITER ';' CSV; -- Run calculations WITH AGG AS( SELECT city, MIN(temperature) min_measure, cast(AVG(temperature) AS DECIMAL(8,1)) mean_measure, MAX(temperature) max_measure FROM test GROUP BY city ) SELECT STRING_AGG(CITY || '=' || CONCAT(min_measure,'/', mean_measure,'/', max_measure),', ' ORDER BY CITY) FROM AGG ;\n\nAnd then timed it with:\n\n    \n    \n    time psql postgres -f test.sql\n\nThe timing was a, not astonishing, 9m16.135s with the majority (6m:24.376s\nspent on copying the data) and 2m:51.443s on aggregating.\n\n### Edit Using PostgreSQL Foreign Data Wrapper (FDW)\n\nAs suggested on HN by using a File Foreign data wrapper we could eliminate the\nneed of loading the data in the PostgreSQL table.\n\nThe test.sql has been changed to:\n\n    \n    \n    \\timing \\o /tmp/output -- Load the data DROP TABLE TEST; CREATE EXTENSION file_fdw; CREATE SERVER stations FOREIGN DATA WRAPPER file_fdw; CREATE FOREIGN TABLE TEST ( city text, temperature float ) SERVER stations OPTIONS (filename '<PATH>/measurements.txt', format 'csv', delimiter ';'); -- Run calculations WITH AGG AS( SELECT city, MIN(temperature) min_measure, cast(AVG(temperature) AS DECIMAL(8,1)) mean_measure, MAX(temperature) max_measure FROM test GROUP BY city ) SELECT STRING_AGG(CITY || '=' || CONCAT(min_measure,'/', mean_measure,'/', max_measure),', ' ORDER BY CITY) FROM AGG ;\n\nWhere the biggest change is that now the TEST table is defined as FOREIGN\nTABLE pointing directly to the measurements.txt file. With The File FDW we\nremoved the need of uploading the data to a table (that was costing us more\nthan 6 minutes), but now the overall query takes 8m:24.572s. Overall, compared\nto the copy and query solution, we are 1 min faster.\n\n## ClickHouse\n\n## Setup local ClickHouse\n\nHow do I load a local file into a ClickHouse database? Well, it depends where\nthe ClickHouse database is. For my test I could have used Aiven but decided\nfor a local installation on my Mac.\n\nI installed ClickHouse locally with (source)\n\n    \n    \n    curl https://clickhouse.com/ | sh\n\n## Query the data in ClickHouse\n\nCompared to PostgreSQL, ClickHouse allows me to directly querying the CSV file\nwithout loading the data in a table.\n\nI can query the measurement.txt file with:\n\n    \n    \n    ./clickhouse local -q \"SELECT c1 as city, c2 as measurement FROM file('measurements.txt', CSV) LIMIT 5\" --format_csv_delimiter=\";\"\n\nIn the above I defined:\n\n  * a pointer to a file called measurements.txt in CSV format\n  * a custom delimiter ;\n  * the first column c1 as city\n  * the second column c2 as measurement\n\nThe input is correctly parsed.\n\n    \n    \n    Wellington 12 Riga 0.30000000000000004 Palermo 18.4 Sochi 10.8 Accra 11.7\n\n### ClickHouse Results\n\nFollowing the same reasoning as Robin, I was able to get to a query like:\n\n    \n    \n    WITH AGG AS( SELECT city, cast(MIN(temperature) AS DECIMAL(8,1)) min_measure, cast(AVG(temperature) AS DECIMAL(8,1)) mean_measure, cast(MAX(temperature) AS DECIMAL(8,1)) max_measure FROM (SELECT c1 as city, c2 as temperature FROM file('measurements.txt', CSV)) GROUP BY city ORDER BY city ) SELECT arrayStringConcat(groupArray(city || '=' || CONCAT(min_measure,'/', mean_measure,'/', max_measure)), ', ') FROM AGG\n\nThe main difference, compared to the PostgreSQL solution is to use groupArray\nto create the list of cities (see the ORDER BY city in the first query to\norder them correctly), and the arrayStringConcat to concatenate the array\nelements in a string.\n\n### ClickHouse Timing\n\nTo get the timing I executed the following:\n\n    \n    \n    time ./clickhouse local -q \"\"\"WITH AGG AS( SELECT city, cast(MIN(temperature) AS DECIMAL(8,1)) min_measure, cast(AVG(temperature) AS DECIMAL(8,1)) mean_measure, cast(MAX(temperature) AS DECIMAL(8,1)) max_measure FROM (SELECT c1 as city, c2 as temperature FROM file('measurements.txt', CSV)) GROUP BY city ORDER BY city ) SELECT arrayStringConcat(groupArray(city || '=' || CONCAT(min_measure,'/', mean_measure,'/', max_measure)), ', ') FROM AGG\"\"\" --format_csv_delimiter=\";\"\n\nThe result is 44.465s!\n\n## Conclusion\n\nBoth PostgreSQL and ClickHouse were able to complete the challenge. PostgreSQL\ninitial solution had the limitiation of forcing the upload of data into a\ntable that took most of the time, when using the file FDW the performances\nstill weren\u2019t great. ClickHouse, on the other side, was able to query directly\nthe CSV and got much faster results, as expected from a database designed for\nanalytics.\n\n> Alert: the above is NOT a benchmark! The test is done with default\n> installations of both databases and NO optimization. The blog only shows the\n> technical viability of the solution.\n\n\u00a9 ftisiot ideas about tech, food and life 2024\n\n", "frontpage": false}
