{"aid": "40021660", "title": "Scalar and binary quantization for pgvector vector search and storage", "url": "https://jkatz05.com/post/postgres/pgvector-scalar-binary-quantization/", "domain": "jkatz05.com", "votes": 1, "user": "kiwicopple", "posted_at": "2024-04-13 09:05:20", "comments": 0, "source_title": "Scalar and binary quantization for pgvector vector search and storage | Jonathan Katz", "source_text": "Scalar and binary quantization for pgvector vector search and storage | Jonathan Katz\n\n### Jonathan Katz\n\n  * \u00a9 2024 Jonathan Katz\n  * Powered by the Anatole Hugo Theme\n  * Opinions are my own.\n\n# Scalar and binary quantization for pgvector vector search and storage\n\nTue, Apr 9, 2024 21-minute read\n\nWhile many AI/ML embedding models generate vectors that provide large amounts\nof information by using high dimensionality, this can come at the cost of\nusing more memory for searches and more overall storage. Both of these can\nhave an impact on the cost and performance of a system that\u2019s storing vectors,\nincluding when using PostgreSQL with the pgvector for these use cases.\n\nWhen I talk about vector search in PostgreSQL, I have a slide that I like to\ncall \u201cno shortcuts without tradeoffs\u201d that calls out the different challenges\naround searching vectors in a database. I first like to highlight that a 1,536\ndimensional vector with 4-byte floating point (fp32) dimensions requires 6KiB\nof storage. That may not seem like a lot, but storing 1,000,000 of these\nvectors in a PostgreSQL database requires 5.7GB of storage without any\nindexing. And again, that may not seem like a lot, but a single database row\nis closer to 600B of data (and typically even less than that) than 6KB.\n\nThe next point I talk about is compression: unfortunately, you just can\u2019t\ncompress a bunch of seemingly random floating point numbers. However, there\nare techniques to reduce the amount of information that you can store, from\nprincipal component analysis (aka PCA), which can reduce the overall\ndimensionality, to quantization techniques that can reduce the size or overall\nnumber of dimensions of a vector. These can reduce your overall storage and\nmemory requirements \u2013 and perhaps even improve performance in certain areas,\nbut recall that I mentioned there are no shortcuts without tradeoffs. To use\nquantization for vector searches, we\u2019ll have to give something up, whether\nit\u2019s in the overall relevancy of our searches (recall) or in some area of\nperformance in our system.\n\nThere are three common quantization techniques around vector databases:\n\n  * Scalar quantization, which reduces the overall size of the dimensions to a smaller data type (e.g. a 4-byte float to a 2-byte float or 1-byte integer).\n  * Binary quantization, which is a subset of scalar quantization that reduces the dimensions to a single bit (e.g. > 0 to 1, <=0 to 0).\n  * Product quantization, which uses a clustering technique to effectively remaps the original vector to a vector with smaller dimensionality and indexes that (e.g. reduce a vector from 128-dim to 8-dim).\n\nThere is already a lot of content available that does better exploration and\nexplanation of these techniques than I will; instead I\u2019ll focus on how to use\n2 of these 3 techniques in the context of pgvector.\n\nThe upcoming pgvector 0.7.0 release is planning to include functionality that\nallows for you to use both scalar and binary quantization as part of your\nindexing strategy. Specifically, pgvector 0.7.0 adds support for indexing\n2-byte floats (halfvec) and bit/binary vectors (using the PostgreSQL bit data\ntype). The 0.7.0 will also add support for sparse vectors via sparsevec, but\nthat will be for a future blog post.\n\nASIDE: These quantization techniques also let you index vectors that are\nlarger than 2,000 dimensions, which has been a major request for pgvector.\nhalfvec can store up to 4,000 dimensions, bit can store up to 64,000\ndimensions, and sparsevec can store up to 1,000 nonzero elements (which means\nit can store vectors with very high dimensionality).\n\nUsing existing pgvector functionality, such as HNSW indexing, along with\nPostgreSQL features like expression indexes, we can explore how scalar and\nbinary quantization can help reduce both space and memory consumption \u2013\nletting us scale vector workloads even further on PostgreSQL, understand what\nperformance gains and tradeoffs we must make, how embedding models can impact\nresults, and determine when it makes sense to use quantization.\n\n## Test setup and system configuration\n\nBefore diving in, let\u2019s do a quick review of how we want to test these\ntechniques. I previously provided guidance on testing approximate nearest\nneighbor (ANN) algorithms over vector indexes; below is a quick summary of\nwhat we will look at over the course of this testing:\n\n  * Index size: This is a key metric with a quantization test, as we should be building indexes that are smaller than a full (or \u201cflat\u201d) vector index. The interesting data is \u201chow much smaller,\u201d and the impacts to the other metrics we\u2019ll review.\n  * Index build time: How does quantization impact overall build time? If the build time is longer, do we gain enough in index size / recall / query performance that it\u2019s OK to make that tradeoff? If the build time is shorter, do we lose anything in recall / query performance such that we should avoid quantization?\n  * Queries per second (QPS): How does quantization impact how many queries we can execute per second, or our overall throughput?\n  * Latency (p99): The time it takes to return a single result, but using a measurement that represents the 99th percentile (\u201cvery slow\u201d) timing. This serves as an \u201cupper bound\u201d on our latency times.\n  * Recall: A measurement of the relevancy of our results - what percentage of the expected results do we return?\n\nFor the test itself, I used a r7gd.16xlarge and stored the data on the local\nNVMe to rule out the impact of network latency. For these tests, I only ran on\nthe ARM-based architecture of the Graviton3; as pgvector does leverage SIMD,\nexact timings may vary on different architectures.\n\nI used PostgreSQL 16.2, with the following (relevant to this test) non-default\nconfiguration (in alphabetical order - thanks \\dconfig!):\n\n  * checkpoint_timeout: 2h\n  * effective_cache_size: 256GB\n  * jit: off\n  * maintenance_work_mem: 64GB\n  * max_parallel_maintenance_workers: 63\n  * max_parallel_workers: 64\n  * max_parallel_workers_per_gather: 64\n  * max_wal_size: 20GB\n  * max_worker_processes: 128\n  * shared_buffers: 128GB\n  * wal_compression: zstd\n  * work_mem: 64MB\n\nFor testing, I used ANN Benchmarks with some modifications to the pgvector\nmodule to work with scalar and binary quantization. The modifications were\npurely to support scalar/binary quantization and would not make a material\ndifference in performance values. I used the following datasets, though I\nwon\u2019t necessarily show the data from all of them:\n\n  * mnist-784-euclidean (60K, 784-dim)\n  * sift-128-euclidean (1M, 128-dim)\n  * gist-960-euclidean (1M, 960-dim)\n  * dbpedia-openai-1000k-angular (1M, 1536-dim)\n  * glove-25-angular (1.1M, 25-dim)\n  * glove-100-angular (1.1M, 100-dim)\n\nFor this blog post, I\u2019ll focus on results from sift-128-euclidean,\ngist-960-euclidean, and dbpedia-openai-1000k-angular.\n\nFor the tests, I used HNSW indexing and used the following values:\n\n  * I fixed m at 16, but ran tests with ef_construction at 32, 64, 128, 256, 512\n  * For each test, I varied ef_search with 10, 20, 40, 80, 120, 200, 400, 800\n\nFinally, for each test, I maintained the original table structure (i.e.\nstoring the original / flat vector in the table) while quantizing the indexed\nvector.\n\nWe\u2019ll first look individually at scalar and binary quantization with pgvector,\nfollowed by a summarization of findings and some recommendations.\n\n## Scalar quantization with 2-byte (fp16) floats (halfvec)\n\nScalar quantization is often the simplest technique to use to shrink vector\nindex storage, as it\u2019s just a matter of converting dimensions to a smaller\ndata type (e.g. a 4-byte float to a 2-byte float). In some ways, quantize to a\n2-byte float makes sense: when performing a distance operation, the biggest\n\u201cdifferentiator\u201d between two dimensions is in the more significant bits of\ndata. If we marginally reduce the information to only care about those bits,\nwe shouldn\u2019t see much difference in recall.\n\nThe first thing needed for this test is to add support for 2-byte float scalar\nquantization when building the HNSW index. As mentioned, we can do this in\nPostgreSQL using an expression index. Below is an example of a table and HNSW\nindex using cosine distance that uses scalar quantization to store a 3,072\ndimensional vector, such as the ones found in the text-embedding-3-large\nmodel:\n\n    \n    \n    CREATE TABLE documents ( id bigint GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, content text, embedding vector(3072) ); CREATE INDEX ON documents USING hnsw ((embedding::halfvec(3072)) halfvec_cosine_ops);\n\nNote that hnsw ((embedding::halfvec(3072)) halfvec_cosine_ops) contains the\nexpression ((embedding::halfvec(3072))) that casts the full, 4-byte vector to\nbecome a 2-byte vector. Also note that we need to use a different operator\nclass for the halfvec data type.\n\nFor completeness, if you want to use scalar quantization with Euclidean / L2\ndistance, you\u2019d use the following query:\n\n    \n    \n    CREATE INDEX ON documents USING hnsw ((embedding::halfvec(3072)) halfvec_l2_ops);\n\nA query that would use scalar quantization would look similar to this:\n\n    \n    \n    SELECT id FROM documents ORDER BY embedding::halfvec(3072) <=> $1::halfvec(3072) LIMIT 10;\n\nwhere $1 represented a parameter that\u2019s a full vector being quantized to a\n2-byte float.\n\nNow, let\u2019s see how scalar quantization impacts vector search in pgvector.\nWe\u2019ll first see the impact on index build times. In the table below, vector\nrepresents the index with no quantization, whereas halfvec contains the\nquantized vector. We\u2019ll only show the ef_construction value, as m is fixed to\n16:\n\n### sift-128-euclidean\n\nef_construction| vector size (MB)| halfvec size (MB)| Space reduction| vector\nbuild-time (s)| halfvec build-time (s)| Speedup  \n---|---|---|---|---|---|---  \n32| 793| 544| 1.46x| 33| 31| 1.06x  \n64| 782| 538| 1.45x| 37| 34| 1.09x  \n128| 782| 538| 1.45x| 44| 40| 1.10x  \n256| 782| 538| 1.45x| 58| 51| 1.14x  \n512| 782| 536| 1.46x| 88| 74| 1.19x  \n  \n### gist-960-euclidean\n\nef_construction| vector size (MB)| halfvec size (MB)| Space reduction| vector\nbuild-time (s)| halfvec build-time (s)| Speedup  \n---|---|---|---|---|---|---  \n32| 7,811| 2,603| 3.00x| 145| 68| 2.13x  \n64| 7,684| 2,561| 3.00x| 161| 77| 2.09x  \n128| 7,680| 2,560| 3.00x| 190| 101| 1.88x  \n256| 7,678| 2,559| 3.00x| 247| 147| 1.68x  \n512| 7,678| 2,559| 3.00x| 349| 229| 1.52x  \n  \n### dbpedia-openai-1000k-angular\n\nef_construction| vector size (MB)| halfvec size (MB)| Space reduction| vector\nbuild-time (s)| halfvec build-time (s)| Speedup  \n---|---|---|---|---|---|---  \n32| 7,734| 3,867| 2.00x| 244| 77| 3.17x  \n64| 7,734| 3,867| 2.00x| 264| 90| 2.93x  \n128| 7,734| 3,867| 2.00x| 301| 115| 2.62x  \n256| 7,734| 3,867| 2.00x| 377| 163| 2.31x  \n512| 7,734| 3,867| 2.00x| 508| 254| 2.00x  \n  \nThe above shows that using halfvec to index a vector has both space savings\nand index build time benefits for vectors with larger dimensions. Without\nquantization, gist-960-euclidean (960-dim) and dbpedia-openai-1000k-angular\n(1536-dim) vectors can respectively only fit 2 and 1 vectors on an index page.\nQuantizing to a halfvec allows to store up to respectively store 4 and 2 on a\npage, which lines up with th space saving numbers we see. With less pages\nrequired, it also makes sense that we\u2019d see a build speedup, as pgvector\ndoesn\u2019t need to look up as many pages when placing a new vector into the\nindex. The sift-128-euclidean (128-dim) tests did showed space savings, the\noverall changes in build times were minimal, though importantly they did not\nshow regression.\n\nIt seems like this means we should start using 2-byte float scalar\nquantization, right? We can\u2019t make that determination just yet \u2013 we need to\nunderstand the impact of these changes on query performance and recall. for\nthe purposes of this blog, we\u2019ll look at the query performance values at\nhnsw.ef_search set to 10, 40, 200, and 800. 10 is the minimum value we can set\nto for this test (given the LIMIT 10) and will give us a lower bound. 40 is\nthe hnsw.ef_search default for pgvector. At 200, we\u2019d expect to see high\nrecall, and at 800 we should be close to converging towards perfect recall (if\nwe haven\u2019t done so already).\n\nVisualizing these results is a bit challenging, as there are several variables\nto consider (dataset, quantization, ef_construction, ef_search) along with\nseveral outputs (QPS, p99 latency, recall). For simplicity, we\u2019ll consider the\ndata at ef_construction=256, as that is the build value I recommend for folks\nto use for index builds when using the parallel build functionality. You can\nsee the results below:\n\n### sift-128-euclidean @ ef_construction=256\n\nhnsw.ef_search| vector recall| halfvec recall| vector QPS| halfvec QPS| vector\np99 latency (ms)| halfvec p99 latency (ms)  \n---|---|---|---|---|---|---  \n10| 77.7%| 77.5%| 2,100| 2,161| 0.71| 0.68  \n40| 95.4%| 95.4%| 1,020| 1,097| 1.19| 1.20  \n200| 99.8%| 99.8%| 268| 293| 4.60| 4.38  \n800| 100.0%| 100.0%| 84| 91| 15.07| 14.80  \n  \n### gist-960-euclidean @ ef_construction=256\n\nhnsw.ef_search| vector recall| halfvec recall| vector QPS| halfvec QPS| vector\np99 latency (ms)| halfvec p99 latency (ms)  \n---|---|---|---|---|---|---  \n10| 50.4%| 50.1%| 1,114| 1,207| 1.36| 1.21  \n40| 78.0%| 78.1%| 513| 579| 2.64| 2.30  \n200| 96.0%| 96.0%| 135| 156| 8.70| 7.49  \n800| 99.6%| 99.6%| 41| 47| 29.20| 25.44  \n  \n### dbpedia-openai-1000k-angular @ ef_construction=256\n\nhnsw.ef_search| vector recall| halfvec recall| vector QPS| halfvec QPS| vector\np99 latency (ms)| halfvec p99 latency (ms)  \n---|---|---|---|---|---|---  \n10| 85.1%| 85.2%| 1,162| 1,163| 1.40| 1.21  \n40| 96.8%| 96.8%| 567| 578| 2.70| 2.61  \n200| 99.6%| 99.6%| 156| 163| 9.01| 8.59  \n800| 99.9%| 99.9%| 48| 51| 30.50| 28.49  \n  \nThese are very good results for 2-byte float quantization with halfvec. In\naddition to the improved build performance, we see that (at least at\nef_construction=256) the recall values between vector and halfvec are nearly\nidenical, and query performance is identical or slightly better (particularly\nwith latency) with halfvec.\n\nAcross all the tests I ran with the ANN Benchmark datasets listed above, I saw\nsimilar results. This makes a strong case that moving forward, you should\nfirst attempt to use HNSW with 2-byte float quantization, as you\u2019ll save space\nand improve index build times while maintaining comparable performance to\nstoring the full vector! If you\u2019re not seeing the recall that you want, you\ncan increase m/ef_construction, or fall back to using the full 4-byte float\nvector in your index.\n\nLet\u2019s now explore binary quantization.\n\n## Binary quantization\n\nBinary quantization is a more extreme quantization technique: it can reduce\nthe full value of the dimension of a vector to a single bit of information.\nSpecifically, binary quantization will reduce any positive value to 1, and any\nzero or negative value to 0. This can certainly lead to a huge reduction in\nmemory/storage utilization, and likely query performance, as you\u2019re storing\nmuch more data on a single page. However, this can have a notable impact on\nrecall, as we\u2019ll see in the experiments below.\n\nI\u2019m going to spoil one of the experiments: using binary quantization on its\nown can lead to poor recall if there\u2019s not enough bit-diversity in your\nvectors. Typically, you\u2019ll see more diversity amongst vectors with higher\ndimensionality, as these vectors are more likely to have varied bits. However,\nyou can improve recall by \u201creranking,\u201d i.e. you use binary quantization to\nnarrow down your set of vectors, and then you reorder the reduced set of\nvectors by using the original (flat) vectors stored in the table.\n\nFirst, let\u2019s see how we can set up a table to support binary quantization.\nWe\u2019ll first look at the results of building a HNSW index that stores binary\nquantized vectors, and then compare results between using binary quantization\nwith and without re-ranking.\n\nRecall that pgvector ultimately gets support for binary quantization through\nsupporting vector operations through the PostgreSQL bit type. The pgvector\n0.7.0 release provides two different bitwise distance functions: Jaccard (<%>)\nand Hamming distance (<~>). For these tests, I chose the Hamming distance\nfunction due to its performance characteristics, though I plan/hope to go into\na deeper dive on these two options in a future blog post.\n\nBelow is an example of how you can create a table and index that supports\nbinary quantization and Hamming distance:\n\n    \n    \n    CREATE TABLE documents ( id bigint GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, content text, embedding vector(3072) ); CREATE INDEX ON documents USING hnsw ((quantize_binary(embedding)::bit(3072)) bit_hamming_ops);\n\nAgain, notice we use a PostgreSQL expression index to quantize the embedding,\nand explicitly cast the bit dimensionality. That final cast is important:\nquantize_binary just returns a bit string, and pgvector index building needs\nto detect and explicit dimension.\n\nThe following query is how to search the index using binary quantization:\n\n    \n    \n    SELECT id FROM documents ORDER BY quantize_binary(embedding)::bit(3072) <~> quantize_binary($1) LIMIT 10;\n\nwhere $1 represents a parameterized value that is a vector type.\n\nAs mentioned earlier, we\u2019ll also run an experiment that involves re-ranking\nthe results before returning them. The following query does exactly that; note\nthat the subquery has a higher limit, but it will ultimately be governed by\nthe value of hnsw.ef_search (you can always set the LIMIT to be the value of\nhnsw.ef_search).\n\n    \n    \n    SELECT i.id FROM ( SELECT id, embedding <=> $1 AS distance FROM items ORDER BY quantize_binary(embedding)::bit(3072) <~> quantize_binary($1) LIMIT 800 -- bound by hnsw.ef_search ) i ORDER BY i.distance LIMIT 10;\n\nwhere $1 represents a parameterized value that is a vector type. Note that in\nthe SELECT list of the subqery, there is the embedding <=> $1 AS distance\nexpression: this performs the distance calculation that is ultimately used in\nthe rerank. This example uses cosine distance, but you can choose your\npreferred distance metric.\n\nNow, let\u2019s see how binary quantization with and without reanking impacts\nvector search in pgvector. As before, we\u2019ll first see the impact on index\nbuild times. In the table below, vector represents the index with no\nquantization, whereas bit contains the binary quantized vector. Note that\nreranking only impacts the query itself, so we don\u2019t have to show multiple\nsets of build times. We\u2019ll only show the ef_construction value, as m is fixed\nto 16:\n\n### sift-128-euclidean\n\nef_construction| vector size (MB)| bit size (MB)| Space reduction| vector\nbuild-time (s)| bit build-time (s)| Speedup  \n---|---|---|---|---|---|---  \n32| 793| 304| 2.61x| 33| 28| 1.18x  \n64| 782| 298| 2.62x| 37| 31| 1.19x  \n128| 782| 298| 2.62x| 44| 37| 1.19x  \n256| 782| 298| 2.62x| 58| 46| 1.26x  \n512| 782| 298| 2.62x| 88| 67| 1.31x  \n  \n### gist-960-euclidean\n\nef_construction| vector size (MB)| bit size (MB)| Space reduction| vector\nbuild-time (s)| bit build-time (s)| Speedup  \n---|---|---|---|---|---|---  \n32| 7,811| 405| 19.29x| 145| 77| 1.88x  \n64| 7,684| 405| 18.97x| 161| 76| 2.12x  \n128| 7,680| 405| 18.96x| 190| 76| 2.50x  \n256| 7,678| 405| 18.96x| 247| 77| 3.20x  \n512| 7,678| 405| 18.96x| 349| 76| 4.59x  \n  \n### dbpedia-openai-1000k-angular\n\nef_construction| vector size (MB)| bit size (MB)| Space reduction| vector\nbuild-time (s)| bit build-time (s)| Speedup  \n---|---|---|---|---|---|---  \n32| 7,734| 473| 16.35x| 244| 151| 1.62x  \n64| 7,734| 473| 16.35x| 264| 155| 1.70x  \n128| 7,734| 473| 16.35x| 301| 162| 1.86x  \n256| 7,734| 473| 16.35x| 377| 187| 2.02x  \n512| 7,734| 473| 16.35x| 508| 227| 2.24x  \n  \nFrom these tests, we see that the space savings becomes significantly more\npronounced as the dimensions of the vectors becomes larger, but we do see the\nreduction trailing off for the 1536-dim vector. This comes back earlier to the\nfact that a PostgreSQL page is 8KB: we can fit about 68 bit(960) on an index\npage, whereas we can only fit about 42 bit(1536) on the same page. However,\nnotice that we don\u2019t have much space savings for the 128-dim vector: this\nmakes sense, as we\u2019re only transforming 128 bytes (1024 bits) into 128 bits,\nand while it\u2019s an 8x reduction in information, we\u2019re already efficiently\nclustering the flat 128-dim vectors in the index pages. The other interesting\nelement is that the speedups, while at least faster for the larger vectors,\ndon\u2019t correlate with the space reduction. There is likely still some\noptimization work we can do in pgvector to speed up the Hamming distance\nfunction.\n\nNow let\u2019s at the impact on query performance and recall. First, let\u2019s evaluate\nperformance and recall without any reranking:\n\n### sift-128-euclidean @ ef_construction=256 (no rerank)\n\nhnsw.ef_search| vector recall| bit recall| vector QPS| bit QPS| vector p99\nlatency (ms)| bit p99 latency (ms)  \n---|---|---|---|---|---|---  \n10| 77.7%| 2.18%| 2,100| 2,412| 0.71| 0.61  \n40| 95.4%| 2.42%| 1,020| 1,095| 1.19| 1.25  \n200| 99.8%| 2.52%| 268| 280| 4.60| 4.91  \n800| 100.0%| 2.52%| 84| 86| 15.07| 16.05  \n  \n### gist-960-euclidean @ ef_construction=256 (no rerank)\n\nhnsw.ef_search| vector recall| bit recall| vector QPS| bit QPS| vector p99\nlatency (ms)| bit p99 latency (ms)  \n---|---|---|---|---|---|---  \n10| 50.4%| 0.00%| 1,114| 6,050| 1.36| 0.18  \n40| 78.0%| 0.00%| 513| 4,847| 2.64| 0.24  \n200| 96.0%| 0.00%| 135| 4,057| 8.70| 0.27  \n800| 99.6%| 0.00%| 41| 3,871| 29.20| 0.28  \n  \n### dbpedia-openai-1000k-angular @ ef_construction=256 (no rerank)\n\nhnsw.ef_search| vector recall| bit recall| vector QPS| bit QPS| vector p99\nlatency (ms)| bit p99 latency (ms)  \n---|---|---|---|---|---|---  \n10| 85.1%| 60.1%| 1,162| 1,556| 1.40| 1.02  \n40| 96.8%| 66.8%| 567| 848| 2.70| 1.79  \n200| 99.6%| 68.3%| 156| 251| 9.01| 5.61  \n800| 99.9%| 68.6%| 48| 71| 30.50| 19.57  \n  \nThe above shows why with approximate nearest neighbor search, you must measure\nperformance and recall. Without doing so, you\u2019d walk away thinking that using\nbinary quantization for the gist-960-euclidean set is a clear, high performant\nwinner, but you\u2019d be returning garbage results. Similarly, sift-128-euclidean\nhas terrible recall with binary quantization, but does not show much\ndifference in query performance between the flat value.\n\nHowever, dbpedia-openai-1000k-angular looks promising: while the recall\nnumbers are not great, they\u2019re significantly higher than the others, likely\ndue to the bit-diversity between the values. We may be able to improve our\nresults if we re-rank. Remember that our rerank query looks something like\nthis (using a 3072-dim vector):\n\n    \n    \n    SELECT i.id FROM ( SELECT id, embedding <=> $1 AS distance FROM items ORDER BY quantize_binary(embedding)::bit(3072) <~> quantize_binary($1) LIMIT 800 -- bound by hnsw.ef_search ) i ORDER BY i.distance LIMIT 10;\n\nLet\u2019s look at the same results with reranking:\n\n### sift-128-euclidean @ ef_construction=256 (with rerank)\n\nhnsw.ef_search| vector recall| bit recall| vector QPS| bit QPS| vector p99\nlatency (ms)| bit p99 latency (ms)  \n---|---|---|---|---|---|---  \n10| 77.7%| 2.31%| 2,100| 2,194| 0.71| 0.65  \n40| 95.4%| 4.19%| 1,020| 971| 1.19| 1.34  \n200| 99.8%| 8.88%| 268| 246| 4.60| 5.32  \n800| 100.0%| 15.69%| 84| 76| 15.07| 17.28  \n  \n### gist-960-euclidean @ ef_construction=256 (with rerank)\n\nhnsw.ef_search| vector recall| bit recall| vector QPS| bit QPS| vector p99\nlatency (ms)| bit p99 latency (ms)  \n---|---|---|---|---|---|---  \n10| 50.4%| 0.00%| 1,114| 2,744| 1.36| 0.40  \n40| 78.0%| 0.00%| 513| 1,030| 2.64| 1.00  \n200| 96.0%| 0.00%| 135| 569| 8.70| 1.79  \n800| 99.6%| 0.00%| 41| 562| 29.20| 1.82  \n  \n### dbpedia-openai-1000k-angular @ ef_construction=256 (with rerank)\n\nhnsw.ef_search| vector recall| bit recall| vector QPS| bit QPS| vector p99\nlatency (ms)| bit p99 latency (ms)  \n---|---|---|---|---|---|---  \n10| 85.1%| 60.1%| 1,162| 1,503| 1.40| 1.03  \n40| 96.8%| 91.6%| 567| 760| 2.70| 1.91  \n200| 99.6%| 99.0%| 156| 222| 9.01| 6.01  \n800| 99.9%| 99.8%| 48| 62| 30.50| 21.34  \n  \nSo, what do we learn with binary quantization and reranking? With\nsift-128-euclidean and re-ranking, we\u2019re able to improve on recall as we\nincrease hnsw.ef_search, but the recall is still very poor. The\ngist-960-euclidean test is still returning garbage, but not as quickly as\nbefore.\n\nBut the dbpedia-openai-1000k-angular results are very interesting. Once we\nexpand our search radius vis-a-vis hnsw.ef_search=40, we see that we get a\n1.34x boost in QPS and a 29% reduction in p99 latency with only sacrificing 5%\nin recall. This is using an index that\u2019s 16x smaller and builds twice as fast!\nMoving up to hnsw.ef_search=200, we get 1.42x faster QPS with a 33% decrease\nin p99 latency with comparable recall!\n\nThere are a few takeaways from this. First, given how binary quantization\nworks, the experiments show that to ensure your results have meaning, you most\ncertainly will need to perform a reranking. Additionally, bit-diversity\nmatters: the most distinct values we have in the index, the more likely we\u2019ll\nbe able to get distinguishable results that lead to higher recall. This leads\nto a decision though: how much recall matters? If you\u2019re doing k=10 / top-10\nqueries, 90% should be good enough, which means in the above example, you can\nuse hnsw.ef_search=40 on the dbpedia-openai-1000k-angular dataset to achieve\nlow latency queries that meet your recall target.\n\nOne indirect observation I have from these tests is around how the overall\nsize of a dataset impacts recall for binary quantization. We see good results\non the dbpedia-openai-1000k-angular dataset because the overall dataset size\nis small (1,000,000) and has good diversity amongst the bit vectors. However,\nlooking at the results of the other two data sets, I would be willing to bet\nthat recall degrades as the 1536-dim data set gets larger (e.g. 1,000,000,000)\nand requires a larger hnsw.ef_search + reranking to achieve the same results.\n\nFinally, for all of these experiments, note that the workload was able to fit\nentirely into memory (the benefit of using a r7gd.16xlarge) to ensure we\u2019re\ngetting a fair comparison between methods. However, in the \u201creal-world,\u201d\nbinary quantization lets you keep your index entirely in memory on much\nsmaller systems - for example, with dbpedia-openai-1000k-angular the binary\nquantized index takes up 473MB vs. 7,734MB (7.5GB) for the full index. If\nyou\u2019re getting the recall results you want from an index using binary\nquantization, this is an effective technique to reduce your overall storage\nand memory footprint in your system.\n\n## Future experiments\n\nWhile the results are really interesting, there is still more to test (and\ndevelop!) around quantization. A few more experiments I\u2019d like to run in the\nfuture:\n\n  * With dbpedia-openai-1000k-angular, quantize the vector to halfvec in the table, and then use binary quantization on the halfvec. I suspect that we reduce the storage/memory footprint even further, get slightly higher QPS/better p99 latency, and don\u2019t impact recall.\n  * Similarly, test storing everything as halfvec in the table and then building the halfvec index. I also suspect comparable recall, but possibly better query performance numbers. This does lose precision in the vector stored in the table, so I think the results will be model dependent.\n  * Test the explicit impact of CPU, i.e. SIMD, acceleration. The unreleased PostgreSQL 17 has planned support for AVX-512 some functions used for computing binary distances; we may be able to further speed up those distance functions.\n  * Test the hypothesis around recall with binary quantization degrading on much larger datasets as there\u2019s less bit diversity amongst the values.\n\n## Conclusion: What (if any) quantization technique should I use?\n\nWhen I give a talk on pgvector or vector search in general, my concluding\nslide always ends with \u201cplan for today and tomorrow.\u201d I think these\nquantization techniques are a great example of that.\n\nFirst and foremost, pgvector maintains its changes as additive, meaning that\nas you upgrade, you will not need to make costly on-disk changes. Adopting a\nnew feature may mean reindexing, though fortunately PostgreSQL and pgvector\nsupport the nonblocking REINDEX CONCURRENTLY operation.\n\nThat said, I think there are a few clear takeaways and tradeoffs to consider\nwhen determining what quantization technique to use:\n\n  * Scalar quantization from 4-byte to 2-byte floats looks like a clear winner, at least amongst these datasets. I feel comfortable recommending storing the vector in the table and quantizing to halfvec in the index. I\u2019d like to see the results over larger datasets, but I still think this is a really good starting point.\n  * Binary quantization works better for datasets that produce larger vectors that can be distinguished by their bits. The more distinguished bits, the better recall we\u2019ll achieve.\n\n    * I also suspect that as these datasets get very large, we start to see degraded recall results.\n  * Effective quantization allows you to reduce your storage/memory footprint, which on one hand means you can use smaller instances, but it also means you can scale your workload even further. This is great news for pgvector, particularly as we see more \u201cbillion-scale\u201d and beyond datasets.\n\nWhile ANN Benchmarks and other ANN testing tools for databases do provide a\ngood set of datasets to evaluate quantization techniques, this may not reflect\nthe results you see with your actual workload. You still need to test to see\nif scalar or binary quantization make sense. However, one of my goals is to\ngive you a guide on how to make that evaluation, and hopefully save some time\nin the process.\n\npostgrespostgresqlpgvector\n\n  * \u00a9 2024 Jonathan Katz\n  * Powered by the Anatole Hugo Theme\n  * Opinions are my own.\n\n", "frontpage": false}
