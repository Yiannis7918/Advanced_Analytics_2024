{"aid": "40014048", "title": "Microsoft RHO-1: Not All Tokens Are What You Need", "url": "https://arxiv.org/abs/2404.07965", "domain": "arxiv.org", "votes": 1, "user": "tosh", "posted_at": "2024-04-12 15:30:33", "comments": 0, "source_title": "Rho-1: Not All Tokens Are What You Need", "source_text": "[2404.07965] Rho-1: Not All Tokens Are What You Need\n\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member\ninstitutions, and all contributors. Donate\n\n> cs > arXiv:2404.07965\n\n# Computer Science > Computation and Language\n\narXiv:2404.07965 (cs)\n\n[Submitted on 11 Apr 2024]\n\n# Title:Rho-1: Not All Tokens Are What You Need\n\nAuthors:Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen\nXu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen\n\nView a PDF of the paper titled Rho-1: Not All Tokens Are What You Need, by\nZhenghao Lin and 10 other authors\n\nView PDF HTML (experimental)\n\n> Abstract:Previous language model pre-training methods have uniformly applied\n> a next-token prediction loss to all training tokens. Challenging this norm,\n> we posit that \"Not all tokens in a corpus are equally important for language\n> model training\". Our initial analysis delves into token-level training\n> dynamics of language model, revealing distinct loss patterns for different\n> tokens. Leveraging these insights, we introduce a new language model called\n> Rho-1. Unlike traditional LMs that learn to predict every next token in a\n> corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively\n> trains on useful tokens that aligned with the desired distribution. This\n> approach involves scoring pretraining tokens using a reference model, and\n> then training the language model with a focused loss on tokens with higher\n> excess loss. When continual pretraining on 15B OpenWebMath corpus, Rho-1\n> yields an absolute improvement in few-shot accuracy of up to 30% in 9 math\n> tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results\n> of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath\n> with only 3% of the pretraining tokens. Furthermore, when pretraining on 80B\n> general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse\n> tasks, increasing both efficiency and performance of the language model pre-\n> training.\n\nComments:| First two authors equal contribution  \n---|---  \nSubjects:| Computation and Language (cs.CL); Artificial Intelligence (cs.AI)  \nCite as:| arXiv:2404.07965 [cs.CL]  \n(or arXiv:2404.07965v1 [cs.CL] for this version)  \nhttps://doi.org/10.48550/arXiv.2404.07965arXiv-issued DOI via DataCite  \n  \n## Submission history\n\nFrom: Zhenghao Lin [view email] [v1] Thu, 11 Apr 2024 17:52:01 UTC (507 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Rho-1: Not All Tokens Are What You Need, by\nZhenghao Lin and 10 other authors\n\n  * View PDF\n  * HTML (experimental)\n  * TeX Source\n  * Other Formats\n\nview license\n\nCurrent browse context:\n\ncs.CL\n\n< prev | next >\n\nnew | recent | 2404\n\nChange to browse by:\n\ncs cs.AI\n\n### References & Citations\n\n  * NASA ADS\n  * Google Scholar\n  * Semantic Scholar\n\na export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer (What is the Explorer?)\n\nLitmaps (What is Litmaps?)\n\nscite Smart Citations (What are Smart Citations?)\n\n# Code, Data and Media Associated with this Article\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\nDagsHub (What is DagsHub?)\n\nGotit.pub (What is GotitPub?)\n\nPapers with Code (What is Papers with Code?)\n\nScienceCast (What is ScienceCast?)\n\n# Demos\n\nReplicate (What is Replicate?)\n\nHugging Face Spaces (What is Spaces?)\n\nTXYZ.AI (What is TXYZ.AI?)\n\n# Recommenders and Search Tools\n\nInfluence Flower (What are Influence Flowers?)\n\nConnected Papers (What is Connected Papers?)\n\nCORE Recommender (What is CORE?)\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new\narXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and\naccepted our values of openness, community, excellence, and user data privacy.\narXiv is committed to these values and only works with partners that adhere to\nthem.\n\nHave an idea for a project that will add value for arXiv's community? Learn\nmore about arXivLabs.\n\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n\n  * About\n  * Help\n\n  * Contact\n  * Subscribe\n\n  * Copyright\n  * Privacy Policy\n\n  * Web Accessibility Assistance\n  * arXiv Operational Status Get status notifications via email or slack\n\n", "frontpage": false}
