{"aid": "40013620", "title": "How we built JSR", "url": "https://deno.com/blog/how-we-built-jsr", "domain": "deno.com", "votes": 20, "user": "vladev", "posted_at": "2024-04-12 14:53:37", "comments": 5, "source_title": "How we built JSR", "source_text": "How we built JSR\n\nSkip to main content\n\n# How we built JSR\n\nApril 12, 2024\n\n  * Luca Casonato\n\n  * Engineering\n  * JSR\n\nWe recently launched the JavaScript Registry - JSR. It\u2019s a new registry for\nJavaScript and TypeScript designed to offer a significantly better experience\nthan npm for both package authors and users:\n\n  * It natively supports publishing TypeScript source code, which is used to auto-generate documentation for your package\n  * It\u2019s secure-by-default, supporting token-less publishing from GitHub Actions and package provenance using Sigstore\n  * It rates packages using our \u201cJSR Score\u201d to give consumers an \u201cat a glance\u201d indication of the quality of a package\n\nWe knew that JSR would only gain adoption if it interoperates with the\nexisting npm ecosystem. Below is the set of \u201cnpm interoperability\u201d\nrequirements we needed to meet with JSR:\n\n  * JSR packages can be seamlessly consumed by any tool that uses node_modules/ folders\n  * You can incrementally adopt JSR packages in your Node projects\n  * npm packages can be imported from JSR packages, and JSR packages can be imported from npm packages\n  * Most existing npm packages written with ESM or TypeScript can be published to JSR with very little effort - just run npx jsr publish\n  * You can use your favorite npm compatible package manager like yarn, or pnpm with JSR\n\nOur open beta launch of JSR was met with enthusiasm from the community, as\nwe\u2019ve already seen some great packages being published already, such as the\ntypesafe validation & parsing library @badrap/valita and the multi-runtime\nHTTP framework @oak/oak.\n\nBut this blog post is not about why you should use JSR. It\u2019s about how I and\nthe rest of the JSR team, over the course of several months, built JSR so meet\nthe technical requirements of a modern, performant, highly available\nJavaScript registry. This post covers nearly every part of JSR:\n\n  * Overview of technical specs\n  * Minimizing latency for the jsr.io website\n  * Building a modern publishing flow (aka say no to probing!)\n  * Serving modules at 100% reliability\n  * So that\u2019s it?\n\nI will try to explain not just how we do something, but also why we do things\nin the way we do. Let\u2019s dive in.\n\n## Overview of technical specs\n\nFor JSR to be a successful modern JavaScript registry, it must be many things:\n\n  * A global CDN to serve package source code and NPM tarballs\n  * A website that users use to browse packages and manage their presence on JSR\n  * An API that the CLI tool talks to publish packages\n  * A system to analyze source code during publishing to check for syntax errors or invalid dependencies, to generate documentation, and to compute package scores\n\nThis diagram shows the high level overview of the JSR system. Don\u2019t worry if\nyou don\u2019t understand everything here yet. I hope that by the end of the post,\nyou\u2019ll understand exactly what\u2019s going on here.\n\nThe challenge with building something that satisfies all those conditions is\nthat they each have different constraints.\n\nFor example, the global CDN must have a near 100% uptime. It is unacceptable\nif downloading a package fails for even 0.1% of users. If a package download\nfails, that is a broken CI run, a confused developer, and a very bad user\nexperience. Any decently sized project will struggle with flaky CI at some\npoint already - I don\u2019t want to add to that \ud83d\ude05.\n\nOn the other hand, if a user receives a scope invite email 10 minutes after\nthey have been invited, that\u2019s much less of an issue. It\u2019s not a great user\nexperience (we still try to avoid this), but it\u2019s not a critical flaw that\nwill require paging an on-call engineer in the middle of the night.\n\nSince reliability is a core part of JSR, these tradeoffs determine how JSR and\nits various components are architected. Each part has varying service level\nobjectives (SLOs \u2014 how reliability is usually defined): we target an SLO of\n100% uptime for serving package source code and NPM packages, whereas other\nservices (like our database) target a more conservative 99.9% uptime.\nThroughout this post, we\u2019ll be using SLO as the starting point for determining\nhow we approached designing a piece of the system.\n\n### Postgres for most data\n\nJSR uses a highly-available Postgres cluster to store most data. We have\ntables for obvious things like users, scopes, or packages. But we also have\nlarger tables, like our package_version_files table, which contains metadata\nlike path, hash, and size of all files ever uploaded to JSR. Because Postgres\nis a relational database, we can combine these tables using JOINs to retrieve\nall kinds of fun information:\n\n  * How much storage space does this user consume?\n  * How many files are duplicated across JSR packages?\n  * What packages were published within an hour of the creator signing up to JSR?\n\nWe use the excellent sqlx Rust package to handle migrations. You can look at\nall of the migrations that create the JSR database on GitHub.\n\nOur Postgres database is hosted on Google Cloud (like the rest of JSR!). We\u2019ve\nhad great experiences with Google Cloud in the past, so we decided to use it\nagain here. Google Cloud has very nice tooling and documentation for\nTerraform, which is the Infra-as-Code tool we use to deploy JSR (could do a\nwhole blog post on this).\n\n### API\n\nRight above the Postgres database sits our API server. JSR does not directly\nexpose the Postgres database to clients. Instead we expose the data in the\ndatabase via JSON over a HTTP REST API. API requests can come from multiple\ndifferent kinds of clients, including from users\u2019 browsers, or from the jsr\npublish / deno publish tool.\n\nThe JSR API server is written in Rust, using the Hyper HTTP server. It\ncommunicates with the Postgres database via the sqlx Rust crate. The service\nis deployed to Google Cloud Run, in one region, right next to the Postgres\ndatabase.\n\nIn addition to proxying data between the database and the JSON API interface,\nthe API server enforces authentication and authorization policies, like\nrequiring that only scope members can update a packages\u2019 description, or\nensuring that only the right GitHub Actions job can be used to publish a\npackage.\n\nAt the end of the day, the API server is a relatively standard service that\ncould exist in a very similar form in any of 100\u2019s of other web applications.\nIt interacts with a SQL database, sends emails using an email service (in our\ncase, Postmark), talks to the GitHub API to verify repository ownership, talks\nto Sigstore to verify publish attestations, and much more.\n\n## Minimizing latency for the jsr.io website\n\nIf you\u2019re writing a service for humans to use, you discover pretty quickly\nthat most humans do not in fact want to manually invoke API calls using curl.\nBecause of this, JSR has a web frontend, which lets you perform every single\noperation that the API exposes (except for publishing packages \u2014 more on this\nlater).\n\nTo keep the jsr.io website feeling fast and snappy, we built it with Fresh, a\nmodern \u201cserver-side render first\u201d web framework built here at Deno. That means\nevery page on the JSR website is rendered on demand in a Deno process running\nin a Google Cloud data center near you, just for you. Let\u2019s explore some\napproaches Fresh uses to ensure site visitors get the best experience.\n\n### Island rendering for performance\n\nFresh is quite unique in that unlike other web frameworks like Next.js or\nRemix, it does not render the entire application both on client and server.\nInstead, every page is always rendered entirely on the server and only\nsections marked interactive are rendered on the client. This is called \u201cisland\nrendering\u201d, because we have little islands of interactivity in a sea of\notherwise server rendered content.\n\nThis is a very powerful model that allows us to provide both incredibly snappy\npage rendering for all users, regardless of their geolocation, internet speed,\ndevice performance, and memory availability, and really nice interactive user\nflows. For example, because of our island architecture the JSR site can still\nsupport \u201csearch as you type\u201d, and client-side validation for scope names\nbefore form submission, even while using serve side rendering. All without\nhaving to ship a markdown renderer, styling library, or a component framework\nto the client.\n\nThis really pays off:\n\nData collected by Chrome shows great performance of the https://jsr.io website\nfor users. View results yourself at [PageSpeed\nInsights](https://pagespeed.web.dev/analysis/https-jsr-io/a3jqrtfhzj).\nRetrieved on Mar 28, 2024, 1:22:52 PM.When looking at the entire origin rather\nthan just the home page with it\u2019s large animation hero, things get even\nbetter!\n\nReal user data from the Chrome UX report (anonymous statistics on performance\ncollected by Chrome in the background), show that https://jsr.io has great\nperformance on a variety of devices, on a variety of networks. Particularly\nexciting is the excellent score for the new Interaction to Next Paint (INP)\nmetric, which measures how contended the main thread is with rendering and\nrepresents how fast interactions \u201cfeel\u201d. Because of Fresh\u2019s island\narchitecture, JSR scores incredibly well on metrics like these.\n\n### Optimizing for time-to-first-byte\n\nAnother thing you may notice here is the TTFB performance. The TTFB metric\nrepresents the 75th percentile of \u201ctime-to-first-byte\u201d - the time between\nhitting enter in your URL bar and the browser receiving back the start of the\nresponse from the server. Our TTFB score of 0.5s is great, especially for a\ndynamically server side rendered site. (Server rendered sites tend to have a\nhigher TTFB, because they wait for dynamic data on the server, rather than\nserving a static shell on the client and then fetching dynamic data from the\nclient.)\n\nWe spent a lot of time optimizing for TTFB, which can be a real challenge for\nserver side rendered pages. Because the entire page rendering is blocked until\nall data is present on the server, the time it takes the server to retrieve\nall of the data it needs needs to be reduced as much as possible. In the first\nfew weeks of our work on JSR, I would frequently get reports from colleagues\nin India and Japan that the JSR package page was unbearably slow to load -\nmultiple seconds for a simple package settings page.\n\nWe managed to narrow this down to a waterfall of requests between the server\nrendering the page, and our API server. We\u2019d first fetch the user profile,\nthen fetch the package metadata, then fetch whether the user is a member of\nthe scope, and so on. Because our API server is hosted in the US (pretty far\naway from India), and even the internet needs to adhere to the physical\nconstraints like the speed of light, it would take seconds for us to have all\nthe data needed to render.\n\nWe are now at a point where there is not a single route on the JSR site that\nrequires more than one network roundtrip between rendering server and API\nserver. We managed to parallelize many of the API calls, and in other cases\nimprove the API in subtle ways to not require multiple requests at all\nanymore. For example, initially the package page would first fetch the package\nversion list, figure out what version is the latest one, then fetch the readme\nfor that version. Now, you can just tell the API endpoint that returns the\nreadme that you would like the readme for the \u201clatest\u201d version and it can\nfigure out what that is quickly from it\u2019s co-located database.\n\nBefore we investigated TTFB, we had large waterfalls of requests between\nrendering server and API server.\n\nNow, we fire off many requests at once, and reduced the total number of calls\nby improving the responses from the API.\n\n### Using <form> wherever possible\n\nIt might seem weird to rely on built-in browser <form> submissions wherever\npossible. (We use <form>\u2019s even in unusual places, like the \u201cRemove\u201d button\nnext to users in a scopes\u2019 scope member list.) However, doing this allows us\nto significantly reduce the amount of code we have to write, ship to users,\nand audit for accessibility.\n\nAccessibility is an important aspect of web development, and using more built-\nin browser primitives significantly decreases the amount of work you have to\ndo yourself to get a good outcome.\n\nHave I mentioned yet that all of this is open source? The JSR frontend is\nprobably the easiest piece to contribute to:\n\n    \n    \n    git clone https://github.com/jsr-io/jsr.git cd jsr echo \"\\n127.0.0.1 jsr.test\" >> /etc/hosts deno task prod:frontend\n\nThis will spin up a local copy of the frontend for you to play with, connected\nto the production API. If you make a change in the frontend/ folder, your\nlocal frontend at http://jsr.test will automatically reload. You can even\nmanage your scopes and packages from the production JSR service using your\nlocal copy of the site!\n\n## Building a modern publishing flow (aka say no to probing!)\n\nWe\u2019ve talked a lot about managing metadata on packages (the API server), and\nviewing this metadata (the frontend), but really - what is a package registry\nif you can\u2019t publish to it?\n\nBuilding a modern JavaScript registry that must interoperate with npm means\nbeing able to accept a wide, diverse set of packages. Module authors should be\nable to publish:\n\n  * existing NPM packages authored in TypeScript with a package.json\n  * packages authored for Deno using an import_map.json\n  * even a single JS file that re-exports a package imported from npm\n\nThe challenge of building a registry that supports accepting a diverse set of\npackages means we must understand and support all kinds of module resolution\nmethods:\n\n  * Files are imported with the actual extension they use (.ts for importing TS files)\n  * Files are imported with no extension at all\n  * Files are imported with the wrong extension (.js imports actually refer to TS files with a .ts extension)\n  * Bare specifiers resolved in import maps\n  * Bare specifiers resolved via package.json\n  * ... and more\n\nDespite these complexities of supporting publishing a diverse set of packages,\nwe knew early on that JSR package consumers should not need to know these\nintricate differences. We\u2019re trying to push the ecosystem into a consistent,\nvastly simpler direction: ESM-only and very explicit resolution behaviour. In\norder to deliver a world class developer experience for package consumers, JSR\nmust ensure a consistent format of code to download.\n\nSo how can we support accepting a diverse set of packages, while still\nproviding a simple standard package consumption experience?\n\nWhen an author publishes to JSR, we automatically \u201cfix it\u201d by converting it\ninto a consistent format for the registry. As the package author, you don\u2019t\nhave to know, care, or understand that it\u2019s happening. But this code\ntransformation speeds up and simplifies the registry.\n\nThen, the question is what is this \u201cconsistent standard\u201d format?\n\n### Say no to probing\n\nBefore we go any further, first a little aside on \u201cprobing\u201d. (Oh ... I shiver\njust saying it.) Probing is the practice of giving unclear instructions to\nsomeone who then tries many things until something works. Confused? Here\u2019s an\nexample.\n\nImagine you work at the supermarket and stocked the fruits today. After coming\nhome, you decide you want some pineapple. You send someone to the supermarket\nto pick some up, but instead of telling them to get a pineapple, you say \u201cget\nme any fruit that is in stock, that starts with a letter \u2018p\u2019, here is my\npriority order: papaya, pear, pineapple, passionfruit, peach\u201d.\n\nYour runner goes to the fruit section in the supermarket and starts looking.\nNo papayas today. Pears? Nope. But lo and behold \u2014 pineapples! They grab some\nand return to you, victorious.\n\nBut that was the neighbourhood grocer, where you can see the entire fruit\ndisplay at a glance, so checking for fruit is quick. What if we were at a\nwholesale fruit market? Each continent\u2019s fruits are now in their own section\nin a giant warehouse. Walking between the papaya\u2019s, the pears, and the\npineapples takes minutes. \u201cProbing\u201d for fruit really falls apart here since it\ntakes so long that it becomes infeasible.\n\nSo how does this clumsy approach to buying fruit tie back to a consistent\nstandard code format? Well, a lot of the resolution algorithms probe: import\n'./foo' needs to check if there is a file called ./foo/index.tsx,\n./foo/index.ts, ./foo/index.js, ./foo.tsx, ./foo.ts, ./foo.js, or even just\n./foo. As the package author, you know which file you want to resolve to. But\nyou\u2019re sending the resolver on a fun adventure of \u201ctry to open a bunch of\nfiles and see if they exist\u201d. Whoops.\n\nWhen you do this on a local file system, the performance is often acceptable.\nReading a file takes a couple hundred nanoseconds on a modern SSD. But \u201cprobe\u201d\non a network drive - already much slower. And over HTTP - you\u2019re waiting 10s\nof milliseconds for every read call. Completely unacceptable for 100\u2019s of\nfiles in a package.\n\nBecause JSR packages can not just be imported from the file system, but also\nusing HTTP imports (like browsers and Deno), probing is a total no go.\n(Coincidentally you now also know why browsers could never ship node_modules/\nresolution like Node: too much probing.)\n\n### Locally rewrite import statements to remove probing during publish\n\nWith probing being out of the question, our consistent standard format for end\nusers and JSR to consume must follow this one main rule:\n\n> Given ONLY the specifier and contents of a module, you can accurately\n> determine the exact names of all files imported from within that package,\n> and the name and version constraints of all external packages.\n\nPractically speaking, this means we cannot rely on package.json to resolve\ndependencies and that all relative imports must have explicit extensions and\npaths. So to support accepting a broad and diverse set of packages, we first\nmust re-write the code before it even hits the JSR API layer.\n\nWhen you call jsr publish or deno publish, the publishing tool will inspect\nyour code, probe for package.json, an import map, and whatever else you use to\nconfigure your resolution, and then walk through all the files in your\npackage, starting at the \"exports\" in your jsr.json. It then finds any imports\nthat would require probing to resolve, and rewrites them to the consistent\nformat that does not require probing:\n\n    \n    \n    - import \"./foo\"; + import \"./foo.ts\";\n    \n    \n    - import \"chalk\"; + import \"npm:chalk@^5\";\n    \n    \n    - import \"oak\"; + import \"jsr:@oak/oak@^14\";\n\nThis all happens in memory, without the package author having to see or know\nthat this is happening. This local code translation to a consistent format is\nthe magic that enables a modern, flexible publishing experience where authors\ncan write in whatever way they want and users can consume packages in a\nsimple, standardized way.\n\n### Using background queues for availability\n\nNext up, the publishing script places all the files into a .tar.gz file. After\ngetting the user to interactively authenticate the publish (this could be it\u2019s\nown whole blog post \ud83d\udc40), it uploads the tarball to the API server.\n\nThe API server then does some initial validations like checking that the\ntarball is smaller than the 20MB allowed and that you have access to publish\nthis version. Then it stores the tarball in a storage bucket and adds the\npublishing task to a background queue.\n\nWhy add it to a queue rather than processing the tarball immediately? For\nreliability.\n\nPart of reliability is handling spikes of traffic. Since publishing is an\nintensive process that requires a lot of CPU and memory resources, a large\nmono repo publishing new versions for 100 different packages all at once could\nslow the entire system to a crawl.\n\nSo we store the tarballs, put them in a queue, and then background workers\npick up publishing tasks out of the queue and process them, as they have\navailability. 99% of publishes go from submission to dequeuing by a background\nworker in under 30 milliseconds. However if we do see large spikes, we can\ngracefully handle these by just slowing down the processing a bit.\n\nWhen a background worker picks up a publishing task, it starts by\ndecompressing the tarball and checking all files inside are within our\nacceptable limits. It then validates that the jsr.json file has a valid\n\"name\", \"version\", and \"exports\" field. After this, we build a module graph of\nthe entire module, which helps validating the package code. The module graph,\nwhich maps the relationship between every module in a package, checks that:\n\n  * your code is valid JavaScript or TypeScript\n  * all your imported modules actually exist\n  * all your dependencies are versioned\n\nThis all happens within 10s of milliseconds for most packages.\n\n### Auto generating docs and uploading the module to storage\n\nAfter validation is complete, we generate documentation for the package based\non this module graph. This is done entirely using TypeScript syntax analysis\nin Rust. The result is uploaded to a storage bucket. We\u2019ll write another blog\npost on how this works at some point, as it\u2019s quite interesting!\n\nThen, we upload each module individually (exactly as it is, no transforms\noccur here) to a modules storage bucket. This bucket will be important later,\nremember it!\n\n### Turning TypeScript into .js and .d.ts files for npm\n\nNext up, we generate a tarball for JSR\u2019s npm compatibility layer. To do this,\nwe transform your TypeScript source code into a .js code file, and .d.ts\ndeclaration file. This is done entirely in Rust - to our knowledge, this is\nthe first large scale deployment of .d.ts generation that does not use\nMicrosoft\u2019s TypeScript compiler written in JavaScript (exciting!). The code in\nthis tarball is rewritten from JSR\u2019s consistent module format back into\nimports that node_modules/ resolution understands, along with a package.json.\nOnce done, this is also uploaded to a bucket.\n\nFinally, we\u2019re done. The Postgres database is updated with the new version,\nthe jsr publish / deno publish command is notified that the publish is done,\nand the package is live on the internet. When you go to https://jsr.io now,\nthe updated version will show up on the package page.\n\n> \ud83d\udea8\ufe0f Side note \ud83d\udea8\ufe0f\n>\n> We\u2019re super excited about our ability to generate .d.ts files from .ts\n> source, without using tsc. This is something TypeScript is actively trying\n> to encourage. For example, Bloomberg and Google, in collaboration with the\n> TypeScript team, have been working on adding an isolatedDeclarations option\n> to TypeScript that will make declaration emit outside of TSC a breeze. We\u2019re\n> bullish that soon, many more tools will have the ability to emit .d.ts files\n> without using tsc.\n\n## Serving modules at 100% reliability\n\nI\u2019ve beaten around the bush enough now - this blog post started with me\ntelling you how it\u2019s really important that our module / npm tarball serving is\nsuper robust. So what\u2019s the magic sauce?\n\nActually, there is no magic. We use incredibly boring, very well understood,\nand very reliable cloud infrastructure.\n\nhttps://jsr.io is hosted on Google Cloud. Traffic is accepted by a Google\nCloud L7 load-balancer via anycast IP addresses. It terminates TLS. It then\nlooks at the path, request method, and headers to determine whether the\nrequest should go to the API server, the frontend, or to a Google Cloud CDN\nbackend that directly fronts a Cloud Storage bucket containing source code and\nnpm tarballs.\n\nSo how do we make serving modules reliable? We defer the entire problem to\nGoogle Cloud. The same infrastructure that serves google.com and YouTube is\nused to host modules on JSR. None of our custom code sits in this hot path -\nwhich is important because that means we can not break it. Only if Google\nitself goes down, will JSR come down too. But at that point - probably half\nthe internet is down so you don\u2019t even notice \ud83d\ude05.\n\n## So that\u2019s it?\n\nFor the registry side, yeah, mostly. I left out how we do the documentation\nrendering, how we compute the JSR score, compute package dependencies and\ndependents, handle the OIDC integration with GitHub Actions, integrate with\nSigstore for provenance attestation... but we can talk about that next time \ud83d\ude42.\n\nIf you are interested in anything I mentioned in this post \u2014 from the API\nserver, to the frontend and Terraform configuration for Google Cloud, to the\nimplementation of deno publish \u2014 you can take a look at them yourself, as JSR\nis completely open source under the MIT license!\n\nWe welcome all contributors! You can open issues, submit PRs, or ask me things\non Twitter.\n\nSee you on JSR!\n\n> Did you enjoy this post on JSR\u2019s internals?\n>\n> We have another post coming soon with all the nitty gritty of how deno\n> installs packages from JSR. Follow us on Twitter to get updated: @jsr_io or\n> @deno_land.\n\n## Learn\n\n  * Node's Security Problem\n  * Node's Complexity Problem\n  * Edge is the Future\n\n## Why Deno?\n\n  * TypeScript Support\n  * Web Standard APIs\n  * All-in-one Tooling\n  * Secure-by-default\n\n## Use Cases\n\n  * Scripts and CLIs\n  * API Servers\n  * Sites and Apps\n  * Modules\n  * Serverless Functions\n\n## Products\n\n  * Deno Runtime\n  * Deno Deploy\n  * Deno KV\n  * Deploy Subhosting\n  * Fresh\n  * SaaSKit\n\n## Sources\n\n  * Runtime Manual\n  * Runtime API\n  * Deploy Docs\n  * Standard Library\n  * Third-Party Modules\n  * Examples\n\n## Company\n\n  * Careers\n  * Blog\n  * Pricing\n  * News\n  * Merch\n  * Privacy Policy\n\nGitHubDiscordTwitter or X or whateverYouTubeMastodon\n\nCopyright \u00a9 2024 Deno Land Inc. All rights reserved.\n\nAll systems operational\n\n", "frontpage": true}
