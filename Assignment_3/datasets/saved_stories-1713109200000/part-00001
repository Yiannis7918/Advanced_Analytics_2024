{"aid": "40029927", "title": "64 Bytes per Embedding, Yee-Haw", "url": "https://www.mixedbread.ai/blog/binary-mrl", "domain": "mixedbread.ai", "votes": 1, "user": "breadislove", "posted_at": "2024-04-14 09:38:21", "comments": 0, "source_title": "64 bytes per embedding, yee-haw \ud83e\udd20", "source_text": "64 bytes per embedding, yee-haw \ud83e\udd20 | mixedbread.ai\n\nmixedbread.ai\n\nSee all posts\n\nPublished on April 12, 202410 min read\n\n# 64 bytes per embedding, yee-haw \ud83e\udd20\n\nDarius Koenig\n\nBaker\n\nAamir Shakir\n\nBaker\n\nWe are happy to introduce a novel embeddings compression method: Binary MRL.\nThis method will make vector search much more scalable and enable a range of\nnew embeddings-based applications that weren't economically feasible before\nour release. Learn how the parameters influence the search results.\n\nRead on to learn more about our approach and to check out our benchmarks. If\nyou want to skip right to the model instead, you can access it here:\n\n  * mxbai-embed-large-v1: Our recently released flagship embedding model supports binary MRL as it is. How cool is that?!\n  * Wikipedia Demo: You can experience the speed and performance of our model in the demo (using binary quantization).\n\n## Why Embeddings?\n\nEmbeddings are one of the most versatile tools in natural language processing,\nsupporting a wide variety of settings and use cases. In essence, embeddings\nare numerical representations of more complex objects like text, images,\naudio, etc. Specifically, the objects are represented as n-dimensional\nvectors.\n\nAfter transforming objects using an embedding model, you can determine their\ninherent semantic similarity by calculating the similarity of the respective\nembeddings. Essentially, you determine how strongly related two objects are by\nmeasuring how close their embeddings are to each other in the n-dimensional\nvector space. This is crucial for many use cases: it serves as the backbone\nfor recommendation systems, retrieval, one-shot or few-shot learning, outlier\ndetection, similarity search, paraphrase detection, clustering,\nclassification, and much more.\n\nUsing embeddings is particularly important for Retrieval-Augmented Generation\n(RAG). The idea behind the concept of RAG is to be able to have an LLM access\ncustom documents that you provide (like analyst reports in your company) and\nimprove its output based on that information. Transforming the documents into\nembeddings (as well as the query given to the model) allows the LLM to\nretrieve the most relevant information from your data and utilize it to\nproduce the most relevant output for the user.\n\n## Embeddings May Struggle to Scale\n\nHowever, embeddings may be challenging to use at scale because of their memory\nusage, which leads to expensive solutions and high latencies. Currently, many\nstate-of-the-art models produce embeddings with 1024 dimensions, each of which\nis encoded in float32, i.e., they require 4 bytes per dimension. To perform\nretrieval over 250 million vectors, you would therefore need around 1TB of\nmemory! With costs estimated at $3.8 per GB/month, using x2gd instances on\nAWS, this would incur monthly cloud storage costs of more than $3,500.\n\n## Matryoshka Representation Learning & Vector Quantization to the Rescue\n\nTo solve the scaling issues of embeddings, two approaches have lately been\ngaining particular traction: Matryoshka Representation Learning (MRL) and\nVector Quantization. Let's first take a look at both concepts.\n\nMRL tries to make embeddings ready to scale by reducing the number of output\ndimensions of an embedding model without sacrificing a lot of accuracy. This\ncan be achieved by storing more important information in earlier dimensions of\nthe embedding, so that the less important later dimensions can be truncated,\nsaving for example on storage cost and improving processing speed in\ndownstream tasks. In essence, the loss function during model training needs to\nbe calibrated in a way that not only accounts for the standard model\nperformance on, say, 1024 output dimensions, but that tracks the performance\nusing the first 512, 256, 128,... dimensions. Training the model to minimize\nthis loss function will lead it to frontload the most important identifying\ninformation within its output vectors.\n\nOn the other hand, vector quantization represents a very different approach to\nthe problem. Here, instead of changing the number of output dimensions, the\nsize of every dimension is reduced. Typically, each dimension of the embedding\nis stored as a float32 value, which requires 4 bytes (32 bits) of storage\nspace. Especially when considering vectors with 1024 dimensions, potentially\nmillions or billions of them, the benefits of reducing this size become\nobvious. A large gain in memory and disk space efficiency as well as retrieval\nspeed under retention of 95% and more of performance can be realized by\nstoring the embedding dimensions as binary values instead.\n\nThis is achieved by simply transforming the float32-values to 1 if they are\ngreater than 0 and to 0 if they are not. In order for this process not to\nresult in greater loss of performance, a rescoring step can be performed when\nusing the model for retrieval tasks. In this approach, first both query and\ndocuments are represented as binary embeddings and the most relevant search\nresults are retrieved with them, which are then also reranked in relation to a\nfloat32-embedding of the query.\n\n## Taking It One Step Further with Binary MRL\n\nRecognizing the potential of both of these approaches, we already published\nsome of our research findings on the subject. On MRL, we published our\ninnovative and novel 2D-Matryoshka model; on binary quantization, we co-\nauthored a post on the hugging face blog, introducing curious members of the\ncommunity to the subject.\n\nNow, we aim to take things one step further by combining both approaches. We\nwant to demonstrate that it is feasible to truncate embedding dimensions and\nreduce the size of each dimension simultaneously, while still retaining most\nof the original model performance using our very own embedding model.\n\nThe following table demonstrates that our model is able to retain over 90% of\nperformance while reducing its output dimensions from 1024 to 512 and also\nreducing the size of each dimension by a factor of 32. In effect, we create a\n64x efficiency gain. Naturally, this decrease in memory usage also leads to a\nproportional - i.e., enormous - decrease in infrastructure cost when processes\nare run via cloud computing or a vector database specifically!\n\nWe evaluated the model performance on the MTEB retrieval benchmark, which\nincludes the 13 publicly available BEIR datasets. The tables show NDCG@10\nscores, relative performance retention, and vector size in bytes of our model\nwith float32 values and binary quantization combined with different output\ndimensions:\n\n1024 Dim.| 512 Dim.| 256 Dim.| 128 Dim.| 64 Dim.  \n---|---|---|---|---  \nNDCG@10  \nfloat32| 54.39| 51.79| 46.78| 36.63| 18.63  \nbinary| 52.46| 49.37| 43.25| 32.80| 17.61  \nPerformance Retention  \nfloat32| 100.00%| 95.22%| 86.01%| 67.34%| 34.25%  \nbinary| 96.46%| 90.76%| 79.52%| 60.31%| 32.38%  \nVector Size [byte]  \nfloat32| 4,096| 2,048| 1,024| 512| 256  \nbinary| 128| 64| 32| 16| 8  \n  \nAs shown, mixedbread's embedding model performs more than 90% as well using\n64-byte vectors as it does using 4,096-byte vectors. In our view, these\n512-dimensional binary embeddings also represent the sweet spot for the trade-\noff between performance and storage capacity.\n\nWe can also take a look at the following graph visualizing the relation\nbetween performance and output dimensions for both float32 and binary\nembeddings:\n\nAs we can see, the curves for float32and binary embeddings exhibit strong\nsimilarities. It's our view that the trade-off between size and performance is\noptimal in the less steep left part of the curve. Due to resource constraints,\nwe did not evaluate the performance retention for int8-quantization, but we\nwould expect that curve to be extremely similar to and inbetween the other two\ncurves.\n\nWhat does all of this mean in practice?\n\nThe following text takes 64 bytes (ASCII) to store: \"Bread the warm and yeasty\ncomfort that feeds both body and soul.\"\n\nAlternatively, you could now store a vector embedding a complex object like\ntext or an image at extremely high quality. Which would you consider more\nuseful?\n\n## The Economic Consequences of the Release\n\nSaving space on embedding sizes is not merely a cosmetic exercise to excite a\nsmall group of experts on the subject - it makes using neural search with\nvector databases significantly cheaper. This can have wide-ranging\nconsequences: we believe it will enable new and exciting embeddings-based\napplications that previously weren't economically feasible!\n\nIn the following table, we compiled an overview of required storage space and\ntherefore monthly cost of performing retrieval over 100m, 250m, and 1b\nvectors, respectively. Again, we assumed costs of $3.8 per GB/month, using\nx2gd instances on AWS:\n\nData type| Dim.| 100M embeddings| 250M embeddings| 1B embeddings  \n---|---|---|---|---  \nfloat32| 1024| 381.47GB $1,449.58 / mo| 953.67GB $3,623.96 / mo| 3.81TB\n$14,495.85 / mo  \nfloat32| 512| 190.73GB $724.79 / mo| 476.84GB $1,811.98 / mo| 1.91TB $7,247.92\n/ mo  \nfloat32| 256| 95.37GB $362.40 / mo| 238.42GB $905.99 / mo| 953.67GB $3,623.96\n/ mo  \nfloat32| 128| 47.68GB $181.20 / mo| 119.21GB $453.00 / mo| 476.84GB $1,811.98\n/ mo  \nbinary| 1024| 11.92GB $45.30 / mo| 29.80GB $113.25 / mo| 119.21GB $453.00 / mo  \nbinary| 512| 5.96GB $22.65 / mo| 14.90GB $56.62 / mo| 59.60GB $226.50 / mo  \nbinary| 256| 2.98GB $11.32 / mo| 7.45GB $28.31 / mo| 29.80GB $113.25 / mo  \nbinary| 128| 1.49GB $5.66 / mo| 3.73GB $14.16 / mo| 14.90GB $56.62 / mo  \n  \n## Using It in Action\n\nWe offer binary MRL through our API and it is also supported through Sentence\nTransformers. Here an example how you can use it:\n\n### Installing\n\n    \n    \n    pip install -u mixedbread-ai\n\n### Usage Example\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 from mixedbread_ai.client import MixedbreadAI mxbai = MixedbreadAI(api_key=\"{MIXEDBREAD_API_KEY}\") res = mxbai.embeddings( model='mixedbread-ai/mxbai-embed-large-v1', input=[ 'Who is german and likes bread?', 'Everybody in Germany.' ], normalized=true, # this has to be True if you want to use binary with faiss encoding_format='ubinary', dimensions=512 )\n\nWe put also a demo online where you can search through the English wikipedia\nusing binary embeddings and which helps you to understand the influence of the\nparameters on the results.\n\n## Practical Considerations\n\nOn a practical level, you will need a vector database that supports this new\nconcept to take full advantage of the benefits it can provide. We understand\nthat many providers will be hesitant in offering this service, as it directly\ncuts into their profits if the number of users and the embeddings they perform\nretrieval on stay constant - even though we believe that making vector search\nmore economically available to users will lead to an increase in demand for\nexpanded old as well as completely new applications that will offset this\neffect for the providers. Already, there are providers that recognize the\npotential of our findings and want to support their customers in using it in\ninnovative and productive ways. Vespa has been particularly vocal about their\nexcitement to support the wonderful things the community will be able to do\nwith binary MRL.\n\nColBERTus Maximus - ...\n\nmixedbread.ai\n\n## Newsletter\n\nSubscribe to our newsletter to get the latest updates.\n\n2024 mixedbread ai inc.. All Rights Reserved.\n\nlinkedIn\n\ngithub\n\nhuggingFace\n\ntwitter\n\ndiscord\n\n", "frontpage": false}
