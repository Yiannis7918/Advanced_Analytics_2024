{"aid": "40032288", "title": "Control and Autofocus Software for Chip-Level Microscopy", "url": "https://www.bunniestudios.com/blog/2024/control-and-autofocus-software-for-chip-level-microscopy/", "domain": "bunniestudios.com", "votes": 1, "user": "todsacerdoti", "posted_at": "2024-04-14 16:30:15", "comments": 0, "source_title": "Control and Autofocus Software for Chip-Level Microscopy \u00ab bunnie's blog", "source_text": "Control and Autofocus Software for Chip-Level Microscopy \u00ab bunnie's blog\n\n\u00ab A 2-Axis, Multihead Light Positioner\n\n## Control and Autofocus Software for Chip-Level Microscopy\n\nThis post is part of a series about giving us a tangible reason to trust our\nhardware through non-destructive IRIS (Infra-Red, in-situ) inspection. Here\u2019s\nthe previous posts:\n\n  * IRIS project overview\n  * Methodology\n  * Light source electronics\n  * Fine focus stage\n  * Light source mechanisms\n\nThis post will discuss the control software used to drive IRIS.\n\nAbove is a screenshot of the IRIS machine control software in action. The top\npart of the window is a preview of the full frame being captured; the middle\nof the window is the specific sub-region used for calculating focus, drawn at\na 1:1 pixel size. Below that are various status readouts and control tweaks\nfor controlling exposure, gain, and autofocus, as well as a graph that plots\nthe \u201cfocus metric\u201d over time, and the current histogram of the visible pixels.\nAt the bottom is a view of the console window, which is separate from the main\nUI but overlaid in screen capture so it all fits in a single image.\n\nThe software itself is written in Python, using the PyQt framework. Why did I\nsubject myself that? No particular reason, other than the demo code for the\ncamera was written with that framework.\n\nThe control software grew out of a basic camera demo application provided by\nthe camera vendor, eventually turning into a multi-threaded abomination. I had\nfully intended to use pyuscope to drive IRIS, but, after testing out the\ncamera, I was like...maybe I can add just one more feature to help with\ntesting...and before you know it, it\u2019s 5AM and you\u2019ve got a heaping pile of\ncode, abandonment issues, and a new-found excitement to read about image\nprocessing algorithms. I never did get around to trying pyuscope, but I\u2019d\nassume it\u2019s probably much better than whatever code I pooped out.\n\nThere were a bunch of things I wish I knew about PyQt before I got started;\nfor example, it plays poorly with multithreading, OpenCV and Matplotlib:\nbasically, everything that draws to the screen (or could draw to the screen)\nhas to be confined to a single thread. If I had known better I would have\nstructured the code a bit differently, but instead it\u2019s a pile of patches and\nshims to shuttle data between a control thread and an imaging/UI thread. I had\nto make some less-than-ideal tradeoffs between where I wanted decisions to be\nmade about things like autofocus and machine trajectory, versus the control\ninputs to guide it and the real-time visualization cues to help debug what was\ngoing on.\n\nFor better or for worse, Python makes it easy and fun to write bad code.\n\nYet somehow, it all runs real-time, and is stable. It\u2019s really amazing how\nfast our desktop PCs have become, and the amount of crimes you can get away\nwith in your code without suffering any performance penalties. I spend most of\nmy time coding Rust for Precursor, a 100MHz 32-bit device with 16MiB of RAM,\nso writing Python for a 16-core, 5GHz x86_64 with 32GiB of RAM is a huge\ncontrast. While writing Python, sometimes I feel like Dr. Evil in the Austin\nPowers series, when he sheepishly makes a \u201cvillian demand\u201d for 1 billion\ndollars \u2013 I\u2019ll write some code allocating one beeeellion bytes of RAM, fully\nexpecting everything to blow up, yet somehow the computer doesn\u2019t even break a\nsweat.\n\nMoore\u2019s Law was pretty awesome. Too bad we don\u2019t have it anymore.\n\nAnyways, before I get too much into the weeds of the software, I have to touch\non one bit of hardware, because, I\u2019m a hardware guy.\n\nI Need Knobs. Lots of Knobs.\n\nWhen I first started bringing up the system, I was frustrated at how\nincredibly limiting traditional UI elements are. Building sliders and\ncontrolling them with a mouse felt so caveman, just pointing a stick and\ngrunting at various rectangles on a slab of glass.\n\nI wanted something more tactile, intuitive, and fast: I needed something with\nlots of knobs and sliders. But I didn\u2019t want to pay a lot for it.\n\nFortunately, such a thing exists:\n\nThe Akai MIDImix (link without affiliate code) is a device that features 24\nknobs, 9 sliders and a bunch of buttons for about $110. Each of the controls\nonly has 7 bits of resolution, but, for the price it\u2019s good enough.\n\nEven better, there\u2019s a good bit of work done already to reverse engineer its\ndesign, and Python already has libraries to talk to MIDI controllers. To\nfigure out what the button mappings are, I use a small test script that I\nwrote to print out MIDI messages when I frob a knob.\n\nIt\u2019s much more immediate and satisfying to tweak and adjust the machine\u2019s\nposition and light parameters in real time, and with this controller, I can\neven adjust multiple things simultaneously.\n\nCore Modules\n\nBelow is a block diagram of the control software platform. I call the control\nsoftware Jubiris. The square blocks represent Python modules. The ovals are\nhardware end points. The hexagons are subordinate threads.\n\nThe code is roughly broken into two primary threads, a Qt thread, and a\ncontrol thread. The Qt thread handles the \u201cbig real time data objects\u201d: image\ndata, mostly. It is responsible for setting up the camera, handling frame\nready events, display the image previews, doing image processing, writing\nfiles to disk, and other ancillary tasks associated with the Qt UI (like\nprocessing button presses and showing status text).\n\nThe control thread contains all the \u201cstrategy\u201d. A set of Event and Queue\nobjects synchronize data between the threads. It would have been nice to do\nall the image processing inside the control thread, but I also wanted the\nfocus algorithm to run really fast. To avoid the overhead of copying raw\n4k-resolution image frames between threads, I settled for the Qt thread doing\nthe heavy lifting of taking the focus region of interest and turning it into a\nsingle floating point number, a \u201cfocus metric\u201d, and passing that into the\ncontrol thread via a Queue. The control thread then considers all the inputs\nfrom the MIDI controller and Events triggered via buttons in the Qt thread,\nand makes decisions about how to set the lights, piezo fine focus stage,\nJubilee motors, and so forth. It also has some nominal \u201cnever to exceed\u201d\nparameters coded into it so if something seems wrong it will ESTOP the machine\nand shut everything down.\n\nSpeaking of which, it\u2019s never a good idea to disable those limits, even for a\nminute. I had a bug once where I had swapped the mapping of the limit switches\non the zenith actuator, causing the motors to stop in the wrong position. For\nsome reason, I thought it\u2019d be a good idea to bypass the safeties to get more\nvisibility into the machine\u2019s trajectory. In a matter of about two seconds, I\nheard the machine groaning under the strain of the zenith motor dutifully\nforcing the lighting platform well past its safe limit, followed by a \u201cping\u201d\nand then an uncontrolled \u201cwhirr\u201d as the motor gleefully shattered its coupling\nand ran freely at maximum speed, scattering debris about the work area.\n\nTurns out, I put the safeties are there for a reason, and it\u2019s never a good\nidea to mix Python debugging practices (\u201cjust frob the variable and see what\nbreaks!\u201d) with hardware debugging, because instead of stack traces you get\nshattered bearings.\n\nThankfully I positioned the master power switch in an extremely accessible\nlocation, and the only things that were broken were a $5 coupling and my\nconfidence.\n\nAutofocus\n\nAutofocus was one of those features that also started out as \u201cjust a test of\nthe piezo actuators\u201d that ended up blooming into a full-on situation. Probably\nthe most interesting part of it, at least to me, was answering the question of\n\u201chow does a machine even know when something is focused?\u201d.\n\nAfter talking to a couple of experts on this, the take-away I gathered is that\nyou don\u2019t, really. Unless you have some sort of structured light or absolute\ndistance measurement sensor, the best you can do is to say you are \u201cmore or\nless focused than before\u201d. This makes things a little tricky for imaging a\nchip, where you have multiple thin films stacked in close proximity: it\u2019s\npretty easy for the focus system to get stuck on the wrong layer. My fix to\nthat was to initially use a manual focus routine to pick three points of\ninterest that define the corners of the region we want to image, extrapolate a\nplane from those three points, and then if the focus algorithm takes us off\nsome micron-scale deviation from the ideal plane we smack it and say \u201cno! Pay\nattention to this plane\u201d, and pray that it doesn\u2019t get distracted again. It\nworks reasonably well for a silicon chip because it is basically a perfect\nplane, but it struggles a bit whenever I exceed the limits of the piezo fine-\nfocus element itself and have to invoke the Jubilee Z-controls to improve the\ndynamic range of the fine-focus.\n\nAbove: visualization of focus values versus an idealized plane. The laser\nmarked area (highlighted in orange) causes the autofocus to fail, and so the\nfocus result is clamped to an idealized plane.\n\nHow does a machine judge the relative focus between two images? The best I\ncould find in the literature is \u0304\\\\_(\u30c4)_/ \u0304 : it all kind of depends on what\nyou\u2019re looking at, and what you care about. Basically, you want some image\nprocessing algorithm that can take an arbitrary image and turn it into a\nsingle number: a \u201cfocused-ness\u201d score. The key observation is that stuff\nthat\u2019s in focus tends to have sharp edges, and so what you want is an image\nprocessing kernel that ignores stuff like global lighting variations and\nreturns you the \u201cedginess\u201d of an image.\n\nThe \u201cLaplacian Operator\u201d in OpenCV does basically this. You can think of it as\ntaking the second derivative of an image in both X and Y. Here\u2019s a before and\nafter example image lifted from the OpenCV documentation.\n\nBefore running the Laplacian:\n\nAfter running the Laplacian:\n\nYou can see how the bright regions in the lower image consists of mostly the\nsharp edges in the original image \u2013 soft gradients are converted to dark\nareas. An \u201cin focus\u201d image would have more and brighter sharp edges than a\nless focused image, and so, one could derive a \u201cfocused-ness\u201d metric by\ncalculating the variance of the Laplacian of an image.\n\nI personally found this representation of the Laplacian insightful:\n\nThe result of the Laplacian is computed by considering the 8 pixels\nsurrounding a source pixel, weighting the pixel in question by -4, and adding\nto it the value of its cardinal neighbors. In the case that you were looking\nat a uniformly shaded region, the sum is 0: the minus four weighting of the\ncenter pixel cancels out the weighting of the neighboring pixels perfectly.\nHowever, in the case that you\u2019re looking at something where neighboring pixels\ndon\u2019t have the same values, you get a non-zero result (and intermediate\nresults are stored using a floating point format, so we don\u2019t end up clamping\ndue to integer arithmetic limitations).\n\nAlso, it took me a long time to figure this out, but I think in \u201cimage\nprocessing nerd speak\u201d, a Laplacian is basically a high-pass filter, and a\nGaussian is a low-pass filter. I\u2019m pretty sure this simplified description is\ngoing to cause some image processing academics to foam in the mouth, because\nof reasons. Sorry!\n\nIf this were a textbook, at this point we would declare success on computing\nfocus, and leave all the other details as an exercise to the reader.\nUnfortunately, I\u2019m the reader, so I had to figure out all the other details.\n\nHere\u2019s the list of other things I had to figure out to get this to work well:\n\n  * Let the machine settle before computing anything. This is done by observing the Laplacian metric in real-time, and waiting until its standard deviation falls below an acceptable threshold.\n  * Do a GaussianBlur before computing the Laplacian. GaussianBlur is basically a low pass filter that reduces noise artifacts, leading to more repeatable results. It may seem counter-intuitive to remove edges before looking for them, but, another insight is, at 10x magnification I get about 4.7 pixels per micron \u2013 and recall that my light source is only 1 micron wavelength. Thus, I have some spatial oversampling of the image, allowing me the luxury of using a GaussianBlur to remove pixel-to-pixel noise artifacts before looking for edges.\n  * Clip bright artifacts from the image before computing the Laplacian. I do this by computing a histogram and determining where most of the desired image intensities are, and then ignoring everything above a manually set threshold. Bright artifacts can occur for a lot of reasons, but are typically a result of dirt or dust in the field of view. You don\u2019t want the algorithm focusing on the dust because it happens to be really bright and contrasting with the underlying circuitry.\n  * It sometimes helps to normalize the image before doing the Laplacian. I have it as an option in the image processing pipeline that I can set with a check-box in the main UI.\n  * You can pick the size of the Laplacian kernel. This effectively sets the \u201csize of the edge\u201d you\u2019re looking for. It has to be an odd number. The example matrix discussed above uses a 3\u00d73 kernel, but in many cases a larger kernel will give better results. Again, because I\u2019m oversampling my image, a 7\u00d77 kernel often gives the best results, but for some chips with larger features, or with a higher magnification objective, I might go even larger.\n  * Pick the right sub-region to focus on. In practice, the image is stitched together by piecing together many images, so as a default I just make sure the very center part is focused, since the edges are mostly used for aligning images. However, some chip regions are really tricky to focus on. Thus, I have an outer loop wrapped around the core focus algorithm, where I divide the image area into nine candidate regions and search across all of the regions to find an area with an acceptable focus result.\n\nNow we know how to extract a \u201cfocus metric\u201d for a single image. But how do we\nknow where the \u201cbest\u201d focal distance is? I use a curve fitting algorithm to\nfind the best focus focus point. It works basically like this:\n\n  1. Compute the metric for the current point\n  2. Pick an arbitrary direction to nudge the focus\n  3. Compute the new metric (i.e. variance of the Laplacian, as discussed above). If the metric is higher, keep going with the same nudge direction; if not, invert the sign of the nudge direction and carry on.\n  4. Keep nudging until you observe the metric getting worse\n  5. Take the last five points and fit them to a curve\n  6. Pick the maximum value of the fitted curve as the focus point\n  7. Check the quality of the curve fit; if the mean squared error of the points versus the fitted curve is too large, probably someone was walking past the machine and the vibrations messed up one of the measurements. Go back to step 4 and redo the measurements.\n  8. Set the focus to the maximum value, and check that the resulting metric matches the predicted value; if not, sweep the proposed region to collect another few points and fit again\n\nAbove is an example of a successful curve fitting to find the maximum focus\npoint. The X-axis plots the stage height in millimeters (for reasons related\nto the Jubilee control software, the \u201czero point\u201d of the Z-height is actually\nat 10mm), and the Y axis is the \u201cfocus metric\u201d. Here we can see that the\noptimal focus point probably lies at around 9.952 mm.\n\nAll of the data is collected in real time, so I use Pandas dataframes to track\nthe focus results versus the machine state and timestamps. Dataframes are a\npretty powerful tool that makes querying a firehose of real-time focus data\nmuch easier, but you have to be a little careful about how you use them:\nappending data to a dataframe is extremely slow, so you can\u2019t implement a FIFO\nfor processing real-time data by simply appending to and dropping rows from a\ndataframe with thousands of elements. Sometimes I just allocate a whole new\ndataframe, other times I manually replace existing entries, and other times I\njust keep the dataframe really short to avoid performance problems.\n\nAfter some performance tuning, the whole algorithm runs quite quickly: the\nlimiting factor ends up being the exposure time of the camera, which is around\n60 ms. The actual piezo stage itself can switch to a new value in a fraction\nof that time, so we can usually find the focus point of an image within a\ncouple of seconds.\n\nIn practice, stray vibrations from the environment limit how fast I can focus.\nThe focus algorithm pauses if it detects stray vibrations, and it will\nrecompute the focus point if it determines the environment was too noisy to\nrun reliably. My building is made out of dense, solid poured concrete, so at\nnight it\u2019s pretty still. However, my building is also directly above a subway\nstation, so during the day the subway rolling in and out (and probably all the\nbuses on the street, too) will regularly degrade imaging performance.\nFortunately, I\u2019m basically nocturnal, so I do all my imaging runs at night,\nafter public transportation stops running.\n\nBelow is a loop showing the autofocus algorithm running in real-time. Because\nwe\u2019re sweeping over such fine increments, the image changes are quite subtle.\nHowever, if you pay attention to the bright artifacts in the lower part of the\nimage (those are laser markings for the part number on the chip surface),\nyou\u2019ll see a much more noticeable change as the focus algorithm does its\nthing.\n\nClosing Thoughts\n\nIf you made it this far, congratulations. You made it through a post about\nsoftware, written by someone who is decidedly not a software engineer. Before\nwe wrap things up, I wanted to leave you with a couple of parting thoughts:\n\n  * OpenCV has just about every algorithm you can imagine, but it\u2019s nearly impossible to find documentation on anything but the most popular routines. It\u2019s often worth it to keep trudging through the documentation tree to find rare gems.\n  * Google sucks at searching for OpenCV documentation. Instead, keep a tab in your browser open to the OpenCV documentation. Be sure to select the version of the documentation that matches your installed version! It\u2019s subtle, but there is a little pull-down menu next to the OpenCV menu that lets you pick that.\n  * Another reason why Google sucks for OpenCV docs is almost every link returned by Google defaults to an ancient version of the docs that does not match what you probably have installed. So if you are in the habit of \u201cGoogle, copy, paste\u201d, you can spend hours debugging subtle API differences until you notice that a Google result reset your doc browser version to 3.2, but you\u2019re on 4.8 of the API.\n  * Because the documentation is often vague or wrong, I write a lot of small, single-use throw-away tests to figure out OpenCV. This is not reflected in the final code, but it\u2019s an absolute nightmare to try and debug OpenCV in a real-time image pipeline. Do not recommend! Keep a little buffer around with some scaffolding to help you \u201csingle-step\u201d through parts of your image processing pipeline until you feel like you\u2019ve figured out what the API even means.\n  * OpenCV is blazing fast if you use it right, thanks in part to all of the important bits being native C++. I think in most cases the Python library is just wrappers for C++ libraries.\n  * OpenCV and Qt do not get along. It\u2019s extremely tricky to get them to co-exist on a single machine, because OpenCV pulls in a version of Qt that is probably incompatible with your Qt installed package. There\u2019s a few fixes for this. In the case that you are only using OpenCV for image processing, you can install the \u201cheadless\u201d version that doesn\u2019t pull in Qt. But if you\u2019re trying to debug OpenCV you probably want to pop up windows using its native API calls, and in that case here\u2019s one weird trick you can use to fix that. Basically, you figure out the location of the Qt binary is that\u2019s bundled inside your OpenCV install, and point your OS environment variable at that.\n  * This is totally fine until you update anything. Ugh. Python.\n  * Google likewise sucks at Qt documentation. Bypass the pages of ad spam, outdated stackoverflow answers, and outright bad example code, and just go straight to the Qt for Python docs.\n  * LLMs can be somewhat helpful for generating Qt boilerplate. I\u2019d say I get about a 50% hallucination rate, so my usual workflow is to ask an LLM to summarize the API options, check the Qt docs that they actually exist, then ask a tightly-formulated question of the LLM to derive an API example, and then cross-check anything suspicious in the resulting example against the Qt docs.\n  * LLMs can also be somewhat helpful for OpenCV boilerplate, but you also get a ton of hallucinations that almost work, some straight-up lies, and also some recommendations that are functional but highly inefficient or would encourage you to use data structures that are dead-ends. I find it more helpful to try and find an actual example program in a repo or in the OpenCV docs first, and then from there form very specific queries to the LLM to get higher quality results.\n  * Threading in Python is terrifying if you normally write concurrent code in Rust. It\u2019s that feeling you get when you step into a taxi and habitually reach for a seat belt, only to find it\u2019s not there or broken. You\u2019ll probably fine! Until you crash.\n\nAnd that\u2019s basically it for the IRIS control software. The code is all located\nin this github repo. miduet.py (portmanteau of MIDI + Duet, i.e. Jubilee\u2019s\ncontrol module) is the top level module; note comments near the top of the\nfile on setting up the environment. However, I\u2019m not quite sure how useful it\nwill be to anyone who doesn\u2019t have an IRIS machine, which at this point is\nprecisely the entire world except for me. But hopefully, the description of\nthe concepts in this post were at least somewhat entertaining and possibly\neven informative.\n\nThanks again to NLnet and my Github Sponsors for making all this research\npossible!\n\nThis entry was posted on Sunday, April 14th, 2024 at 11:58 pm and is filed\nunder IRIS. You can follow any responses to this entry through the RSS 2.0\nfeed. You can leave a response, or trackback from your own site.\n\n### One Response to \u201cControl and Autofocus Software for Chip-Level Microscopy\u201d\n\n  1. willmore says:\n\nApril 15, 2024 at 1:05 am\n\nThat was a bit of a slog, but well worth it. Great work, sir!\n\nReply\n\n### Leave a Reply\n\nClick here to cancel reply.\n\nbunnie's blog is proudly powered by WordPress Entries (RSS) and Comments\n(RSS).\n\n", "frontpage": false}
