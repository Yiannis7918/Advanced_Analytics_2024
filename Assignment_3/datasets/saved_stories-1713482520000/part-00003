{"aid": "40078534", "title": "Multi-agent collaboration design patterns", "url": "https://www.deeplearning.ai/the-batch/issue-245/", "domain": "deeplearning.ai", "votes": 1, "user": "helloericsf", "posted_at": "2024-04-18 17:20:23", "comments": 0, "source_title": "AI Agents With Low/No Code, Hallucinations Create Security Holes, and more", "source_text": "AI Agents With Low/No Code, Hallucinations Create Security Holes, and more\n\n\ud83c\udf1f New Course! Enroll in Quantization Fundamentals With Hugging Face\n\nStart Learning\n\nPublished\n\nApr 17, 2024\n\nReading time\n\n13 min read\n\nPublished\n\nApr 17, 2024\n\nReading time\n\n13 min read\n\nShare\n\nDear friends,\n\nMulti-agent collaboration is the last of the four key AI agentic design\npatterns that I\u2019ve described in recent letters. Given a complex task like\nwriting software, a multi-agent approach would break down the task into\nsubtasks to be executed by different roles \u2014 such as a software engineer,\nproduct manager, designer, QA (quality assurance) engineer, and so on \u2014 and\nhave different agents accomplish different subtasks.\n\nDifferent agents might be built by prompting one LLM (or, if you prefer,\nmultiple LLMs) to carry out different tasks. For example, to build a software\nengineer agent, we might prompt the LLM: \u201cYou are an expert in writing clear,\nefficient code. Write code to perform the task . . ..\u201d\n\nIt might seem counterintuitive that, although we are making multiple calls to\nthe same LLM, we apply the programming abstraction of using multiple agents.\nI\u2019d like to offer a few reasons:\n\n  * It works! Many teams are getting good results with this method, and there\u2019s nothing like results! Further, ablation studies (for example, in the AutoGen paper cited below) show that multiple agents give superior performance to a single agent.\n  * Even though some LLMs today can accept very long input contexts (for instance, Gemini 1.5 Pro accepts 1 million tokens), their ability to truly understand long, complex inputs is mixed. An agentic workflow in which the LLM is prompted to focus on one thing at a time can give better performance. By telling it when it should play software engineer, we can also specify what is important in that role\u2019s subtask. For example, the prompt above emphasized clear, efficient code as opposed to, say, scalable and highly secure code. By decomposing the overall task into subtasks, we can optimize the subtasks better.\n  * Perhaps most important, the multi-agent design pattern gives us, as developers, a framework for breaking down complex tasks into subtasks. When writing code to run on a single CPU, we often break our program up into different processes or threads. This is a useful abstraction that lets us decompose a task, like implementing a web browser, into subtasks that are easier to code. I find thinking through multi-agent roles to be a useful abstraction as well.\n\nIn many companies, managers routinely decide what roles to hire, and then how\nto split complex projects \u2014 like writing a large piece of software or\npreparing a research report \u2014 into smaller tasks to assign to employees with\ndifferent specialties. Using multiple agents is analogous. Each agent\nimplements its own workflow, has its own memory (itself a rapidly evolving\narea in agentic technology: how can an agent remember enough of its past\ninteractions to perform better on upcoming ones?), and may ask other agents\nfor help. Agents can also engage in Planning and Tool Use. This results in a\ncacophony of LLM calls and message passing between agents that can result in\nvery complex workflows.\n\nWhile managing people is hard, it's a sufficiently familiar idea that it gives\nus a mental framework for how to \"hire\" and assign tasks to our AI agents.\nFortunately, the damage from mismanaging an AI agent is much lower than that\nfrom mismanaging humans!\n\nEmerging frameworks like AutoGen, Crew AI, and LangGraph, provide rich ways to\nbuild multi-agent solutions to problems. If you're interested in playing with\na fun multi-agent system, also check out ChatDev, an open source\nimplementation of a set of agents that run a virtual software company. I\nencourage you to check out their GitHub repo and perhaps clone the repo and\nrun the system yourself. While it may not always produce what you want, you\nmight be amazed at how well it does.\n\nLike the design pattern of Planning, I find the output quality of multi-agent\ncollaboration hard to predict, especially when allowing agents to interact\nfreely and providing them with multiple tools. The more mature patterns of\nReflection and Tool Use are more reliable. I hope you enjoy playing with these\nagentic design patterns and that they produce amazing results for you!\n\nIf you're interested in learning more, I recommend:\n\n  * \u201cCommunicative Agents for Software Development,\u201d Qian et al. (2023) (the ChatDev paper)\n  * \u201cAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,\u201d Wu et al. (2023)\n  * \u201cMetaGPT: Meta Programming for a Multi-Agent Collaborative Framework,\u201d Hong et al. (2023)\n\nKeep learning!\n\nAndrew\n\nP.S. Large language models (LLMs) can take gigabytes of memory to store, which\nlimits your ability to run them on consumer hardware. Quantization can reduce\nmodel size by 4x or more while maintaining reasonable performance. In our new\nshort course \u201cQuantization Fundamentals,\u201d taught by Hugging Face's Younes\nBelkada and Marc Sun, you\u2019ll learn how to quantize LLMs and how to use int8\nand bfloat16 (Brain Float 16) data types to load and run LLMs using PyTorch\nand the Hugging Face Transformers library. You\u2019ll also dive into the technical\ndetails of linear quantization to map 32-bit floats to 8-bit integers. I hope\nyou\u2019ll check it out!\n\n# News\n\n# Custom Agents, Little Coding\n\nGoogle is empowering developers to build autonomous agents using little or no\ncustom code.\n\nWhat\u2019s new: Google introduced Vertex AI Agent Builder, a low/no-code toolkit\nthat enables Google\u2019s AI models to run external code and ground their\nresponses in Google search results or custom data.\n\nHow it works: Developers on Google\u2019s Vertex AI platform can build agents and\nintegrate them into multiple applications. The service costs $12 per 1,000\nqueries and can use Google Search for $2 per 1,000 queries.\n\n  * You can set an agent\u2019s goal in natural language (such as \u201cYou are a helpful assistant. Return your responses in markdown format.\u201d) and provide instructions (such as \u201cGreet the user, then ask how you can help them today\u201d).\n  * Agents can ground their outputs in external resources including information retrieved from Google\u2019s Enterprise Search or BigQuery data warehouse. Agents can generate a confidence score for each grounded response. These scores can drive behaviors such as enabling an agent to decide whether its confidence is high enough to deliver a given response.\n  * Agents can use tools, including a code interpreter that enables agents to run Python scripts. For instance, if a user asks about popular tourist locations, an agent can call a tool that retrieves a list of trending attractions near the user\u2019s location. Developers can define their own tools by providing instructions to call a function, built-in extension, or external API.\n  * The system integrates custom code via the open source library LangChain including the LangGraph extension for building multi-agent workflows. For example, if a user is chatting with a conversational agent and asks to book a flight, the agent can route the request to a subagent designed to book flights.\n\nBehind the news: Vertex AI Agent Builder consolidates agentic features that\nsome of Google\u2019s competitors have rolled out in recent months. For instance,\nOpenAI\u2019s Assistants API lets developers build agents that respond to custom\ninstructions, retrieve documents (limited by file size), call functions, and\naccess a code interpreter. Anthropic recently launched Claude Tools, which\nlets developers instruct Claude language models to call customized tools.\nMicrosoft\u2019s Windows Copilot and Copilot Builder can call functions and\nretrieve information using Bing search and user documents stored via Microsoft\nGraph.\n\nWhy it matters: Making agents practical for commercial use can require\ngrounding, tool use, multi-agent collaboration, and other capabilities.\nGoogle\u2019s new tools are a step in this direction, taking advantage of\ninvestments in its hardware infrastructure as well as services such as search.\nAs tech analyst Ben Thompson writes, Google\u2019s combination of scale,\ninterlocking businesses, and investment in AI infrastructure makes for a\ncompelling synergy.\n\nWe\u2019re thinking: Big-tech offerings like Vertex Agent Builder compete with an\nexpanding universe of open source tools such as AutoGen, CrewAI, and\nLangGraph. The race is on to provide great agentic development frameworks!\n\n# Hallucination Creates Security Holes\n\nLanguage models can generate code that erroneously points to software\npackages, creating vulnerabilities that attackers can exploit.\n\nWhat\u2019s new: A cybersecurity researcher noticed that large language models,\nwhen used to generate code, repeatedly produced a command to install a package\nthat was not available on the specified path, The Register reported. He\ncreated a dummy package of the same name and uploaded it to that path, and\ndevelopers duly installed it.\n\nHow it works: Bar Lanyado, a researcher at Lasso Security, found that the\nerroneous command pip install huggingface-cli appeared repeatedly in generated\ncode. The package huggingface-cli does exist, but it is installed using the\ncommand pip install -U \u201chuggingface_hub[cli]\". The erroneous command attempts\nto download a package from a different repository. Lanyado published some of\nhis findings in a blog post.\n\n  * Lanyado uploaded a harmless package with that name. Between December 2023 and March 2024, the dummy package was downloaded more than 15,000 times. It is not clear whether the downloads resulted from generated code, mistaken advice on bulletin boards, or user error.\n  * Several repositories on Github used or recommended the dummy package, including GraphTranslator, which has been updated to remove the reference. Hugging Face itself called the package in one of its own projects; the company removed the call after Lanyado notified it.\n  * In research published last year, Lanyado described ChatGPT\u2019s tendency to recommend a nonexistent Node.js package called arangodb. (ArangoDB is a real database query system, but its official Node.js package is arangojs.) Lanyado demonstrated that it was possible to create a new package with the erroneous name and install it using ChatGPT\u2019s instructions.\n\nTesting: Lanyado tested Cohere AI\u2019s Coral, Google\u2019s Gemini Pro, and OpenAI\u2019s\nGPT-4 and GPT-3.5. His aim was to determine how often they hallucinated\npackages and how often they referred repeatedly to the same hallucinated\npackage. First he collected roughly 47,000 \u201chow to\u201d questions related to over\n100 subjects in Go, .NET, Node.js, Python, and Ruby. Then he identified\nquestions that produced hallucinated packages from a zero-shot prompt. He\nselected 20 of these questions at random and prompted each model 100 times to\nsee whether it would refer to the same package every time.\n\n  * Of the models tested, Gemini Pro hallucinated packages most often, while Coral hallucinated packages most repeatedly. Here's (a) how often each model hallucinated packages and (b) how often it hallucinated the same package repeatedly. Coral: (a) 29.1 percent, (b) 24.2 percent. Gemini Pro: (a) 64.5 percent, (b) 14 percent. GPT-4: (a) 24.2 percent, (b) 19.6 percent. GPT-3.5 (a) 22.2 percent, (b) 13.6 percent.\n  * The percentage of references to hallucinated packages also varied depending on the programming language. Using GPT-4, for example, 30.9 percent of Go queries referred to a hallucinated package compared to 28.7 percent of .NET queries, 19.3 percent of Node.js queries, 25 percent of Python queries, and 23.5 percent of Ruby queries.\n  * Generally, Python and Node.js are more vulnerable to this type of attack than Go and .NET, which block access to certain paths and filenames. Of the Go and .NET prompts that returned a hallucinated package name, 2.9 percent and 21.2 percent were exploitable, respectively.\n\nWhy it matters: Lanyado\u2019s method is not known to have been used in an attack,\nbut it may be only a matter of time given its similarity to hacks like\ntyposquatting, dependency confusion, and masquerading.\n\nWe\u2019re thinking: Improved AI-driven coding tools should help to address this\nissue. Meanwhile, the difference between a command like pip install\nhuggingface-cli and pip install -U \"huggingface_hub[cli]\" is subtle. In cases\nlike this, package providers can look out for potential doppelgangers and warn\nusers from being misled.\n\n## NEW FROM DEEPLEARNING.AI\n\nIn the short course \u201cQuantization Fundamentals with Hugging Face,\u201d you\u2019ll\nlearn how to cut the computational and memory costs of AI models through\nquantization. Learn to quantize nearly any open source model! Join today\n\n# GPT Store Shows Lax Moderation\n\nOpenAI has been moderating its GPT Store with a very light touch.\n\nWhat\u2019s new: In a survey of the GPT Store\u2019s offerings, TechCrunch found\nnumerous examples of custom ChatGPT instances that appear to violate the\nstore\u2019s own policies.\n\nHow it works: The GPT Store has a low bar for entry by design \u2014 any paid\nChatGPT user can create a custom-prompted variation of the chatbot, known as a\nGPT, and include it in the store. The store lists GPTs in several categories,\nsuch as Writing, Productivity, Programming, and Lifestyle. While many are\nuseful, some are questionable.\n\n  * Some GPTs purported to jailbreak ChatGPT. In TechCrunch\u2019s survey, some of them were able to circumvent OpenAI\u2019s own guardrails. Since then, they have been tamed. The GPT Store\u2019s terms of use prohibit efforts to thwart OpenAI\u2019s safeguards and safety measures.\n  * GPTs like Humanizer Pro, the second-ranked instance in the Writing category at the time of writing, purport to rewrite text and make it undetectable to programs designed to detect generated text. These GPTs may violate OpenAI\u2019s ban on GPTs that enable academic dishonesty.\n  * Many GPTs purport to allow users to chat with trademarked characters without clear authorization from the trademark owners. The store prohibits use of content owned by third parties without their permission.\n  * Other GPTs purport to represent real-life figures such as Elon Musk, Donald Trump, and Joe Rogan, or companies such as Microsoft and Apple (many of them obviously satirical). OpenAI allows GPTs to respond in the style of a real person if they do not impersonate that person. However, many such GPTs don\u2019t indicate that they are not associated with the genuine person.\n\nBehind the news: OpenAI launched the GPT Store in January. Since then, users\nhave uploaded more than 3 million GPTs that include enhanced search engines,\ncreative writing aids, and tools that produce short videos. The most popular\nGPTs have millions of downloads. Despite its \u201cstore\u201d name, the GPT Store\u2019s\ncontents are free to download. OpenAI is piloting a program in which\nU.S.-based uploaders of popular GPTs can earn money.\n\nWhy it matters: The GPT Store is the chatbot era\u2019s answer to Apple\u2019s App Store\nor Android\u2019s Google Play Store. If it succeeds, it could democratize chatbot\ndevelopment just as the App Store helped to popularize building smartphone\napplications. How OpenAI moderates the store may have real financial and\nreputational impacts on developers in the years ahead.\n\nWe\u2019re thinking: The GPT Store\u2019s low barrier to entry is a boon to well-meaning\ndevelopers, but it may encourage less responsible actors to take advantage of\nlax moderation. We applaud OpenAI\u2019s willingness to execute an ambitious vision\nand hope it finds a workable balance.\n\n# Tuning LLMs for Better RAG\n\nRetrieval-augmented generation (RAG) enables large language models to generate\nbetter output by retrieving documents that are relevant to a user\u2019s prompt.\nFine-tuning further improves RAG performance.\n\nWhat\u2019s new: Xi Victoria Lin, Xilun Chen, Mingda Chen, and colleagues at Meta\nproposed RA-DIT, a fine-tuning procedure that trains an LLM and retrieval\nmodel together to improve the LLM\u2019s ability to capitalize on retrieved\ncontent.\n\nRetrieval augmented generation (RAG) basics: When a user prompts an LLM, RAG\nsupplies documents that are relevant to the prompt. A separate retrieval model\ncomputes the probability that each chunk of text in a separate dataset is\nrelevant to the prompt. Then it grabs the chunks with the highest probability\nand provides them to the LLM to append to the prompt. The LLM generates each\ntoken based on the chunks plus the prompt and tokens generated so far.\n\nKey insight: Typically LLMs are not exposed to retrieval-augmented inputs\nduring pretraining, which limits how well they can use retrieved text to\nimprove their output. Such methods have been proposed, but they\u2019re costly\nbecause they require processing a lot of data. A more data-efficient, and\ntherefore compute-efficient, approach is to (i) fine-tune the LLM to better\nuse retrieved knowledge and then (ii) fine-tune the retrieval model to select\nmore relevant text.\n\nHow it works: The authors fine-tuned Llama 2 (65 billion parameters) and\nDRAGON+, a retriever. They call the system RA-DIT 65B.\n\n  * The authors fine-tuned Llama 2 on prompts that consist of retrieved text and a question or instruction. They used 20 datasets including dialogue, question-answering, answering questions about a given text passage, summarization, and datasets in which the model must answer questions and explain its reasoning.\n  * They fine-tuned DRAGON+\u2019s encoder to increase the probability that it retrieved a given chunk if the chunk improved the LLM\u2019s chance of generating the correct answer. Fine-tuning was supervised for the tasks listed above. Fine-tuning was self-supervised for completion of 37 million text chunks from Wikipedia and 362 million text chunks from CommonCrawl.\n\nResults: On average, across four collections of questions from datasets such\nas MMLU that cover topics like elementary mathematics, United States history,\ncomputer science, and law, RA-DIT 65B achieved 49.1 percent accuracy, while\nthe combination of LLaMA 2 65B and DRAGON+ without fine-tuning achieved 45.1\npercent accuracy, and LLaMA 2 65B without retrieval achieved 32.9 percent\naccuracy. When the input included five examples that showed the model how to\nperform the task, RA-DIT 65B achieved 51.8 percent accuracy, LLaMA 2 65B\ncombined with DRAGON+ achieved 51.1 percent accuracy, and LLaMA 2 65B alone\nachieved 47.2 percent accuracy. On average, over eight common-sense reasoning\ntasks such as ARC-C, which involves common-sense physics such as the buoyancy\nof wood, RA-DIT 65B achieved 74.9 percent accuracy, LLaMA 2 65B with DRAGON+\nachieved 74.5 percent accuracy, and LLaMA 2 achieved 72.1 percent accuracy.\n\nWhy it matters: This method offers an inexpensive way to improve LLM\nperformance with RAG.\n\nWe\u2019re thinking: Many developers have found that putting more effort into the\nretriever, to make sure it provides the most relevant text, improves RAG\nperformance. Putting more effort into the LLM helps, too.\n\n# Data Points\n\nIn this week\u2019s Data Points, find new model and feature releases from Google,\nMicrosoft, Mistral, OpenAI, and Spotify, plus AI art projects and government\ninvestments.\n\nRead your short-form digest of this week\u2019s AI news now\n\nShare\n\n## Subscribe to The Batch\n\nStay updated with weekly AI News and Insights delivered to your inbox\n\n", "frontpage": false}
