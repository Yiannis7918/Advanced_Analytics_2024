{"aid": "39967983", "title": "Moving fast breaks things: the importance of a staging environment", "url": "https://graphite.dev/blog/staging-environment", "domain": "graphite.dev", "votes": 2, "user": "kiyanwang", "posted_at": "2024-04-08 09:45:03", "comments": 0, "source_title": "Moving fast breaks things: the importance of a staging environment", "source_text": "Moving fast breaks things: the importance of a staging environment\n\nHomeFeaturesPricingDocsBlogLog inSign up\n\n\u00a9 2024 Graphite, All rights reserved.\n\n#Engineering\n\n# Moving fast breaks things: the importance of a staging environment\n\nFebruary 18, 2024\n\nWritten by\n\nGreg Foster\n\nIn this post\n\nMoving fast breaks things\n\nDogfooding our changes before production\n\nEarly concerns with our staging bake period\n\nWaiting an hour\n\nTwo DBs to migrate\n\nPausing deployments for too long\n\nMaintaining realistic data on staging\n\nLater improvements to our deployment process\n\nInspiration from Airbnb\n\nHow you can implement your own staging bake\n\nGet a pipeline\n\nGet data\n\nGet traffic\n\nMonitor for issues\n\nReflections\n\nStay unblocked. Ship faster.\n\nExperience the new developer workflow - create, review, and merge code\ncontinuously. Get started with one command.\n\nGet started\n\n## Moving fast breaks things\n\nGraphite's engineering team has a culture of moving extremely fast. This is\nvery much a stylistic choice - some engineering teams move more carefully, but\nwe like to iteratively ship small, fast changes out to users as quickly as\npossible. That culture is reflected in the nature of the Graphite product\nitself, which aims to accelerate code changes - and I\u2019d argue this is somewhat\na Conway Law-esque result.\n\nThe downside to our organization\u2019s need for speed is that I\u2019m also personally\nresponsible for keeping the site online. For years, I was the first to get\npaged, and oh boy was I paged a lot during those early days.\n\nIn its first year, Graphite was plagued with regressions. Our small\nengineering team and our blistering pace meant that we\u2019d frequently ship a\nregression to production and then have to scramble to roll back or fix\nforward. The approach was functional, but we were relying on paper-cut users\ngiving us feedback. Without slowing down our velocity (culturally\nunattractive), or getting ten times better at unit testing (tricky because of\nour strong integration to GitHub\u2019s API), there was little we could do to catch\nregressions before they impacted users.\n\n## Dogfooding our changes before production\n\nSomething needed to change. We needed a reliable way to catch unknown unknowns\nwithout imposing extra work on developers. Around the end of 2022, after much\ndebate, I chose to build a Graphite staging environment where we\u2019d bake all\ndeployments automatically, pre-production.\n\nWhile I could have simply duplicated our application servers and created a\nsecond Postgres cluster, I decided to go whole-hog and create an entirely\nseparate AWS account, complete with duplicate load balancers, VPCs, S3\nbuckets, ECS containers, and more. I used a third \u201ctooling\u201d AWS account to\nhouse what little was shared - in particular, an AWS code pipeline that\ndeployed changes to both staging and production.\n\nAfter creating a staging account, I tried dogfooding Graphite on the staging\nservers for a week. Initially, I was blocked by a few hardcoded url bugs - but\neventually, I got the experience workable. Because Graphite used GitHub\nproduction as our shared database, I was still able to interface with\nteammates\u2019 PRs, even if we were working across two separate Postgres clusters.\n\nWith me happily doing all my work on the staging cluster, I next added a\nbanner visible only to employees on prod, reminding them that they should\ninstead be using the staging version of the site to help us dogfood the stream\nof daily changes. This nudging banner proved to be a perfect balance of\ndriving employees to use staging for daily work while still making production\neasily accessible when necessary.\n\nAt this point, I had two duplicate environments, with employees on one and all\nother Graphite users on the other. Each new release however, would deploy to\nboth environments simultaneously, meaning that unexpected regressions would\nhit external users at the same time as employees.\n\n> \ud83d\udca1 How did I do all this without breaking deployments for our team you may\n> ask? I did indeed break deployments - but I tactically did it over a long\n> weekend. While my partner was out of town on a work trip, I was left alone\n> re-watching Scott Pilgrim vs. The World on one screen while trial-and-error\n> running hundreds of deployments on another. The stress of knowing I had to\n> reassemble our deployments before the work week pushed me to land the first\n> pass all at once. After a few sleepless, Scott Pilgrim filled nights, I\n> eventually got things into a stable state.\n\nWith everything in place, the final step was to reap the actual benefits of\nthe project. With begrudging team buy-in, I updated our AWS code pipeline to\nsequence staging deployments, followed by a one-hour wait stage and, finally,\nan automatic promotion to production. We now had a buffer before production\ndeployments.\n\nI didn\u2019t need to wait long to test the new capability. A few days after adding\nthe staging bake, we released a regression. A bug broke our diffing algorithm,\npreventing the site from loading pull requests. Half of our site was\ninaccessible - Nick on our team noticed within 60 seconds of deploying.\nPreviously, this would have taken us half an hour to roll back or longer to\nfix forward, meaning real external engineers would be blocked from reviewing\nor merging code. What before would have been a post-mortem-worthy incident was\nnow a minor distraction.\n\nWe calmly navigated to AWS and paused our code pipeline, disabling automated\npromotion from staging to production. With deployments paused, we could take\nall the time we needed to debug, fix, and re-deploy to staging without\nstressing that external users would be having a bad time. Folks immediately\nfelt the benefit.\n\nSubsequently, we found ourselves pausing the deployment pipeline for\naccidental regression about one-to-two times a week. Our rate of production\nregression dropped by three-quarters, and engineers were able to continue\niterating just as fast as before.\n\nWe are heavy users of the pause-deployments capability...\n\n## Early concerns with our staging bake period\n\nI\u2019d be lying if I said everything was perfect off the bat.\n\n### Waiting an hour\n\nTeammates initially complained about the extra hour they needed to wait to see\ntheir changes out in the wild. Their fears were slowly assuaged as folks\nshifted their focus to the staging environment, which still received new\nbuilds just as quickly as before.\n\n### Two DBs to migrate\n\nFolks also were initially annoyed at needing to maintain two environments. In\nparticular, Postgres migrations became a spot of possible drift - someone\nmight apply a DB schema migration to staging but forget to apply it to prod\nand vice-versa. Also, to migrate both was net-more work than previously\nrequired. We considered building some auto-migration system, but felt there\nwere too many edge cases in our DB to ever guarantee safety there. In fact,\nI\u2019ve come to see running migrations twice as more of a feature than a bug.\nWhile doing so indeed takes more work, it gives engineers a chance to test out\napplying a risky migration to the site before repeating in production. It only\ntook a few scary DB operations before the sentiment on the team shifted to\nappreciating having an additional DB to stage migrations on.\n\n### Pausing deployments for too long\n\nBeing able to pause the deployment pipeline isn\u2019t a perfect catch-all. While\nit buys time to fix a regression, it also blocks folks across the team from\nshipping new code. Because pipelines are linear, no one can get new changes\nout while an issue is being triaged, meaning it\u2019s somewhat unhealthy to lock\nfor more than 12 hours. For one, you block unrelated bug fixes from deploying,\nwhich makes it difficult to handle two different incidents at the same time.\nSecondly, locking deployments for too long risks unleashing a tidal wave of\nchanges onto production. Any regressions beyond this point become hard to\ncorrelate back to a specific PR as the accumulated changeset grows to be\nunwieldy.\n\n### Maintaining realistic data on staging\n\nLastly, we were initially concerned that seeding test data into our staging\nenvironment would be a tricky maintenance burden. In practice, this proved to\nbe no issue at all. Because Graphite as a service does not share data across\norganizational boundaries, we could simply shift all our internal usage to the\nstaging environment and populate it with an organic, long-lived dataset for\njust our org.\n\n## Later improvements to our deployment process\n\nOver the following year, the team improved upon my initial setup. First, we\nadded an \u201cemergency deployment pipeline\u201d that could be manually triggered.\nThis pipeline would build the latest code artifact but skip both staging and\nbake stages, instead deploying straight to production as fast as possible. We\nonly trigger this pipeline on rare occasions, but it\u2019s nice to have a break-\nglass way of fixing forward as quickly as possible.\n\nSecondly, we added a manual \u201cskip-bake\u201d command that would call AWS and\nautomatically finish whatever bake period was running. This command has proven\nuseful in times when an engineer is confident that the staging environment is\nhealthy and simply wants to roll their change out to all users faster than\nwould otherwise be possible.\n\nThirdly, we added code to programmatically lock staging-to-prod promotions\noutside of weekdays, 9-6 working hours. This small change has saved more on-\ncall sleep than anything else we\u2019ve done. Best of all, deployments continue\nreleasing to staging on weekends or nights, which helps off-hour engineers\nfeel like they can keep releasing code without endangering users.\n\nLastly, we updated our employee-only page within Graphite to include GUI-based\ndeployment controls. We added the ability to see what SHA each environment was\nrunning on and a button to lock deployments during an incident. These changes\nadded quality-of-life improvements to on call where locking our deployment\npromotion became muscle memory each time we suspected a regression had\ndeployed.\n\n## Inspiration from Airbnb\n\nI take no credit for being the first to invent a staging bake period, nor even\nself-discovering the idea. Rather, everything I\u2019ve built at Graphite has been\ninspired by my time working on Airbnb\u2019s Continuous Delivery team. There, I\nworked to help migrate thousands of microservices onto the open-source\nSpinnaker and create a default pattern for all deployments to release with\nautomated canary analysis. While not every service used this pattern, the\nmajority adopted it, and I witnessed firsthand the major reduction in\nincidents. You can read more about the amazing work my old teammates completed\nhere.\n\nThat being said, not everything has been the same between Airbnb and Graphite.\nWhile Airbnb had a cross-service staging environment, there was no continuous\nhuman traffic on it. That made test data a real struggle while I was there. We\ntried various processes of automatically generating data and mirroring\nproduction traffic, but in practice, everything we tried at Airbnb paled in\ncomparison to the steady stream of dogfood activity I see today at Graphite.\n\nWhile Graphite\u2019s deployments have Airbnb beat when it comes to staging\ntraffic, Airbnb had a more sophisticated blue-green prod rollout. Without\nrealistic staging traffic, gradual canary releases to production became\nimportant. Each production deployment would gently ramp traffic up to the new\nrelease before cutting over, and Spinnaker would call Datadog\u2019s API to monitor\nfor any spikes in errors. If a pre-specified metric regressed, Spinnaker would\nautomatically halt the production rollback and cut traffic back to the\npreviously safe service version. We don\u2019t have this sophistication at Graphite\n(yet) - if we don't spot a regression through our organic staging usage within\nan hour, we still promote the bug to prod and wreak havoc on unwitting users.\n\n## How you can implement your own staging bake\n\nIf you\u2019ve enjoyed my story about how we implemented a staging bake at Graphite\nand are interested in doing something similar on your team. Here\u2019s what you\u2019ll\nneed:\n\n### Get a pipeline\n\nFirst off, you\u2019ll need some kind of deployment pipeline. Automatically\ndeploying the top of your main branch to your servers is not enough. You need\na system that allows you to build an artifact and progress it through a\nsequence of stages. You need that system to support manual pausing, waiting,\nand resuming, as well as the ability to continue deploying to staging even if\nthe prod-promotion transition is paused. You could hand code this system or\nself-host Spinnaker - though these are options that are too time-intensive for\na smaller startup. In my experience, the only pre-built tool I\u2019ve found that\nfits my needs is AWS\u2019 CodePipeline, though there might be something better out\nthere. If you know of a good one, please let me know!\n\n### Get data\n\nSecondly, you need a way to populate your staging environment with realistic\ndata. There are three options I know of:\n\n  1. Write custom logic to generate fake data in the environment\n\n  2. Clone and sanitize production data\n\n  3. Have some set of real people living on staging creating the data.\n\nIn my opinion, the third option is the best, followed by option two. I\u2019d\nrecommend against the first option - creating test data is a Sisyphean\nmaintenance burden that never quite lives up to the real thing. Back in 2018,\nI created a system for generating fake test data at Airbnb, and I was never\nquite satisfied with the outcome.\n\n### Get traffic\n\nThirdly, you need activity on the staging database. Having data there is not\nenough; you need something to trigger user flows and peck at APIs. At\nGraphite, this is our own engineering team using our application to create all\nour own PRs, reviews, and merges. At Airbnb, it was a mixture of API traffic\nreplayed against specific services, coupled with begging engineers to \u201cpoke\nstaging\u201d and make sure their most recent merge looked functional. In someone\nelse\u2019s application, it might be QA testers or even a free tier of users. If\npossible, I\u2019d strongly recommend finding a way for your traffic to be real-\ntime humans.\n\n### Monitor for issues\n\nLastly, you need a way to detect and alert on regressions. At Airbnb, this was\nmostly done through Datadog monitors and automated server monitoring for\nstatistical regressions. At Graphite, this is an engineer on our team who\nnoticed that the comment button no longer works and reported it on our\ninternal Slack. We also have the added benefit that the person spotting the\nregression is the same person who\u2019s qualified to pause the pipeline and debug\nthe issue - though I respect that not all products benefit from being so\nthoroughly dogfooded by the engineering team creating them.\n\nIf you can think of an answer to each of these requirements, I\u2019d strongly\nrecommend you consider implementing a staging bake. The upfront cost of\nsetting one up will pay back tenfold, and anecdotally, I can assert that the\nmaintenance cost is lower than any other approach I know to catch regressions.\n\n## Reflections\n\nChoosing to build a staging bake process at Graphite took me longer than it\nshould have. I was more afraid of drift than I should have been. I was worried\nthat the maintenance cost of maintaining dual Postgres instances, each needing\nthe same schema migrations, would become cripplingly annoying. I wondered if\nengineers on the team would ever become comfortable with the added wait time.\nIn the end however, none of these things were true, and I\u2019d say every member\nof our engineering team appreciates the staging-bake\u2019s existence.\n\nRelated posts\n\n#Engineering\n\nWhy we use AWS instead of Vercel to host our Next.js app\n\nNovember 28, 2023\n\nGreg Foster\n\n#Engineering\n\nThe core principles of building a good AI feature\n\nMarch 21, 2024\n\nGreg Foster\n\n#Engineering\n\nOnboarding roulette: deleting our employee accounts daily\n\nMarch 14, 2024\n\nGreg Foster\n\n\u00a9 2024\n\nProduct\n\nFeaturesPricingDocsCustomers\n\nCompany\n\nBlogCareersContact us\n\nResources\n\nCommunityPrivacy & securityTerms of serviceStacking workflow\n\nDevelopers\n\nStatusGitHub\n\n", "frontpage": false}
