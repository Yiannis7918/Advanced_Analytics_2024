{"aid": "39965028", "title": "Architecture Decisions in Neon \u2013 A Serverless Postgres Service", "url": "https://neon.tech/blog/architecture-decisions-in-neon", "domain": "neon.tech", "votes": 1, "user": "shenli3514", "posted_at": "2024-04-08 00:04:44", "comments": 0, "source_title": "Architecture decisions in Neon - Neon", "source_text": "Architecture decisions in Neon - Neon\n\n\u25b2 Vercel Integration now GA - Create a database branch for every preview\ndeployment, automatically.Learn here\n\nNeon\n\n...Log InSign Up\n\nEngineeringJul 08, 2022\n\n# Architecture decisions in Neon\n\nDecisions we made on Neon architecture\n\nThe idea behind Neon is to create a new serverless Postgres service with a\nmodern cloud-native architecture. When building for the cloud it usually is a\ngood idea to separate storage and compute. For operational databases such\ndesign was first introduced by AWS Aurora ^1, followed by many others ^2 3,\nhowever none of the implementations were open source and native to Postgres.\n\nWe wanted to make Neon the best platform to run Postgres on. As we started to\nfigure out the details we needed to understand what exactly the architecture\nshould look like for an OLTP cloud database. We also knew that we couldn\u2019t\ndeviate from Postgres. People choose Postgres for many reasons. It\u2019s open\nsource, feature-rich, and has a large ecosystem of extensions and tools. But\nincreasingly, it\u2019s simply the default choice. There are a lot of databases out\nthere with different strengths and weaknesses, but unless you have a\nparticular reason to pick something else, you should just go with Postgres.\nTherefore, we don\u2019t want to compete with Postgres itself or maintain a fork.\nWe understood that Neon would only work in the market if it doesn\u2019t fork\nPostgres and gives users 100% compatibility with their apps written for\nPostgres.\n\nSo before we wrote a single line of code, we had some big upfront decisions to\nmake on the architecture.\n\n## Separating storage and compute\n\nThe core idea of an Aurora-like architecture of separation of storage and\ncompute is to replace the regular filesystem and local disk with a smart\nstorage layer.\n\nSeparating compute and storage allows you to do things that are difficult or\nimpossible to do otherwise:\n\n  * Run multiple compute instances without having multiple copies of the data.\n  * Perform a fast startup and shutdown of compute instances.\n  * Provide instant recovery for your database.\n  * Simplify operations, like backups and archiving, to be handled by the storage layer without affecting the application.\n  * Scale CPU and I/O resources independently.\n\nThe first major decision for us was if we should just use a SAN or an off-the-\nshelf distributed file system. You can certainly get some of these benefits\nfrom smart filesystems or SANs, and there are a lot of tools out there to\nmanage these in a traditional installation. But having a smart storage system\nthat knows more about the database system and the underlying infrastructure of\nthe cloud provider makes the overall system simpler, and gives a better\ndeveloper experience. We were aware of Delphix \u2013 a company that is built on\nthe premise of providing dev and test environments for database products using\nzfs. If we took a similar approach due to the fact that we don\u2019t control the\nfilesystem tier, it would be hard to efficiently integrate it with the cloud\nand result in a clunky and expensive solution. We could still sell it to large\nenterprises, but we knew we could do better. So the first decision was made:\nno SANs, no third party filesystems. Let\u2019s build our own storage from the\nfirst principles.\n\n## Storage Interface\n\nWe started to think about what the interface should be between compute and\nstorage. Since we have many Postgres hackers on the team, we already knew how\nit works in vanilla Postgres. Postgres generates WAL (write ahead log) for all\ndata modifications, and the WAL is written to disk. Each WAL record references\none or more pages, and an operation and some payload to apply to them. In\nessence, each WAL record is a diff against the previous version of the page.\nAs Postgres processes a WAL record it applies the operation encoded in the WAL\nrecord to the page cache, which will eventually write the page to disk. If a\ncrash occurs before this happens, the page is reconstructed using the old\nversion of the page and the WAL.\n\nPostgres architecture gave us a hint of how to integrate our cloud native\nstorage. We can make Postgres stream WAL to Neon storage over the network and\nsimilarly read pages from Neon storage using RPC calls. If we did that,\nPostgres changes would be minimal and we can even hope to push them upstream.\n\nIt was clear that we will need to have a consensus algorithm for persisting\nthe WAL \u2013 the database is the log, it has to be incredibly robust. And it was\nalso clear that we need to organize pages so that we can quickly return them\nwhen requested by Postgres. What was not clear was if we should have two\nservices: one for WAL and one for serving pages or one that combines all of\nit. Aurora has one, SQL Server, which came later, has two. There was a\ndecision to make.\n\n## Separating Page servers and the WAL service\n\nOne early decision was to separate the WAL and page service. The WAL service\nconsists of multiple WAL safekeeper nodes that receive the WAL from Postgres,\nand run a consensus algorithm. The consensus algorithm ensures durability,\neven if one of the safekeeper nodes is down. It also ensures that only one\nPostgres instance is acting as the primary at any given time, avoiding split-\nbrain problems. Pageservers store committed WAL and can reconstruct a page at\nany given point of WAL on request from the compute layer.\n\nSeparating the WAL service has several advantages:\n\n  * The WAL service and the page servers can be developed independently and in parallel.\n  * It is easier to reason about and verify the correctness of the consensus algorithm when it is a separate component.\n  * We can use hardware optimized for different purposes efficiently; the I/O pattern and workload of the safekeepers is very different from the page servers \u2013 one is append-only, and the other one is both read, write, and update.\n\n## Relationship between compute and pageservers\n\nDoes one compute only talk to one pageserver or should we spread out pages\nfrom one database across multiple pageservers? Also, does one pageserver only\ncontain data for one or many databases? The latter question is a simple one.\nWe need to build multi-tenancy to support a large number of small databases\nefficiently. So one pageserver can contain pages from many databases.\n\nThe former question is a trade-off between simplicity and availability. If we\nspread database pages across many pageservers, and especially if we cache the\nsame page on multiple page servers, we can provide better availability in case\na page server goes down. To get started, we implemented a simple solution with\none pageserver, but will add a pageserver \u201csharding\u201d feature later to support\nhigh availability and very large databases.\n\n## Treat historical data the same as recent data\n\nThe most straightforward model for the page servers would be to replay the WAL\nas it is received, to keep an up-to-date copy of the database \u2013 just like a\nPostgres replica. However, replicas connected to the storage system can lag\nbehind the primary, and need to see older versions of pages. So at least you\nneed some kind of a buffer to hold old page versions, in case a read replica\nrequests them. But for how long? There\u2019s no limitation on how far behind a\nread replica can lag.\n\nThen we started to think about WAL archiving, backups and Point-in-Time\nRecovery (PITR). How are those things going to work in Neon? Do we need to\nbuild them as separate features or can we do better? Could the storage handle\nall of those?\n\nPITR is a standard feature in most serious OLTP installations. The canonical\nuse case for PITR is that you accidentally drop a table, and want to restore\nthe database to the state just before that. You don\u2019t do PITR often, but you\nwant to have the capability. To allow PITR, you need to retain all old page\nversions in some form, as far back as you want to allow PITR. Traditionally,\nthat\u2019s done by taking daily or weekly backups and archiving all the WAL.\n\nYou don\u2019t do PITR often, because it has traditionally been a very expensive\noperation. You start from the last backup and replay all the archived WAL to\nget to the desired point in time. This can take hours. And if you pick the\nwrong point to recover to, you have to start all over again.\n\nWhat if PITR was a quick and computationally cheap operation? If you don\u2019t\nknow the exact point to recover to, that\u2019s OK; you can do PITR as many times\nas you need to. You could use it for many things that are not feasible\notherwise. For example, if you want to run an ad hoc analytical query against\nan OLTP database, you could do that against a PITR copy instead, without\naffecting the primary.\n\nIf you have a storage system that keeps all the old page versions, such\noperations become cheap. You can query against an older point in time just the\nsame as the latest version.\n\nWe decided to embrace the idea of keeping old page versions, and build the\nstorage system so that it can do that efficiently. It replaces the traditional\nbackups and the WAL archive, and makes all of the history instantly\naccessible. The immediate question is \u201cBut at what cost\u201d? If you have a PITR\nhorizon of several weeks or months, that can be a lot of old data, even for a\nsmall database. We needed a way to store the old and cold data efficiently and\nthe solution was to move cold and old data to cloud object storage such as S3.\n\n## Leverage cloud object storage\n\nThe most efficient I/O pattern is to write incoming data sequentially and\navoid random updates of old data. In Neon, incoming WAL is processed as it\narrives, and indexed and buffered in memory. When the buffer fills up, it is\nwritten to a new file. Files are never modified in place. In the background,\nold data is reorganized by merging and deleting old files, to keep read\nlatency in check and to garbage collect old page versions that are no longer\nneeded for PITR. This design was inspired by Log-Structured Merge-trees.\n\nThis system, based on immutable files, has a few important benefits. Firstly,\nit makes compression easy. You can compress one file at a time, without having\nto worry about updating parts of the file later. Secondly, it makes it easy to\nscale the storage, and swap in and out parts of the database as needed. You\ncan move a file to cold storage, and fetch it back if it\u2019s needed again.\n\nNeon utilizes cloud object storage to make the storage cost-efficient and\nrobust. By relying on object storage, we don\u2019t necessarily need multiple\ncopies of data in the page servers, and we can utilize fast but less reliable\nlocal SSDs. Neon offloads cold parts of the database to object storage, and\ncan bring it back online when needed. In a sense, the page servers are just a\ncache of what\u2019s stored in the object storage, to allow fast random access to\nit. Object storage provides for the long-term durability of the data, and\nallows easy sharding and scaling of the storage system.\n\n## One year later\n\nWe built a modern, cloud-native architecture that separates storage from\ncompute to provide an excellent Postgres experience. Different decisions would\nhave made some things easier and others harder, but so far, we haven\u2019t\nregretted any of these choices. The immutable file format made it\nstraightforward to support branching, for example, and we have been able to\ndevelop the page server and safekeeper parts fairly independently, just like\nwe thought. You can get early access to our service and experience the\nbenefits directly.\n\n1\\. A. Verbitski et al., \u201cAmazon Aurora,\u201d Proceedings of the 2017 ACM\nInternational Conference on Management of Data. ACM, May 09, 2017 [Online].\nAvailable: https://dx.doi.org/10.1145/3035918.3056101\u21a9\n\n2\\. P. Antonopoulos et al., \u201cSocrates,\u201d Proceedings of the 2019 International\nConference on Management of Data. ACM, Jun. 25, 2019 [Online]. Available:\nhttps://www.microsoft.com/en-us/research/uploads/prod/2019/05/socrates.pdf\u21a9\n\n3\\. W. Cao et al., \u201cPolarDB Serverless,\u201d Proceedings of the 2021 International\nConference on Management of Data. ACM, Jun. 09, 2021 [Online]. Available:\nhttp://dx.doi.org/10.1145/3448016.3457560\u21a9\n\n### Posted by\n\nHeikki LinnakangasCo-Founder at Neon\n\n### More articles\n\n  * # Build and Deploy a Global Serverless Nuxt SSR App with Cloudflare Hyperdrive and Postgres\n\nStephen Siegert\n\n  * # Why You Want a Database That Scales to Zero\n\nBryan Clark\n\n  * # Performance tips for Neon Postgres\n\nCarlota Soto\n\nShare:\n\nShare:\n\n## Subscribe to receive our latest updates\n\n## More from Neon\n\n  * Community\n\n# Build and Deploy a Global Serverless Nuxt SSR App with Cloudflare Hyperdrive\nand Postgres\n\nStephen SiegertApr 05, 2024\n\n  * Community\n\n# Why You Want a Database That Scales to Zero\n\nBryan ClarkApr 05, 2024\n\n  * Postgres\n\n# Performance tips for Neon Postgres\n\nCarlota SotoApr 04, 2024\n\nNeonAll systems operational\n\nMade in SF and the World\n\nCopyright C 2022 \u2013 2024 Neon, Inc.\n\nCompany\n\n  * About us\n  * Careers\n  * Partners\n  * Case studies\n  * Trust\n  * Pricing\n  * Contact Sales\n\nResources\n\n  * AI\n  * Blog\n  * Docs\n  * Changelog\n  * Demos\n  * Support\n  * Security\n\nCommunity\n\n  * X\n  * LinkedIn\n  * GitHub\n  * Discord\n  * Discourse\n  * YouTube\n\nLegal\n\n  * Privacy Policy\n  * Terms of Service\n  * DPA\n  * Subprocessors List\n  * Privacy Guide\n  * Cookie Policy\n  * Business Information\n\nWe use cookies to improve our services. Learn more in our Cookie Policy.\n\n", "frontpage": false}
