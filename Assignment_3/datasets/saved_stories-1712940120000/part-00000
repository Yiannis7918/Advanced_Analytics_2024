{"aid": "40011129", "title": "AI Accelerators: The Cambrian Explosion", "url": "https://thechipletter.substack.com/p/ai-accelerators-the-cambrian-explosion", "domain": "thechipletter.substack.com", "votes": 1, "user": "rbanffy", "posted_at": "2024-04-12 10:39:24", "comments": 0, "source_title": "AI Accelerators: The Cambrian Explosion", "source_text": "AI Accelerators: The Cambrian Explosion - by Babbage\n\n# The Chip Letter\n\nShare this post\n\n#### AI Accelerators: The Cambrian Explosion\n\nthechipletter.substack.com\n\n# AI Accelerators: The Cambrian Explosion\n\n### A sample of the designs that followed Google's first Tensor Processing\nUnit.\n\nBabbage\n\nApr 07, 2024\n\n\u2219 Paid\n\n28\n\nShare this post\n\n#### AI Accelerators: The Cambrian Explosion\n\nthechipletter.substack.com\n\n8\n\nShare\n\nThe next decade will see a Cambrian explosion of novel computer architectures,\nmeaning exciting times for computer architects in academia and in industry.\n\nJohn L. Hennessy and David A. Patterson (Turing Lecture, Communications of the\nAssociation for Computing Machinery, Feb 2019)\n\nThis post explores this Cambrian Explosion in hardware AI accelerators,\nperhaps aiming to reproduce some of the excitement that David Patterson refers\nto!\n\nPlease note that this is a long post (printed out it\u2019s more than 50 pages) so\nto read it all you\u2019ll need to view in your browser.\n\nThe complete post, with seventeen extra accelerator architectures, is for\npremium subscribers. Click on the button below to upgrade. Thank you!\n\nIn Google\u2019s First Tensor Processing Unit : Architecture we included a quote\nfrom one of the papers published by members of Google\u2019s original TPU team:\n\n> Drawing an historical analogy to Helen of Troy\u2014\u201cthe face that launched a\n> thousand ships\u201d\u2014we say tongue-in-cheek that TPU v1 \u201claunched a thousand\n> chips.\u201d\n\n\u2018A thousand chips\u2019 is obviously an exaggeration but the decade following the\nlaunch of Google\u2019s TPU v1 in 2015 has certainty seen a lot of activity in the\nfield of machine learning, and more specifically deep learning, accelerators.\n\nThis post aims to give a broad overview of activity in the field over the\ndecade 2015 from 2023, by listing the most prominent accelerators developed\nover this period. There has been too much going on to claim to offer a\ncomplete list, with less emphasis on designs from outside the U.S. than is\nprobably merited. I\u2019m also sure that there are some architectures that are\nstill \u2018in stealth\u2019 and not publicly described. There also isn\u2019t space to go\ninto depth on any single architecture. Instead we\u2019ll provide just a few lines\non each architecture with links for further viewing and reading. I\u2019ve also\nsaved the additions made to established CPU / GPU ranges, such as Tensor cores\nin Nvidia GPUs, for later posts. I\u2019ve omitted the projects that are unlikely\nto be commercialised and designs mainly focused on inference at \u2018the edge\u2019.\n\nI had a couple of objectives in writing this post.\n\n  * To provide a (hopefully) helpful starting point for further exploration of AI accelerator hardware. To help with that exploration, there are links to presentations on each of the architectures listed.\n\n  * To show just how much activity there has been on the development of AI accelerator hardware over the last decade or so.\n\nIt\u2019s useful to group the activity in this field into a small number of\ncategories:\n\n  * TPU v1 and its Application Specific Integrated Circuit (ASIC) successors developed within Google;\n\n  * ASIC\u2019s developed by other hyperscalers or by established semiconductor or adjacent firms (such as Intel and Tesla);\n\n  * ASIC\u2019s developed by startups;\n\n  * Machine learning accelerators added to existing CPU / GPU designs.\n\nThere is some overlap between these categories. For example, Intel has\nacquired multiple startups over this period. Some of the ASICs use established\nCPU ISAs (most notably RISC-V) extensively.\n\nOmitting GPU designs leaves Nvidia, as the 800 pound gorilla of accelerated\nmachine learning, out of the picture. One can sometimes feel the shadow of\nNvidia\u2019s presence in these designs, most obviously in the efforts by the\nhyperscalers to develop their own ASICs, which are probably, in large part,\nattempts to escape from (almost complete) dependency on Nvidia.\n\nThese ASIC designs have one thing in common. They all accelerate matrix\nmultiplication using, from one (e.g. Google TPU v1) to very many, matrix\nmultiply units. They differ in how these units are controlled, connected\ntogether, how large they are, their memory access, their software stacks and\nthe business models for the companies that make them.\n\n> The chips / firms we\u2019ll mention include : Google TPU v1-v5, Amazon\n> Inferentium and Trainium, Microsoft Maia, Meta MTIA, Intel Nervana, Intel\n> Gaudi, Alibaba Hanguang, Baidu Kunlun, Esperanto, Cerebras, Graphcore,\n> Tenstorrent, Groq, SambaNova, IBM, Encharge AI, Moffett AI and more.\n>\n> That\u2019s a lot of chips but I know that it\u2019s an incomplete list. Do let me\n> know of interesting designs that I\u2019ve missed!\n\nNote that some of the most interesting designs are from the startups listed at\nthe end of the post. It\u2019s well worth browsing to the end.\n\nShare\n\nIt\u2019s striking that almost all of this activity started well before the latest\nexcitement (or hype, depending on your taste) with generative AI triggered by\nthe launch of ChatGPT in November 2022. With further influx of interest and\ninvestment it seems certain that the list will grow and the capabilities of\nmany of the designs here will expand substantially.\n\nI suspect that most of these architectures won\u2019t survive though. Some are\nalmost certainly already doomed. The ones that survive are likely to be the\nones that have a strategy to avoid the steamroller that is Nvidia, just like\nArm avoided Intel in the RISC \u2018Cambrian Explosion\u2019 in the late 1980s and\n1990s.\n\nIt\u2019s just a subjective feeling, but it seems to me that we may have already\nmoved on from the era that saw the conception and launch of these designs. In\n2024 the stakes are higher and the focus has changed, for example to what can\nbe achieved at Data Center level, as\n\nDoug O'Laughlin\n\nhas described in his recent post.\n\nFabricated Knowledge\n\nThe Data Center is the New Compute Unit: Nvidia's Vision for System-Level\nScaling\n\nRead more\n\n5 days ago \u00b7 78 likes \u00b7 6 comments \u00b7 Doug O'Laughlin\n\nThe focus of the content linked to in this list is on architectures, so some\nbrief words on three other key aspects of these designs: fabrication,\ndeployment and software stacks.\n\n##### Fabrication\n\nAlmost all of the designs listed here are fabricated by TSMC. Google\u2019s TPU v1\nused a 28nm process. More recent designs have typically used TSMC N7 or more\nadvanced.\n\nWe have seen that (what is now) Broadcom provided substantial assistance to\nGoogle in building the TPU v1. They continue to support Google\u2019s TPU efforts\nand also support other designs, including from other hyperscalers.\n\n##### Software\n\nOn software the picture is complex and patchy. Most support use of one or both\nof Tensorflow and PyTorch. There isn\u2019t space to go into detail on the software\nstack for each design but it\u2019s interesting to see the varying emphasis on\nsoftware in the presentations and supporting material.\n\n##### Deployment\n\nWhere these designs are deployed, if at all, isn\u2019t always clear. I\u2019ve tried to\nhighlight where they are available to access on public clouds.\n\nSome points that jump out from this overview:\n\n  * Interest in AI accelerators from Venture Capital has been substantial, as can be seen from the number of startups on this list.\n\n  * Many, but not all, come from Silicon Valley. There has, of course, been strong interest from China for both commercial and political reasons. There are also designs from Toronto and the UK along with other locations in the U.S.\n\n  * Many of these designs have attracted industry veterans such as Jim Keller at Tenstorrent and of course a number of members of the original Google TPU team. I suspect it\u2019s not just the (potential) financial rewards but also the attractions of creating innovative designs.\n\nThis post links to over 10 hours of presentations on these designs. If you\nhaven\u2019t got time to watch them all then I recommend Jim Keller\u2019s relatively\nshort talk for a useful introduction to the issues and trade-offs in designing\nan AI ASIC, albeit with obvious bias towards the approach that his team at\nTenstorrent have taken.\n\nIf you haven\u2019t read them already and aren\u2019t familiar with this area then it\u2019s\nproblem helpful to read the earlier posts on Google\u2019s TPU v1 first.\n\n#### Google's First Tensor Processing Unit : Origins\n\nBabbage\n\n\u00b7\n\nFeb 25\n\nRead full story\n\n#### Google's First Tensor Processing Unit : Architecture\n\nBabbage\n\n\u00b7\n\nMar 24\n\nRead full story\n\nBefore we get started I do want to correct any possible misapprehension that\nthe development of hardware machine learning accelerators only started in\n2015. We can go back all the way to the Mark 1 Perceptron Machine, first\nannounced to the world in 1958.\n\nThe Mark 1 Perceptron, being adjusted by Charles Wightman (Mark I Perceptron\nproject engineer)\n\nOver the following decades there have been many attempts, using a wide variety\nof approaches, to the challenge of accelerating the computations required to\nsimulate neural networks. Many of the designs here adopt approaches that\naren\u2019t new. But mirroring the increase in interest in deep learning we have\ncertainly seen a \u2018Cambrian explosion\u2019 of interest in hardware since 2015.\nLet\u2019s start where this latest surge started: at Google.\n\nShare\n\n###\n\nGoogle\n\n##### TPUs V1-V5\n\nThe TPU v4 package (ASIC in center plus 4 HBM stacks) and printed circuit\nboard (PCB) with 4 liquid-cooled packages. - By Norman P. Jouppi, George\nKurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil,\nSuvinaySubramanian, Andy Swing, Brian Towles, Cliff Young, Xiang Zhou, Zongwei\nZhou, and David Patterson -\nhttps://arxiv.org/ftp/arxiv/papers/2304/2304.01433.pdf, CC BY 4.0,\nhttps://commons.wikimedia.org/w/index.php?curid=130427664\n\nWe\u2019ve already covered TPU v1 in some detail in two posts. In this post we\u2019ll\njust provide brief details of TPU v1 and its successors along with links to\npresentations that provide a lot more detail:\n\n##### TPU v1\n\n  * Introduced in 2015 (but announced in 2016)\n\n  * Inference only\n\n  * PCIe peripheral\n\n  * DDR3 Memory\n\n  * 256 x 256 Systolic Matrix Multiply Unit (8-bit integer multiply)\n\n##### TPU v2\n\n  * Introduced in 2017\n\n  * Inference and Training\n\n  * High Bandwidth Memory (HBM)\n\n  * Two Matrix Multiply Units per chip using floating point (bfloat16)\n\n##### TPU v3\n\n  * Introduced in 2018\n\n  * As TPU v2 but with more memory bandwidth, higher clock frequency\n\n  * Four Matrix Multiply Units per chip\n\n##### TPU v4 & TPU v4i\n\n  * Introduced in 2021\n\n  * Inference and Training (TPU v4) with liquid cooling or Inference (TPU v4i) with no liquid cooling\n\n##### TPU v5p (performance) & TPUv5e (economy)\n\n  * Introduced in 2023\n\n  * \u201cDesigned for performance, flexibility, and scale, TPU v5p can train large LLM models 2.8X faster than the previous-generation TPU v4. Moreover, with second-generation SparseCores, TPU v5p can train embedding-dense models 1.9X faster than TPU v4^2.\u201d Source\n\nFrom Peripherals to Supercomputers\n\nIt\u2019s interesting to see how focus in the TPU project has moved from creating a\nperipheral that can be attached to one of Google\u2019s existing servers to \u2018Cloud\nAI Supercomputers\u2019 that can work independently and combine thousands of chips\nwith shared memory and high-speed interconnect.\n\nTPUs, VLIW and RISC-V\n\nReaders may recall that TPU v1 used a CISC instruction set architecture with a\nsmall number of instructions.\n\nThe TPU has now changed to use a Very Long Instruction Word (VLIW) approach\nalong with modified RISC-V cores from SiFive.\n\nSource\nhttps://www.kisacoresearch.com/sites/default/files/presentations/1pdf._sifive_ai_presentation-\nkasanovic.pdf\n\nPresentations\n\nHot Chips 32 (TPU v2 & v3)\n\nSlides Hot Chips 32 (TPU v2 & v3)\n\nHot Chips 2023 (TPUv4)\n\nTPU v4 Paper\n\nTPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with\nHardware Support for Embeddings\n\nSemianalysis on TPU v5\n\nAs usual\n\nDylan Patel\n\nhas detailed analysis of the latest TPUs.\n\nSemiAnalysis\n\nTPUv5e: The New Benchmark in Cost-Efficient Inference and Training for <200B\nParameter Models\n\nDuring its Cloud Next 2023 event, Google has announced general availability of\nits latest AI chip, the TPUv5e (TPUv5 lite), and it is a game changer, due to\nthe performance/TCO that it brings for both Google and the new Cloud TPU\ncustomers. It is straight up a massive cost advantage for many external\nparties to train and inference models with less than 200 billion parameters...\n\nRead more\n\n7 months ago \u00b7 73 likes \u00b7 21 comments \u00b7 Dylan Patel and Aleksandar Kostovic\n\n###\n\nOther Hyperscalers and Established Firms\n\nWe\u2019ll start with the \u2018Hyperscalers\u2019, like AWS and Microsoft, along with other\nsemiconductor firms and established firms with an interest in machine\nlearning.\n\n####\n\nAmazon\n\n##### Inferentia & Trainium\n\n> AWS Inferentia accelerators are designed by AWS to deliver high performance\n> at the lowest cost in Amazon EC2 for your deep learning (DL) and generative\n> AI inference applications. (Amazon)\n\nAnnounced : 2018\n\nInference (Inferentia) and Training (Trainium)\n\nAvailable on AWS\n\nWebsites :\n\nhttps://aws.amazon.com/machine-learning/inferentia/\n\nhttps://aws.amazon.com/machine-learning/trainium/\n\nInferentia2 and Trainium Architectures\n\n> NeuronCore-v2 is the second generation of the NeuronCore engine, powering\n> the Trainium NeuronDevices. Each NeuronCore-v2 is a fully-independent\n> heterogenous compute-unit, with 4 main engines (Tensor/Vector/Scalar/GPSIMD\n> Engines), and on-chip software-managed SRAM memory, for maximizing data\n> locality (compiler managed, for maximum data locality and optimized data\n> prefetch).\n\nSources : Trainium and Inferentia\n\nArchitecture Highlights\n\n  * GPSIMD-Engine, which consists of eight fully-programmable 512-bit wide vector processors;\n\nPresentations :\n\n####\n\nMicrosoft\n\nMicrosoft\u2019s original approach to machine learning hardware was to use Field\nProgrammable Gate Arrays (FPGAs). Project \u2018Brainwave\u2019 implemented many of the\nfeatures we have discussed already, on Intel FPGAs, in Microsoft\u2019s cloud (see\nbelow for the Hot Chips 2017 presentation).\n\nThis approach changed last year when Microsoft announced Azure Maia.\n\n##### Azure Maia AI Accelerator\n\n> The company\u2019s new Maia 100 AI Accelerator will power some of the largest\n> internal AI workloads running on Microsoft Azure. Additionally, OpenAI has\n> provided feedback on Azure Maia and Microsoft\u2019s deep insights into how\n> OpenAI\u2019s workloads run oninfrastructure tailored for its large language\n> models is helping inform future Microsoft designs. (Microsoft)\n\nAnnounced : 2023\n\nInference and Training\n\nArchitectural Highlights\n\n  * Uses Microscaling Formats (MX) 8-bit data types\n\nPresentations :\n\nMaia Announcement\n\n> As a side note, announcing Maia didn\u2019t preclude inviting Jensen Huang to the\n> stage!\n\nFurther Reading on Maia\n\nThe Verge\n\nBrainwave Presentation (Hot Chips 2017)\n\nHot Chips Brainwave Slides\n\n####\n\nMeta\n\n##### MTIA\n\n> In 2020, we designed the first-generation MTIA ASIC for Meta\u2019s internal\n> workloads. This inference accelerator is a part of a co-designed full-stack\n> solution that includes silicon, PyTorch, and the recommendation models\n> (Meta).\n\nLaunched : 2023\n\nInference and Training\n\nWebsite : https://ai.meta.com/blog/meta-training-inference-accelerator-AI-\nMTIA/\n\nArchitecture Highlights\n\n  * Use of dual RISC-V cores - one of which with vector extension, coupled with dedicated matrix multiplication units.\n\nPresentations\n\n####\n\nAlibaba\n\n##### Hanguang 800\n\nAnnounced : 2019\n\nInference Only\n\nWebsite : https://www.alibabacloud.com/blog/announcing-hanguang-800-alibabas-\nfirst-ai-inference-chip_595482\n\nPresentations\n\nHot Chips 2020\n\nHot Chips 2020 Slides\n\n####\n\nBaidu\n\nAs with Microsoft Baidu started with an FPGA based accelerator before moving\nto an ASIC version of a similar architecture in 2020.\n\n##### Kunlun (1-3)\n\nAnnounced : 2020\n\nPresentations :\n\nHot Chips 2020\n\nHot Chips 2020 Slides\n\nAfter the paywall, premium subscribers get more information and links to\npresentations and papers on seventeen further deep learning accelerators.\n\nThe Chip Letter is a reader-supported publication. To receive new posts and\nsupport my work, consider becoming a free or paid subscriber.\n\n## This post is for paid subscribers\n\nAlready a paid subscriber? Sign in\n\n\u00a9 2024 The Chip Letter\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great writing\n\nShare\n\n", "frontpage": false}
