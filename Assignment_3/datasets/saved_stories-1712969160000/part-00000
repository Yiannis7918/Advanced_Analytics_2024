{"aid": "40016300", "title": "Dagster and SkyPilot: Orchestrating LLM training cost-effectively", "url": "https://dagster.io/blog/use-dagster-and-skypilot-to-orchestrate-cost-effective-ai-training-jobs", "domain": "dagster.io", "votes": 2, "user": "covi", "posted_at": "2024-04-12 18:44:36", "comments": 0, "source_title": "Use Dagster and SkyPilot to Orchestrate Cost-Effective AI Training Jobs | Dagster Blog", "source_text": "Use Dagster and SkyPilot to Orchestrate Cost-Effective AI Training Jobs | Dagster Blog\n\nJoin us on April 17th for the launch of Dagster + ... the next generation of\nDagster Cloud\n\nApril 11, 2024 \u2022 6 minute read \u2022\n\n# Use Dagster and SkyPilot to Orchestrate Cost-Effective AI Training Jobs\n\nName\n\n    Muhammad Jarir Kanji\nHandle\n\n    @muhammad\n\nAI is becoming increasingly essential for organizations, but the associated\ncosts can be prohibitive. GPU scarcity and the added complexity of standing up\ninfrastructure and managing AI training jobs are other limiting factors\npreventing smaller enterprises from fully participating in the AI revolution.\n\nAnother challenge is the awkward handoff between data engineering and ML\nteams, as both functions often work in silos and use different tooling. These\nissues can be addressed through better data platform design. To this end, we\nbelieve that a data platform should:\n\n  * Be heterogeneous: A data platform should seamlessly accommodate a diversity of user types (data engineers, data scientists, business stakeholders, etc.) using a variety of data storage, processing, and cloud technologies.\n  * Be monolithic: Teams generating and consuming data assets across the organization benefit from having a single pane of glass, leading to improved visibility, productivity, consistency, and alignment.\n  * Enable declarative flows: Imperative approaches can often result in additional complexity and cognitive overhead. Standardizing processes using a domain-specific language (DSL) can help tame some of that complexity and facilitate collaboration across teams.\n\nThese are the principles we adopt at Dagster Labs. But these goals also align\nwith the capabilities of SkyPilot, a powerful Sky Computing framework that\nenables resilient and cost-effective AI/ML training jobs across cloud\nenvironments and regions.\n\nThis article explores the cost-effective and efficient solution of using\nDagster and SkyPilot to orchestrate ML training jobs within a single data\nplatform. This combination abstracts the resource acquisition and job\nexecution through an intuitive declarative DSL.\n\nCritically, this solution allows data engineering to invite ML teams to bring\ntheir existing ML training and inference pipelines into Dagster and\norchestrate them with minimal code changes and without the need to learn\nDagster internals.\n\nExisting Dagster Cloud users can enhance their Serverless deployment with\nSkyPilot, allowing for seamless scalability and the ability to tap into\nadditional computing resources (like GPUs) to accelerate machine learning\nworkloads, without the additional overhead of spinning up your own\ninfrastructure and migrating to a Hybrid deployment.\n\n## What is SkyPilot and Sky Computing?\n\nSkyPilot is a framework that implements the Sky Computing paradigm, where\nworkloads can be transparently executed on one or more clouds, abstracting the\nprovision of resources and execution of arbitrary workloads across cloud\nvendors while automatically maximizing cost savings and availability for\nusers.\n\nIn doing so, SkyPilot reduces the barrier of entry to AI by not only allowing\nyou to find the cheapest vendor and region for resources but also making it\neasy to run managed jobs on spot instances, with automatic recovery after\npreemption. These features combined can yield both massive cost savings for\nyour business and also significantly reduce development time reworking\nexisting pipelines to work with other cloud vendors.\n\nAll of this takes place via a user-friendly DSL and, much like Dagster,\nSkyPilot boasts a great local development experience.\n\n## Project Overview\n\nFor this project, we\u2019ll be showing how you can fine-tune Gemma, the newest\nopen-source LLMs from Google. We\u2019ll use the Abirate/english_quotes dataset and\nfine-tune Gemma to mimic the quotability of literary geniuses like Oscar\nWilde.\n\n## Setup\n\nBefore we get started, here are a few preliminaries you\u2019ll need to satisfy:\n\n  * Step 1: Fork the project repo into your GitHub account.\n  * Step 2: Sign up for a Dagster Cloud Serverless account. While the demo can be run locally, this article will focus on deploying to Dagster Cloud.\n  * Step 3: Go through the Getting Started flow under the Import a Dagster project. Select the fork you created in Step 1 as your repository. (You may need to approve the Dagster integration in GitHub.)\n  * Step 4: The above step will create some files under .github/workflows/ in the repo. Edit both deploy.yml and branch_deployments.yml to disable fast deployments by making the following change and commit the change. This will deploy the Dagster project using Docker. SkyPilot requires openssh and rsync as native dependencies, which are installed in the Docker image by the dagster_cloud_post_install.sh script.\n\n    \n    \n    env: ... ENABLE_FAST_DEPLOYS: 'false' # This was originally true\n\nOnce the GitHub Actions workflow has finished running, the project will be\ndeployed to Dagster Cloud.\n\n  * Step 5: While SkyPilot allows you to orchestrate jobs on a variety of cloud providers (and even choose between them for the best deals), we\u2019ll be using AWS for this demo. To that end, please set up the minimal required permissions for SkyPilot by following the instructions here.\n\n    * If you want to experiment with using other cloud providers, you may also want to refer to the instructions on how to set up permissions for other cloud platforms and the API keys and other credentials SkyPilot needs to operate.\n    * If your cloud account is new, you may need to request a quota increase in order to provision GPU machines. You can do so by following the instructions here.\n  * Step 6: Create an S3 bucket to use for this example.\n  * Step 7: Create an account on Hugging Face, generate a read-only access token for your account, and agree to Google\u2019s terms and conditions for using Gemma on the model page here.\n  * Step 8: Go to the Deployment > Environment Variables tab in Dagster Cloud and set up the following variables:\n\n    * SKYPILOT_BUCKET with the name of the bucket you created in Step 6.\n    * HF_TOKEN with the token you got in Step 7.\n    * AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY with your AWS credentials.\n\n## Defining Tasks in SkyPilot\n\nBefore spinning up a training run, let\u2019s go over how you can define SkyPilot\njobs. SkyPilot allows you to use either a YAML file or its Python API for this\npurpose. We\u2019ll go over SkyPilot\u2019s DSL for defining jobs and go through the\ndifferent sections of the finetune.yaml file below.\n\nOne of SkyPilot\u2019s greatest strengths lies in abstracting the provision of\ncloud resources across multiple vendors. In the snippet below, we declare that\nwe want to run our job on AWS using any of the accelerators in the list.\nSkyPilot will then automatically search for instances with one of these\naccelerators, check their availability, and provision a cluster with the most\ncost effective option that satisfies your requirements.\n\n    \n    \n    resources: cloud: aws accelerators: {L4, A10g, A10, L40, A40, A100, A100-80GB} disk_tier: best\n\nWe set the values of the environment variables we want to propagate to the\nremote cluster using the env key. Some of the variables are set to an empty\nstring, because we will override them with the Dagster process that invokes\nSkyPilot.\n\n    \n    \n    envs: # The first three env vars are left empty and overwritten by the Dagster process. DAGSTER_RUN_ID: \"no-run\" # The ID of the Dagster run that triggered the job. HF_TOKEN: \"\" SKYPILOT_BUCKET: \"\" MAX_STEPS: 10 TERM: \"dumb\" NO_COLOR: 1\n\nSkyPilot lets you easily sync local files to the remote cluster, as well as\nmounting cloud storage onto the cluster. See the docs on Syncing Code and\nArtifacts on their website for more information.\n\nThe workdir key can be used to define the working directory for the setup and\nrun commands (covered below) and its contents will be synced to the remote\ncluster in the ~/sky_workdir directory. In this case, we want to sync the\nscripts subdirectory containing the lora.py script.\n\n    \n    \n    workdir: dagster_skypilot/scripts\n\nAdditionally, we are also mounting the S3 bucket we created earlier to the\n/artifacts path in the remote cluster. This is where we\u2019ll save intermediate\ncheckpoints and the trained model. If we use spot instances to run the task,\nSkyPilot can use these checkpoints to recover from preemptions/interruptions\nwithout losing progress.\n\n    \n    \n    file_mounts: /artifacts: source: ${SKYPILOT_BUCKET} mode: MOUNT\n\nThe setup command defines a setup script to be run when you launch the cluster\nfor the first time. This is useful for installing any dependencies and setting\nup your environment correctly, which is exactly what we use it for here.\n\n    \n    \n    setup: | conda activate gemma if [ $? -ne 0 ]; then conda create -q -y -n gemma python=3.10 conda activate gemma fi echo \"Installing Python dependencies.\" pip install -q -U bitsandbytes==0.42.0 pip install -q -U peft==0.8.2 pip install -q -U trl==0.7.10 pip install -q -U accelerate==0.27.1 pip install -q -U datasets==2.17.0 pip install -q -U transformers==4.38.1 pip install -q \"torch<2.2\" torchvision --index-url https://download.pytorch.org/whl/cu121\n\nFinally, the run command defines the main program that\u2019s run on every node in\nthe remote cluster. We use it to run the lora.py training script, which\ncontains the logic for training the Gemma model.\n\n    \n    \n    run: | conda activate gemma NUM_NODES=`echo \"$SKYPILOT_NODE_IPS\" | wc -l` HOST_ADDR=`echo \"$SKYPILOT_NODE_IPS\" | head -n1` # Turn off wandb WANDB_MODE=\"offline\" TERM=dumb NO_COLOR=1 torchrun \\ --nnodes=$NUM_NODES \\ --nproc_per_node=$SKYPILOT_NUM_GPUS_PER_NODE \\ --master_port=12375 \\ --master_addr=$HOST_ADDR \\ --node_rank=${SKYPILOT_NODE_RANK} \\ lora.py \\ --model_name_or_path google/gemma-7b \\ --save_steps 4 \\ --max_steps ${MAX_STEPS} \\ --output_dir /artifacts/${DAGSTER_RUN_ID}\n\nTake note of the fact that the output_dir is set to change based on the\nDAGSTER_RUN_ID. This ensures that the outputs of each run are stored in\nseparate sub-directories in the S3 bucket, instead of overwriting the files\nfrom the previous run.\n\nThe details of the lora.py training script are out of the scope of this\narticle and won\u2019t be discussed in detail, though you\u2019re welcome to go through\nthe code yourself.\n\n## Defining the Dagster Asset\n\nYou can interactively call the task defined above by running the following\ncommand from the terminal:\n\n    \n    \n    # Run this from the root of the project SKYPILOT_BUCKET=\"s3://<your-skypilot-bucket>\" HF_TOKEN=\"<your-token>\" \\ sky launch -c gemma dagster_skypilot/finetune.yaml \\ --env HF_TOKEN --env SKYPILOT_BUCKET\n\nHowever, we want to orchestrate the job using Dagster. Thankfully, SkyPilot\nprovides an easy-to-use Python API for triggering jobs, alongside the CLI.\nRunning the task defined in the YAML file is as simple as creating a software-\ndefined asset.\n\nWe first define a dagster.Config object with different parameters that we want\nto be configurable from within the Dagster UI. In this case, that\u2019s just the\nnumber of training steps and whether the task is run as a Managed Spot Job by\nSkyPilot (more on that later).\n\n    \n    \n    class *SkyPilotConfig(Config): \"\"\"A minimal set of configurations for SkyPilot. This is NOT intended as a complete or exhaustive representation of a Task YAML config.\"\"\" max_steps: int = Field( default=10, description=\"Number of training steps to perform.\" ) spot_launch: bool = Field( default=False, description=\"Should the task be run as a managed spot job?\" )\n\nWe then use this Config object in our software-defined asset, which reads the\ntask\u2019s YAML definition and overrides the required environment variables based\non their definitions in Dagster Cloud.\n\nNote that because HF_TOKEN is a secret, we don\u2019t want to have the real value\ndefined in the YAML file itself. Instead, we set the value as an environment\nvariable in Dagster Cloud and override the default values at runtime. We use\nthe same trick to dynamically populate the Dagster Run ID, the name of the\nbucket to be used, and the number of training steps at runtime.\n\n    \n    \n    @asset(group_name=\"ai\") def skypilot_model(context: AssetExecutionContext, config: SkyPilotConfig) -> None: # SkyPilot doesn't support reading credentials from environment variables. # So, we need to populate the required keyfiles. populate_keyfiles() skypilot_bucket = os.getenv(\"SKYPILOT_BUCKET\") # The parent of the current script* parent_dir = UPath(__file__).parent yaml_file = parent_dir / \"finetune.yaml\" with yaml_file.open(\"r\", encoding=\"utf-8\") as f: task_config = yaml.safe_load(f) task = sky.Task().from_yaml_config( config=task_config, env_overrides={ \"HF_TOKEN\": os.getenv(\"HF_TOKEN\", \"\"), \"DAGSTER_RUN_ID\": context.run_id, \"BUCKET_NAME\": skypilot_bucket, \"MAX_STEPS\": config.max_steps, }, ) task.workdir = str(parent_dir.absolute() / \"scripts\") ...\n\nHaving defined the task, we check the configuration to determine if the task\nshould be run as a managed spot job and launch the task.\n\n    \n    \n    @asset(group_name=\"ai\") def skypilot_model(context: AssetExecutionContext, config: SkyPilotConfig) -> None: ... try: if config.spot_launch: ... sky.spot_launch(task, name=\"gemma\") else: ... sky.launch(task, cluster_name=\"gemma\") context.log.info(\"Task completed.\") context.add_output_metadata(get_metrics(context, skypilot_bucket)) finally: teardown_all_clusters(context.log)\n\nIt\u2019s important to note here that in order to shut down any clusters it\nstarted, SkyPilot needs some metadata that it stores locally. If you\u2019re on\nDagster Cloud, however, the runner that an asset op executes on is ephemeral\nand will be shut down after the run is complete.\n\nTo ensure that an error in the op logic doesn\u2019t immediately kill the runner\nmachine and prevent us from shutting down the remote cluster, we wrap that\nlogic in a finally clause. In order to be absolutely sure you don\u2019t have idle\nresources that continue to cost you, though, it\u2019s a good idea to check the EC2\nDashboard for any running machines.\n\nFinally, because SkyPilot cannot directly return data from the remote cluster\nto the Python process, we save the final metrics to the S3 bucket (see the\nlora.py script for the details of the implementation). The get_metrics\nfunction reads the metrics file from S3 and logs them as metadata for the\nDagster run. This will be used to visualize the training metrics across runs.\n\n## Using Dagster to Orchestrate the SkyPilot Job\n\nTo run the job, go to your Dagster Cloud instance and select the\nskypilot_model asset in the Asset Graph. Then, hold the Shift key and click on\nthe Materialize Selected button. This will open the Launchpad and allow us to\npopulate the parameters we defined as part of SkyPilotConfig.\n\nFor now, leave the default values and click on Materialize. This will launch a\nnew run and show you the logs from the process.\n\nThe logs from SkyPilot and the remote cluster are sent to stdout and are not\nvisible under the Events pane. Click on the stdout pane to see them. Note that\nif you\u2019re using Dagster Cloud, they will not be visible until after the run\nfinishes; this should take 10 - 15 minutes. If you\u2019re running the job locally,\nthe stdout logs will start streaming immediately.\n\nOnce the run has completed, the training metrics are logged into the run\nmetadata in Dagster. We can use the Plots tab in the Asset Catalog page to\ncompare metrics across runs.\n\nI re-ran the same job for 20, 30, 40, and 50 max_steps, and as one might\nexpect, training for a larger number of epochs decreases the training loss.\n\nYou can use Dagster's robust metadata logging to similarly track other useful\nbits of information, such as which instance type SkyPilot chose for a given\nrun or how much the run cost.\n\n## Cutting Costs with Managed Spot Jobs\n\nOne other SkyPilot feature that we\u2019d like to call out here is managed spot\njobs. Cloud providers like AWS offer spot instances for up to 90% cheaper than\non-demand. Given how expensive GPU instances can be, using spot instances can\nresult in significant savings for your organization.\n\nThe catch, of course, is that the instance can be preempted (i.e., shut down)\nby AWS at any time in response to a surge in demand. SkyPilot helps you\nnavigate this uncertainty by managing the job for you. Not only does it find\nthe region with the best pricing (the same spot instance can be 2x cheaper in\nanother region) but if an instance is preempted, SkyPilot will also\nimmediately spin up a new cluster in another region or on another cloud and\ncontinue your training job with minimal loss of progress!\n\nLet\u2019s try doing another job and setting the spot_launch parameter in the\nLaunchpad to true. This will run the same task (with zero code changes\nrequired) as a spot job.\n\nComparing the logs between the earlier runs (above) and the spot job (below),\nwe can see that SkyPilot was able to find a spot instance with the same specs\nfor almost 3x cheaper than before!\n\n## Conclusion\n\nOrchestrating AI training jobs using Dagster and SkyPilot can help you:\n\n  * Execute AI training jobs in a cost-effective fashion with plug-in support for spot instances and automatic recovery from preemption.\n  * Benefit from declarative code and a simple DSL to quickly onboard ML teams and centralize orchestration with Dagster as your single pane of glass.\n  * Overcome the limitations of a Serverless deployment without migrating to a Hybrid deployment or spinning up your own clusters.\n\nLearn more about SkyPilot at their GitHub repo and docs. If you\u2019re interested\nin exploring more, make sure to fork the project repo and spin up a few\ntraining runs on Dagster Cloud.\n\nWe're always happy to hear your feedback, so please reach out to us! If you\nhave any questions, ask them in the Dagster community Slack (join here!) or\nstart a Github discussion. If you run into any bugs, let us know with a Github\nissue. And if you're interested in working with us, check out our open roles!\n\nFollow us:\n\nSign up for our newsletter\n\nSubscribe to the RSS feed\n\nStar us on GitHub\n\nFollow us on Twitter\n\nSubscribe to our YouTube Channel\n\nRead more filed under\n\nBlog Post\n\nApr 10, 2024\n\n#### The Data Engineering Impedance Mismatch\n\nA case for asset-oriented over workflow-oriented in data orchestration.\n\nName\n\n    Pete Hunt\nHandle\n\n    @floydophone\n\nApr 5, 2024\n\n#### Expanding the Dagster Embedded ELT Ecosystem with dltHub for Data\nIngestion\n\nWe now have an officially supported dlt integration.\n\nName\n\n    Colton Padden\nHandle\n\n    @colton\n\nApr 3, 2024\n\n#### Sling Out Your ETL Provider with Embedded ELT\n\nHow we saved $40k and gained better control over our ingestion steps.\n\nName\n\n    Nick Roach\nHandle\n\nJoin us on Slack\n\n#### Resources\n\nPlatform OverviewIntegrationsDocsBlogData Engineering GlossaryChangelogDagster\nVs. Others\n\n#### Dagster Cloud\n\nCloud OverviewEnterprisePricingStatusContact Sales\n\n#### Community\n\nCommunity OverviewImplementation PartnersUpcoming EventsBrowse Discussions\n\n#### Company\n\nAbout Careers Brand Assets\n\n#### Get updates delivered to your inbox\n\nDagster is an open-source project maintained by Dagster Labs.\n\nCopyright \u00a9 2024 Elementl, Inc. d.b.a. Dagster Labs. All rights reserved.\n\nThis site is protected by reCAPTCHA and the Google Privacy Policy and Terms of\nService apply.\n\n", "frontpage": false}
