{"aid": "40032262", "title": "Third-party testing as a key ingredient of AI policy", "url": "https://www.anthropic.com/news/third-party-testing?trk=feed_main-feed-card_feed-article-content", "domain": "anthropic.com", "votes": 1, "user": "andy99", "posted_at": "2024-04-14 16:26:23", "comments": 0, "source_title": "Third-party testing as a key ingredient of AI policy", "source_text": "Third-party testing as a key ingredient of AI policy \\ Anthropic\n\nPolicy\n\n# Third-party testing as a key ingredient of AI policy\n\nMar 25, 2024\u25cf18 min read\n\nWe believe that the AI sector needs effective third-party testing for frontier\nAI systems. Developing a testing regime and associated policy interventions\nbased on the insights of industry, government, and academia is the best way to\navoid societal harm\u2014whether deliberate or accidental\u2014from AI systems.\n\nOur deployment of large-scale, generative AI systems like Claude has shown us\nthat work is needed to set up the policy environment to respond to the\ncapabilities of today\u2019s most powerful AI models, as well as those likely to be\nbuilt in the future. In this post, we discuss what third-party testing looks\nlike, why it\u2019s needed, and describe some of the research we\u2019ve done to arrive\nat this policy position. We also discuss how ideas around testing relate to\nother topics on AI policy, such as openly accessible models and issues of\nregulatory capture.\n\n#### Policy overview\n\nToday\u2019s frontier AI systems demand a third-party oversight and testing regime\nto validate their safety. In particular, we need this oversight for\nunderstanding and analyzing model behavior relating to issues like election\nintegrity, harmful discrimination, and the potential for national security\nmisuse. We also expect more powerful systems in the future will demand deeper\noversight - as discussed in our \u2018Core views on AI safety\u2019 post, we think\nthere\u2019s a chance that today\u2019s approaches to AI development could yield systems\nof immense capability, and we expect that increasingly powerful systems will\nneed more expansive testing procedures. A robust, third-party testing regime\nseems like a good way to complement sector-specific regulation as well as\ndevelop the muscle for policy approaches that are more general as well.\n\nDeveloping a third-party testing regime for the AI systems of today seems to\ngive us one of the best tools to manage the challenges of AI today, while also\nproviding infrastructure we can use for the systems of the future. We expect\nthat ultimately some form of third-party testing will be a legal requirement\nfor widely deploying AI models, but designing this regime and figuring out\nexactly what standards AI systems should be assessed against is something\nwe\u2019ll need to iterate on in the coming years - it\u2019s not obvious what would be\nappropriate or effective today, and the way to learn that is to prototype such\na regime and generate evidence about it.\n\nAn effective third-party testing regime will:\n\n  * Give people and institutions more trust in AI systems\n  * Be precisely scoped, such that passing its tests is not so great a burden that small companies are disadvantaged by them\n  * Be applied only to a narrow set of the most computationally-intensive, large-scale systems; if implemented correctly, the vast majority of AI systems would not be within the scope of such a testing regime\n  * Provide a means for countries and groups of countries to coordinate with one another via developing shared standards and experimenting with Mutual Recognition agreements\n\nSuch a regime will have the following key ingredients [1]:\n\n  * Effective and broadly-trusted tests for measuring the behavior and potential misuses of a given AI system\n  * Trusted and legitimate third-parties who can administer these tests and audit company testing procedures\n\n#### Why we need an effective testing regime\n\nThis regime is necessary because frontier AI systems\u2014specifically, large-scale\ngenerative models that consume substantial computational resources\u2014don\u2019t\nneatly fit into the use-case and sector-specific frameworks of today. These\nsystems are designed to be 'everything machines' - Gemini, ChatGPT, and Claude\ncan all be adapted to a vast number of downstream use-cases, and the behavior\nof the downstream systems always inherits some of the capabilities and\nweaknesses of the frontier system it relies on.\n\nThese systems are extremely capable and useful, but they also present risks\nfor serious misuse or AI-caused accidents. We want to help come up with a\nsystem that greatly reduces the chance of major misuses or accidents caused by\nAI technology, while still allowing for the wide deployment of its beneficial\naspects. In addition to obviously wanting to prevent major accidents or misuse\nfor its own sake, major incidents are likely to lead to extreme, knee-jerk\nregulatory actions, leading to a 'worst of both worlds' where regulation is\nboth stifling and ineffective. We believe it is better for multiple reasons to\nproactively design effective and carefully thought through regulation.\n\nSystems also have the potential to display emergent, autonomous behaviors\nwhich could lead to serious accidents - for instance, systems might insert\nvulnerabilities into code that they are asked to produce or, when asked to\ncarry out a complex task with many steps, carry some actions which contradict\nhuman intentions. Though these kinds of behaviors are inherently hard to\nmeasure, it\u2019s worth developing tools to measure for them today as insurance\nagainst these manifesting in widely deployed systems.\n\nAt Anthropic, we\u2019ve implemented self-governance systems that we believe should\nmeaningfully reduce the risk of misuse or accidents from the technologies\nwe\u2019ve developed. Our main approach is our Responsible Scaling Policy (RSP),\nwhich commits us to testing our frontier systems, like Claude, for misuses and\naccident risks, and to deploy only models that pass our safety tests. Multiple\nother AI developers have subsequently adopted or are adopting frameworks that\nbear a significant resemblance to Anthropic's RSP.\n\nHowever, although Anthropic is investing in our RSP (and other organizations\nare doing the same), we believe that this type of testing is insufficient as\nit relies on self-governance decisions made by single, private sector actors.\nUltimately, testing will need to be done in a way which is broadly trusted,\nand it will need to be applied to everyone developing frontier systems. This\ntype of industry-wide testing approach isn\u2019t unusual - most important sectors\nof the economy are regulated via product safety standards and testing regimes,\nincluding food, medicine, automobiles, and aerospace.\n\n#### What would a robust testing regime look like?\n\nA robust third-party testing regime can help identify and prevent the\npotential risks of AI systems. It will require:\n\n  * A shared understanding across industry, government, and academia of what an AI safety testing framework looks like - what it should and shouldn\u2019t include\n  * An initial period where companies complete practice runs of implementing such testing, sometimes with third-party oversight, to make sure the tests work, are feasible to run, and can be validated by a third party\n  * A two-stage testing regime: there should be a very fast, automated testing stage that companies apply to their systems. This stage should cover a wide area and be biased towards avoiding false negatives. If this stage spots potential problems, there should be a more thorough secondary test, likely using expert human-led elicitation\n  * Increased resources to the parts of government that will oversee and validate tests - building and analyzing tests is detailed, expensive, technical work, so governments will need to find a way to fund the entities that do this\n  * A carefully scoped set of mandated tests - we\u2019ll need specific, legally mandated tests where it becomes clear there are poor incentives for industry self-governance, and the benefits of public safety from government oversight outweigh the regulatory burdens. We should ensure this is a well scoped, small set of tests, or else we\u2019ll create regulatory burdens and increase the possibility of regulatory capture\n  * An effective balance of the assurance of safety with ease of administration of these tests\n\nWhen it comes to tests, we can already identify one area today where testing\nby third-parties seems helpful and draws on the natural strengths of\ngovernments: national security risks. We should identify a set of AI\ncapabilities that, if misused, could compromise national security, then test\nour systems for these capabilities. Such capabilities might include the\nability to meaningfully speed up the creation of bioweapons or to carry out\ncomplex cyberattacks. (If systems are capable of this, then that would lead to\nus changing how we deployed the model - e.g, remove certain capabilities from\nbroadly deployed models and/or gate certain model capabilities behind \u2018know\nyour customer\u2019 regimes, and ensuring relevant government agencies were aware\nwe had systems with these capabilities.) We expect there are several areas\nwhere society will ultimately demand there be legitimate, third-party testing\napproaches, and national security is just one of them.\n\nWhen it comes to the third party doing the testing, there will be a multitude\nof them and the tests will be carried out for different reasons, which we\noutline here:\n\n  * Private companies: Companies may subcontract other companies to build tests and evaluations for their systems, as we have done with firms like Gryphon Scientific. We can also imagine companies doing tests for other companies where the tests are mandated by law but not carried out by government agencies, similar to how accounting firms audit the books of private companies.\n  * Universities: Today, many researchers at many academic institutions have free or subsidized access to models developed by AI labs; in the future, we could imagine some of these research institutions administering their own testing initiatives, some of which may be supervised or elevated via government bodies.\n  * Governments: Some tests (we suspect, a relatively small number) may be mandated by law and carried out by government actors - for instance, for testing for national security misuses of AI systems. Here, government agencies may carry out the tests directly.\n\nUltimately, we expect that third-party testing will be accomplished by a\ndiverse ecosystem of different organizations, similar to how product safety is\nachieved in other parts of the economy today. Because broadly commercialized,\ngeneral purpose AI is a relatively new technology, we don\u2019t think the\nstructure of this ecosystem is clear today and it will become clearer through\nall the actors above running different testing experiments. We need to start\nworking on this testing regime today, because it will take a long time to\nbuild.\n\nWe believe that we - and other participants in AI development - will need to\nrun multiple testing experiments to get this right. The stakes are high: if we\nland on an approach that doesn\u2019t accurately measure safety but is easy to\nadminister, we risk not doing anything substantive or helpful. If we land on\nan approach that accurately measures safety but is hard to administer, we risk\ncreating a testing ecosystem that favors companies with greater resources and\nthus reduces the ability for smaller actors to participate.\n\n#### How Anthropic will support fair, effective testing regimes\n\nIn the future, Anthropic will carry out the following activities to support\ngovernments in the development of effective third-party testing regimes:\n\n  * Prototyping a testing regime via implementing our RSP and sharing what we learn\n  * Testing third-party assessment of our systems via contractors and government partners\n  * Deepening our frontier red teaming work to give us and the broader sector a clearer sense of the risks of AI systems and their mitigations\n  * Advocating for governments to fund the agencies and organizations that could help to develop an effective third-party testing regime (e.g, in the United States, NIST, the US AI Safety Institute, the National AI Research Resource, the usage of DoE supercomputers for AI testing, and so on)\n  * Encouraging governments to build their own \u2018National Research Clouds\u2019 (like the National AI Research Resource in the US) so that they can a) develop independent capacity in academia and government to build, study, and test frontier AI systems, and b) work on the science of evaluating AI systems, including those developed by private companies like Anthropic\n\nDeveloping a testing regime and associated policy interventions based on the\ninsights of industry, government, and academia is the best way to avoid\nsocietal harm\u2014whether deliberate or accidental\u2014from AI systems.\n\n#### How testing connects to our broader policy priorities\n\nOur overarching policy goal is to have appropriate oversight of the AI sector.\nWe believe this will mostly be achieved via there being an effective ecosystem\nfor third-party testing and evaluation of AI systems. Here are some AI policy\nideas you can expect to see us advocating for in support of that:\n\nGreater funding for AI testing and evaluation in government\n\n  * Effective testing and evaluation procedures are a necessary prerequisite of any effective form of AI policy. We think that governments should stand up and support institutions that develop AI evaluations, as well as bringing together industry, academia, and other stakeholders to agree on standards for the safety of AI systems. In the US, we specifically advocate for greater funding for NIST.\n\nSupport greater evaluation of AI systems through public sector infrastructure\nfor doing AI research\n\n  * We urgently need to increase the number and breadth of people working to test and evaluate AI systems, for both current and future risks. It\u2019s therefore crucial that governments create experimental infrastructure to help academic researchers test out and evaluate frontier AI systems, and develop their own frontier systems for beneficial purposes. For more information, see our support for a US national research cloud via the CREATE AI Act, and our written Senate testimony.\n\nDeveloping tests for specific, national security-relevant capabilities\n\n  * We should know if AI systems can be used in ways that strengthen or (if fielded by another entity) weaken national security. Whereas the private sector and academia can develop the vast majority of tests, some testing and evaluation questions relate to national security capabilities which are classified, so only certain governments are able to effectively evaluate them. Therefore, we want to support US Government efforts to develop ways of testing AI systems for national security-relevant capabilities. We will also continue our own work to better understand the capabilities of our own systems.\n\nScenario planning and test development for increasingly advanced systems\n\n  * Our Responsible Scaling Policy is designed to frontload work about evaluating and testing future, hypothetical capabilities of AI systems. This is to ensure we have the relevant tests to better assess and minimize accident and misuse risks from increasingly powerful AI systems. But we don\u2019t claim that our RSP delineates all the tests that need to be run on increasingly powerful models. As AI advances driven by growing computational power increase, a broader set of actors should work to anticipate the future capabilities of AI systems, and develop tests for them.\n\n#### Aspects of AI policy we believe are important to discuss\n\nWhile developing our policy approach, we\u2019ve also found ourselves returning\nagain and again to a few specific issues such as openly accessible models and\nregulatory capture. We\u2019ve outlined our current policy thinking below but\nrecognize these are complicated issues where people often disagree.\n\n  * Openly-disseminated and/or open-source models: Science moves forward largely due to a culture of openness and transparency around research. This is especially true in AI, where much of the currently-unfolding revolution is built on the open publication of research and models like the Transformer, BERT, Vision Transformers, and so on. There is also a long history of open source and openly accessible systems increasing the robustness of the security environment by helping a greater number of people experiment with technologies and identify their potential weaknesses.\n\nWe believe that the vast majority of AI systems today (perhaps even all of\nthem) are safe to openly disseminate and will be safe to broadly disseminate\nin the future. However, we believe in the future it may be hard to reconcile a\nculture of full open dissemination of frontier AI systems with a culture of\nsocietal safety.\n\nIf \u2014 and \u2018if\u2019 is a key and unresolved point \u2014 increasingly capable AI models\ncan lead to detrimental effects, or hold the possibility of catastrophic\naccidents, then we\u2019ll need to adjust the norms of what is openly disseminated\nat the frontier.\n\nSpecifically, we\u2019ll need to ensure that AI developers release their systems in\na way that provides strong guarantees for safety - for example, if we were to\ndiscover a meaningful misuse of our model, we might put in place classifiers\nto detect and block attempts to elicit that misuse, or we might gate the\nability to finetune a system behind a \u2018know your customer\u2019 rule along with\ncontractual obligations to not finetune towards a specific misuse. By\ncomparison, if someone wanted to openly release the weights of a model which\nwas capable of the same misuse, they would need to both harden the model\nagainst that misuse (e.g, via RLHF or RLAIF training) and find a way to make\nthis model resilient to attempts to fine-tune it onto a dataset that would\nenable this misuse. We will also need to experiment with disclosure processes,\nsimilar to how the security community has developed norms around pre-\nnotification of disclosures of zero days.\n\nThough what we\u2019ve described is inherently very costly we also believe it is\nnecessary - we must do everything we can to avoid AI systems enabling\nsignificant misuses or causing major accidents. But carrying out any\nrestrictions on the open dissemination of AI systems depends on there being\nbroad agreement on what unacceptable misuses of AI systems or system behaviors\nare.\n\nAnthropic is not an impartial actor here - we are a company that primarily\ndevelops proprietary systems, and we don\u2019t have the legitimacy to make claims\nhere about what should or shouldn\u2019t be acceptable in openly disseminated\nsystems. Therefore, to resolve questions of open source models we need\nlegitimate third parties to develop testing and evaluation approaches that are\nbroadly accepted as legitimate, we need these third parties (or other trusted\nentities) to define a narrow and serious set of misuses of AI systems as well\nas adverse AI system behaviors, and we will need to apply these tests to\nmodels that are both controlled (e.g., via API) or openly disseminated (e.g.,\nvia the weights being released).\n\nThird party testing of openly disseminated and closed proprietary models can\ngenerate the essential information we need to understand the safety properties\nof the AI landscape [2]. If we don\u2019t do this, then you could end up in a\nsituation where either a proprietary model or openly accessible model directly\nenables a serious misuse or causes a major AI accident - and if that happens,\nthere could be significant harm to people and also likely adverse regulations\napplied to the AI sector.\n\n  * Regulatory capture: Any form of policy can suffer regulatory capture by a sufficiently motivated and well-resourced actor: for example, a well-capitalized AI company. Some of the ideas we discuss above about openly accessible models are the kinds of things which themselves are prone to regulatory capture. It\u2019s important that the AI ecosystem remains robust and competitive - AI is a complicated field and humanity\u2019s best chance at getting it right likely comes from there being a diverse, broad set of actors engaged in its development and oversight.\n\nWe generally advocate for third-party testing and measurement initiatives\nbecause they seem like the kind of policy infrastructure that helps us to\nidentify and prevent concrete harms as well as building capacity that exists\nindependently of large companies. Therefore, we think that focusing on the\ndevelopment of third-party testing capacity can reduce the risk of regulatory\ncapture and create a level playing field for developers. Conversely, industry-\nled consortia might have a tendency to favor approaches that involve high\ncompliance costs on the parts of companies regardless of their scale - an\napproach that inherently advantages larger businesses which can spend more\nmoney on policy compliance.\n\n#### Why we\u2019re being careful in what we advocate for in AI policy\n\nWhen developing our policy positions, we assume that regulations tend to\ncreate an administrative burden both for the party that enforces the\nregulation (e.g, the government), and for the party targeted by the regulation\n(e.g, AI developers). Therefore, we should advocate for policies that are both\npractical to enforce and feasible to comply with. We also note that\nregulations tend to be accretive - once passed, regulations are hard to\nremove. Therefore, we advocate for what we see as the \u2018minimal viable policy\u2019\nfor creating a good AI ecosystem, and we will be open to feedback.\n\n#### Why AI policy is important\n\nThe AI systems of today and those of the future are immensely powerful and are\ncapable of yielding great benefits to society. We also believe these systems\nhave the potential for non-trivial misuses, or could cause accidents if\nimplemented poorly. Though the vast majority of our work is technical in\nnature, we\u2019ve come to believe that testing is fundamental to the safety of our\nsystems - it\u2019s not only how we better understand the capabilities and safety\nproperties of our own models, but also how third-parties can validate claims\nwe make about AI systems.\n\nWe believe that building out a third-party testing ecosystem is one of the\nbest ways for bringing more of society into the development and oversight of\nAI systems. We hope that by publishing this post we\u2019ve been able to better\narticulate the benefits of third-party testing as well as outline our own\nposition for others to critique and build upon.\n\n#### Footnotes\n\n[1] Some countries may also experiment with \u2018regulatory markets\u2019 where AI\ndevelopers can buy and sell AI testing services and compete with one another\nto try to build and deploy successively safer, more useful systems.\n\n[2] For example, if you openly release an AI model, it\u2019s relatively easy for a\nthird-party to fine-tune that model on a dataset of their own choosing. Such a\ndataset could be designed to optimize for a misuse (e.g, phishing or offensive\nhacking). If you were able to develop technology that made it very hard to\nfine-tune an AI model away from its original capability distribution, then\nit\u2019d be easier to confidently release models without potentially compromising\non downstream safety.\n\n  * Claude\n  * API\n  * Research\n  * Company\n  * Customers\n  * News\n  * Careers\n\n  * Press Inquiries\n  * Support\n  * Status\n  * Twitter\n  * LinkedIn\n  * Availability\n\n  * Terms of Service \u2013 Consumer\n  * Terms of Service \u2013 Commercial\n  * Privacy Policy\n  * Acceptable Use Policy\n  * Responsible Disclosure Policy\n  * Compliance\n\n\u00a9 2024 Anthropic PBC\n\n", "frontpage": false}
