{"aid": "40011137", "title": "AMD's AI Strategy", "url": "https://morethanmoore.substack.com/p/quick-bytes-amds-ai-hardware-strategy", "domain": "morethanmoore.substack.com", "votes": 2, "user": "rbanffy", "posted_at": "2024-04-12 10:41:53", "comments": 0, "source_title": "Quick Bytes: AMD\u2019s AI Strategy", "source_text": "Quick Bytes: AMD\u2019s AI Strategy - by George Cozma\n\n# More Than Moore\n\nShare this post\n\n#### Quick Bytes: AMD\u2019s AI Hardware Strategy\n\nmorethanmoore.substack.com\n\n#### Discover more from More Than Moore\n\nThe latest in semiconductors: silicon, AI, processors, market trends.\n\nOver 4,000 subscribers\n\nContinue reading\n\nSign in\n\n# Quick Bytes: AMD\u2019s AI Hardware Strategy\n\n### Quotes from CEO Lisa Su and SVP Jack Huynh\n\nGeorge Cozma\n\nApr 08, 2024\n\n11\n\nShare this post\n\n#### Quick Bytes: AMD\u2019s AI Hardware Strategy\n\nmorethanmoore.substack.com\n\nShare\n\n##### Guest Post By George Cozma, Editor-in-Chief of Chips And Cheese\n\nTo say that AMD\u2019s story has been a roller coaster would be an understatement -\nthere is a massive contrast between the AMD of 2014 and AMD of 2024. Where the\nAMD of a decade ago was floundering, the AMD of today is resurgent and is a\nkey player in many markets. As with many other players in this space, AI is a\nprimary focus, with the company building a dedicated AI team internally to\ncover the full end-to-end strategy for a rapidly blooming AI market.\n\nShare\n\nIn recent weeks, AMD CEO Lisa Su, and SVP/GM of Computing and Graphics Jack\nHuynh, both answered questions from industry analysts as to the nature of\nAMD\u2019s AI hardware strategy and how to look at its portfolio.\n\nAMD\u2019s AI hardware strategy comes in three prongs.\n\nThe first is AMD\u2019s Instinct series of Datacenter GPUs, which retail in the\nform of the MI300 series. There are two variants, and the MI300X is focused on\nAI - it has seen success in getting adoption from large cloud players such as\nMicrosoft and Azure along with some smaller AI-centric clouds like TensorWave.\nIn the latest earnings call, Lisa Su commented about an expanding demand for\nthese chips, pushing up from $2b to $3.5b revenue by the end of 2024. At the\nlaunch, AMD compared itself to NVIDIA\u2019s H100 favorably, marking an eight-chip\nsystem as equal in ML training, but better in ML inference.\n\nThe other variant of this series is the MI300A, offering similar\nspecifications but is a combination CPU/GPU, and targeted at High Performance\nComputing. It has been adopted into the largest planned global supercomputer,\nEl Captian, which is going to use machine learning models to assist with\nprotecting the United States Nuclear Stockpile.\n\nEVP/GM of Datacenter, Forrest Norrod\n\nSpeaking to the adoption of MI300, Lisa stated\n\n    \n    \n    \u201cWe have been pleasantly surprised and [it has] been great to see the momentum with MI300, and where that momentum is coming from. Large cloud [customers] often move the fastest - from workload [to workload]. LLMs play very well to MI300 - our memory capacity and memory bandwidth [are market leading]. AI is the leading workload. [We have] quite a broad set of customers that come in with needs - some are training, some are fine tuning, some are mixed. [But out] confidence from the pattern as we start with customers. [We\u2019ve spent] a lot of work with the software environment as well. New customers are [finding it] easier to reach their performance expectations, because ROCm (AMD\u2019s softwate stack) is getting mature. [Our] largest [MI300] workload are large language models.\u201d\n\nIt should also be noted that AMD recently announced it was expanding its chip-\nto-chip communication protocol, known as Infinity Fabric, to specific\nnetworking partners such as Arista, Broadcom, and Cisco. We expect these\ncompanies to go and build Infinity Fabric switches, enabling enable chip-to-\nchip communication for MI300 beyond a single system.\n\nThe second prong of AMD\u2019s strategy is their client GPU lineup. This consists\nof both AMD\u2019s Radeon discrete graphics cards (GPUs), and their APUs which\nconsist of a GPU integrated on to a client CPU which are mostly used in\nlaptops. Both the first and second prong of AMD\u2019s AI strategy relies on their\ncompute stack, called ROCm, which is AMD\u2019s competitor to NVIDIA\u2019s CUDA stack.\nA long running grumble about ROCm, even the latest version, is inconsistent\nsupport across enterprise and consumer hardware - only AMD\u2019s Instinct GPUs\nhave proper support for ROCm and its associated libraries and select discrete\nGPUs, whereas CUDA runs on nearly every piece of NVIDIA hardware.\n\nPresident of AI and Adaptive/Embedded, Victor Peng\n\nHowever, Jack said in our Q&A,\n\n    \n    \n    \u201cWe [currently] enable ROCm on our 7900 flagships to allow you to do some AI applications. We are going to expand ROCm more broadly.\u201d \u201cThere are schools and universities and startups that maybe can't afford a very high-end GPU, but they want to tinker. We want to enable that community as a developer tool.\u201d\n\nWe are hoping this implies a wider ROCm support for current generation\nhardware as well as all future releases - more than just their flagship RX7900\nseries.\n\nLisa has also commented on AMD\u2019s software stack saying,\n\n    \n    \n    \u201cThe big question [as of] recent has been software. We've made a ton of progress on software. The ROCm 6 software stack was a significant step up. There's still a lot more [to do] on software... we want to address the large opportunities.\u201d\n\nAMD\u2019s third prong is their XDNA AI engines. While the technology comes from\nXilinx, this IP was already licensed to AMD before the acquisition. These AI\nengines are being integrated into laptop processors, and will present as an\nNPU for Microsoft\u2019s AIPC initiative to compete against Intel and Qualcomm\nofferings. These AI engines are designed for low power inference rather than\nthe high throughput inference or training that the higher power GPUs are\ncapable of.\n\nCommenting on the position of NPUs versus GPUs, Lisa said,\n\n    \n    \n    \u201cThere are places where the AI engines will be more prevalent, such as PC and notebooks. If you\u2018re looking at large[r] scale, more workstation notebooks, [they\u2019ll] probably use the GPU in that framework.\u201d\n\nAMD sees a future of multiple AI workloads and engines: CPU, GPU and NPU. It\nis worth noting everyone else in the space is making the same noises.\n\nSVP/GM of Computing and Graphics, Jack Huynh\n\nJack commented,\n\n    \n    \n    \u201c[For the] NPU, MS is driving [it] heavily because of power efficiency. The NPU can still drive experiences, but not hurt battery [life]. We're going to bet on the NPU. We're going to 2x and 3x on AI... The NPU is all about battery life - in a desktop, you tend not to worry about battery, but also custom data formats supported [by the NPU can be brought] into the desktop.\u201d\n\nThis three pronged approach allows AMD is tackle the AI space on various\nfronts, showcasing not all the eggs have to be in the same basket. AMD has\nalready seen some success using this approach - in the datacenter AMD is\nconsidered the closest competitor to NVIDIA. MI300\u2019s memory capacity and\nbandwidth puts it in good competition against NVIDIA\u2019s H100 hardware (B100\nbenchmarks we\u2019re still waiting for). The NPU space is still too new and fluid\nto really determine if AMD\u2019s strategy is paying off; however it is likely that\nMicrosoft will use the NPU for local ML models, such as assistants or \u2018co-\npilot\u2019 models.\n\nFrom our perspective, the weakness of AMD\u2019s strategy is in the desktop GPU\nside of things, due to the lack of near-universal ROCm support all across\nAMD\u2019s hardware stack. This is an issue that will take time to resolve - one of\nthe downsides of having a split front is a division of resources. AMD will\nrequire strict management to ensure work is not duplicated across the company.\nHowever, there are positives, with AMD ever increasing its forecast of\ndatacenter revenue in 2024 claiming the limit is only in demand, not supply.\n\nShare\n\nAMD\u2019s Q1 2024 Financial Results are expected at the end of April (29th or 30th\nwe think, TBC), with the Annual Stockholders Meeting on May 8th.\n\nMore Than Moore is a reader-supported publication. To receive new posts and\nsupport my work, consider becoming a free or paid subscriber.\n\n###### More Than Moore, as with other research and analyst firms, provides or\nhas provided paid research, analysis, advising, or consulting to many high-\ntech companies in the industry, which may include advertising on the More Than\nMoore newsletter or TechTechPotato YouTube channel and related social media.\nThe companies that fall under this banner include AMD, Applied Materials,\nArmari, Ayar Labs, Baidu, Facebook, IBM, Infineon, Intel, Lattice Semi,\nLinode, MediaTek, NordPass. NVIDIA, ProteanTecs, Qualcomm, SiFive, Supermicro,\nSynopsys, Tenstorrent, TSMC.\n\n### Subscribe to More Than Moore\n\nBy Dr. Ian Cutress \u00b7 Launched 2 years ago\n\nThe latest in semiconductors: silicon, AI, processors, market trends.\n\n11 Likes\n\n\u00b7\n\n2 Restacks\n\n11\n\nShare this post\n\n#### Quick Bytes: AMD\u2019s AI Hardware Strategy\n\nmorethanmoore.substack.com\n\nShare\n\nA guest post by| George CozmaEditor in Chief  \n---  \n  \nComments\n\nThe Risk of RISC-V: What's Going on at SiFive?\n\nUpdated 10/25\n\nOct 24, 2023 \u2022\n\nDr. Ian Cutress\n\n28\n\nShare this post\n\n#### The Risk of RISC-V: What's Going on at SiFive?\n\nmorethanmoore.substack.com\n\n5\n\nNVIDIA Enabling Computational Lithography on GPUs\n\nEliminating the fuzzies\n\nMar 21, 2023 \u2022\n\nDr. Ian Cutress\n\n16\n\nShare this post\n\n#### NVIDIA Enabling Computational Lithography on GPUs\n\nmorethanmoore.substack.com\n\n5\n\nInterview with Intel\u2019s Dr. Ann Kelleher\n\nThe One In Charge of The Nodes\n\nNov 22, 2023 \u2022\n\nDr. Ian Cutress\n\n9\n\nShare this post\n\n#### Interview with Intel\u2019s Dr. Ann Kelleher\n\nmorethanmoore.substack.com\n\nReady for more?\n\n\u00a9 2024 Dr. Ian Cutress\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great writing\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
