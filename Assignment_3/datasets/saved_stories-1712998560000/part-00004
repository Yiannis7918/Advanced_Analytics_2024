{"aid": "40019704", "title": "Grok-1.5 Vision Preview", "url": "https://x.ai/blog/grok-1.5v", "domain": "x.ai", "votes": 42, "user": "runesoerensen", "posted_at": "2024-04-13 01:50:37", "comments": 8, "source_title": "Grok-1.5 Vision Preview", "source_text": "Grok-1.5 Vision Preview\n\nBlogAboutCareers\n\nApril 12, 2024\n\n# Grok-1.5 Vision Preview\n\nApril 12, 2024\n\nConnecting the digital and physical worlds with our first multimodal model.\n\nIntroducing Grok-1.5V, our first-generation multimodal model. In addition to\nits strong text capabilities, Grok can now process a wide variety of visual\ninformation, including documents, diagrams, charts, screenshots, and\nphotographs. Grok-1.5V will be available soon to our early testers and\nexisting Grok users.\n\n## Capabilities\n\nGrok-1.5V is competitive with existing frontier multimodal models in a number\nof domains, ranging from multi-disciplinary reasoning to understanding\ndocuments, science diagrams, charts, screenshots, and photographs. We are\nparticularly excited about Grok\u2019s capabilities in understanding our physical\nworld. Grok outperforms its peers in our new RealWorldQA benchmark that\nmeasures real-world spatial understanding. For all datasets below, we evaluate\nGrok in a zero-shot setting without chain-of-thought prompting.\n\nBenchmark| Grok-1.5V| GPT-4V| Claude 3 Sonnet| Claude 3 Opus| Gemini Pro 1.5  \n---|---|---|---|---|---  \nMMMU Multi-discipline| 53.6%| 56.8%| 53.1%| 59.4%| 58.5%  \nMathvista Math| 52.8%| 49.9%| 47.9%| 50.5%| 52.1%  \nAI2D For samples missing annotations such as A, B, C, etc., we render bounding\nboxes and corresponding letters at the relevant areas in the image. Diagrams|\n88.3%| 78.2%| 88.7%| 88.1%| 80.3%  \nTextVQA Text reading| 78.1%| 78.0%| -| -| 73.5%  \nChartQA Charts| 76.1%| 78.5%| 81.1%| 80.8%| 81.3%  \nDocVQA Documents| 85.6%| 88.4%| 89.5%| 89.3%| 86.5%  \nRealWorldQA Real-world understanding| 68.7%| 61.4%| 51.9%| 49.8%| 67.5%  \n  \n### Writing code from a diagram\n\nSample 1/7\n\n##### User\n\nCan you translate this into Python code?\n\n##### Grok\n\nCertainly! The flowchart you\u2019ve provided describes a simple guessing game\nwhere the computer generates a random number, and the user has to guess it.\nHere\u2019s the Python code that represents the logic in the flowchart:\n\npython\n\n    \n    \n    1import random 2 3def guess_number(): # Generate a random number between 1 and 10 4 5 target = random.randint(1, 10) 6 7 while True: 8 guess = int(input(\"Guess a number between 1 and 10: \")) 9 10 if guess == target: 11 print(\"You won!\") 12 break 13 else: 14 print(\"Wrong guess, try again!\") 15 16if __name__ == \"__main__\": 17 guess_number()\n\nThis code will run the guessing game as described in the flowchart. The user\nwill be prompted to enter their guess until they guess the correct number.\n\n## Real-World Understanding\n\nIn order to develop useful real-world AI assistants, it is crucial to advance\na model's understanding of the physical world. Towards this goal, we are\nintroducing a new benchmark, RealWorldQA. This benchmark is designed to\nevaluate basic real-world spatial understanding capabilities of multimodal\nmodels. While many of the examples in the current benchmark are relatively\neasy for humans, they often pose a challenge for frontier models.\n\nWhich object is larger the pizza cutter or the scissors? A. The pizza cutter\nis larger. B. The scissors is larger. C. They are about the same size.\n\nWhere can we go from the current lane? A. Turn left. B. Go straight. C. Turn\nleft and go straight. D. Turn right.\n\nGiven this front camera view from our sedan, do we have enough space to drive\naround the gray car in front of us? A. Yes. B. No.\n\nGiven the picture, in which cardinal direction is the dinosaur facing? A.\nNorth. B. South. C. East. D. West.\n\nThe initial release of the RealWorldQA consists of over 700 images, with a\nquestion and easily verifiable answer for each image. The dataset consists of\nanonymized images taken from vehicles, in addition to other real-world images.\nWe are excited to release RealWorldQA to the community, and we intend to\nexpand it as our multimodal models improve. RealWorldQA is released under CC\nBY-ND 4.0. Click here (677MB) to download the dataset.\n\n## Into the future\n\nAdvancing both our multimodal understanding and generation capabilities are\nimportant steps in building beneficial AGI that can understand the universe.\nIn the coming months, we anticipate to make significant improvements in both\ncapabilities, across various modalities such as images, audio, and video.\n\nIf you want to be a part of this journey, we are hiring.\n\nTerms of service\n\nPrivacy policy\n\nDevelopers\n\nPromptIDE\n\nGrok\n\n", "frontpage": true}
