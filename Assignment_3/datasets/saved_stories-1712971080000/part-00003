{"aid": "40016617", "title": "I think therefore I am, but does AI think?", "url": "https://beabytes.com/ai-does-not-think/", "domain": "beabytes.com", "votes": 1, "user": "azhenley", "posted_at": "2024-04-12 19:16:26", "comments": 0, "source_title": "I think therefore I am, but does AI think ?", "source_text": "I think therefore I am, but does AI think ?\n\nB\u00e9atrice Moissinac, PhD Hello, world! \ud83d\udc4b Welcome to BeaBytes. My goal is to\nhelp you understand AI and equip you with enough conceptual (but not\ntechnical) fluency to fight off the snake oil merchants. I reserve the right\nto change my mind at any time\n\ncontact -at- beabytes -dot- com\n\n  * Blog\n  * About\n  * Talks\n\n\u00a9 2013-2024 B\u00e9atrice Moissinac, all rights reserved. Any opinions, findings,\nand conclusions or recommendations expressed in this material are those of the\nauthor(s) and do not necessarily reflect the views of my employer or sponsors.\n\n# I think therefore I am, but does AI think ?\n\nThe sudden democratization of Generative Artificial Intelligence (genAI)\ntools, especially those backed by Large Language Models (LLMs), has enflamed\npassions and questioned our fundamental understanding of intelligence,\nconsciousness, and reasoning. For some, LLMs are the latest expression of age-\nold questions. Can machines think? Can machines become sentient? What is\nconsciousness? From philosophers to Star Trek fans, many have asked those\nquestions. For others, it is an excellent source of click-bait titles:\n\n  * This AI says it has feelings. It\u2019s wrong. Right?\n  * Google Engineer claims AI Chatbot is sentient: Why That Matters\n  * A conversation with Bing\u2019s Chatbot left me deeply unsettled\n\nIn this article, I would like to demystify what is Al, and specifically LLMs,\nand explain why they do not \u201cthink\u201d in the way you might be - indeed -\nthinking.\n\n# Intelligence\n\nTo talk about artificial intelligence, I must first talk about human\nintelligence. Our story started presumably 200,000 to 300,000 years ago, with\nthe emergence of the Homo Sapiens species. In Latin, homo sapiens means\n\u201cintelligent human\u201d. Today, modern Humans are grouped into a subspecies of\nHomo Sapiens Sapiens, which can be interpreted as \u201chuman who thinks about how\nit thinks\u201d. Scientists have longed believed that one major aspect of our\ncognitive mark-up is the ability to have metacognition, to think about how we\nthink. This ability to think about how we think, and therefore the ability to\nchange how we think, is the keystone of modern humans\u2019 ability to innovate and\ninvent.\n\n# Logic\n\nWith the invention of the computer program (see Ada Lovelace), we stepped from\nthe world of mechanical computing (i.e., the abacus) to the world of logical\nsystems. A logical system is a way to encode a reasoning without encumbering\nitself with the details. It abstracts details away to form rules that can be\napplied in many different settings to obtain a conclusion.\n\nFor instance, we could create a logical system to count how many objects are\nin my backpack. We don\u2019t need to define every type of object. \u201cObject\u201d is\ntherefore an abstraction of wallet, keys, phone, etc. (If you are a software\nengineer, things are starting to look familiar). Computers are the physical\nexpression of logical systems, thus computers are machines that, given the\nproper logical system, can logically reason.\n\n# Reason\n\nYes, computers can logically reason, but computers do not have Reason - Note\nthe capitalization. \u201cReason is the capacity of applying logic consciously by\ndrawing conclusions from new or existing information, with the aim of seeking\nthe truth.\u201d Logical reasoning by following a set of instructions is not\nReason. A computer does not think, it computes (see Automated reasoning and\nFirst-order logic). We are telling the computer how to reason.\n\nMore importantly, I am in strong agreement with Mark H. Bickard and Robert L.\nCampbell, who argued that logical systems \u201ccan\u2019t construct new logical systems\nmore powerful than themselves\u201d, so reasoning and rationality must involve more\nthan a system of logic. (see Reason compared to Logic). This means that a\ncomputer cannot - by itself start solving harder problems than the ones its\nfirst system could solve. Humans, on the other hand, have metacognition. This\nis how we can create logical systems that are more powerful than the previous\nones. This is where innovation and invention come from. \u201cBut what about\ndata!\u201d, you cheekily ask! \ud83d\ude1b I\u2019ll get to it in a moment... (if you can\u2019t wait,\nread this).\n\n# Computation\n\nIf you are still with me at this point, let\u2019s go even deeper! If computers can\ncompute following a logical system, then we must strive to create more\npowerful logical systems for computers to compute! The greater the system, the\nharder the problem it can solve! In the XXth century, mathematicians became\nvery interested in understanding how to measure the ability of a logical\nsystem to answer questions, and also, how long it would take to compute.\nWelcome to Theory of computation! You very likely already know the most famous\nconcept of theory of computation: The Turing machine!\n\nAI was invented the day computers were invented, because understanding\ncomputation is understanding the power of logical systems, which are in\nessence intelligent. AI and computation are so intertwined that it is also\nAlan Turing who proposed the Turing test. The Turing Test is an experiment\nwhere a human is asked to differentiate between a human and an Al based on\ntheir outputs. Where have you heard that before? Google\u2019s AI passed a famous\ntest \u2014 and showed how the test is broken. With the Turing test, Alan Turing\ngave us the definition of an AI:\n\n> AI is a computer-made decision, which by its \u201csmartness\u201d (computation) is\n> indistinguishable from a human-made decision.\n\nOn a side note, theory of computation is a very profound and philosophical\ntopic. It gave us the compiler, problem reduction, and so many beautiful ways\nof understanding the world.\n\n# Data\n\nAt the time of Alan Turing, computers were all deterministic systems: given an\ninput, a computer would always give the same output. And human computers were\nmore powerful, reliable, and trusted than non-human computers (Katherine\nJohnson). As computer engineering improved with hard drive, flash memory, CPUs\ncomputers were made to handle larger and larger amount of data.\n\nAt first, AI algorithms were purely analytical, meaning that given a problem,\nthe solution was a mathematical equation that provided an exact and\ndeterministic answer every time. Even with data, algorithms were\ndeterministic, like the Linear regression algorithm. As data and processing\npower grew, it became harder to compute analytical solutions (e.g., Linear\nprogramming). Thus, we needed to start approximating things, and use\nstatistics to derive conclusions. Things became stochastic, that is, dependent\non probabilities. A stochastic system may produce a different output every\ntime, hence the probabilities.\n\nMachine Learning (ML) emerged as subfield of AI. It focuses on grouping\nthings, whether we already know what groups exist (supervised ML) or not\n(unsupervised ML). For instance, ML can calculate a customer\u2019s probability to\nrenew (Which group does this customer belong? Renew or Churn?), or wether an\nimage contains a cat or a dog. This grouping is done using various statistical\nprocesses, which produce a stochastic answer (i.e., it may change depending on\nyour data). Every \u201ctruth\u201d derived by an ML comes from data + its statistical\nprocessing. And that, includes LLMs as well. Thus, the quality of your data\nwill strongly define the quality of the system\u2019s conclusions.\n\nMandatory SMBC reference:\n\n> A philosophical note. We are experiencing the real-life debate between\n> empiricists (Knowledge comes from data) and rationalists (Knowledge comes\n> from reason), and it\u2019s ok. We need both.\n\n> Can an Al tell us something that we didn\u2019t know before? No, because logical\n> systems \u201ccan\u2019t construct new logical systems more powerful than themselves.\"\n>\n> Even with data? The field of Epistemology is the study of how we know what\n> we know. Logical systems are excellent at proving what they know. If an\n> algorithm generates a new discovery, it is mathematically only possible if\n> the discovery was contained in the data. An excellent example of this is the\n> application of Machine learning in physics (see Scientific Machine\n> Learning). Those cases do not violate the epistemological assumption that\n> knowledge comes from somewhere. The same way a microscope doesn\u2019t know\n> something we don\u2019t, algorithms only accelerated the emergence of evidence,\n> they do not invent it.\n\n# Deep Learning\n\nAlright. \u2615 Let\u2019s recap. \u2615\n\nComputers use logical reasoning to compute things. We can create special types\nof logical reasoning, called AI, to perform specific tasks in a very smart\nway. Data and statistical processing really help achieve a wider type of\ntasks, faster and better. Machine Learning (ML) is an example of such tasks.\n\nDeep learning is a subfield within ML, that use a specific logical system\ncalled Neural Network (Think of it as a specific mathematical equation), with\nan approximation (see Rectifier) to make it easier to compute, with the help\nof a TON of data.\n\n> On a side note, Neural Networks were invented in the 1960s (Perceptron) but\n> became useful only recently, when we could make them very large due to GPU\n> improvement, and use a lot of data due to storage improvement.\n\nBut something was missing. Data and computational power by themselves are not\nsufficient to derive very complicated conclusions. Data is not knowledge, it\u2019s\ninformation. How do we represent data such that it forms knowledge? w\nKnowledge representation and reasoning has been a field of study in Al since\nthe beginning, but mathematically, it was very difficult. We tried Knowledge\ngraph and other crazy stuff, until..\n\nUntil the Transformers \ud83d\ude97\ud83e\udd16 and Encoders! I am oversimplifying, but in essence,\nwe developed approximations that allowed to immensely increase the amount of\nparameters a model could compute AND simultaneously, told the model what was\ninteresting about the data.\n\n# Language\n\nAlso, it turns out that language is very logical... given enough data, you can\neasily approximate stochastically what comes next in a sentence. DUN DUN\nDUNNN...\n\nNow, let\u2019s put it all together.\n\n# Large Language Models\n\nLarge Language Models (LLMs) are a specific type of Deep Learning algorithm,\nwhose logical system uses statistical approximation and VAST amount of data to\nsimulate language.\n\nAn LLM does not think. An LLM is a stochastic parrot \ud83e\udd9c.\n\nStill don\u2019t believe me? Watch this!\n\nAn LLM computes - in a very mathematically impressive way - the statistically\nbest response to a prompt and the context given. LLMs do not think like you\nand I would colloquially say \u201cI think\u201d.\n\nAI does NOT think. Yet.\n\n# Conclusion\n\nThere might be a day, where computers armed with powerful algorithms and data,\ncan achieve general cognition at similar levels than humans. Given any new\nproblem, computers could create a new logical system to solve it. But this day\nhas not come yet. And the people claiming that we are close to this day have a\nstrong financial interest in making you believe the hype. (Open AI - Planning\nfor AGI and beyond ).\n\nIn conclusion, AI is great at automating many tasks, but thinking is not one\nof them!\n\n", "frontpage": false}
