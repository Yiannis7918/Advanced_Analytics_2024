{"aid": "39965126", "title": "Visualizing Attention, a Transformer's Heart [video]", "url": "https://www.youtube.com/watch?v=eMlx5fFNoYc", "domain": "youtube.com", "votes": 5, "user": "jonbaer", "posted_at": "2024-04-08 00:23:32", "comments": 0, "source_title": "Visualizing Attention, a Transformer's Heart | Chapter 6, Deep Learning", "source_text": "Visualizing Attention, a Transformer's Heart | Chapter 6, Deep Learning - YouTube\n\n\u2022\n\nNaN / NaN\n\nBack\n\nDE\n\nDE\n\nVisualizing Attention, a Transformer's Heart | Chapter 6, Deep Learning\n\n2x\n\nIf playback doesn't begin shortly, try restarting your device.\n\n\u2022\n\nIn the last chapter, you and I started to step\n\nUp next\n\nLive\n\nUpcoming\n\nPlay Now\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV\nrecommendations. To avoid this, cancel and sign in to YouTube on your\ncomputer.\n\nBut what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning\n\n27:14\n\n3Blue1Brown is creating videos animating math | Patreon\n\npatreon.com\n\nLearn more\n\n3Blue1Brown\n\nSubscribe\n\nSubscribed\n\nMy name is Grant Sanderson. Videos here cover a variety of topics in math, or\nadjacent fields like physics and CS, all with an emphasis on visualizing the\ncore ideas. To goal is to use animation to help elucidate and motivate\notherwise tricky topics, and for difficult problems to be made simple with\nchanges in perspective. For more information, other projects, FAQs, and\ninquiries see the website: https://www.3blue1brown.com\n\n3Blue1Brown\n\nSubscribe\n\nSubscribed\n\nShare\n\nAn error occurred while retrieving sharing information. Please try again\nlater.\n\n0:00\n\n0:00 / 26:09\u2022Watch full video\n\n## Chapters\n\n#### Recap on embeddings\n\n#### Recap on embeddings\n\n0:00\n\n#### Recap on embeddings\n\n0:00\n\n#### Motivating examples\n\n#### Motivating examples\n\n1:39\n\n#### Motivating examples\n\n1:39\n\n#### The attention pattern\n\n#### The attention pattern\n\n4:29\n\n#### The attention pattern\n\n4:29\n\n#### Masking\n\n#### Masking\n\n11:08\n\n#### Masking\n\n11:08\n\n#### Context size\n\n#### Context size\n\n12:42\n\n#### Context size\n\n12:42\n\n#### Values\n\n#### Values\n\n13:10\n\n#### Values\n\n13:10\n\n#### Counting parameters\n\n#### Counting parameters\n\n15:44\n\n#### Counting parameters\n\n15:44\n\n#### Cross-attention\n\n#### Cross-attention\n\n18:21\n\n#### Cross-attention\n\n18:21\n\n#### Multiple heads\n\n#### Multiple heads\n\n19:19\n\n#### Multiple heads\n\n19:19\n\n#### The output matrix\n\n#### The output matrix\n\n22:16\n\n#### The output matrix\n\n22:16\n\n#### Going deeper\n\n#### Going deeper\n\n23:19\n\n#### Going deeper\n\n23:19\n\n#### Ending\n\n#### Ending\n\n24:54\n\n#### Ending\n\n24:54\n\nSync to video time\n\n## Description\n\nVisualizing Attention, a Transformer's Heart | Chapter 6, Deep Learning\n\n10KLikes\n\n174,673Views\n\n12hAgo\n\nDemystifying attention, the key mechanism inside transformers and LLMs. Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support Special thanks to these supporters: https://www.3blue1brown.com/lessons/a... An equally valuable form of support is to simply share the videos. Demystifying self-attention, multiple heads, and cross-attention. Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support The first pass for the translated subtitles here is machine-generated, and therefore notably imperfect. To contribute edits or fixes, visit https://translate.3blue1brown.com/ ------------------ Here are a few other relevant resources Build a GPT from scratch, by Andrej Karpathy \u2022 Let's build GPT: from scratch, in cod... If you want a conceptual understanding of language models from the ground up, @vcubingx just started a short series of videos on the topic: \u2022 What does it mean for computers to un... If you're interested in the herculean task of interpreting what these large networks might actually be doing, the Transformer Circuits posts by Anthropic are great. In particular, it was only after reading one of these that I started thinking of the combination of the value and output matrices as being a combined low-rank map from the embedding space to itself, which, at least in my mind, made things much clearer than other sources. https://transformer-circuits.pub/2021... Site with exercises related to ML programming and GPTs https://www.gptandchill.ai/codingprob... History of language models by Brit Cruise, @ArtOfTheProblem \u2022 ChatGPT: 30 Year History | How AI Lea... An early paper on how directions in embedding spaces have meaning: https://arxiv.org/pdf/1301.3781.pdf ------------------ Timestamps: 0:00 - Recap on embeddings 1:39 - Motivating examples 4:29 - The attention pattern 11:08 - Masking 12:42 - Context size 13:10 - Values 15:44 - Counting parameters 18:21 - Cross-attention 19:19 - Multiple heads 22:16 - The output matrix 23:19 - Going deeper 24:54 - Ending ------------------ These animations are largely made using a custom Python library, manim. See the FAQ comments here: https://3b1b.co/faq#manim https://github.com/3b1b/manim https://github.com/ManimCommunity/manim/ All code for specific videos is visible here: https://github.com/3b1b/videos/ The music is by Vincent Rubinetti. https://www.vincentrubinetti.com https://vincerubinetti.bandcamp.com/a... https://open.spotify.com/album/1dVyjw... ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. If you're reading the bottom of a video description, I'm guessing you're more interested than the average viewer in lessons here. It would mean a lot to me if you chose to stay up to date on new ones, either by subscribing here on YouTube or otherwise following on whichever platform below you check most regularly. Mailing list: https://3blue1brown.substack.com Twitter: / 3blue1brown Instagram: / 3blue1brown Reddit: / 3blue1brown Facebook: / 3blue1brown Patreon: / 3blue1brown Website: https://www.3blue1brown.com\n\nShow less Show more\n\n##\n\nChapters\n\n#### Recap on embeddings\n\n#### Recap on embeddings\n\n0:00\n\n#### Recap on embeddings\n\n0:00\n\n#### Motivating examples\n\n#### Motivating examples\n\n1:39\n\n#### Motivating examples\n\n1:39\n\n#### The attention pattern\n\n#### The attention pattern\n\n4:29\n\n#### The attention pattern\n\n4:29\n\n#### Masking\n\n#### Masking\n\n11:08\n\n#### Masking\n\n11:08\n\nFeatured course\n\n6 videos\n\nNeural networks\n\n3Blue1Brown\n\nTranscript\n\nFollow along using the transcript.\n\n### 3Blue1Brown\n\n6.07M subscribers\n\nVideos\n\nAbout\n\nVideos\n\nAbout\n\nPatreon\n\nTwitter\n\nReddit\n\nInstagram\n\nFacebook\n\nBut what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning\n\nby 3Blue1Brown\n\nWhat does it mean for computers to understand language? | LM1\n\nby vcubingx\n\n## Comments 531\n\nTop comments\n\nNewest first\n\n## Transcript\n\nNaN / NaN\n\n# Visualizing Attention, a Transformer's Heart | Chapter 6, Deep Learning\n\n3Blue1Brown\n\n3Blue1Brown\n\n6.07M subscribers\n\n<__slot-el>\n\n<__slot-el>\n\nDownload\n\n174,673 views 12 hours ago 3Blue1Brown series S3 E6\n\n174,673 views \u2022 Apr 7, 2024 \u2022 3Blue1Brown series S3 E6\n\nShow less\n\nDemystifying attention, the key mechanism inside transformers and LLMs.\nInstead of sponsored ad reads, these lessons are funded directly by viewers:\nhttps://3b1b.co/support Special thanks to these supporters:\nhttps://www.3blue1brown.com/lessons/a... ......more\n\n...more\n\n##\n\nChapters\n\n#### Recap on embeddings\n\n#### Recap on embeddings\n\n0:00\n\n#### Recap on embeddings\n\n0:00\n\n#### Motivating examples\n\n#### Motivating examples\n\n1:39\n\n#### Motivating examples\n\n1:39\n\n#### The attention pattern\n\n#### The attention pattern\n\n4:29\n\n#### The attention pattern\n\n4:29\n\n#### Masking\n\n#### Masking\n\n11:08\n\n#### Masking\n\n11:08\n\n#### Context size\n\n#### Context size\n\n12:42\n\n#### Context size\n\n12:42\n\n#### Values\n\n#### Values\n\n13:10\n\n#### Values\n\n13:10\n\n#### Counting parameters\n\n#### Counting parameters\n\n15:44\n\n#### Counting parameters\n\n15:44\n\n#### Cross-attention\n\n#### Cross-attention\n\n18:21\n\n#### Cross-attention\n\n18:21\n\nFeatured course\n\n6 videos\n\nNeural networks\n\n3Blue1Brown\n\nTranscript\n\nFollow along using the transcript.\n\n### 3Blue1Brown\n\n6.07M subscribers\n\nVideos\n\nAbout\n\nVideos\n\nAbout\n\nPatreon\n\nTwitter\n\nReddit\n\nInstagram\n\nFacebook\n\nBut what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning\n\nby 3Blue1Brown\n\nWhat does it mean for computers to understand language? | LM1\n\nby vcubingx\n\nShow less\n\n3Blue1Brown series S3 E6\n\n# Visualizing Attention, a Transformer's Heart | Chapter 6, Deep Learning\n\n174,673 views\n\nApr 7, 2024\n\nDownload\n\n###\n\n3Blue1Brown series S3 E5\n\nBut what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning\n\n3Blue1Brown\n\n3Blue1Brown\n\n\u2022\n\nNew\n\n\u2022\n\n1.5M views 6 days ago\n\n###\n\n3Blue1Brown series S3 E1\n\nBut what is a neural network? | Chapter 1, Deep learning\n\n3Blue1Brown\n\n3Blue1Brown\n\n\u2022\n\n\u2022\n\n15M views 6 years ago\n\n### Convolutions | Why X+Y in probability is a beautiful mess\n\n3Blue1Brown\n\n3Blue1Brown\n\n\u2022\n\n\u2022\n\n610K views 9 months ago\n\n### How TSMC Handled an Earthquake\n\nAsianometry\n\nAsianometry\n\n\u2022\n\n\u2022\n\n7.5K views 2 hours ago\n\nNew\n\n### Understanding Bayesian Statistics Without Frequentist Language -- Richard\nMcElreath (MPI)\n\nTeaching and Learning Mathematics Online\n\nTeaching and Learning Mathematics Online\n\n\u2022\n\n\u2022\n\n2K views 4 days ago\n\nNew\n\n### Is the iPhone \"Illegal?\"\n\nMarques Brownlee\n\nMarques Brownlee\n\n\u2022\n\n\u2022\n\n1.1M views 9 hours ago\n\nNew\n\n### Elon Musk SpaceX Presentation Leaves Audience SPEECHLESS\n\nFarzad\n\nFarzad\n\n\u2022\n\n\u2022\n\n194K views 1 day ago\n\nNew\n\n### But why would light \"slow down\"? | Optics puzzles 3\n\n3Blue1Brown\n\n3Blue1Brown\n\n\u2022\n\n\u2022\n\n991K views 4 months ago\n\n### MIT 6.S191: Recurrent Neural Networks, Transformers, and Attention\n\nAlexander Amini\n\nAlexander Amini\n\n\u2022\n\n\u2022\n\n611K views 1 year ago\n\n### Which Planet Has the Best Eclipse?\n\nminutephysics\n\nminutephysics\n\n\u2022\n\n\u2022\n\n137K views 2 days ago\n\nNew\n\n### If this ships, it will change javascript forever\n\nTheo - t3.gg\n\nTheo - t3.gg\n\n\u2022\n\n\u2022\n\n93K views 21 hours ago\n\nNew\n\n### How Deep Neural Networks Work - Full Course for Beginners\n\nfreeCodeCamp.org\n\nfreeCodeCamp.org\n\n\u2022\n\n\u2022\n\n2.9M views 4 years ago\n\n### HE WON AGAIN!!!!!!!!!!!!!!!!!!!!!!!\n\nGothamChess\n\nGothamChess\n\n\u2022\n\n\u2022\n\n20K views 40 minutes ago\n\nNew\n\n### Solving Wordle using information theory\n\n3Blue1Brown\n\n3Blue1Brown\n\n\u2022\n\n\u2022\n\n10M views 2 years ago\n\n### Goliath & Leviathan Numbers - Numberphile\n\nNumberphile\n\nNumberphile\n\n\u2022\n\n\u2022\n\n35K views 9 hours ago\n\nNew\n\n### Can This Forgotten Invention Save Your Life?\n\nI did a thing\n\nI did a thing\n\n\u2022\n\n\u2022\n\n1.8M views 1 day ago\n\nNew\n\nSaving your choice\n\nAn error occurred while saving your choice. Please try again.\n\nA Google company\n\nSign in\n\nSign in\n\n## Before you continue to YouTube\n\nWe use cookies and data to\n\n  * Deliver and maintain Google services\n\n  * Track outages and protect against spam, fraud, and abuse\n\n  * Measure audience engagement and site statistics to understand how our services are used and enhance the quality of those services\n\nIf you choose to \u201cAccept all,\u201d we will also use cookies and data to\n\n  * Develop and improve new services\n\n  * Deliver and measure the effectiveness of ads\n\n  * Show personalized content, depending on your settings\n\n  * Show personalized ads, depending on your settings\n\nIf you choose to \u201cReject all,\u201d we will not use cookies for these additional\npurposes.\n\nNon-personalized content and ads are influenced by things like the content\nyou\u2019re currently viewing and your location (ad serving is based on general\nlocation). Personalized content and ads can also include things like video\nrecommendations, a customized YouTube homepage, and tailored ads based on past\nactivity, like the videos you watch and the things you search for on YouTube.\nWe also use cookies and data to tailor the experience to be age-appropriate,\nif relevant.\n\nSelect \u201cMore options\u201d to see additional information, including details about\nmanaging your privacy settings. You can also visit g.co/privacytools at any\ntime.\n\nMore options\n\nPrivacy Policy \u2022 Terms of Service\n\n", "frontpage": true}
