{"aid": "40010698", "title": "Modern Microprocessors A 90-Minute Guide (2016)", "url": "https://www.lighterra.com/papers/modernmicroprocessors/", "domain": "lighterra.com", "votes": 1, "user": "Tomte", "posted_at": "2024-04-12 09:07:46", "comments": 0, "source_title": "Modern Microprocessors", "source_text": "Modern Microprocessors - A 90-Minute Guide!\n\n# Modern Microprocessors A 90-Minute Guide!\n\nA brief, pulls-no-punches, fast-paced introduction to the main design aspects\nof modern processor microarchitecture.\n\nToday's robots are very primitive, capable of understanding only a few simple\ninstructions such as 'go left', 'go right' and 'build car'.\n\n\u2014 John Sladek\n\nby Jason Robert Carey Patterson, last updated Aug 2016 (orig Feb 2001)\n\nWARNING: This article is meant to be informal and fun!\n\nOkay, so you're a CS graduate and you did a hardware course as part of your\ndegree, but perhaps that was a few years ago now and you haven't really kept\nup with the details of processor designs since then.\n\nIn particular, you might not be aware of some key topics that developed\nrapidly in recent times...\n\n  * pipelining (superscalar, OOO, VLIW, branch prediction, predication)\n  * multi-core and simultaneous multi-threading (SMT, hyper-threading)\n  * SIMD vector instructions (MMX/SSE/AVX, AltiVec, NEON)\n  * caches and the memory hierarchy\n\nFear not! This article will get you up to speed fast. In no time, you'll be\ndiscussing the finer points of in-order vs out-of-order, hyper-threading,\nmulti-core and cache organization like a pro.\n\nBut be prepared \u2013 this article is brief and to-the-point. It pulls no punches\nand the pace is pretty fierce (really). Let's get into it...\n\n## More Than Just Megahertz\n\nThe first issue that must be cleared up is the difference between clock speed\nand a processor's performance. They are not the same thing. Look at the\nresults for processors of a few years ago (the late 1990s)...\n\nSPECint95| SPECfp95  \n---|---  \n195 MHz| MIPS R10000| 11.0| 17.0  \n400 MHz| Alpha 21164| 12.3| 17.2  \n300 MHz| UltraSPARC| 12.1| 15.5  \n300 MHz| Pentium II| 11.6| 8.8  \n300 MHz| PowerPC G3| 14.8| 11.4  \n135 MHz| POWER2| 6.2| 17.6  \n  \nTable 1 \u2013 Processor performance circa 1997.\n\nA 200 MHz MIPS R10000, a 300 MHz UltraSPARC and a 400 MHz Alpha 21164 were all\nabout the same speed at running most programs, yet they differed by a factor\nof two in clock speed. A 300 MHz Pentium II was also about the same speed for\nmany things, yet it was about half that speed for floating-point code such as\nscientific number crunching. A PowerPC G3 at that same 300 MHz was somewhat\nfaster than the others for normal integer code, but still far slower than the\ntop three for floating-point. At the other extreme, an IBM POWER2 processor at\njust 135 MHz matched the 400 MHz Alpha 21164 in floating-point speed, yet was\nonly half as fast for normal integer programs.\n\nHow can this be? Obviously, there's more to it than just clock speed \u2013 it's\nall about how much work gets done in each clock cycle. Which leads to...\n\n## Pipelining & Instruction-Level Parallelism\n\nInstructions are executed one after the other inside the processor, right?\nWell, that makes it easy to understand, but that's not really what happens. In\nfact, that hasn't happened since the middle of the 1980s. Instead, several\ninstructions are all partially executing at the same time.\n\nConsider how an instruction is executed \u2013 first it is fetched, then decoded,\nthen executed by the appropriate functional unit, and finally the result is\nwritten into place. With this scheme, a simple processor might take 4 cycles\nper instruction (CPI = 4)...\n\nFigure 1 \u2013 The instruction flow of a sequential processor.\n\nModern processors overlap these stages in a pipeline, like an assembly line.\nWhile one instruction is executing, the next instruction is being decoded, and\nthe one after that is being fetched...\n\nFigure 2 \u2013 The instruction flow of a pipelined processor.\n\nNow the processor is completing 1 instruction every cycle (CPI = 1). This is a\nfour-fold speedup without changing the clock speed at all. Not bad, huh?\n\nFrom the hardware point of view, each pipeline stage consists of some\ncombinatorial logic and possibly access to a register set and/or some form of\nhigh-speed cache memory. The pipeline stages are separated by latches. A\ncommon clock signal synchronizes the latches between each stage, so that all\nthe latches capture the results produced by the pipeline stages at the same\ntime. In effect, the clock \"pumps\" instructions down the pipeline.\n\nAt the beginning of each clock cycle, the data and control information for a\npartially processed instruction is held in a pipeline latch, and this\ninformation forms the inputs to the logic circuits of the next pipeline stage.\nDuring the clock cycle, the signals propagate through the combinatorial logic\nof the stage, producing an output just in time to be captured by the next\npipeline latch at the end of the clock cycle...\n\nFigure 3 \u2013 A pipelined microarchitecture.\n\nSince the result from each instruction is available after the execute stage\nhas completed, the next instruction ought to be able to use that value\nimmediately, rather than waiting for that result to be committed to its\ndestination register in the writeback stage. To allow this, forwarding lines\ncalled bypasses are added, going backwards along the pipeline...\n\nFigure 4 \u2013 A pipelined microarchitecture with bypasses.\n\nAlthough the pipeline stages look simple, it is important to remember the\nexecute stage in particular is really made up of several different groups of\nlogic (several sets of gates), making up different functional units for each\ntype of operation the processor must be able to perform...\n\nFigure 5 \u2013 A pipelined microarchitecture in more detail.\n\nThe early RISC processors, such as IBM's 801 research prototype, the MIPS\nR2000 (based on the Stanford MIPS machine) and the original SPARC (derived\nfrom the Berkeley RISC project), all implemented a simple 5-stage pipeline not\nunlike the one shown above. At the same time, the mainstream 80386, 68030 and\nVAX CISC processors worked largely sequentially \u2013 it's much easier to pipeline\na RISC because its reduced instruction set means the instructions are mostly\nsimple register-to-register operations, unlike the complex instruction sets of\nx86, 68k or VAX. As a result, a pipelined SPARC running at 20 MHz was way\nfaster than a sequential 386 running at 33 MHz. Every processor since then has\nbeen pipelined, at least to some extent. A good summary of the original RISC\nresearch projects can be found in the 1985 CACM article by David Patterson.\n\n## Deeper Pipelines \u2013 Superpipelining\n\nSince the clock speed is limited by (among other things) the length of the\nlongest, slowest stage in the pipeline, the logic gates that make up each\nstage can be subdivided, especially the longer ones, converting the pipeline\ninto a deeper super-pipeline with a larger number of shorter stages. Then the\nwhole processor can be run at a higher clock speed! Of course, each\ninstruction will now take more cycles to complete (latency), but the processor\nwill still be completing 1 instruction per cycle (throughput), and there will\nbe more cycles per second, so the processor will complete more instructions\nper second (actual performance)...\n\nFigure 6 \u2013 The instruction flow of a superpipelined processor.\n\nThe Alpha architects in particular liked this idea, which is why the early\nAlphas had deep pipelines and ran at such high clock speeds for their era.\nToday, modern processors strive to keep the number of gate delays down to just\na handful for each pipeline stage, about 12-25 gates deep (not total!) plus\nanother 3-5 for the latch itself, and most have quite deep pipelines...\n\nPipeline Depth| Processors  \n---|---  \n6| UltraSPARC T1  \n7| PowerPC G4e  \n8| UltraSPARC T2/T3, Cortex-A9  \n10| Athlon, Scorpion  \n11| Krait  \n12| Pentium Pro/II/III, Athlon 64/Phenom, Apple A6  \n13| Denver  \n14| UltraSPARC III/IV, Core 2, Apple A7/A8  \n14/19| Core i*2/i*3 Sandy/Ivy Bridge, Core i*4/i*5 Haswell/Broadwell  \n15| Cortex-A15/A57  \n16| PowerPC G5, Core i*1 Nehalem  \n18| Bulldozer/Piledriver, Steamroller  \n20| Pentium 4  \n31| Pentium 4E Prescott  \n  \nTable 2 \u2013 Pipeline depths of common processors.\n\nThe x86 processors generally have deeper pipelines than the RISCs (of\ncomparable era) because they need to do extra work to decode the complex x86\ninstructions (more on this later). UltraSPARC T1/T2/T3 Niagara are a recent\nexception to the deep-pipeline trend \u2013 just 6 for UltraSPARC T1 and 8 for\nT2/T3 to keep those cores as small as possible (more on this later, too).\n\n## Multiple Issue \u2013 Superscalar\n\nSince the execute stage of the pipeline is really a bunch of different\nfunctional units, each doing its own task, it seems tempting to try to execute\nmultiple instructions in parallel, each in its own functional unit. To do\nthis, the fetch and decode/dispatch stages must be enhanced so they can decode\nmultiple instructions in parallel and send them out to the \"execution\nresources\"...\n\nFigure 7 \u2013 A superscalar microarchitecture.\n\nOf course, now that there are independent pipelines for each functional unit,\nthey can even have different numbers of stages. This allows the simpler\ninstructions to complete more quickly, reducing latency (which we'll get to\nsoon). Since such processors have many different pipeline depths, it's normal\nto refer to the depth of a processor's pipeline when executing integer\ninstructions, which is usually the shortest of the possible pipeline paths,\nwith the memory and floating-point pipelines implied as having a few\nadditional stages. Thus, a processor with a \"10-stage pipeline\" would use 10\nstages for executing integer instructions, perhaps 12 or 13 stages for memory\ninstructions, and maybe 14 or 15 stages for floating-point. There are also a\nbunch of bypasses within and between the various pipelines, but these have\nbeen left out of the diagram for simplicity.\n\nIn the above example, the processor could potentially issue 3 different\ninstructions per cycle \u2013 for example 1 integer, 1 floating-point and 1 memory\ninstruction. Even more functional units could be added, so that the processor\nmight be able to execute 2 integer instructions per cycle, or 2 floating-point\ninstructions, or whatever the target applications could best use.\n\nOn a superscalar processor, the instruction flow looks something like...\n\nFigure 8 \u2013 The instruction flow of a superscalar processor.\n\nThis is great! There are now 3 instructions completing every cycle (CPI =\n0.33, or IPC = 3, also written as ILP = 3 for instruction-level parallelism).\nThe number of instructions able to be issued, executed or completed per cycle\nis called a processor's width.\n\nNote that the issue width is less than the number of functional units \u2013 this\nis typical. There must be more functional units because different code\nsequences have different mixes of instructions. The idea is to execute 3\ninstructions per cycle, but those instructions are not always going to be 1\ninteger, 1 floating-point and 1 memory operation, so more than 3 functional\nunits are required.\n\nThe IBM POWER1 processor, the predecessor of PowerPC, was the first mainstream\nsuperscalar processor. Most of the RISCs went superscalar soon after\n(SuperSPARC, Alpha 21064). Intel even managed to build a superscalar x86 \u2013 the\noriginal Pentium \u2013 however the complex x86 instruction set was a real problem\nfor them (more on this later).\n\nOf course, there's nothing stopping a processor from having both a deep\npipeline and multiple instruction issue, so it can be both superpipelined and\nsuperscalar at the same time...\n\nFigure 9 \u2013 The instruction flow of a superpipelined-superscalar processor.\n\nToday, virtually every processor is a superpipelined-superscalar, so they're\njust called superscalar for short. Strictly speaking, superpipelining is just\npipelining with a deeper pipe anyway.\n\nThe widths of modern processors vary considerably...\n\nIssue Width| Processors  \n---|---  \n1| UltraSPARC T1  \n2| UltraSPARC T2/T3, Scorpion, Cortex-A9  \n3| Pentium Pro/II/III/M, Pentium 4, Krait, Apple A6, Cortex-A15/A57  \n4| UltraSPARC III/IV, PowerPC G4e  \n4/8| Bulldozer/Piledriver, Steamroller  \n5| PowerPC G5  \n6| Athlon, Athlon 64/Phenom, Core 2, Core i*1 Nehalem, Core i*2/i*3 Sandy/Ivy\nBridge, Apple A7/A8  \n7| Denver  \n8| Core i*4/i*5 Haswell/Broadwell  \n  \nTable 3 \u2013 Issue widths of common processors.\n\nThe exact number and type of functional units in each processor depends on its\ntarget market. Some processors have more floating-point execution resources\n(IBM's POWER line), others are more integer-biased (Pentium Pro/II/III/M),\nsome devote much of their resources to SIMD vector instructions (PowerPC\nG4/G4e), while most try to take the \"balanced\" middle ground.\n\n## Explicit Parallelism \u2013 VLIW\n\nIn cases where backward compatibility is not an issue, it is possible for the\ninstruction set itself to be designed to explicitly group instructions to be\nexecuted in parallel. This approach eliminates the need for complex\ndependency-checking logic in the dispatch stage, which should make the\nprocessor easier to design, smaller, and easier to ramp up the clock speed\nover time (at least in theory).\n\nIn this style of processor, the \"instructions\" are really groups of little\nsub-instructions, and thus the instructions themselves are very long, often\n128 bits or more, hence the name VLIW \u2013 very long instruction word. Each\ninstruction contains information for multiple parallel operations.\n\nA VLIW processor's instruction flow is much like a superscalar, except the\ndecode/dispatch stage is much simpler and only occurs for each group of sub-\ninstructions...\n\nFigure 10 \u2013 The instruction flow of a VLIW processor.\n\nOther than the simplification of the dispatch logic, VLIW processors are much\nlike superscalar processors. This is especially so from a compiler's point of\nview (more on this later).\n\nIt is worth noting, however, that most VLIW designs are not interlocked. This\nmeans they do not check for dependencies between instructions, and often have\nno way of stalling instructions other than to stall the whole processor on a\ncache miss. As a result, the compiler needs to insert the appropriate number\nof cycles between dependent instructions, even if there are no instructions to\nfill the gap, by using nops (no-operations, pronounced \"no ops\") if necessary.\nThis complicates the compiler somewhat, because it is doing something that a\nsuperscalar processor normally does at runtime, however the extra code in the\ncompiler is minimal and it saves precious resources on the processor chip.\n\nNo VLIW designs have yet been commercially successful as mainstream CPUs,\nhowever Intel's IA-64 architecture, which is still in production in the form\nof the Itanium processors, was once intended to be the replacement for x86.\nIntel chose to call IA-64 an \"EPIC\" design, for \"explicitly parallel\ninstruction computing\", but it was essentially a VLIW with clever grouping (to\nallow long-term compatibility) and predication (see below). The programmable\nshaders in graphics processors (GPUs) are sometimes VLIW designs, as are many\ndigital signal processors (DSPs), and there was also Transmeta (see the x86\nsection, coming up soon).\n\n## Instruction Dependencies & Latencies\n\nHow far can pipelining and multiple issue be taken? If a 5-stage pipeline is 5\ntimes faster, why not build a 20-stage superpipeline? If 4-issue superscalar\nis good, why not go for 8-issue? For that matter, why not build a processor\nwith a 50-stage pipeline which issues 20 instructions per cycle?\n\nWell, consider the following two instructions...\n\n    \n    \n    a = b * c; d = a + 1;\n\nThe second instruction depends on the first \u2013 the processor can't execute the\nsecond instruction until after the first has completed calculating its result.\nThis is a serious problem, because instructions that depend on each other\ncannot be executed in parallel. Thus, multiple issue is impossible in this\ncase.\n\nIf the first instruction was a simple integer addition then this might still\nbe okay in a pipelined single-issue processor, because integer addition is\nquick and the result of the first instruction would be available just in time\nto feed it back into the next instruction (using bypasses). However in the\ncase of a multiply, which will take several cycles to complete, there is no\nway the result of the first instruction will be available when the second\ninstruction reaches the execute stage just one cycle later. So, the processor\nwill need to stall the execution of the second instruction until its data is\navailable, inserting a bubble into the pipeline where no work gets done.\n\nIt can be confusing when the word \"latency\" is used for related, but\ndifferent, meanings. Here, I'm talking about the latency as seen by a\ncompiler. Some hardware engineers may think of latency as the number of cycles\nrequired for execution (the number of pipeline stages). So a hardware engineer\nmight say the instructions in a simple integer pipeline have a latency of 5\nbut a throughput of 1, whereas from a compiler's point of view they have a\nlatency of 1 because their results are available for use in the very next\ncycle. The compiler view is the more common, and is generally used even in\nhardware manuals.\n\nThe number of cycles between when an instruction reaches the execute stage and\nwhen its result is available for use by other instructions is called the\ninstruction's latency. The deeper the pipeline, the more stages and thus the\nlonger the latency. So a very deep pipeline is not much more effective than a\nshort one, because a deep one just gets filled up with bubbles thanks to all\nthose nasty instructions depending on each other.\n\nFrom a compiler's point of view, typical latencies in modern processors range\nfrom a single cycle for integer operations, to around 3-6 cycles for floating-\npoint addition and the same or perhaps slightly longer for multiplication,\nthrough to over a dozen cycles for integer division.\n\nLatencies for memory loads are particularly troublesome, in part because they\ntend to occur early within code sequences, which makes it difficult to fill\ntheir delays with useful instructions, and equally importantly because they\nare somewhat unpredictable \u2013 the load latency varies a lot depending on\nwhether the access is a cache hit or not (we'll get to caches later).\n\n## Branches & Branch Prediction\n\nAnother key problem for pipelining is branches. Consider the following code\nsequence...\n\n    \n    \n    if (a > 7) { b = c; } else { b = d; }\n\n...which compiles into something like...\n\n    \n    \n    cmp a, 7 ; a > 7 ? ble L1 mov c, b ; b = c br L2 L1: mov d, b ; b = d L2: ...\n\nNow consider a pipelined processor executing this code sequence. By the time\nthe conditional branch at line 2 reaches the execute stage in the pipeline,\nthe processor must have already fetched and decoded the next couple of\ninstructions. But which instructions? Should it fetch and decode the if branch\n(lines 3 and 4) or the else branch (line 5)? It won't really know until the\nconditional branch gets to the execute stage, but in a deeply pipelined\nprocessor that might be several cycles away. And it can't afford to just wait\n\u2013 the processor encounters a branch every six instructions on average, and if\nit was to wait several cycles at every branch then most of the performance\ngained by using pipelining in the first place would be lost.\n\nSo the processor must make a guess. The processor will then fetch down the\npath it guessed and speculatively begin executing those instructions. Of\ncourse, it won't be able to actually commit (writeback) those instructions\nuntil the outcome of the branch is known. Worse, if the guess is wrong the\ninstructions will have to be cancelled, and those cycles will have been\nwasted. But if the guess is correct, the processor will be able to continue on\nat full speed.\n\nThe key question is how the processor should make the guess. Two alternatives\nspring to mind. First, the compiler might be able to mark the branch to tell\nthe processor which way to go. This is called static branch prediction. It\nwould be ideal if there was a bit in the instruction format in which to encode\nthe prediction, but for older architectures this is not an option, so a\nconvention can be used instead, such as backward branches are predicted to be\ntaken while forward branches are predicted not-taken. More importantly,\nhowever, this approach requires the compiler to be quite smart in order for it\nto make the correct guess, which is easy for loops but might be difficult for\nother branches.\n\nThe other alternative is to have the processor make the guess at runtime.\nNormally, this is done by using an on-chip branch prediction table containing\nthe addresses of recent branches and a bit indicating whether each branch was\ntaken or not last time. In reality, most processors actually use two bits, so\nthat a single not-taken occurrence doesn't reverse a generally taken\nprediction (important for loop back edges). Of course, this dynamic branch\nprediction table takes up valuable space on the processor chip, but branch\nprediction is so important that it's well worth it.\n\nUnfortunately, even the best branch prediction techniques are sometimes wrong,\nand with a deep pipeline many instructions might need to be cancelled. This is\ncalled the mispredict penalty. The Pentium Pro/II/III was a good example \u2013 it\nhad a 12-stage pipeline and thus a mispredict penalty of 10-15 cycles. Even\nwith a clever dynamic branch predictor that correctly predicted an impressive\n90% of the time, this high mispredict penalty meant about 30% of the Pentium\nPro/II/III's performance was lost due to mispredictions. Put another way, one\nthird of the time the Pentium Pro/II/III was not doing useful work, but\ninstead was saying \"oops, wrong way\".\n\nModern processors devote ever more hardware to branch prediction in an attempt\nto raise the prediction accuracy even further, and reduce this cost. Many\nrecord each branch's direction not just in isolation, but in the context of\nthe couple of branches leading up to it, which is called a two-level adaptive\npredictor. Some keep a more global branch history, rather than a separate\nhistory for each individual branch, in an attempt to detect any correlations\nbetween branches even if they're relatively far away in the code. That's\ncalled a gshare or gselect predictor. The most advanced modern processors\noften implement several branch predictors and select between them based on\nwhich one seems to be working best for each individual branch!\n\nNonetheless, even the very best modern processors with the best, smartest\nbranch predictors only reach a prediction accuracy of about 95%, and still\nlose quite a lot of performance due to branch mispredictions. The bottom line\nis simple \u2013 very deep pipelines naturally suffer from diminishing returns,\nbecause the deeper the pipeline, the further into the future you must try to\npredict, the more likely you'll be wrong, and the greater the mispredict\npenalty when you are.\n\n## Eliminating Branches with Predication\n\nConditional branches are so problematic that it would be nice to eliminate\nthem altogether. Clearly, if statements cannot be eliminated from programming\nlanguages, so how can the resulting branches possibly be eliminated? The\nanswer lies in the way some branches are used.\n\nConsider the above example once again. Of the five instructions, two are\nbranches, and one of those is an unconditional branch. If it was possible to\nsomehow tag the mov instructions to tell them to execute only under some\nconditions, the code could be simplified...\n\n    \n    \n    cmp a, 7 ; a > 7 ? mov c, b ; b = c cmovle d, b ; if le, then b = d\n\nHere, a new instruction has been introduced called cmovle, for \"conditional\nmove if less than or equal\". This instruction works by executing as normal,\nbut only commits itself if its condition is true. This is called a predicated\ninstruction, because its execution is controlled by a predicate (a true/false\ntest).\n\nGiven this new predicated move instruction, two instructions have been\neliminated from the code, and both were costly branches. In addition, by being\nclever and always doing the first mov then overwriting it if necessary, the\nparallelism of the code has also been increased \u2013 lines 1 and 2 can now be\nexecuted in parallel, resulting in a 50% speedup (2 cycles rather than 3).\nMost importantly, though, the possibility of getting the branch prediction\nwrong and suffering a large mispredict penalty has been eliminated.\n\nOf course, if the blocks of code in the if and else cases were longer, then\nusing predication would mean executing more instructions than using a branch,\nbecause the processor is effectively executing both paths through the code.\nWhether it's worth executing a few more instructions to avoid a branch is a\ntricky decision \u2013 for very small or very large blocks the decision is simple,\nbut for medium-sized blocks there are complex tradeoffs which the optimizer\nmust consider.\n\nThe Alpha architecture had a conditional move instruction from the very\nbeginning. MIPS, SPARC and x86 added it later. With IA-64, Intel went all-out\nand made almost every instruction predicated in the hope of dramatically\nreducing branching problems in inner loops, especially ones where the branches\nare unpredictable, such as compilers and OS kernels. Interestingly, the ARM\narchitecture used in many phones and tablets was the first architecture with a\nfully predicated instruction set. This is even more intriguing given that the\nearly ARM processors only had short pipelines and thus relatively small\nmispredict penalties.\n\n## Instruction Scheduling, Register Renaming & OOO\n\nIf branches and long-latency instructions are going to cause bubbles in the\npipeline(s), then perhaps those empty cycles can be used to do other work. To\nachieve this, the instructions in the program must be reordered so that while\none instruction is waiting, other instructions can execute. For example, it\nmight be possible to find a couple of other instructions from further down in\nthe program and put them between the two instructions in the earlier multiply\nexample.\n\nThere are two ways to do this. One approach is to do the reordering in\nhardware at runtime. Doing dynamic instruction scheduling (reordering) in the\nprocessor means the dispatch logic must be enhanced to look at groups of\ninstructions and dispatch them out of order as best it can to use the\nprocessor's functional units. Not surprisingly, this is called out-of-order\nexecution, or just OOO for short (sometimes written OoO or OOE).\n\nIf the processor is going to execute instructions out of order, it will need\nto keep in mind the dependencies between those instructions. This can be made\neasier by not dealing with the raw architecturally-defined registers, but\ninstead using a set of renamed registers. For example, a store of a register\ninto memory, followed by a load of some other piece of memory into the same\nregister, represent different values and need not go into the same physical\nregister. Furthermore, if these different instructions are mapped to different\nphysical registers they can be executed in parallel, which is the whole point\nof OOO execution. So, the processor must keep a mapping of the instructions in\nflight at any moment and the physical registers they use. This process is\ncalled register renaming. As an added bonus, it becomes possible to work with\na potentially larger set of real registers in an attempt to extract even more\nparallelism out of the code.\n\nAll of this dependency analysis, register renaming and OOO execution adds a\nlot of complex logic to the processor, making it harder to design, larger in\nterms of chip area, and more power-hungry. The extra logic is particularly\npower-hungry because those transistors are always working, unlike the\nfunctional units which spend at least some of their time idle (possibly even\npowered down). On the other hand, out-of-order execution offers the advantage\nthat software need not be recompiled to get at least some of the benefits of\nthe new processor's design, though typically not all.\n\nAnother approach to the whole problem is to have the compiler optimize the\ncode by rearranging the instructions. This is called static, or compile-time,\ninstruction scheduling. The rearranged instruction stream can then be fed to a\nprocessor with simpler in-order multiple-issue logic, relying on the compiler\nto \"spoon feed\" the processor with the best instruction stream. Avoiding the\nneed for complex OOO logic should make the processor quite a lot easier to\ndesign, less power-hungry and smaller, which means more cores, or extra cache,\ncould be placed onto the same amount of chip area (more on this later).\n\nThe compiler approach also has some other advantages over OOO hardware \u2013 it\ncan see further down the program than the hardware, and it can speculate down\nmultiple paths rather than just one, which is a big issue if branches are\nunpredictable. On the other hand, a compiler can't be expected to be psychic,\nso it can't necessarily get everything perfect all the time. Without OOO\nhardware, the pipeline will stall when the compiler fails to predict something\nlike a cache miss.\n\nMost of the early superscalars were in-order designs (SuperSPARC, hyperSPARC,\nUltraSPARC, Alpha 21064 & 21164, the original Pentium). Examples of early OOO\ndesigns included the MIPS R10000, Alpha 21264 and to some extent the entire\nPOWER/PowerPC line (with their reservation stations). Today, almost all high-\nperformance processors are out-of-order designs, with the notable exceptions\nof UltraSPARC III/IV, POWER6 and Denver. Most low-power, low-performance\nprocessors, such as Cortex-A7/A53 and Atom, are in-order designs because OOO\nlogic consumes a lot of power for a relatively small performance gain.\n\n## The Brainiac Debate\n\nA question that must be asked is whether the costly out-of-order logic is\nreally warranted, or whether compilers can do the task of instruction\nscheduling well enough without it. This is historically called the brainiac vs\nspeed-demon debate. This simple (and fun) classification of design styles\nfirst appeared in a 1993 Microprocessor Report editorial by Linley Gwennap,\nand was made widely known by Dileep Bhandarkar's Alpha Implementations &\nArchitecture book.\n\nBrainiac designs are at the smart-machine end of the spectrum, with lots of\nOOO hardware trying to squeeze every last drop of instruction-level\nparallelism out of the code, even if it costs millions of logic transistors\nand years of design effort to do it. In contrast, speed-demon designs are\nsimpler and smaller, relying on a smart compiler and willing to sacrifice a\nlittle bit of instruction-level parallelism for the other benefits that\nsimplicity brings. Historically, the speed-demon designs tended to run at\nhigher clock speeds, precisely because they were simpler, hence the \"speed-\ndemon\" name, but today that's no longer the case because clock speed is\nlimited mainly by power and thermal issues.\n\nClearly, OOO hardware should make it possible for more instruction-level\nparallelism to be extracted, because things will be known at runtime that\ncannot be predicted in advance \u2013 cache misses, in particular. On the other\nhand, a simpler in-order design will be smaller and use less power, which\nmeans you can place more small in-order cores onto the same chip as fewer,\nlarger out-of-order cores. Which would you rather have: 4 powerful brainiac\ncores, or 8 simpler in-order cores?\n\nExactly which is the more important factor is currently open to hot debate. In\ngeneral, it seems both the benefits and the costs of OOO execution have been\nsomewhat overstated in the past. In terms of cost, appropriate pipelining of\nthe dispatch and register-renaming logic allowed OOO processors to achieve\nclock speeds competitive with simpler designs by the late 1990s, and clever\nengineering has reduced the power overhead of OOO execution considerably in\nrecent years, leaving mainly the chip area cost. This is a testament to some\noutstanding engineering by processor architects.\n\nUnfortunately, however, the effectiveness of OOO execution in dynamically\nextracting additional instruction-level parallelism has been disappointing,\nwith only a relatively small improvement being seen, perhaps 20-40% or so over\nan equivalent in-order design. To quote Andy Glew, a pioneer of out-of-order\nexecution and one of the chief architects of the Pentium Pro/II/III: \"The\ndirty little secret of OOO is that we are often not very much OOO at all\".\nOut-of-order execution has also been unable to deliver the degree of\n\"recompile independence\" originally hoped for, with recompilation still\nproducing large speedups even on aggressive OOO processors.\n\nWhen it comes to the brainiac debate, many vendors have gone down one path\nthen changed their mind and switched to the other side...\n\nShow Lineage:\n\nFigure 11 \u2013 Brainiacs vs speed-demons.\n\nDEC, for example, went primarily speed-demon with the first two generations of\nAlpha, then changed to brainiac for the third generation. MIPS did similarly.\nSun, on the other hand, went brainiac with their first superscalar SPARC, then\nswitched to speed-demon for more recent designs. The POWER/PowerPC camp also\ngradually moved away from brainiac designs over the years (until recently),\nalthough the reservation stations in all POWER/PowerPC designs do offer a\ndegree of OOO execution between different functional units even if the\ninstructions within each functional unit's queue are executed strictly in\norder. ARM processors, in contrast, have shown a consistent move towards more\nbrainiac designs, coming up from the low-power, low-performance embedded world\nas they have, but still remaining mobile-centric and thus unable to push the\nclock speed too high.\n\nIntel has been the most interesting of all to watch. Modern x86 processors\nhave no choice but to be at least somewhat brainiac due to limitations of the\nx86 architecture (more on this soon), and the Pentium Pro embraced that\nsentiment wholeheartedly. Then the race against AMD to reach 1 GHz ensued,\nwhich AMD won by a nose in March 2000. Intel changed their focus to clock\nspeed at all cost, and made the Pentium 4 about as speed-demon as possible for\na decoupled x86 microarchitecture, sacrificing some ILP and using a deep\n20-stage pipeline to pass 2 and then 3 GHz, and with a later revision\nfeaturing a staggering 31-stage pipeline, reach as high as 3.8 GHz. At the\nsame time, with IA-64 Itanium (not shown above), Intel again bet solidly on\nthe smart-compiler approach, with a simple design relying totally on static,\ncompile-time scheduling. Faced with the failure of IA-64, the enormous power\nand heat issues of the Pentium 4, and the fact that AMD's more slowly clocked\nAthlon processors, in the 2 GHz range, were actually outperforming the Pentium\n4 on real-world code, Intel then reversed its position once again, and revived\nthe older Pentium Pro/II/III brainiac design to produce the Pentium M and its\nCore successors, which have been a great success.\n\n## The Power Wall & The ILP Wall\n\nThe Pentium 4's severe power and heat issues demonstrated there are limits to\nclock speed. It turns out power usage goes up even faster than clock speed\ndoes \u2013 for any given level of chip technology, increasing the clock speed of a\nprocessor by, say 20%, will typically increase its power usage by even more,\nmaybe 50%, because not only are the transistors switching 20% more often, but\nthe voltage also generally needs to be increased, in order to drive the\nsignals through the circuits faster to reliably meet the shorter timing\nrequirements, assuming the circuits work at all at the increased speed, of\ncourse. And while power increases linearly with clock frequency, it increases\nas the square of voltage, making for a kind of \"triple whammy\" at very high\nclock speeds (f*V*V).\n\nIt gets even worse, because in addition to the normal switching power, there\nis also a small amount of leakage power, since even when a transistor is off,\nthe current flowing through it isn't completely reduced to zero. And just like\nthe good, useful current, this leakage current also goes up as the voltage is\nincreased. If that wasn't bad enough, leakage generally goes up as the\ntemperature increases as well, due to the increased movement of the hotter,\nmore energetic electrons within the silicon.\n\nThe net result is that today, increasing a modern processor's clock speed by a\nrelatively modest 30% can take as much as double the power, and produce double\nthe heat...\n\nFigure 12 \u2013 The heatsink of a modern desktop processor, with front fan\nremoved.\n\nUp to a point this increase in power is okay, but at a certain point,\ncurrently somewhere around 150-200 watts, the power and heat problems become\nunmanageable, because it's simply not possible to provide that much power and\ncooling to a silicon chip in any practical fashion, even if the circuits\ncould, in fact, operate at higher clock speeds. This is called the power wall.\n\nProcessors which focused too much on clock speed, such as the Pentium 4, IBM's\nPOWER6 and most recently AMD's Bulldozer, quickly hit the power wall and found\nthemselves unable to push the clock speed as high as originally hoped,\nresulting in them being beaten by slower-clocked but smarter processors which\nexploited more instruction-level parallelism.\n\nThus, going purely for clock speed is not the best strategy. And of course,\nthis is even more true for portable, mobile devices, such as laptops, tablets\nand phones, where the power wall hits much sooner, around 50W for \"pro\"\nlaptops, 15W for ultralight laptops, 10W for tablets and less than 5W for\nphones, due to the constraints of battery capacity and limited, often fanless\ncooling.\n\nSo, if going primarily for clock speed is a problem, is going purely brainiac\nthe right approach then? Sadly, no. Pursuing more and more ILP also has\ndefinite limits, because unfortunately, normal programs just don't have a lot\nof fine-grained parallelism in them, due to a combination of load latencies,\ncache misses, branches and dependencies between instructions. This limit of\navailable instruction-level parallelism is called the ILP wall.\n\nProcessors which focused too much on ILP, such as the early POWER processors,\nSuperSPARC and the MIPS R10000, soon found their ability to extract additional\ninstruction-level parallelism was only modest, while the additional complexity\nseriously hindered their ability to reach fast clock speeds, resulting in\nthose processors being beaten by dumber but higher-clocked processors which\nweren't so focused on ILP.\n\nA 4-issue superscalar processor wants 4 independent instructions to be\navailable, with all their dependencies and latencies met, at every cycle. In\nreality this is virtually never possible, especially with load latencies of 3\nor 4 cycles. Currently, real-world instruction-level parallelism for\nmainstream, single-threaded applications is limited to about 2-3 instructions\nper cycle at best. In fact, the average ILP of a modern processor running the\nSPECint benchmarks is less than 2 instructions per cycle, and the SPEC\nbenchmarks are somewhat \"easier\" than most large, real-world applications.\nCertain types of applications do exhibit more parallelism, such as scientific\ncode, but these are generally not representative of mainstream applications.\nThere are also some types of code, such as \"pointer chasing\", where even\nsustaining 1 instruction per cycle is extremely difficult. For those programs,\nthe key problem is the memory system, and yet another wall, the memory wall\n(which we'll get to later).\n\n## What About x86?\n\nSo where does x86 fit into all this, and how have Intel and AMD been able to\nremain competitive through all of these developments in spite of an\narchitecture that's now more than 35 years old?\n\nWhile the original Pentium, a superscalar x86, was an amazing piece of\nengineering, it was clear the big problem was the complex and messy x86\ninstruction set. Complex addressing modes and a minimal number of registers\nmeant few instructions could be executed in parallel due to potential\ndependencies. For the x86 camp to compete with the RISC architectures, they\nneeded to find a way to \"get around\" the x86 instruction set.\n\nThe solution, invented independently (at about the same time) by engineers at\nboth NexGen and Intel, was to dynamically decode the x86 instructions into\nsimple, RISC-like micro-instructions, which can then be executed by a fast,\nRISC-style register-renaming OOO superscalar core. The micro-instructions are\nusually called \u03bcops (pronounced \"micro-ops\"). Most x86 instructions decode\ninto 1, 2 or 3 \u03bcops, while the more complex instructions require a larger\nnumber.\n\nFor these decoupled superscalar x86 processors, register renaming is\nabsolutely critical due to the meager 8 registers of the x86 architecture in\n32-bit mode (64-bit mode added an additional 8 registers). This differs\nstrongly from the RISC architectures, where providing more registers via\nrenaming only has a modest effect. Nonetheless, with clever register renaming,\nthe full bag of RISC tricks become available to the x86 world, with the two\nexceptions of advanced static instruction scheduling (because the \u03bcops are\nhidden behind the x86 layer and thus are less visible to compilers) and the\nuse of a large register set to avoid memory accesses.\n\nThe basic scheme works something like this...\n\nFigure 13 \u2013 A \"RISCy x86\" decoupled microarchitecture.\n\nNexGen's Nx586 and Intel's Pentium Pro (also known as the P6) were the first\nprocessors to adopt a decoupled x86 microarchitecture design, and today all\nmodern x86 processors use this technique. Of course, they all differ in the\nexact design of their core pipelines, functional units and so on, just like\nthe various RISC processors, but the fundamental idea of translating from x86\nto internal \u03bcop RISC-like instructions is common to all of them.\n\nSome of the more recent x86 processors even store the translated \u03bcops in a\nsmall buffer, or even a dedicated \"L0\" \u03bcop instruction cache, to avoid having\nto re-translate the same x86 instructions over and over again during loops,\nsaving both time and power. That's why, for example, the pipeline depth of\nCore i*2/i*3 Sandy/Ivy Bridge was shown as 14/19 stages in the earlier section\non superpipelining \u2013 it is 14 stages when the processor is running from its L0\n\u03bcop cache (which is the common case), but 19 stages when running from the L1\ninstruction cache and having to decode x86 instructions and translate them\ninto \u03bcops.\n\nThe decoupling of x86 instruction fetch and decode from internal RISC-like \u03bcop\ninstruction dispatch and execution also makes defining the width of a modern\nx86 processor a bit tricky, and it gets even more unclear because internally\nsuch processors often group or \"fuse\" \u03bcops into common pairs where possible\nfor ease of tracking (such as load-and-add or compare-and-branch). A processor\nsuch as Core i*4/i*5 Haswell/Broadwell, for example, can decode up to 5 x86\ninstructions per cycle, producing a maximum of up to 4 fused \u03bcops per cycle,\nwhich are then stored in an L0 \u03bcop cache, from which up to 4 fused \u03bcops per\ncycle are fetched, then register-renamed and placed into a reorder buffer,\nfrom which up to 8 un-fused individual \u03bcops are issued per cycle to the\nfunctional units, where they proceed down the various pipelines until they\ncomplete, whereupon up to 4 fused \u03bcops per cycle can be committed and retired.\nSo what does that make the width of Haswell/Broadwell? It's really an 8-issue\nprocessor at heart, since up to 8 un-fused \u03bcops can be fetched, issued and\ncompleted per cycle if they're paired/fused in just the right way (and an un-\nfused \u03bcop is the most direct equivalent of a simple RISC instruction), but\neven experts disagree on exactly what to call the width of such a design,\nsince 4-issue would also be valid, in terms of fused \u03bcops, which is what the\nprocessor mostly \"thinks in terms of\" for tracking purposes, and 5-issue is\nalso valid if thinking in terms of original x86 instructions. Of course, this\nwidth-labelling conundrum is largely academic, since no processor is likely to\nactually sustain such high levels of ILP when running real-world code anyway.\n\nOne of the most interesting members of the RISC-style x86 group was the\nTransmeta Crusoe processor, which translated x86 instructions into an internal\nVLIW form, rather than internal superscalar, and used software to do the\ntranslation at runtime, much like a Java virtual machine. This approach\nallowed the processor itself to be a simple VLIW, without the complex x86\ndecoding and register-renaming hardware of decoupled x86 designs, and without\nany superscalar dispatch or OOO logic either. The software-based x86\ntranslation did reduce the system's performance compared to hardware\ntranslation (which occurs as additional pipeline stages and thus is almost\nfree in performance terms), but the result was a very lean chip which ran fast\nand cool and used very little power. A 600 MHz Crusoe processor could match a\nthen-current 500 MHz Pentium III running in its low-power mode (300 MHz clock\nspeed) while using only a fraction of the power and generating only a fraction\nof the heat. This made it ideal for laptops and handheld computers, where\nbattery life is crucial. Today, of course, x86 processor variants designed\nspecifically for low power use, such as the Pentium M and its Core\ndescendants, have made the Transmeta-style software-based approach\nunnecessary, although a very similar approach is currently being used in\nNVIDIA's Denver ARM processors, again in the quest for high performance at\nvery low power.\n\n## Threads \u2013 SMT, Hyper-Threading & Multi-Core\n\nAs already mentioned, the approach of exploiting instruction-level parallelism\nthrough superscalar execution is seriously weakened by the fact that most\nnormal programs just don't have a lot of fine-grained parallelism in them.\nBecause of this, even the most aggressively brainiac OOO superscalar\nprocessor, coupled with a smart and aggressive compiler to spoon feed it, will\nstill almost never exceed an average of about 2-3 instructions per cycle when\nrunning most mainstream, real-world software, due to a combination of load\nlatencies, cache misses, branching and dependencies between instructions.\nIssuing many instructions in the same cycle only ever happens for short bursts\nof a few cycles at most, separated by many cycles of executing low-ILP code,\nso peak performance is not even close to being achieved.\n\nIf additional independent instructions aren't available within the program\nbeing executed, there is another potential source of independent instructions\n\u2013 other running programs, or other threads within the same program.\nSimultaneous multi-threading (SMT) is a processor design technique which\nexploits exactly this type of thread-level parallelism.\n\nOnce again, the idea is to fill those empty bubbles in the pipelines with\nuseful instructions, but this time rather than using instructions from further\ndown in the same code (which are hard to come by), the instructions come from\nmultiple threads running at the same time, all on the one processor core. So,\nan SMT processor appears to the rest of the system as if it were multiple\nindependent processors, just like a true multi-processor system.\n\nOf course, a true multi-processor system also executes multiple threads\nsimultaneously \u2013 but only one in each processor. This is also true for multi-\ncore processors, which place two or more processor cores onto a single chip,\nbut are otherwise no different from traditional multi-processor systems. In\ncontrast, an SMT processor uses just one physical processor core to present\ntwo or more logical processors to the system. This makes SMT much more\nefficient than a multi-core processor in terms of chip space, fabrication\ncost, power usage and heat dissipation. And of course there's nothing\npreventing a multi-core implementation where each core is an SMT design.\n\nFrom a hardware point of view, implementing SMT requires duplicating all of\nthe parts of the processor which store the \"execution state\" of each thread \u2013\nthings like the program counter, the architecturally-visible registers (but\nnot the rename registers), the memory mappings held in the TLB, and so on.\nLuckily, these parts only constitute a tiny fraction of the overall\nprocessor's hardware. The really large and complex parts, such as the decoders\nand dispatch logic, the functional units, and the caches, are all shared\nbetween the threads.\n\nOf course, the processor must also keep track of which instructions and which\nrename registers belong to which threads at any given point in time, but it\nturns out this only adds a small amount to the complexity of the core logic.\nSo, for the relatively cheap design cost of around 10% more logic in the core,\nand an almost negligible increase in total transistor count and final\nproduction cost, the processor can execute several threads simultaneously,\nhopefully resulting in a substantial increase in functional-unit utilization\nand instructions per cycle, and thus overall performance.\n\nThe instruction flow of an SMT processor looks something like...\n\nFigure 14 \u2013 The instruction flow of an SMT processor.\n\nThis is really great! Now that we can fill those bubbles by running multiple\nthreads, we can justify adding more functional units than would normally be\nviable in a single-threaded processor, and really go to town with multiple\ninstruction issue. In some cases, this may even have the side effect of\nimproving single-thread performance (for particularly ILP-friendly code, for\nexample).\n\nSo 20-issue here we come, right? Unfortunately, the answer is no.\n\nSMT performance is a tricky business. First, the whole idea of SMT is built\naround the assumption that either lots of programs are simultaneously\nexecuting (not just sitting idle), or if just one program is running, it has\nlots of threads all executing at the same time. Experience with existing\nmulti-processor systems shows this isn't always true. In practice, at least\nfor desktops, laptops, tablets, phones and small servers, it is rarely the\ncase that several different programs are actively executing at the same time,\nso it usually comes down to just the one task the machine is currently being\nused for.\n\nSome applications, such as database systems, image and video processing, audio\nprocessing, 3D graphics rendering and scientific code, do have obvious high-\nlevel (coarse-grained) parallelism available and easy to exploit, but\nunfortunately even many of these applications have not been written to make\nuse of multiple threads in order to exploit multiple processors. In addition,\nmany of the applications which are easy to parallelize, because they're\ninherently \"embarrassingly parallel\" in nature, are primarily limited by\nmemory bandwidth, not by the processor (image processing, audio processing,\nsimple scientific code), so adding a second thread or processor won't help\nthem much unless memory bandwidth is also dramatically increased (we'll get to\nthe memory system soon). Worse yet, many other types of software, such as web\nbrowsers, multimedia design tools, language interpreters, hardware simulations\nand so on, are currently not written in a way which is parallel at all, or\ncertainly not enough to make effective use of multiple processors.\n\nOn top of this, the fact that the threads in an SMT design are all sharing\njust one processor core, and just one set of caches, has major performance\ndownsides compared to a true multi-processor (or multi-core). Within the\npipelines of an SMT processor, if one thread saturates just one functional\nunit which the other threads need, it effectively stalls all of the other\nthreads, even if they only need relatively little use of that unit. Thus,\nbalancing the progress of the threads becomes critical, and the most effective\nuse of SMT is for applications with highly variable code mixtures, so the\nthreads don't constantly compete for the same hardware resources. Also,\ncompetition between the threads for cache space may produce worse results than\nletting just one thread have all the cache space available \u2013 particularly for\nsoftware where the critical working set is highly cache-size sensitive, such\nas hardware simulators/emulators, virtual machines and high-quality video\nencoding (with a large motion-estimation window).\n\nThe bottom line is that without care, and even with care for some\napplications, SMT performance can actually be worse than single-thread\nperformance and traditional context switching between threads. On the other\nhand, applications which are limited primarily by memory latency (but not\nmemory bandwidth), such as database systems, 3D graphics rendering and a lot\nof general-purpose code, benefit dramatically from SMT, since it offers an\neffective way of using the otherwise idle time during load latencies and cache\nmisses (we'll cover caches later). Thus, SMT presents a very complex and\napplication-specific performance picture. This also makes it a difficult\nchallenge for marketing \u2013 sometimes almost as fast as two \"real\" processors,\nsometimes more like two really lame processors, sometimes even worse than one\nprocessor, huh?\n\nThe Pentium 4 was the first processor to use SMT, which Intel calls \"hyper-\nthreading\". Its design allowed for 2 simultaneous threads (although earlier\nrevisions of the Pentium 4 had the SMT feature disabled due to bugs). Speedups\nfrom SMT on the Pentium 4 ranged from around -10% to +30% depending on the\napplication(s). Subsequent Intel designs then eschewed SMT during the\ntransition back to the brainiac designs of the Pentium M and Core 2, along\nwith the transition to multi-core. Many other SMT designs were also cancelled\naround the same time (Alpha 21464, UltraSPARC V), and for a while it almost\nseemed as if SMT was out of favor, before it finally made a comeback with\nPOWER5, a 2-thread SMT design as well as being multi-core (2 threads per core\ntimes 2 cores per chip equals 4 threads per chip). Intel's Core i series are\nalso 2-thread SMT, so a typical quad-core Core i processor is thus an 8-thread\nchip. Sun was the most aggressive of all on the thread-level parallelism\nfront, with UltraSPARC T1 Niagara providing 8 simple in-order cores each with\n4-thread SMT, for a total of 32 threads on a single chip. This was\nsubsequently increased to 8 threads per core in UltraSPARC T2, and then 16\ncores per chip in UltraSPARC T3, for a whopping 128 threads!\n\n## More Cores or Wider Cores?\n\nGiven SMT's ability to convert thread-level parallelism into instruction-level\nparallelism, coupled with the advantage of better single-thread performance\nfor particularly ILP-friendly code, you might now be asking why anyone would\never build a multi-core processor when an equally wide (in total) SMT design\nwould be superior.\n\nWell unfortunately it's not quite as simple as that. As it turns out, very\nwide superscalar designs scale very badly in terms of both chip area and clock\nspeed. One key problem is that the complex multiple-issue dispatch logic\nscales up as roughly the square of the issue width, because all n candidate\ninstructions need to be compared against every other candidate. Applying\nordering restrictions or \"issue rules\" can reduce this, as can some clever\nengineering, but it's still in the order of n^2. That is, the dispatch logic\nof a 5-issue processor is more than 50% larger than a 4-issue design, with\n6-issue being more than twice as large, 7-issue over 3 times the size, 8-issue\nmore than 4 times larger than 4-issue (for only 2 times the width), and so on.\nIn addition, a very wide superscalar design requires highly multi-ported\nregister files and caches, to service all those simultaneous accesses. Both of\nthese factors conspire to not only increase size, but also to massively\nincrease the amount of longer-distance wiring at the circuit-design level,\nplacing serious limits on the clock speed. So a single 10-issue core would\nactually be both larger and slower than two 5-issue cores, and our dream of a\n20-issue SMT design isn't really viable due to circuit-design limitations.\n\nNevertheless, since the benefits of both SMT and multi-core depend so much on\nthe nature of the target application(s), a broad spectrum of designs might\nstill make sense with varying degrees of SMT and multi-core. Let's explore\nsome possibilities...\n\nToday, a \"typical\" SMT design implies both a wide execution core and OOO\nexecution logic, including multiple decoders, the large and complex\nsuperscalar dispatch logic and so on. Thus, the size of a typical SMT core is\nquite large in terms of chip area. With the same amount of chip space, it\nwould be possible to fit several simpler, single-issue, in-order cores (either\nwith or without basic SMT). In fact, it may be the case that as many as half a\ndozen small, simple cores could fit within the chip area taken by just one\nmodern OOO superscalar SMT design!\n\nNow, given that both instruction-level parallelism and thread-level\nparallelism suffer from diminishing returns (in different ways), and\nremembering that SMT is essentially a way to convert TLP into ILP, but also\nremembering that wide superscalar designs scale very non-linearly in terms of\nchip area (and design complexity, and power usage), the obvious question is\nwhere is the sweet spot? How wide should the cores be made to reach a good\nbalance between ILP and TLP? Right now, many different approaches are being\nexplored...\n\nFigure 15 \u2013 Design extremes: Core i*2 \"Sandy Bridge\" vs UltraSPARC T3 \"Niagara\n3\".\n\nAt one extreme we have processors like Intel's Core i*2 Sandy Bridge (above\nleft), consisting of 4 large, wide, 6-issue, out-of-order, aggressively\nbrainiac cores (along the top, with shared L3 cache below), each running 2\nthreads, for a total of 8 \"fast\" threads. At the other end of the spectrum,\nSun/Oracle's UltraSPARC T3 Niagara 3 (above right) contains 16 much smaller,\nsimpler, 2-issue in-order cores (top and bottom, with shared L2 cache towards\nthe center), each running 8 threads, for a massive 128 threads in total,\nalthough these threads are considerably slower than those of Sandy Bridge.\nBoth chips are of the same era \u2013 early 2011. Both contained around 1 billion\ntransistors and are drawn approximately to scale above (assuming similar\ntransistor density). Note just how much smaller the simple, in-order cores\nreally are!\n\nWhich is the better approach? Alas, there's no simple answer here \u2013 once again\nit's going to depend very much on the application(s). For applications with\nlots of active but memory-latency-limited threads (database systems, 3D\ngraphics rendering), more simple cores would be better because big/wide cores\nwould spend most of their time waiting for memory anyway. For most\napplications, however, there simply are not enough threads active to make this\nviable, and the performance of just a single thread is much more important, so\na design with fewer but bigger, wider, more brainiac cores is more appropriate\n(at least for today's applications).\n\nOf course, there are also a whole range of options between these two extremes\nthat have yet to be fully explored. IBM's POWER7, for example, was of the same\ngeneration, also having approximately 1 billion transistors, and used them to\ntake the middle ground with an 8-core, 4-thread SMT design with moderately but\nnot overly aggressive OOO execution hardware. AMD's Bulldozer design used a\nmore unusual approach, with a shared, SMT-style front-end for each pair of\ncores, feeding a back-end with unshared, multi-core-style integer execution\nunits but shared, SMT-style floating-point units, blurring the line between\nSMT and multi-core.\n\nFigure 16 \u2013 Xeon Haswell with 18 brainiac cores, the best of both worlds?\n\nToday (early 2015), with several billion transistors now available thanks to\nMoore's Law, even aggressively brainiac designs can have quite a lot of cores\n\u2013 Intel's Xeon Haswell, the server version of Core i*4 Haswell, uses 5.7\nbillion transistors to provide 18 cores (up from 8 in Xeon Sandy Bridge), each\na very aggressively brainiac 8-issue design (up from 6-issue in Sandy Bridge),\neach still with 2-thread SMT, while IBM's POWER8 uses 4.4 billion transistors\nto move to a considerably more brainiac core design than POWER7, and at the\nsame time provide 12 cores (up from 8 in POWER7), each with 8-thread SMT (up\nfrom 4 in POWER7). Of course, whether such large, brainiac core designs are an\nefficient use of all those transistors is a separate question.\n\nGiven the multi-core performance-per-area efficiency of small cores, but the\nmaximum outright single-threaded performance of large cores, perhaps in the\nfuture we might see asymmetric designs, with one or two big, wide, brainiac\ncores plus a large number of smaller, narrower, simpler cores. In many ways,\nsuch a design makes the most sense \u2013 highly parallel programs would benefit\nfrom the many small cores more than a few large ones, but single-threaded,\nsequential programs want the might of at least one large, wide, brainiac core,\neven if it does take four times the area to provide only twice the single-\nthreaded performance.\n\nIBM's Cell processor (used in the Sony PlayStation 3) was arguably the first\nsuch design, but unfortunately it suffered from severe programmability\nproblems because the small, simple cores in Cell were not instruction-set\ncompatible with the large main core, and only had limited, awkward access to\nmain memory, making them more like special-purpose coprocessors than general-\npurpose CPU cores. Some modern ARM designs also use an asymmetric approach,\nwith several large cores paired with one or a few smaller, simpler \"companion\"\ncores, not for maximum multi-core performance, but so the large, power-hungry\ncores can be powered down if the phone or tablet is only being lightly used,\nin order to increase battery life, a strategy ARM calls \"big.LITTLE\".\n\nOf course, with all of those transistors available, it might also make sense\nto integrate other secondary functionality into the main CPU chip, such as I/O\nand networking (usually part of the motherboard chipset), dedicated video\nencoding/decoding hardware (usually part of the graphics system), or even an\nentire low-end GPU (graphics processing unit). This integration is\nparticularly attractive in cases where a reduction in chip count, physical\nspace or cost is more important than the performance advantage of more cores\non the main CPU chip and separate, dedicated chips for those other purposes,\nmaking it ideal for phones, tablets and small, low-performance laptops. Such a\nheterogeneous design is called a system-on-chip, or SoC...\n\nFigure 17 \u2013 A typical SoC: NVIDIA Tegra 2.\n\n## Data Parallelism \u2013 SIMD Vector Instructions\n\nIn addition to instruction-level parallelism and thread-level parallelism,\nthere is yet another source of parallelism in many programs \u2013 data\nparallelism. Rather than looking for ways to execute groups of instructions in\nparallel, the idea is to look for ways to make one instruction apply to a\ngroup of data values in parallel.\n\nThis is sometimes called SIMD parallelism (single instruction, multiple data).\nMore often, it's called vector processing. Supercomputers used to use vector\nprocessing a lot, with very long vectors, because the types of scientific\nprograms which are run on supercomputers are quite amenable to vector\nprocessing.\n\nToday, however, vector supercomputers have long since given way to multi-\nprocessor designs where each processing unit is a commodity CPU. So why revive\nvector processing?\n\nIn many situations, especially in imaging, video and multimedia applications,\na program needs to execute the same instruction for a small group of related\nvalues, usually a short vector (a simple structure or small array). For\nexample, an image-processing application might want to add groups of 8-bit\nnumbers, where each 8-bit number represents one of the red, green, blue or\nalpha (transparency) values of a pixel...\n\nFigure 18 \u2013 A SIMD vector addition operation.\n\nWhat's happening here is exactly the same operation as a 32-bit addition,\nexcept that every 8th carry is not being propagated. Also, it might be\ndesirable for the values not to wrap to zero once all 8 bits are full, and\ninstead to hold at 255 as a maximum value in those cases (called saturation\narithmetic). In other words, every 8th carry is not carried across but instead\ntriggers an all-ones result. So, the vector addition operation shown above is\nreally just a modified 32-bit add.\n\nFrom the hardware point of view, adding these types of vector instructions is\nnot terribly difficult \u2013 existing registers can be used and in many cases the\nfunctional units can be shared with existing integer or floating-point units.\nOther useful packing and unpacking instructions can also be added, for byte\nshuffling and so on, and a few predicate-like instructions for bit masking\netc. With some thought, a small set of vector instructions can enable some\nimpressive speedups.\n\nOf course, there's no reason to stop at 32 bits. If there happen to be some\n64-bit registers, which architectures usually have for floating-point (at\nleast), they could be used to provide 64-bit vectors, thereby doubling the\nparallelism \u2013 SPARC VIS and x86 MMX did this. If it is possible to define\nentirely new registers, then they might as well be even wider \u2013 x86 SSE added\n8 new 128-bit registers, later increased to 16 registers in 64-bit mode, then\nwidened to 256 bits with AVX, while POWER/PowerPC AltiVec provided a full set\nof 32 new 128-bit registers from the start (in keeping with POWER/PowerPC's\nmore separated design style, where even the branch instructions have their own\nregisters). An alternative to widening the registers is to use pairing, where\neach pair of registers is treated as a single operand by the SIMD vector\ninstructions \u2013 ARM NEON does this, with its registers usable both as 32 64-bit\nregisters or as 16 128-bit registers.\n\nNaturally, the data in the registers can also be divided up in other ways, not\njust as 8-bit bytes \u2013 for example as 16-bit integers for high-quality image\nprocessing, or as floating-point values for scientific number crunching. With\nAltiVec, NEONv2 and recent versions of SSE/AVX, for example, it is possible to\nexecute a 4-way parallel floating-point multiply-add as a single, fully\npipelined instruction.\n\nFor applications where this type of data parallelism is available and easy to\nextract, SIMD vector instructions can produce amazing speedups. The original\ntarget applications were primarily in the area of image and video processing,\nhowever suitable applications also include audio processing, speech\nrecognition, some parts of 3D graphics rendering and many types of scientific\ncode. For other types of software, such as compilers and database systems, the\nspeedup is generally much smaller, perhaps even nothing at all.\n\nUnfortunately, it's quite difficult for a compiler to automatically make use\nof vector instructions when working from normal source code, except in trivial\ncases. The key problem is that the way programmers write programs tends to\nserialize everything, which makes it difficult for a compiler to prove two\ngiven operations are independent and can be done in parallel. Progress is\nslowly being made in this area, but at the moment programs must basically be\nrewritten by hand to take advantage of vector instructions (except for simple\narray-based loops in scientific code).\n\nLuckily, however, rewriting just a small amount of code in key places within\nthe graphics and video/audio libraries of your favorite operating system has a\nwidespread effect across many applications. Today, most OSs have enhanced\ntheir key library functions in this way, so virtually all multimedia and 3D\ngraphics applications do make use of these highly effective vector\ninstructions. Chalk up yet another win for abstraction!\n\nAlmost every architecture has now added SIMD vector extensions, including\nSPARC (VIS), x86 (MMX/SSE/AVX), POWER/PowerPC (AltiVec) and ARM (NEON). Only\nrelatively recent processors from each architecture can execute some of these\nnew instructions, however, which raises backward-compatibility issues,\nespecially on x86 where the SIMD vector instructions evolved somewhat\nhaphazardly (MMX, 3DNow!, SSE, SSE2, SSE3, SSE4, AVX, AVX2).\n\n## Memory & The Memory Wall\n\nAs mentioned earlier, latency is a big problem for pipelined processors, and\nlatency is especially bad for loads from memory, which make up about a quarter\nof all instructions.\n\nLoads tend to occur near the beginning of code sequences (basic blocks), with\nmost of the other instructions depending on the data being loaded. This causes\nall the other instructions to stall, and makes it difficult to obtain large\namounts of instruction-level parallelism. Things are even worse than they\nmight first seem, because in practice most superscalar processors can still\nonly issue one, or at most two, load instructions per cycle.\n\nThe core problem with memory access is that building a fast memory system is\nvery difficult, in part because of fixed limits like the speed of light, which\nimpose delays while a signal is transferred out to RAM and back, and more\nimportantly because of the relatively slow speed of charging and draining the\ntiny capacitors which make up the memory cells. Nothing can change these facts\nof nature \u2013 we must learn to work around them.\n\nFor example, access latency for main memory, using a modern SDRAM with a CAS\nlatency of 11, will typically be 24 cycles of the memory system bus \u2013 1 to\nsend the address to the DIMM (memory module), RAS-to-CAS delay of 11 for the\nrow access, CAS latency of 11 for the column access, and a final 1 to send the\nfirst piece of data up to the processor (or E-cache), with the remaining data\nblock following over the next few bus cycles. Of course, that's just the\ntypical latency, because memory latency varies quite a lot. For any given\naccess, we might get lucky and be accessing a memory bank where the memory\ncontroller already has the right row open for us, skipping the RAS-to-CAS\ndelay and saving nearly 50% of the latency (1+11+1), or we might be unlucky\nand access a bank where the memory controller has a row open but it's the\nwrong one, needing to first be put away, waiting an additional RAS precharge\ndelay and adding nearly 50% to the latency (1+11+11+11+1). On a multi-\nprocessor system, even more bus cycles may be required to support cache\ncoherency between the processors. And then there are the cycles within the\nprocessor itself, checking the various on-chip caches before the address even\ngets sent to the memory controller, and then when the data arrives from RAM to\nthe memory controller and is sent to the relevant processor core. Luckily,\nthose are faster internal CPU cycles, not memory bus cycles, but they still\naccount for 20 CPU cycles or so in most modern processors.\n\nAssuming a typical 800 MHz SDRAM memory system (DDR3-1600), and assuming a 2.4\nGHz processor, this makes (1+11+11+1) * 2400/800 + 20 = 92 cycles of the CPU\nclock to access main memory! Yikes, you say! And it gets worse \u2013 a 2.8 GHz\nprocessor would take it to 104 cycles, a 3.2 GHz processor to 116 cycles, a\n3.6 GHz processor 128 cycles, and a 4.0 GHz processor would wait a staggering\n140 cycles to access main memory!\n\nNote that although a DDR SDRAM memory system transfers data on both the rising\nand falling edges of the clock signal (ie: at double data rate), the true\nclock speed of the memory system bus is only half that, and it is the bus\nclock speed which applies for control signals. So the latency of a DDR memory\nsystem is the same as a non-DDR system, even though the bandwidth is doubled\n(more on the difference between bandwidth and latency later).\n\nOlder generations of processors were even worse, because their memory\ncontrollers weren't on-chip, but rather were part of the chipset on the\nmotherboard, adding another 2 bus cycles for the transfer of the address and\ndata between the processor and that motherboard chipset \u2013 and this was at a\ntime when the memory bus was only 200 MHz or less, not 800 MHz, so those 2 bus\ncycles often added another 20+ CPU cycles to the total. Some processors\nattempted to mitigate this issue by increasing the speed of their frontside\nbus (FSB) between the processor and the chipset (800 MHz QDR in Pentium 4,\n1.25 GHz DDR in PowerPC G5). A far better approach, used by all modern\nprocessors, is to integrate the memory controller directly into the processor\nchip, which allows those 2 bus cycles to be converted into much faster CPU\ncycles instead. UltraSPARC IIi and Athlon 64 were the first mainstream\nprocessors to do this, while Intel was late to the party and only integrated\nthe memory controller into their CPU chips starting with the Core i series.\n\nUnfortunately, both DDR SDRAM memory and on-chip memory controllers are only\nable to do so much, and memory latency continues to be a major problem. This\nproblem of the large, and slowly growing, gap between the processor and main\nmemory is called the memory wall. It was, at one time, the single most\nimportant problem facing processor architects, although today the problem has\neased considerably because processor clock speeds are no longer climbing at\nthe rate they previously did, due to power and heat constraints \u2013 the power\nwall.\n\nNonetheless, the memory wall is still a big problem.\n\n## Caches & The Memory Hierarchy\n\nModern processors solve the problem of the memory wall with caches. A cache is\na small but fast type of memory located on or near the processor chip. Its\nrole is to keep copies of small pieces of main memory. When the processor asks\nfor a particular piece of main memory, the cache can supply it much more\nquickly than main memory would be able to \u2013 if the data is in the cache.\n\nThe word \"cache\" is pronounced like \"cash\"... as in \"a cache of weapons\" or \"a\ncache of supplies\". It means a place for hiding or storing things. It is not\npronounced \"ca-shay\" or \"kay-sh\".\n\nTypically, there are small but fast \"primary\" level-1 (L1) caches on the\nprocessor chip itself, inside each core, usually around 8-64k in size, with a\nlarger level-2 (L2) cache further away but still on-chip (a few hundred KB to\na few MB), and possibly an even larger and slower L3 cache etc. The\ncombination of the on-chip caches, any off-chip external cache (E-cache) and\nmain memory (RAM) together form a memory hierarchy, with each successive level\nbeing larger but slower than the one before it. At the bottom of the memory\nhierarchy, of course, is virtual memory (paging/swapping), which provides the\nillusion of an almost infinite amount of main memory by moving pages of RAM to\nand from file storage (which is slower again, by a large margin).\n\nIt's a bit like working at a desk in a library... You might have two or three\nbooks open on the desk itself. Accessing them is fast (you can just look), but\nyou can't fit more than a couple on the desk at the same time \u2013 and even if\nyou could, accessing 100 books laid out on a huge desk would take longer\nbecause you'd have to walk between them. Instead, in the corner of the desk\nyou might have a pile of a dozen more books. Accessing them is slower, because\nyou have to reach over, grab one and open it up. Each time you open a new one,\nyou also have to put one of the books already on the desk back into the pile\nto make room. Finally, when you want a book that's not on the desk, and not in\nthe pile, it's very slow to access because you have to get up and walk around\nthe library looking for it. However the size of the library means you have\naccess to thousands of books, far more than could ever fit on your desk.\n\nA typical modern memory hierarchy looks something like...\n\nLevel| Size| Latency| Physical Location  \n---|---|---|---  \nL1 cache| 32 KB| 4 cycles| inside each core  \nL2 cache| 256 KB| 12 cycles| beside each core  \nL3 cache| 6 MB| ~21 cycles| shared between all cores  \nL4 E-cache| 128 MB| ~58 cycles| separate eDRAM chip  \nRAM| 4+ GB| ~117 cycles| SDRAM DIMMs on motherboard  \nSwap| 100+ GB| 10,000+ cycles| hard disk or SSD  \n  \nTable 4 \u2013 The memory hierarchy of a modern desktop/laptop: Core i*4 Haswell.\n\nEven phones have such a memory hierarchy...\n\nLevel| Size| Latency| Physical Location  \n---|---|---|---  \nL1 cache| 64 KB| 4 cycles| inside each core  \nL2 cache| 1 MB| ~20 cycles| beside the cores  \nL3 cache| 4 MB| ~107 cycles| beside the memory controller  \nRAM| 1 GB| ~261 cycles| separate SDRAM chip  \nSwap| N/A| N/A| paging/swapping not used on iOS  \n  \nTable 5 \u2013 The memory hierarchy of a modern phone: Apple A8 in the iPhone 6.\n\nThe amazing thing about caches is that they work really well \u2013 they\neffectively make the memory system seem almost as fast as the L1 cache, yet as\nlarge as main memory. A modern primary (L1) cache has a latency of just 2 to 4\nprocessor cycles, which is dozens of times faster than accessing main memory,\nand modern primary caches achieve hit rates of around 90% for most software.\nSo 90% of the time, accessing memory only takes a few cycles!\n\nCaches can achieve these seemingly amazing hit rates because of the way\nprograms work. Most programs exhibit locality in both time and space \u2013 when a\nprogram accesses a piece of memory, there's a good chance it will need to re-\naccess the same piece of memory in the near future (temporal locality), and\nthere's also a good chance it will need to access other nearby memory in the\nfuture as well (spatial locality). Temporal locality is exploited by merely\nkeeping recently accessed data in the cache. To take advantage of spatial\nlocality, data is transferred from main memory up into the cache in blocks of\na few dozen bytes at a time, called a cache line.\n\nFrom the hardware point of view, a cache works like a two-column table \u2013 one\ncolumn is the memory address and the other is the block of data values\n(remember that each cache line is a whole block of data, not just a single\nvalue). Of course, in reality the cache need only store the necessary higher-\nend part of the address, since lookups work by using the lower part of the\naddress to index the cache. When the higher part, called the tag, matches the\ntag stored in the table, this is a hit and the appropriate piece of data can\nbe sent to the processor core...\n\nFigure 19 \u2013 A cache lookup.\n\nIt is possible to use either the physical address or the virtual address to do\nthe cache lookup. Each has pros and cons (like everything else in computing).\nUsing the virtual address might cause problems because different programs use\nthe same virtual addresses to map to different physical addresses \u2013 the cache\nmight need to be flushed on every context switch. On the other hand, using the\nphysical address means the virtual-to-physical mapping must be performed as\npart of the cache lookup, making every lookup slower. A common trick is to use\nvirtual addresses for the cache indexing but physical addresses for the tags.\nThe virtual-to-physical mapping (TLB lookup) can then be performed in parallel\nwith the cache indexing so that it will be ready in time for the tag\ncomparison. Such a scheme is called a virtually-indexed physically-tagged\ncache.\n\nThe sizes and speeds of the various levels of cache in modern processors are\nabsolutely crucial to performance. The most important by far are the primary\nL1 data cache (D-cache) and L1 instruction cache (I-cache). Some processors go\nfor small L1 caches (Pentium 4E Prescott, Scorpion and Krait have 16k L1\ncaches (for each of I- and D-cache), earlier Pentium 4s and UltraSPARC\nT1/T2/T3 are even smaller at just 8k), most have settled on 32k as the sweet\nspot, and a few are larger at 64k (Athlon, Athlon 64/Phenom, UltraSPARC\nIII/IV, Apple A7/A8) or occasionally even 128k (the I-cache of Denver, with a\n64k D-cache).\n\nFor modern L1 data caches, the load latency is usually 3 or 4 cycles,\ndepending on the processor's general clock speed, but occasionally shorter (2\ncycles on UltraSPARC III/IV thanks to clock-less \"wave\" pipelining, 2 cycles\nin earlier processors due to their slower clock speeds and shorter pipelines,\nwhere a clock cycle was more real time). Increasing the load latency by a\ncycle, say from 3 to 4, or from 4 to 5, can seem like a minor change but is\nactually a serious hit to performance, and is something rarely noticed or\nunderstood by end users. For normal, everyday pointer-chasing code, a\nprocessor's load latency is a major factor in real-world performance.\n\nMost modern processors have a large second or third level of on-chip cache,\nusually shared between all cores. This cache is also very important, but its\nsize sweet spot depends heavily on the type of application being run and the\nsize of that application's active working set \u2013 the difference between 2 MB of\nL3 cache and 8 MB will be barely measurable for some applications, while for\nothers it will be enormous. Given that the relatively small L1 caches already\ntake up a significant percentage of the chip area for many modern processor\ncores, you can imagine how much area a large L2 or L3 cache would take, yet\nthis is still absolutely essential to combat the memory wall. Often, the large\nL2/L3 cache takes as much as half the total chip area, so much that it's\nclearly visible in chip photographs, standing out as a relatively clean,\nrepetitive structure against the more \"messy\" logic transistors of the cores\nand memory controller.\n\n## Cache Conflicts & Associativity\n\nIdeally, a cache should keep the data that is most likely to be needed in the\nfuture. Since caches aren't psychic, a good approximation of this is to keep\nthe most recently used data.\n\nUnfortunately, keeping exactly the most recently used data would mean that\ndata from any memory location could be placed into any cache line. The cache\nwould thus contain exactly the most recently used n KB of data, which would be\ngreat for exploiting locality but unfortunately is not suitable for allowing\nfast access \u2013 accessing the cache would require checking every cache line for\na possible match, which would be very slow for a modern cache with hundreds of\nlines.\n\nInstead, a cache usually only allows data from any particular address in\nmemory to occupy one, or at most a handful, of locations within the cache.\nThus, only one or a handful of checks are required during access, so access\ncan be kept fast (which is the whole point of having a cache in the first\nplace). This approach does have a downside, however \u2013 it means the cache\ndoesn't store the absolutely best set of recently accessed data, because\nseveral different locations in memory will all map to the same one location in\nthe cache. When two such memory locations are wanted at the same time, such a\nscenario is called a cache conflict.\n\nCache conflicts can cause \"pathological\" worst-case performance problems,\nbecause when a program repeatedly accesses two memory locations which happen\nto map to the same cache line, the cache must keep storing and loading from\nmain memory and thus suffering the long main-memory latency on each access\n(100 cycles or more, remember!). This type of situation is called thrashing,\nsince the cache is not achieving anything and is simply getting in the way \u2013\ndespite obvious temporal locality and reuse of data, the cache is unable to\nexploit the locality offered by this particular access pattern due to\nlimitations of its simplistic mapping between memory locations and cache\nlines.\n\nTo address this problem, more sophisticated caches are able to place data in a\nsmall number of different places within the cache, rather than just a single\nplace. The number of places a piece of data can be stored in a cache is called\nits associativity. The word \"associativity\" comes from the fact that cache\nlookups work by association \u2013 that is, a particular address in memory is\nassociated with a particular location in the cache (or set of locations for a\nset-associative cache).\n\nAs described above, the simplest and fastest caches allow for only one place\nin the cache for each address in memory \u2013 each piece of data is simply mapped\nto address % size within the cache by simply looking at the lower bits of the\naddress (as in the above diagram). This is called a direct-mapped cache. Any\ntwo locations in memory whose addresses are the same for the lower address\nbits will map to the same cache line in a direct-mapped cache, causing a cache\nconflict.\n\nA cache which allows data to occupy one of 2 locations based on its address is\ncalled 2-way set-associative. Similarly, a 4-way set-associative cache allows\nfor 4 possible locations for any given piece of data, and an 8-way cache 8\npossible locations. Set-associative caches work much like direct-mapped ones,\nexcept there are several tables, all indexed in parallel, and the tags from\neach table are compared to see whether there is a match for any one of them...\n\nFigure 20 \u2013 A 4-way set-associative cache.\n\nEach table, or way, may also have marker bits so that only the line of the\nleast recently used way is evicted when a new line is brought in, or perhaps\nsome faster approximation of that ideal.\n\nUsually, set-associative caches are able to avoid the problems that\noccasionally occur with direct-mapped caches due to unfortunate cache\nconflicts. Adding even more ways allows even more conflicts to be avoided.\nUnfortunately, the more highly associative a cache is, the slower it is to\naccess, because there are more operations to perform during each access. Even\nthough the comparisons themselves are performed in parallel, additional logic\nis required to select the appropriate hit, if any, and the cache also needs to\nupdate the marker bits appropriately within each way. More chip area is also\nrequired, because relatively more of the cache's data is consumed by tag\ninformation rather than data blocks, and extra datapaths are needed to access\neach individual way of the cache in parallel. Any and all of these factors may\nnegatively affect access time. Thus, a 2-way set-associative cache is slower\nbut smarter than a direct-mapped cache, with 4-way and 8-way being slower but\nsmarter again.\n\nIn most modern processors, the instruction cache can afford to be highly set-\nassociative because its latency is hidden somewhat by the fetching and\nbuffering of the early stages of the processor's pipeline. The data cache, on\nthe other hand, is usually set-associative to some degree, but often not\noverly so, to minimize the all-important load latency. Most processors have\nsettled on 4-way set-associative as the sweet spot, but a few are less\nassociative (2-way in Athlon, Athlon 64/Phenom, PowerPC G5 and\nCortex-A15/A57), and a handful are more associative (8-way in PowerPC G4e,\nPentium M and its Core descendants). As the last resort before heading off to\nfar-away main memory, the large L2/L3 cache (sometimes called LLC for \"last-\nlevel cache\") is also usually highly associative, perhaps as much as 12- or\n16-way, although external E-cache is sometimes direct-mapped for flexibility\nof size and implementation.\n\nThe concept of caches also extends up into software systems. For example, the\noperating system uses main memory to cache the contents of the filesystem to\nspeed up file I/O, web browsers cache recently viewed pages, images and\nJavaScript files in case you revisit those sites, and web cache servers (also\nknown as proxy caches) cache the contents of remote web servers on a more\nlocal server (your ISP almost certainly uses one). With respect to main memory\nand virtual memory (paging/swapping), it can be thought of as being a smart,\nfully associative cache, like the ideal cache mentioned initially (above).\nAfter all, virtual memory is managed by the (hopefully) intelligent software\nof the OS kernel.\n\n## Memory Bandwidth vs Latency\n\nSince memory is transferred in blocks, and since cache misses are an urgent\n\"show stopper\" type of event with the potential to halt the processor in its\ntracks (or at least severely hamper its progress), the speed of those block\ntransfers from memory is critical. The transfer rate of a memory system is\ncalled its bandwidth. But how is that different from latency?\n\nA good analogy is a highway... Suppose you want to drive in to the city from\n100 miles away. By doubling the number of lanes, the total number of cars that\ncan travel per hour (the bandwidth) is doubled, but your own travel time (the\nlatency) is not reduced. If all you want to do is increase cars-per-second,\nthen adding more lanes (wider bus) is the answer, but if you want to reduce\nthe time for a specific car to get from A to B then you need to do something\nelse \u2013 usually either raise the speed limit (bus and RAM speed), or reduce the\ndistance, or perhaps build a regional mall so that people don't need to go to\nthe city as often (a cache).\n\nWhen it comes to memory systems, there are often subtle tradeoffs between\nlatency and bandwidth. Lower-latency designs will be better for pointer-\nchasing code, such as compilers and database systems, whereas bandwidth-\noriented systems have the advantage for programs with simple, linear access\npatterns, such as image processing and scientific code. Of course, it's\nreasonably easy to increase bandwidth \u2013 simply adding more memory banks and\nmaking the busses wider can easily double or quadruple bandwidth. In fact,\nmany high-end systems do this to increase their performance, but it comes with\ndownsides as well. In particular, wider busses mean a more expensive\nmotherboard, restrictions on the way RAM can be added to a system (install in\npairs or groups of four) and a higher minimum RAM configuration.\n\nUnfortunately, latency is much harder to improve than bandwidth \u2013 as the\nsaying goes: \"you can't bribe god\". Even so, there have been some good\nimprovements in effective memory latency in past years, chiefly in the form of\nsynchronously clocked DRAM (SDRAM), which uses the same clock as the memory\nbus. The main benefit of SDRAM was that it allowed pipelining of the memory\nsystem, because the internal timing aspects and interleaved structure of SDRAM\nchip operation are exposed to the system and can thus be taken advantage of.\nThis reduces effective latency because it allows a new memory access to be\nstarted before the current one has completed, thereby eliminating the small\namounts of waiting time found in older asynchronous DRAM systems, which had to\nwait for the current access to complete before starting the next (on average,\nan asynchronous memory system had to wait for the transfer of half a cache\nline from the previous access before starting a new request, which was often\nseveral bus cycles, and we know how slow those are!).\n\nIn addition to the reduction in effective latency, there is also a substantial\nincrease in bandwidth, because in an SDRAM memory system, multiple memory\nrequests can be outstanding at any one time, all being processed in a highly\nefficient, fully pipelined fashion. Pipelining of the memory system has\ndramatic effects for memory bandwidth \u2013 an SDRAM memory system generally\nprovided double or triple the sustained memory bandwidth of an asynchronous\nmemory system of the same era, even though the latency of the SDRAM system was\nonly slightly lower, and the same underlying memory-cell technology was in use\n(and still is).\n\nWill further improvements in memory technology, along with even more levels of\ncaching, be able to continue to hold off the memory wall, while at the same\ntime scaling up to the ever higher bandwidth demanded by more and more\nprocessor cores? Or will we soon end up constantly bottlenecked by memory,\nboth bandwidth and latency, with neither the processor microarchitecture nor\nthe number of cores making much difference, and the memory system being all\nthat matters? It will be interesting to watch, and while predicting the future\nis never easy, there are good reasons to be optimistic...\n\n## Acknowledgments\n\nThe overall style of this article, particularly with respect to the style of\nthe processor \"instruction flow\" and microarchitecture diagrams, is derived\nfrom a combination of a well-known 1989 ASPLOS paper by Norman Jouppi and\nDavid Wall, the book POWER & PowerPC by Shlomo Weiss and James Smith, and the\ntwo very famous Hennessy/Patterson textbooks Computer Architecture: A\nQuantitative Approach and Computer Organization and Design.\n\nThere have, of course, been many other presentations of this same material,\nand naturally they are all somewhat similar, however the above four are\nexceptionally good (in my opinion). To learn more about these topics, those\nbooks are an excellent place to start.\n\n## More Information?\n\nIf you want more detail on the specifics of recent processor designs, and\nsomething more insightful than the raw technical manuals, here are a few good\narticles...\n\n  * The Intel Skylake Mobile and Desktop Launch, with Architecture Analysis \u2013 the current Intel x86 processor design, Core i*6 \"Skylake\", an incremental step from Haswell.\n  * Intel's Haswell CPU Microarchitecture \u2013 the previous Intel x86 processor design, Core i*4 \"Haswell\", largely based on the prior \"Sandy Bridge\" design.\n  * Intel's Sandy Bridge Microarchitecture \u2013 the most significant recent Intel x86 processor design, Core i*2 \"Sandy Bridge\", blending the Pentium Pro and Pentium 4 design styles.\n  * AMD's Bulldozer Microarchitecture \u2013 the novel resource-sharing approach used in AMD's Bulldozer-based processor designs, blurring the line between SMT and multi-core.\n  * Intel's Next Generation Microarchitecture Unveiled \u2013 Intel's revival of the venerable P6 core from the Pentium Pro/II/III/M to produce the Core microarchitecture.\n  * The Pentium 4 and the PowerPC G4e (and Part II) \u2013 a comparison of the very different designs of two extremely popular and successful, if somewhat maligned, processors.\n  * Into the K7 (and Part II) \u2013 the AMD Athlon, the only competitor to ever really challenge Intel's dominance in the world of x86 processors.\n  * The AMD Opteron Microprocessor (video) \u2013 a 1-hour presentation covering both the Opteron/Athlon 64 processor and AMD's 64-bit extensions to the x86 architecture.\n  * Niagara II: The Hydra Returns \u2013 Sun's innovative UltraSPARC T Niagara processor, revised for a second generation and taking thread-level parallelism to the extreme.\n  * Crusoe Explored \u2013 the Transmeta Crusoe processor and its software-based approach to x86 compatibility.\n\nAnd here are some articles not specifically related to any particular\nprocessor, but still very interesting...\n\n  * Designing an Alpha Microprocessor \u2013 a fascinating look at what really goes on in the various stages of a project to make a new processor.\n  * Things CPU Architects Need To Think About (video) \u2013 an interesting 80-minute presentation given by Bob Colwell, one of the principle architects of the Pentium Pro/II/III.\n\nAnd if you want to keep up with the latest news in the world of\nmicroprocessors...\n\n  * Ars Technica\n  * AnandTech\n  * Microprocessor Report\n  * Real World Tech\n\nThat should keep you busy!\n\n", "frontpage": false}
