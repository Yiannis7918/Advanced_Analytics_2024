{"aid": "40077233", "title": "Multi-Tenant Queues in Postgres", "url": "https://docs.hatchet.run/blog/multi-tenant-queues", "domain": "hatchet.run", "votes": 9, "user": "abelanger", "posted_at": "2024-04-18 15:26:57", "comments": 0, "source_title": "An unfair advantage: multi-tenant queues in Postgres \u2013 Nextra", "source_text": "Hatchet Documentation\n\nCTRL K\n\nBlog\n\nAn unfair advantage: multi-tenant queues in Postgres\n\n# An unfair advantage: multi-tenant queues in Postgres\n\n#####\n\nAlexander Belanger\n\nPublished on April 18, 2024\n\nTL;DR - we've been implementing fair queueing strategies for Postgres-backed\ntask queues, so processing Bob's 10,000 files doesn't crowd out Alice's 1-page\nPDF. We've solved this in Hatchet (opens in a new tab) and Hatchet Cloud\n(opens in a new tab) so you don't have to \u2014 here's a look at how we did it.\n\n## Introduction\n\nWe set the scene with a simple user request: they'd like to upload and parse a\nPDF. Or an image, CSV, audio file \u2014 it doesn't really matter. What matters is\nthat the processing of this file can take ages, and scales \u2265 linearly with the\nsize of the file.\n\nPerhaps you're an astute developer and realized that processing this file\nmight impact the performance of your API \u2014 or more likely, the new file upload\nfeature you pushed on Friday has you explaining to your family that nephew\nJimmy's baseball game on Saturday will have to wait.\n\nIn the postmortem, you decide to offload processing this file to somewhere\noutside of the core web server, asynchronously on a new worker process. The\nuser can now upload a file, the web server quickly sends it off to the worker,\nand life goes on.\n\nThat is, until Bob decides to upload his entire hard drive \u2014 probably also on\na Saturday \u2014 and your document processing worker now goes down.\n\nAt this point (or ideally before this point), you introduce...the task queue.\nThis allows you to queue each file processing task and only dispatch the\namount of tasks each worker can handle at a time.\n\nBut while this solves the problem of the worker crashing, it introduces a new\nproblem, because you've intentionally bottlenecked the system. Which means\nthat when Bob uploads his second hard drive, a new issue emerges - Alice's\n1-page file upload gets stuck at the back of the queue:\n\nYou're now worried about fairness \u2014 specifically, how can you guarantee fair\nexecution time to both Bob and Alice? We'd like to introduce a strategy that's\neasy to implement in a Postgres-backed queue \u2014 and more difficult in other\nqueueing systems \u2014 deterministic round-robin queueing.\n\n## The setup\n\nLet's start with some code! We're implementing a basic Postgres-backed task\nqueue, where workers poll for events off the queue at some interval. You can\nfind all the code used in these examples \u2014 along with some nice helper seed\nand worker commands \u2014 in this repo: github.com/abelanger5/postgres-fair-queue\n(opens in a new tab). Note that I chose sqlc to write these examples, so you\nmight see some sqlc.arg and sqlc.narg in the example queries.\n\nOur tasks are very simple \u2014 they have a created_at time, some input data, and\nan auto-incremented id:\n\n    \n    \n    -- CreateEnum CREATE TYPE \"TaskStatus\" AS ENUM ( 'QUEUED', 'RUNNING', 'SUCCEEDED', 'FAILED', 'CANCELLED' ); -- CreateTable CREATE TABLE tasks ( id BIGSERIAL NOT NULL, created_at timestamp, status \"TaskStatus\" NOT NULL, args jsonb, PRIMARY KEY (id) );\n\nThe query which pops tasks off the queue looks like the following:\n\n    \n    \n    -- name: PopTasks :many WITH eligible_tasks AS ( SELECT * FROM tasks WHERE \"status\" = 'QUEUED' ORDER BY id ASC FOR UPDATE SKIP LOCKED LIMIT COALESCE(sqlc.narg('limit'), 10) ) UPDATE tasks SET \"status\" = 'RUNNING' FROM eligible_tasks WHERE tasks.id = eligible_tasks.id RETURNING tasks.*;\n\nNote the use of FOR UPDATE SKIP LOCKED: this means that workers which\nconcurrently pull tasks off the queue won't pull duplicate tasks, because they\nwon't read any rows locked by other worker transactions.\n\nThe polling logic looks something like this:\n\n    \n    \n    type HandleTask func(ctx context.Context, task *dbsqlc.Task) func poll(ctx context.Context, handleTask HandleTask) { for { select { case <-ctx.Done(): return case <-time.After(5 * time.Second): tasks, err := queries.PopTasks(ctx, pool, 10) if err != nil { log.Printf(\"could not pop tasks: %v\", err) continue } for _, task := range tasks { handleTask(ctx, task) } } } }\n\nThe ORDER BY id statement gives us a default ordering by the auto-incremented\nindex. We've now implemented the basic task queue shared above, with long-\npolling for tasks. We could also add some nice features, like listen/notify to\nget new tasks immediately, but that's not the core focus here.\n\n## Fair queueing\n\nWe'd now like to guarantee fair execution time to Bob and Alice. A simple way\nto support this is a round-robin strategy: pop 1 task from Alice, 1 task from\nBob, and...Bob's your uncle? To achieve this, we can imagine separate queues\nfor each group of users -- in this case, \"purple,\" \"orange\" and \"green\":\n\nEven though we're essentially creating a set of smaller queues within our\nlarger queue, we don't want workers to manage their subscriptions across all\npossible queues. The ugliness of adding a new queue per group should be\nabstracted from the worker, which should use a single query to pop the next\ntasks out of the queue.\n\nTo define our groups, let's modify our implementation above slightly: we're\ngoing to introduce a group_key to each table:\n\n    \n    \n    CREATE TABLE tasks ( id BIGSERIAL NOT NULL, created_at timestamp, status \"TaskStatus\" NOT NULL, args jsonb, group_key text, PRIMARY KEY (id) );\n\nThe group key simply identifies which group the task belongs to \u2014 for example,\nis this one of Bob's or Alice's tasks? This can refer to individual users,\ntenants, or even a custom group key based on some combination of other fields.\n\n### First attempt: PARTITION BY\n\nLet's try our hand at writing a query to do this. While we have a few options,\nthe most straightforward solution is to use PARTITION BY. Here's what we'd\nlike the query to look like:\n\n    \n    \n    WITH eligible_tasks AS ( SELECT t.id, t.\"status\", t.\"group_key\", row_number() OVER (PARTITION BY t.\"group_key\" ORDER BY t.\"id\" ASC) AS rn FROM tasks t WHERE \"status\" = 'QUEUED' ORDER BY rn, t.id ASC LIMIT COALESCE(sqlc.narg('limit'), 10) FOR UPDATE SKIP LOCKED ) UPDATE tasks SET \"status\" = 'RUNNING' FROM eligible_tasks WHERE tasks.id = eligible_tasks.id AND tasks.\"status\" = 'QUEUED' RETURNING tasks.*;\n\nThis assigns a row number of 1 to the first task in each group, a row number\nof 2 to the second task in each group, and so on.\n\nHowever, if we run this, we'll get the error: ERROR: FOR UPDATE is not allowed\nwith window functions (SQLSTATE 0A000) . Easy, let's tweak our query to solve\nfor this - we'll load up the rows with PARTITION BY and pass them to a new\nexpression which uses SKIP LOCKED:\n\n    \n    \n    WITH ordered_tasks AS ( SELECT t.id, t.\"status\", t.\"group_key\", row_number() OVER (PARTITION BY t.\"group_key\" ORDER BY t.\"id\" ASC) AS rn FROM tasks t WHERE \"status\" = 'QUEUED' ORDER BY rn, t.id ASC LIMIT COALESCE(sqlc.narg('limit'), 10) ), eligible_tasks AS ( SELECT t1.id FROM ordered_tasks t1 FOR UPDATE SKIP LOCKED ) UPDATE tasks SET \"status\" = 'RUNNING' FROM eligible_tasks WHERE tasks.id = eligible_tasks.id AND tasks.\"status\" = 'QUEUED' RETURNING tasks.*;\n\n...but not so fast. We've introduced an issue by adding the first CTE (Common\nTable Expression - the queries using the WITH clause). If we run 3 workers\nconcurrently and log the number of rows that each worker receives, with a\nlimit of 100 rows per worker, we'll find only 1 worker is picking up tasks,\neven if there are more rows to return!\n\n    \n    \n    2024/04/05 12:52:50 (worker 1) popped 0 tasks 2024/04/05 12:52:50 (worker 0) popped 0 tasks 2024/04/05 12:52:50 (worker 2) popped 100 tasks 2024/04/05 12:52:51 (worker 1) popped 0 tasks 2024/04/05 12:52:51 (worker 2) popped 0 tasks 2024/04/05 12:52:51 (worker 0) popped 100 tasks 2024/04/05 12:52:52 (worker 0) popped 0 tasks 2024/04/05 12:52:52 (worker 2) popped 0 tasks 2024/04/05 12:52:52 (worker 1) popped 100 tasks 2024/04/05 12:52:53 (worker 0) popped 0 tasks 2024/04/05 12:52:53 (worker 1) popped 0 tasks 2024/04/05 12:52:53 (worker 2) popped 100 tasks\n\nWhat's happening here? By introducing the first CTE, we are now selecting\nlocked rows which are excluded by FOR UPDATE SKIP LOCKED in the second CTE -\nin other words, we might not enqueue any runs on some workers if we're polling\nconcurrently for new tasks. While we are still guaranteed to enqueue in the\nmanner which we'd like, we may reduce throughput if there's high contention\namong workers for the same rows.\n\nUnfortunately, using PARTITION BY isn't the right approach here. But before we\ndive into a better approach, this query does show some interesting properties\nof queueing systems more generally.\n\n### Aside: queueing woes\n\nA hotfix for the slow polling query would be adding 3 lines of code to our\nworker setup:\n\n    \n    \n    // sleep for random duration between 0 and polling interval to avoid thundering herd sleepDuration := time.Duration(id) * interval / time.Duration(numWorkers) log.Printf(\"(worker %d) sleeping for %v\\n\", id, sleepDuration) time.Sleep(sleepDuration)\n\nWhich gives us much more promising output:\n\n    \n    \n    2024/04/05 12:54:19 (worker 2) sleeping for 666.666666ms 2024/04/05 12:54:19 (worker 0) sleeping for 0s 2024/04/05 12:54:19 (worker 1) sleeping for 333.333333ms 2024/04/05 12:54:21 (worker 0) popped 100 tasks 2024/04/05 12:54:21 (worker 1) popped 100 tasks 2024/04/05 12:54:21 (worker 2) popped 100 tasks 2024/04/05 12:54:22 (worker 0) popped 100 tasks 2024/04/05 12:54:22 (worker 1) popped 100 tasks 2024/04/05 12:54:22 (worker 2) popped 100 tasks 2024/04/05 12:54:23 (worker 0) popped 100 tasks 2024/04/05 12:54:23 (worker 1) popped 100 tasks 2024/04/05 12:54:23 (worker 2) popped 100 tasks 2024/04/05 12:54:24 (worker 0) popped 100 tasks\n\nThis works \u2014 and you can modify this logic to be more distributed by\nmaintaining a lease when a worker starts for a set amount of time \u2014 as long as\nthe polling interval is below the query duration time (or more specifically,\npollingTime / numWorkers is below the query duration time). But what happens\nwhen our queue starts to fill up? Let's add 10,000 enqueued tasks and run an\nEXPLAIN ANALYZE for this query to take a look at performance:\n\n    \n    \n    QUERY PLAN --------------------------------------------------------------------------------------------------------------------------------------------- Update on tasks (cost=3060.81..3602.70 rows=77 width=42) (actual time=142.749..160.682 rows=100 loops=1) CTE tasks_1 -> Limit (cost=2479.94..2480.19 rows=100 width=36) (actual time=116.248..116.405 rows=100 loops=1) -> Sort (cost=2479.94..2517.65 rows=15087 width=36) (actual time=116.128..116.203 rows=100 loops=1) Sort Key: (row_number() OVER (?)), (random()) Sort Method: top-N heapsort Memory: 32kB -> WindowAgg (cost=1563.86..1903.32 rows=15087 width=36) (actual time=70.788..105.097 rows=10000 loops=1) -> Sort (cost=1563.86..1601.58 rows=15087 width=20) (actual time=69.491..72.651 rows=10000 loops=1) Sort Key: t.group_key, t.id Sort Method: quicksort Memory: 1010kB -> Seq Scan on tasks t (cost=0.00..516.75 rows=15087 width=20) (actual time=0.587..12.837 rows=10000 loops=1) Filter: (status = 'QUEUED'::\"TaskStatus\") CTE eligible_tasks -> LockRows (cost=3.25..578.12 rows=77 width=46) (actual time=119.137..133.640 rows=100 loops=1) -> Hash Join (cost=3.25..577.35 rows=77 width=46) (actual time=118.408..132.472 rows=100 loops=1) Hash Cond: (t_1.id = tasks_1.id) -> Seq Scan on tasks t_1 (cost=0.00..516.75 rows=15087 width=14) (actual time=0.660..9.547 rows=10000 loops=1) Filter: (status = 'QUEUED'::\"TaskStatus\") -> Hash (cost=2.00..2.00 rows=100 width=40) (actual time=117.536..117.569 rows=100 loops=1) Buckets: 1024 Batches: 1 Memory Usage: 18kB -> CTE Scan on tasks_1 (cost=0.00..2.00 rows=100 width=40) (actual time=116.848..117.191 rows=100 loops=1) -> Hash Join (cost=2.50..544.40 rows=77 width=42) (actual time=140.519..152.972 rows=100 loops=1) Hash Cond: (tasks.id = eligible_tasks.id) -> Seq Scan on tasks (cost=0.00..468.00 rows=19500 width=14) (actual time=5.947..13.352 rows=10000 loops=1) -> Hash (cost=1.54..1.54 rows=77 width=40) (actual time=134.125..134.126 rows=100 loops=1) Buckets: 1024 Batches: 1 Memory Usage: 16kB -> CTE Scan on eligible_tasks (cost=0.00..1.54 rows=77 width=40) (actual time=119.320..134.017 rows=100 loops=1) Planning Time: 38.426 ms Execution Time: 165.835 ms\n\nThe important part here is the WindowAgg cost - computing a partition across\nall rows on the groupKey naturally involves querying every QUEUED row (in this\ncase, 10000 tasks). We expect this to scale sublinearly with the number of\nrows in the input - let's take a guess and look at how our workers do on\n25,000 enqueued rows:\n\n    \n    \n    2024/04/05 13:06:24 (worker 2) sleeping for 666.666666ms 2024/04/05 13:06:24 (worker 0) sleeping for 0s 2024/04/05 13:06:24 (worker 1) sleeping for 333.333333ms 2024/04/05 13:06:26 (worker 0) popped 100 tasks 2024/04/05 13:06:26 (worker 1) popped 0 tasks 2024/04/05 13:06:26 (worker 2) popped 100 tasks 2024/04/05 13:06:27 (worker 0) popped 100 tasks 2024/04/05 13:06:27 (worker 1) popped 0 tasks 2024/04/05 13:06:28 (worker 2) popped 100 tasks 2024/04/05 13:06:29 (worker 0) popped 100 tasks 2024/04/05 13:06:29 (worker 1) popped 0 tasks 2024/04/05 13:06:29 (worker 2) popped 100 tasks\n\nSure enough, because we're seeing execution times greater than 333ms, we start\nlosing tasks on worker 1. This is very problematic, because not only is our\nqueue backlog increasing, but the throughput of our workers is decreasing, and\nthis isn't a problem we can solve by throwing more workers at the queue. This\nis a general problem in systems that are stable for a long time until some\nexternal trigger (for example, workers going down for an hour) causes the\nsystem to fail in an unexpected way, leading to the system being\nunrecoverable.\n\nA second practical solution to this issue is to create an OVERFLOW status on\nthe task queue, and set an upper bound on the number of enqueued tasks, to\nensure worker performance doesn't drop below a certain threshold. We then can\nperiodically check the overflow queue and place the overflow into the queued\nstatus. This is a good idea regardless of the query we write to get new tasks.\n\nBut practical advice aside, let's take a look at how to write this query to\navoid performance degradation at such a small number of enqueued tasks.\n\n## Improving performance\n\n### Sequencing algorithm\n\nThe main issue, as we've identified, is the window function which is searching\nacross every row that is QUEUED. What we were hoping to accomplish with the\npartition method was filling up each group's queue, ordering each group by the\ntask id, and order the tasks by their rank within each group.\n\nOur goal is to write a query that is constant-time (or as close as possible to\nconstant-time) when reading from the queue, so we can avoid our system being\nunrecoverable. Even using a JOIN LATERAL instead of PARTITION BY will get\nslower as the number of partitions (i.e. groups) increases. Also, maintaining\neach task's rank after reads (for example, decrementing the task's rank within\nthe group after read) will also get slower the more tasks we add to a group.\n\nWhat if instead of computing the rank within the group via the PARTITION BY\nmethod at read time, we wrote a sequence number at write time which guarantees\nround-robin enqueueing? At first glance, this seems difficult - we don't know\nthat Alice will need to enqueue 1 task in the future if Bob enqueued 10,000\ntasks now.\n\nWe can solve for this by reserving contiguous blocks of IDs for future\nenqueued runs which belong to groups which don't exist yet or don't have a\ntask assigned for that block yet. We're going to partition BIGINT\n(max=9,223,372,036,854,775,807) into blocks of blockLength:\n\nNext, let's assign task IDs according to the following algorithm:\n\n  1. Maintain a unique numerical id i for each distinct group, and maintain a pointer p to the last block that was enqueued for each group - we'll call this p(i) .\n  2. Maintain a pointer p to the block containing the maximum task ID which doesn't have a QUEUED status (in other words, the maximum assigned task), call this p_max_assigned. If there are no tasks in the queue, set this to the maximum block across all p(i). Initialize p_max_assigned at 0.\n  3. When a task is created in group j:\n\n    1. If this is a new group j is added, initialize p(j) to p_max_assigned\n    2. If this is an existing group j, set p(j) to the greater of p_max_assigned or p(j) + 1\n    3. Set the id of the task to j + blockLength * p(j)\n\n*Note: we are making a critical assumption that the number of unique group keys will always be below the blockLength , and increasing the blockLength in the future would be a bit involved. A blockLength of ~1 million gives us ~1 billion task executions. To increase the block length, it's recommended that you add an offset equal to the the maximum task id, and start assigning task ids from there. We will also (in the worst case) cap out at 1 billion executed tasks, though this can be fixed by reassigning IDs when close to this limit.*\n\n### SQL implemenation\n\nTo actually implement this, let's add a new set of tables to our queue\nimplementation. We'll add a table for task_groups, which maintains the pointer\np(i) from above, along with a table called task_addr_ptrs which maintains\np_max_assigned from above:\n\n    \n    \n    CREATE TABLE task_groups ( id BIGSERIAL NOT NULL, group_key text, block_addr BIGINT, PRIMARY KEY (id) ); ALTER TABLE task_groups ADD CONSTRAINT unique_group_key UNIQUE (group_key); ALTER TABLE tasks ADD CONSTRAINT fk_tasks_group_key FOREIGN KEY (group_key) REFERENCES task_groups (group_key); CREATE TABLE task_addr_ptrs ( max_assigned_block_addr BIGINT NOT NULL, onerow_id bool PRIMARY KEY DEFAULT true, CONSTRAINT onerow_uni CHECK (onerow_id) );\n\nNext, we'll write our CreateTask query using a blockLength of 1024*1024:\n\n    \n    \n    WITH group_key_task AS ( INSERT INTO task_groups ( id, group_key, block_addr ) VALUES ( COALESCE((SELECT max(id) FROM task_groups), -1) + 1, sqlc.arg('group_key')::text, (SELECT max_assigned_block_addr FROM task_addr_ptrs) ) ON CONFLICT (group_key) DO UPDATE SET group_key = EXCLUDED.group_key, block_addr = GREATEST( task_groups.block_addr + 1, (SELECT max_assigned_block_addr FROM task_addr_ptrs) ) RETURNING id, group_key, block_addr ) INSERT INTO tasks ( id, created_at, status, args, group_key ) VALUES ( (SELECT id FROM group_key_task) + 1024 * 1024 * (SELECT block_addr FROM group_key_task), COALESCE(sqlc.arg('created_at')::timestamp, now()), 'QUEUED', COALESCE(sqlc.arg('args')::jsonb, '{}'::jsonb), sqlc.arg('group_key')::text ) RETURNING *;\n\nThe great thing about this is that our PopTasks query doesn't change, we've\njust changed how we assign IDs. However, we do need to make sure to update\ntask_addr_ptrs in the same transaction that we pop tasks from the queue:\n\n    \n    \n    -- name: UpdateTaskPtrs :one WITH max_assigned_id AS ( SELECT id FROM tasks WHERE \"status\" != 'QUEUED' ORDER BY id DESC LIMIT 1 ) UPDATE task_addr_ptrs SET max_assigned_block_addr = COALESCE( FLOOR((SELECT id FROM max_assigned_id)::decimal / 1024 / 1024), COALESCE( (SELECT MAX(block_addr) FROM task_groups), 0 ) ) FROM max_assigned_id RETURNING task_addr_ptrs.*;\n\nAgainst 1 million enqueued tasks with 1000 partitions, we still only need to\nsearch across 100 rows:\n\n    \n    \n    QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------------------------------- Nested Loop (cost=12.89..853.72 rows=100 width=77) (actual time=17.521..20.227 rows=100 loops=1) CTE eligible_tasks -> Limit (cost=0.42..10.21 rows=100 width=14) (actual time=1.669..16.365 rows=100 loops=1) -> LockRows (cost=0.42..97842.23 rows=999484 width=14) (actual time=1.662..16.231 rows=100 loops=1) -> Index Scan using tasks_pkey on tasks tasks_1 (cost=0.42..87847.39 rows=999484 width=14) (actual time=0.711..13.331 rows=100 loops=1) Filter: (status = 'QUEUED'::\"TaskStatus\") -> HashAggregate (cost=2.25..3.25 rows=100 width=8) (actual time=17.299..17.497 rows=100 loops=1) Group Key: eligible_tasks.id Batches: 1 Memory Usage: 24kB -> CTE Scan on eligible_tasks (cost=0.00..2.00 rows=100 width=8) (actual time=1.720..16.959 rows=100 loops=1) -> Index Scan using tasks_pkey on tasks (cost=0.42..8.40 rows=1 width=77) (actual time=0.022..0.022 rows=1 loops=100) Index Cond: (id = eligible_tasks.id) Planning Time: 13.979 ms Execution Time: 21.433 ms\n\nYou may also have noticed that because we stopped using the window function,\nwe've removed the issue of selecting for previously locked rows. So even if we\nstart 10 workers at the same time, we're guaranteed to select unique rows\nagain:\n\n    \n    \n    2024/04/08 16:28:08 (worker 9) sleeping for 0s 2024/04/08 16:28:08 (worker 8) sleeping for 0s 2024/04/08 16:28:08 (worker 4) sleeping for 0s 2024/04/08 16:28:08 (worker 0) sleeping for 0s 2024/04/08 16:28:08 (worker 1) sleeping for 0s 2024/04/08 16:28:08 (worker 2) sleeping for 0s 2024/04/08 16:28:08 (worker 6) sleeping for 0s 2024/04/08 16:28:08 (worker 3) sleeping for 0s 2024/04/08 16:28:08 (worker 5) sleeping for 0s 2024/04/08 16:28:08 (worker 7) sleeping for 0s 2024/04/08 16:28:09 (worker 1) popped 100 tasks 2024/04/08 16:28:09 (worker 2) popped 100 tasks 2024/04/08 16:28:09 (worker 7) popped 100 tasks 2024/04/08 16:28:09 (worker 0) popped 100 tasks 2024/04/08 16:28:09 (worker 8) popped 100 tasks 2024/04/08 16:28:09 (worker 9) popped 100 tasks 2024/04/08 16:28:09 (worker 3) popped 100 tasks 2024/04/08 16:28:09 (worker 6) popped 100 tasks 2024/04/08 16:28:09 (worker 5) popped 100 tasks 2024/04/08 16:28:09 (worker 4) popped 100 tasks\n\nThis doesn't come without a tradeoff: our writes are slower due to\ncontinuously updating the block_addr parameter on the task_group. However,\neven the writes are constant-time, so the throughput on writes is still on the\norder of 500 to 1k tasks/second. If you'd prefer a higher write throughput,\nsetting a small limit for placing tasks in the OVERFLOW queue and using the\npartition method from above may be a better approach.\n\n## Introducing concurrency limits\n\nIn the above implementation, we had a simple LIMIT statement to set an upper\nbound of the number of tasks a worker should execute. But what if we want to\nset a concurrency limit for each group of tasks? For example, not only do we\nwant to limit a worker to 100 tasks globally, but we limit each group to 5\nconcurrent tasks (we'll refer to this number as concurrency below). This\nensures that even if there are slots available on the worker, they are not\nautomatically filled by the same user, which could again crowd out other users\nin the near future.\n\nLuckily, this is quite simple with the implementation above. Because of the\nway we've divided task ids across different block addresses, we can simply\nlimit concurrency by searching only from the minimum queued ID min_id to\nmin_id + blockLength * concurrency:\n\n    \n    \n    -- name: PopTasksWithConcurrency :many WITH min_id AS ( SELECT COALESCE(min(id), 0) AS min_id FROM tasks WHERE \"status\" = 'QUEUED' ), eligible_tasks AS ( SELECT tasks.id FROM tasks WHERE \"status\" = 'QUEUED' AND \"id\" >= (SELECT min_id FROM min_id) AND \"id\" < (SELECT min_id FROM min_id) + sqlc.arg('concurrency')::int * 1024 * 1024 ORDER BY id ASC FOR UPDATE SKIP LOCKED LIMIT COALESCE(sqlc.narg('limit')::int, 10) ) UPDATE tasks SET \"status\" = 'RUNNING' FROM eligible_tasks WHERE tasks.id = eligible_tasks.id RETURNING tasks.*;\n\nThis guarantees an additional level of fairness which makes it even harder for\nBob's workloads to interfere with Alice's.\n\n## Final thoughts\n\nWe've covered deterministic round-robin queueing, but it turns out that many\nsystems just need approximate fairness guarantees (\"deterministic\" in this\ncase refers to the fact that tasks are processed in a deterministic order on\nsubsequent reads - as opposed to using something like ORDER BY RANDOM()). But\nthere are other approaches which provide approximate fairness, such as shuffle\nsharding (opens in a new tab), which we'll show how to implement in Postgres\nin a future post.\n\nIf you have suggestions on making these queries more performant - or perhaps\nyou spotted a bug - I'd love to hear from you in our Discord (opens in a new\ntab).\n\nHatchet Cloud (opens in a new tab) is our managed Hatchet offering. Give it a\nspin and let us know what you think!\n\nLast updated on April 18, 2024\n\nHatchet\n\n", "frontpage": true}
