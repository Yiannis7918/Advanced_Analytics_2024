{"aid": "39964617", "title": "Mixture of Finetuned and GPT4 Model", "url": "https://huggingface.co/leeroo/LeerooDedicated-Math-7b", "domain": "huggingface.co", "votes": 1, "user": "zmy999", "posted_at": "2024-04-07 22:44:33", "comments": 0, "source_title": "leeroo/LeerooDedicated-Math-7b \u00b7 Hugging Face", "source_text": "leeroo/LeerooDedicated-Math-7b \u00b7 Hugging Face\n\nHugging Face\n\n#\n\nleeroo\n\n/\n\nLeerooDedicated-Math-7b\n\nText Generation Transformers Safetensors\n\nmistral math orchestration_of_experts custom_code text-generation-inference\n\nModel card Files Files and versions Community\n\nEdit model card\n\n# Leeroo Dedidcated Math Expert \ud83e\udd17\n\nThe model is built by applying Orchestration of Expert for the math domain.\nThe dedicated model either generates solutions or, when necessary, utilizes\nGPT-4 (or similar performing LLM) to fill in gaps in its knowledge base.\nSpecifically, when given an input, the dedicated model first determines if the\ninput question is solvable with the base model. If solvable, the orchestrator\nis detached, and token generation is invoked using the base LLM expert. If the\nproblem is difficult and requires a larger model like GPT-4, it produces\n<GPT4> token (i.e., token_id=32000).\n\nThe Orchestrator is first trained to estimate the knowledge of the base model,\nfor any given query, then we marge into the base model (MetaMath7b, here). In\ngeneral for any domain, you can build it by:\n\n  * Choose a base LLM expert \ud83e\udd17\n  * Train a domain-specific Orchestrator\n  * Merge Orchestrator with the base expert\n\n\u2705 In evaluations using the GSM8k dataset of OpenLLM Leaderboard, Leeroo Math\n7b model achieves an accuracy of 84.77% in 5-shot setting, positioning it\namong the top performers in its class and notably surpassing its base model,\nwhich scores 68.84% on the same dataset. This was accomplished while relying\non GPT-4 for responses to half of the questions posed by GSM8k.\n\n## Sample Usage\n\n    \n    \n    from transformers import AutoModelForCausalLM, AutoTokenizer model = AutoModelForCausalLM.from_pretrained(\"leeroo/LeerooDedicated-Math-7b\", trust_remote_code=True) tokenizer = AutoTokenizer.from_pretrained(\"leeroo/LeerooDedicated-Math-7b\") device = model.device # the following question is answered by the leeroo expert question = \"Natalia sold clips to 48 of her friends in April,and then she sold half as many clips in May.How many clips did Natalia sell altogether in April and May?\" encodeds = tokenizer([question], return_tensors=\"pt\") model_inputs = encodeds['input_ids'].to(device) generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=False) decoded = tokenizer.batch_decode(generated_ids) print(decoded[0]) # Natalia sold 48 clips in April.\\nIn May, she sold half as many clips as in April, # so she sold 48/2 = 24 clips.\\nAltogether, Natalia sold 48 + 24 = 72 clips in April and May.\\n#### 72\\nThe answer is: 72</s> # sends the following question to GPT4 question = \"James loves to go swimming and has to swim across a 20-mile lake. He can swim at a pace of 2 miles per hour. He swims 60% of the distance. After that, he stops on an island and rests for half as long as the swimming time. He then finishes the remaining distance while going half the speed. How long did it take him to get across the lake?\" encodeds = tokenizer([question], return_tensors=\"pt\") model_inputs = encodeds['input_ids'].to(device) generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=False) decoded = tokenizer.batch_decode(generated_ids) print(decoded[0]) # <GPT4></s>\n\nYou can also add your OpenAI API to get the full answer when <GPT4> token is\ngenerated:\n\n    \n    \n    from openai import OpenAI from transformers import AutoModelForCausalLM, AutoTokenizer model = AutoModelForCausalLM.from_pretrained(\"leeroo/LeerooDedicated-Math-7B-OOE\", trust_remote_code=True) tokenizer = AutoTokenizer.from_pretrained(\"leeroo/LeerooDedicated-Math-7B-OOE\") openai_client = OpenAI( api_key= \"OPENAI_API_KEY\", base_url= \"https://api.openai.com/v1\" ) def generate(prompt, tokenizer, model, openai_client, max_new_tokens=100, verbose=True): inputs = tokenizer(prompt, return_tensors=\"pt\") inputs = {k:v.to(model.device) for k,v in inputs.items()} gen_tokens = model.generate( **inputs , max_new_tokens=max_new_tokens, do_sample=False, pad_token_id= tokenizer.pad_token_id) if gen_tokens[0, inputs['input_ids'].shape[1]] != tokenizer.unk_token_id: if verbose: print(\"\\033[94mGenerating using MetaMath7b.\\033[0m\") gen_text = tokenizer.decode( gen_tokens[0, inputs['input_ids'].shape[1]:].tolist() ) else: if verbose: print(\"\\033[94mGenerating using gpt4.\\033[0m\") gen_text = openai_client.completions.create( model = \"gpt-4-1106-preview\", # NOTE you can use any bigger mode here having performance similar to gpt4 prompt = prompt, max_tokens = max_new_tokens, temperature = 0.0 ).choices[0].text return gen_text # the following question is answered by the leeroo expert prompt = \"Question: Natalia sold clips to 48 of her friends in April,and then she sold half as many clips in May.How many clips did Natalia sell altogether in April and May?\\nAnswer:\" generation = generate(prompt, tokenizer, model, openai_client, max_new_tokens=500) print(generation) #> Generating using MetaMath7b. # Natalia sold 48 clips in April.\\nIn May, she sold half as many clips as in April, # so she sold 48/2 = 24 clips.\\nAltogether, Natalia sold 48 + 24 = 72 clips in April and May.\\n#### 72\\nThe answer is: 72</s> # sends the following question to GPT4 prompt = \"James loves to go swimming and has to swim across a 40-mile lake. He can swim at a pace of 2 miles per hour. He swims 60% of the distance. After that, he stops on an island and rests for half as long as the swimming time. He then finishes the remaining distance while going half the speed. How many hours did it take him to get across the lake?\" generation = generate(prompt, tokenizer, model, openai_client, max_new_tokens=500) print(generation) #> Generating using gpt4. # He swam 40*.6=24 miles # So he swam for 24/2=12 hours # He rested for 12/2=6 hours # He had 40-24=16 miles left to swim # He swam at 2/2=1 mile per hour # So he swam for 16/1=16 hours # So in total, it took him 12+6+16=34 hours # 34\n\n## Learn More\n\n\ud83d\udd0d To a deeper dive into our method and results, refer to HF blog \ud83e\udd17,\npublication, and repository. \ud83c\udf0d Join Leeroo community for further updates:\nLinkedin, Discord, X, Website.\n\n## Citation\n\n    \n    \n    @misc{mohammadshahi2024leeroo, title={Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration}, author={Alireza Mohammadshahi and Ali Shaikh and Majid Yazdani}, year={2024}, eprint={2401.13979}, archivePrefix={arXiv}, primaryClass={cs.CL} }\n\nDownloads last month\n\n    237\n\nSafetensors\n\nModel size\n\n7.26B params\n\nTensor type\n\nFP16\n\n\u00b7\n\nText Generation\n\nInference API (serverless) does not yet support model repos that contain\ncustom code.\n\n## Dataset used to train leeroo/LeerooDedicated-Math-7b\n\n", "frontpage": false}
