{"aid": "40015157", "title": "InfLLM: LLMs for Understanding Long Sequences with Training-Free Memory", "url": "https://github.com/thunlp/InfLLM", "domain": "github.com/thunlp", "votes": 1, "user": "throwaway888abc", "posted_at": "2024-04-12 17:03:22", "comments": 0, "source_title": "GitHub - thunlp/InfLLM: The code of our paper \"InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory\"", "source_text": "GitHub - thunlp/InfLLM: The code of our paper \"InfLLM: Unveiling the Intrinsic\nCapacity of LLMs for Understanding Extremely Long Sequences with Training-Free\nMemory\"\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nthunlp / InfLLM Public\n\n  * Notifications\n  * Fork 15\n  * Star 162\n\nThe code of our paper \"InfLLM: Unveiling the Intrinsic Capacity of LLMs for\nUnderstanding Extremely Long Sequences with Training-Free Memory\"\n\n162 stars 15 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# thunlp/InfLLM\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n2 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nguyan364Merge pull request #25 from thunlp/faiss18bc061 \u00b7\n\n## History\n\n37 Commits  \n  \n### benchmark\n\n|\n\n### benchmark\n\n| feat minicpm  \n  \n### config\n\n|\n\n### config\n\n| add fattn mistral config  \n  \n### image\n\n|\n\n### image\n\n| init  \n  \n### inf_llm\n\n|\n\n### inf_llm\n\n| add perhead option  \n  \n### scripts\n\n|\n\n### scripts\n\n| fastchat integration  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| init  \n  \n### README.md\n\n|\n\n### README.md\n\n| update readme  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| add accelerate  \n  \n### setup.py\n\n|\n\n### setup.py\n\n| init  \n  \n## Repository files navigation\n\n# InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely\nLong Sequences with Training-Free Memory\n\nThe code of our paper \"InfLLM: Unveiling the Intrinsic Capacity of LLMs for\nUnderstanding Extremely Long Sequences with Training-Free Memory\" [pdf].\n\n## Updates\n\n  * March 3, 2024: Initial code release. See init.\n  * March 24, 2024: Refactor the code. Improve inference speed and reduce GPU memory usage.\n  * April 4, 2024: Supports topk retrieval using faiss.\n\n## Quick Links\n\n  * Overview\n  * Requirements\n  * Usage\n  * Citation\n\n## Overview\n\nLarge language models (LLMs) have emerged as a cornerstone in real-world\napplications with lengthy streaming inputs, such as LLM-driven agents.\nHowever, existing LLMs, pre-trained on sequences with restricted maximum\nlength, cannot generalize to longer sequences due to the out-of-domain and\ndistraction issues. To alleviate these issues, existing efforts employ sliding\nattention windows and discard distant tokens to achieve the processing of\nextremely long sequences. Unfortunately, these approaches inevitably fail to\ncapture long-distance dependencies within sequences to deeply understand\nsemantics. This paper introduces a training-free memory-based method, InfLLM,\nto unveil the intrinsic ability of LLMs to process streaming long sequences.\nSpecifically, InfLLM stores distant contexts into additional memory units and\nemploys an efficient mechanism to lookup token-relevant units for attention\ncomputation. Thereby, InfLLM allows LLMs to efficiently process long sequences\nwhile maintaining the ability to capture long-distance dependencies. Without\nany training, InfLLM enables LLMs pre-trained on sequences of a few thousand\ntokens to achieve superior performance than competitive baselines continually\ntraining these LLMs on long sequences. Even when the sequence length is scaled\nto 1, 024K, InfLLM still effectively captures long-distance dependencies.\n\n## Requirements\n\n    \n    \n    torch>=1.13.1 transformers>=4.37.2 fschat>=0.2.35 datasets>=2.17.0 omegaconf flash-attn rouge==1.0.1 fuzzywuzzy==0.18.0 jieba==0.42.1\n\n## Usage\n\n### Configuration\n\nWe use YAML files for configuration, and you can see the configuration files\nwe use for benchmark in the config/ directory.\n\nThe description of the configuration files is as follows:\n\n    \n    \n    model: # attention type. # inf-llm/infinite-lm/stream-lm/origin(full attention) type: inf-llm # huggingface or model-center model path path: mistralai/Mistral-7B-Instruct-v0.2 # Use flash-attention or not. # For inf-llm/infinite-lm/stream-llm, we implemented multi-stage flash-attention by OpenAI's Triton. fattn: false # RoPE base and distance_scale base: 1000000 distance_scale: 1.0 # inf-llm/infinite-lm/stream-lm settings # Initital tokens as attention sinks n_init: 128 # Local sliding window size n_local: 4096 # inf-llm settings # Number of memory units to retrieve for attention computation. topk: 16 # The number of top-scoring tokens per memory unit considered as representative elements. repr_topk: 4 # Maximum number of memory units stored in GPU memory. max_cached_block: 32 # Number of tokens queried at a time as an execution block. # Each execution block retrieves topk memory units once. exc_block_size: 512 # The strategy for replacing cached memory units. # Supported strategies include LRU (Least Recently Used), FIFO (First In, First Out), # and LRU-S (LRU in our paper). cache_strategy: lru # score_decay for LRU-S # score_decay: 0.1 # Use overlap local and global calculation. # Can accelerate, but may not be compatible. async_global_stream: false # Use faiss for topk retrieval of memory units. # It will increase inference time and ensure constant GPU memory usage. faiss: false # Use perhead topk. # Enabling it will be very time-consuming and is intended for research use only. # perhead: false # Model max input length. # A truncation will be employed if the input length exceeds. max_len: 2147483647 # truncation type. Now supports suffix only. truncation: suffix # Chunked input in decoding. # To save GPU memory. (FFN block) chunk_size: 8192 # Conversation type. # mistral/vicuna/qwen/minicpm conv_type: mistral\n\n### Evaluation\n\nData Preparation We adopt InfiniteBench and LongBench for model evaluation.\nYou can download the datasets by running the following command.\n\n    \n    \n    bash scripts/download.sh\n\nResponse Generation You can evaluate InfLLM by running the following command.\nNotably, the provided code is used to run evaluate with only one GPU, and you\ncan accelerate the experiments with multiple GPUs.\n\n    \n    \n    bash scripts/[infinitebench,longbench].sh\n\n### Run a Chatbot with InfLLM\n\nWe integrated fastchat's CLI chat.\n\n    \n    \n    python -m inf_llm.chat \\ --model-path mistralai/Mistral-7B-Instruct-v0.2 \\ --inf-llm-config-path config/mistral-inf-llm.yaml\n\n## Citation\n\nIf you find InfLLM useful, please cite the following paper:\n\n    \n    \n    @article{xiao2024infllm, author = {Chaojun Xiao and Pengle Zhang and Xu Han and Guangxuan Xiao and Yankai Lin and Zhengyan Zhang and Zhiyuan Liu and Song Han and Maosong Sun}, title = {InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory}, journal = {arXiv}, year = {2024} }\n\n## About\n\nThe code of our paper \"InfLLM: Unveiling the Intrinsic Capacity of LLMs for\nUnderstanding Extremely Long Sequences with Training-Free Memory\"\n\n### Topics\n\nlarge-language-models llm long-context training-free\n\n### Resources\n\nReadme\n\nActivity\n\nCustom properties\n\n### Stars\n\n162 stars\n\n### Watchers\n\n13 watching\n\n### Forks\n\n15 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 2\n\n  * guyan364 Pengle Zhang\n  * xcjthu xiaocj\n\n## Languages\n\n  * Python 97.9%\n  * Shell 2.1%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
