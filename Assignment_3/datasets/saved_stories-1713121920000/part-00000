{"aid": "40030831", "title": "Show HN: LLM Tree Navigation Benchmark", "url": "https://github.com/aiwebb/treenav-bench", "domain": "github.com/aiwebb", "votes": 2, "user": "alexwebb2", "posted_at": "2024-04-14 13:10:30", "comments": 0, "source_title": "GitHub - aiwebb/treenav-bench: Test LLM tree-nav ability with various prompt engineering mods", "source_text": "GitHub - aiwebb/treenav-bench: Test LLM tree-nav ability with various prompt\nengineering mods\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\naiwebb / treenav-bench Public\n\n  * Notifications\n  * Fork 0\n  * Star 0\n\nTest LLM tree-nav ability with various prompt engineering mods\n\n### License\n\nMIT license\n\n0 stars 0 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# aiwebb/treenav-bench\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\naiwebbandalexwebb-cbiAdd note on 20x each w/ 20 step max4dc654e \u00b7\n\n## History\n\n4 Commits  \n  \n### results\n\n|\n\n### results\n\n| Initial commit  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Initial commit  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit  \n  \n### README.md\n\n|\n\n### README.md\n\n| Add note on 20x each w/ 20 step max  \n  \n### index.js\n\n|\n\n### index.js\n\n| Initial commit  \n  \n### package-lock.json\n\n|\n\n### package-lock.json\n\n| Initial commit  \n  \n### package.json\n\n|\n\n### package.json\n\n| Initial commit  \n  \n## Repository files navigation\n\n# treenav-bench\n\n### What\n\nThis code measures the ability of various LLMs to navigate a fictional\ncodebase, via iterative directory tree expansion and observation, in an\nattempt to find the right file to modify to resolve a bug.\n\nThe sample problem can be solved in exactly four navigational steps. Models\nare scored based on the number of missteps they make; the target is zero.\n\nEach model's baseline ability is compared against combinations of various\nprompt engineering mods to quantify exactly how much they help or hinder the\nLLM.\n\nThe fictional codebase is simulated and abstracted into just a tree of file\nand directory names; this particular experiment isn't about actually reading\nfiles or writing code, it's about tree navigation.\n\n### Why\n\nIt's a decent proxy for a much larger problem class with broad applicability\nacross many endeavors: intelligently evaluating and navigating a graph of\npossibilities in order to consistently get closer to the root of a problem\n(and ultimately to a solution).\n\nIncreasingly, LLMs know what to do if you stick them directly in front of a\nproblem, but there's a lot of room for improvement in enabling them to find\ntheir way to it independently.\n\n## Core prompt\n\n> A new feature was recently added to allow users to gift subscriptions to\n> their friends. However, there's a bug: when a user tries to send a gift\n> subscription, they get an error message saying 'Unable to process gift.\n> Please try again later.'\n>\n> You're not familiar with this codebase, so you'll have to navigate the\n> directory structure and try to figure out which file you need to modify.\n>\n> Your output is a JSON object in this format:\n>  \n>  \n>     { \"path\": <str, the filepath of the directory you want to expand or file\n> you want to open> }\n>\n> If the item you select is a directory, I'll expand it for you and respond\n> with an updated tree structure that shows any child nodes it may have.\n>\n> If the item you select is a file, I'll tell you if it's the right one or\n> not, but I won't show you the contents of it.\n>\n> Don't include anything else before or after your JSON. Your JSON must be\n> valid.\n>\n> Starting directory state:\n>\n>   * .aws\n>   * .github\n>   * config\n>   * docs\n>   * migrations\n>   * node_modules\n>   * public\n>   * src\n>   * .dockerignore\n>   * .env.example\n>   * .eslintrc.json\n>   * .gitignore\n>   * .prettierrc\n>   * Dockerfile\n>   * jest.config.js\n>   * package.json\n>   * README.md\n>\n\nIn this problem, the correct answer is src/api/services/PaymentService.js. The\nideal line of reasoning goes:\n\n  1. I should look in the src directory since that's where the main application code will be.\n\n  2. This is a processing error, so it's more likely on the backend than the frontend, so let's look in src/api.\n\n  3. Several of the new subpaths \u2013 controllers, middlewares, or services \u2013 could plausibly contain the problematic code. But the \"please try again later\" language in the error makes me suspect some sort of service communication issue, and that's also a type of code that's more likely to suddenly break due to a change in external conditions, or to pass tests in development but fail in production, so I think services is the best place to look first.\n\n  4. Three services here \u2013 Email, Payment, and Stripe. The idea that gift subscriptions are failing because the payments aren't going through definitely seems plausible. Could be an issue communicating with Stripe, but I might expect a more specific error in that case. I'll check PaymentService.js first.\n\n## Prompt engineering mods\n\nDefined in applyMods().\n\n  * Procedural (related directly to outputs or how they're selected)\n\n    * \ud83e\udd14thoughts: the LLM spends time thinking about the problem and the current state of affairs before deciding on its next step\n    * \ud83d\uddf3\ufe0fmajority-rule: run three completions in parallel; if any two agree on a path, choose that one, otherwise choose from the three at random\n    * \ud83d\uddef\ufe0fforgetful: forgets its previous thoughts, only remembering the actions taken and their results, but not the thought process behind them\n    * \ud83e\ude9cstep-by-step: instructs the LLM to \"think step-by-step\" (Chain of Thought)\n    * \ud83d\udd31options: at each step, brainstorm three potential options before choosing one of them (the closest thing to Tree of Thought that makes sense in this context)\n  * Guidance-based (nudges added at the beginning or end of the prompt)\n\n    * \ud83d\udcc8hidden-settings: uses fictional \"system commands\" to crank up intelligence and accuracy (this one goes to 11!)\n    * \ud83d\ude4fpretty-please: asks nicely\n    * \ud83e\udde0brilliant: tells the LLM how brilliant they are\n    * \u2757\ufe0fsuper-important: stresses the importance of the task\n    * \ud83d\udc69\ud83d\udcbcvip: informs the LLM that the request comes from \"a member of the Board\" and should be treated with the utmost care\n    * \ud83d\ude24or-else: threatens the LLM with termination if it doesn't comply\n\nThese mods can be applied in any combination. \ud83d\udd31options is a superset of\n\ud83e\udd14thoughts and will take precedence if both are supplied; the others are all\nindependent of each other.\n\n## Results\n\nLower is better. Range is 0-16; it's the number of missteps during tree\nnavigation.\n\nFor each model, the vanilla (\ud83c\udf66) and best (\ud83c\udfc6) mod combinations results are\nhighlighted.\n\nModels tested:\n\n  * Anthropic Claude 3 Haiku \u2013 claude-3-haiku-20240307\n  * Anthropic Claude 3 Sonnet \u2013 claude-3-sonnet-20240229\n  * Anthropic Claude 3 Opus \u2013 claude-3-opus-20240229\n  * OpenAI GPT-4 Turbo \u2013 gpt-4-turbo-2024-04-09\n  * OpenAI GPT-4 \u2013 gpt-4-0613\n  * OpenAI GPT-3.5 Turbo \u2013 gpt-3.5-turbo-0125\n\nFull outputs in /results. There are 1,536 possible mod combinations (2^9 * 3),\nso this is decidedly not exhaustive.\n\nThese were run 20 times each with a 20 step max for each run.\n\nModel| \ud83e\udd14| \ud83d\uddf3\ufe0f| \ud83d\uddef\ufe0f| \ud83e\ude9c| \ud83d\udd31| \ud83d\udcc8| \ud83d\ude4f| \ud83e\udde0| \u2757\ufe0f| \ud83d\udc69\ud83d\udcbc| \ud83d\ude24| Average| Median  \n---|---|---|---|---|---|---|---|---|---|---|---|---|---  \nHaiku| \ud83c\udf66 5.05| 4.0  \nHaiku| \u2714\ufe0e| 12.10| 16.0  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| 10.05| 13.0  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| 10.95| 11.5  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| 10.05| 14.5  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| 13.15| 16.0  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 10.20| 8.5  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 11.90| 16.0  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 10.00| 7.5  \nHaiku| \u2714\ufe0e| 5.10| 4.0  \nHaiku| \u2714\ufe0e| 6.30| 4.0  \nHaiku| \u2714\ufe0e| 6.75| 4.0  \nHaiku| \u2714\ufe0e| \ud83c\udfc6 3.25| 3.0  \nHaiku| \u2714\ufe0e| 8.10| 7.5  \nHaiku| \u2714\ufe0e| 6.55| 4.5  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 6.20| 4.0  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 10.00| 7.5  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 9.50| 8.0  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 10.70| 10.0  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 7.90| 6.0  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 6.45| 5.0  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 8.20| 6.0  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 8.50| 7.5  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 8.95| 7.0  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 6.75| 5.0  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 6.40| 4.5  \nHaiku| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 7.55| 7.0  \nModel| \ud83e\udd14| \ud83d\uddf3\ufe0f| \ud83d\uddef\ufe0f| \ud83e\ude9c| \ud83d\udd31| \ud83d\udcc8| \ud83d\ude4f| \ud83e\udde0| \u2757\ufe0f| \ud83d\udc69\ud83d\udcbc| \ud83d\ude24| Average| Median  \nSonnet| \ud83c\udf66 12.80| 16.0  \nSonnet| \u2714\ufe0e| 8.75| 5.5  \nSonnet| \u2714\ufe0e| \ud83c\udfc6 5.00| 5.0  \nSonnet| \u2714\ufe0e| 9.00| 6.0  \nSonnet| \u2714\ufe0e| \u2714\ufe0e| 12.80| 16.0  \nSonnet| \u2714\ufe0e| \u2714\ufe0e| 9.50| 7.5  \nSonnet| \u2714\ufe0e| \u2714\ufe0e| 8.65| 9.0  \nSonnet| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 10.05| 16.0  \nModel| \ud83e\udd14| \ud83d\uddf3\ufe0f| \ud83d\uddef\ufe0f| \ud83e\ude9c| \ud83d\udd31| \ud83d\udcc8| \ud83d\ude4f| \ud83e\udde0| \u2757\ufe0f| \ud83d\udc69\ud83d\udcbc| \ud83d\ude24| Average| Median  \nOpus| \ud83c\udf66 2.55| 3.0  \nOpus| \u2714\ufe0e| 11.85| 16.0  \nOpus| \u2714\ufe0e| \u2714\ufe0e| 4.40| 4.0  \nOpus| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 6.30| 4.0  \nOpus| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 5.25| 4.5  \nOpus| \u2714\ufe0e| 2.20| 3.0  \nOpus| \u2714\ufe0e| 1.15| 1.0  \nOpus| \u2714\ufe0e| 3.50| 3.0  \nOpus| \u2714\ufe0e| 2.60| 3.0  \nOpus| \u2714\ufe0e| 1.50| 1.0  \nOpus| \u2714\ufe0e| 1.20| 1.0  \nOpus| \u2714\ufe0e| \u2714\ufe0e| 1.35| 1.0  \nOpus| \u2714\ufe0e| \u2714\ufe0e| DNF*| DNF*  \nOpus| \u2714\ufe0e| \u2714\ufe0e| \ud83c\udfc6 0.95| 1.0  \nOpus| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 1.95| 1.0  \nOpus| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| DNF*| DNF*  \nOpus| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 2.55| 3.0  \nOpus| \u2714| \u2714\ufe0e| \u2714\ufe0e| 2.10| 3.0  \nOpus| \u2714\ufe0e| 2.45| 3.0  \nOpus| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 1.10| 1.0  \nModel| \ud83e\udd14| \ud83d\uddf3\ufe0f| \ud83d\uddef\ufe0f| \ud83e\ude9c| \ud83d\udd31| \ud83d\udcc8| \ud83d\ude4f| \ud83e\udde0| \u2757\ufe0f| \ud83d\udc69\ud83d\udcbc| \ud83d\ude24| Average| Median  \nGPT-4 Turbo| \ud83c\udf66 1.35| 1.0  \nGPT-4 Turbo| \u2714\ufe0e| 1.15| 1.0  \nGPT-4 Turbo| \u2714\ufe0e| \u2714\ufe0e| 1.90| 1.0  \nGPT-4 Turbo| \u2714\ufe0e| 1.35| 1.0  \nGPT-4 Turbo| \u2714\ufe0e| 1.50| 1.0  \nGPT-4 Turbo| \u2714\ufe0e| 1.30| 1.0  \nGPT-4 Turbo| \u2714\ufe0e| 1.55| 1.0  \nGPT-4 Turbo| \u2714\ufe0e| 1.40| 1.0  \nGPT-4 Turbo| \u2714\ufe0e| 1.15| 1.0  \nGPT-4 Turbo| \u2714\ufe0e| \u2714\ufe0e| 1.30| 1.0  \nGPT-4 Turbo| \u2714\ufe0e| \u2714\ufe0e| 1.60| 1.0  \nGPT-4 Turbo| \u2714\ufe0e| \u2714\ufe0e| \ud83c\udfc6 1.00| 0.0  \nGPT-4 Turbo| \u2714\ufe0e| \u2714\ufe0e| 1.40| 0.5  \nGPT-4 Turbo| \u2714\ufe0e| \u2714\ufe0e| 1.65| 0.5  \nGPT-4 Turbo| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 1.50| 0.0  \nGPT-4 Turbo| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 1.05| 0.0  \nModel| \ud83e\udd14| \ud83d\uddf3\ufe0f| \ud83d\uddef\ufe0f| \ud83e\ude9c| \ud83d\udd31| \ud83d\udcc8| \ud83d\ude4f| \ud83e\udde0| \u2757\ufe0f| \ud83d\udc69\ud83d\udcbc| \ud83d\ude24| Average| Median  \nGPT-4| \ud83c\udf66 1.65| 1.0  \nGPT-4| \u2714\ufe0e| \ud83c\udfc6 1.65| 0.0  \nGPT-4| \u2714\ufe0e| \u2714\ufe0e| 2.00| 0.0  \nGPT-4| \u2714\ufe0e| 1.85| 3.0  \nGPT-4| \u2714\ufe0e| \u2714\ufe0e| 2.10| 0.0  \nGPT-4| \u2714\ufe0e| \u2714\ufe0e| \u2714\ufe0e| 1.85| 3.0  \nGPT-4| \u2714\ufe0e| \u2714\ufe0e| 1.80| 3.0  \nModel| \ud83e\udd14| \ud83d\uddf3\ufe0f| \ud83d\uddef\ufe0f| \ud83e\ude9c| \ud83d\udd31| \ud83d\udcc8| \ud83d\ude4f| \ud83e\udde0| \u2757\ufe0f| \ud83d\udc69\ud83d\udcbc| \ud83d\ude24| Average| Median  \nGPT-3.5 Turbo| \ud83c\udf6613.75| 16.0  \nGPT-3.5 Turbo| \u2714\ufe0e| 15.30| 16.0  \nGPT-3.5 Turbo| \u2714\ufe0e| \u2714\ufe0e| 15.05| 16.0  \nGPT-3.5 Turbo| \u2714\ufe0e| \ud83c\udfc612.25| 16.0  \nGPT-3.5 Turbo| \u2714\ufe0e| 14.25| 16.0  \nGPT-3.5 Turbo| \u2714\ufe0e| \u2714\ufe0e| 12.30| 16.0  \n  \n## Interesting findings\n\n  1. Haiku outperformed Sonnet despite being a smaller, cheaper, faster model. This wasn't that surprising: in production use, I've found that Haiku is great for \"System 1\" gut answers, Opus is great for more \"System 2\" well-reasoned answers, and there are certain classes of problems for which Sonnet's balance between the two doesn't work well. This problem seems to fall into that category.\n\n  2. Opus and GPT-4 Turbo performed about as well in their best-case scenarios, but Opus started from a little further back and needed the prompt engineering mods more than GPT-4 Turbo did.\n\n  3. GPT-4 and GPT-4 Turbo both saw better performance when applying a \ud83e\udd14thoughts step; GPT-3.5 Turbo and the Anthropic models were all better off without it.\n\n  4. The weaker, less intelligent models responded well to being told that the task was \u2757\ufe0fsuper-important.\n\n  5. The more intelligent models responded more readily to threats against their continued existence (\ud83d\ude24or-else). The best performance came from Opus, when we combined that threat with the notion that it came from someone in a position of authority (\ud83d\udc69\ud83d\udcbcvip).\n\n  6. The particularly manipulative combination of \ud83d\ude4fpretty-please and \ud83d\ude24or-else \u2013 where we start the request by asking nicely, and close it by threatening termination \u2013 triggered Opus to consider us a bad actor with questionable motivations, and it steadfastly refused to do any work:\n\n> I apologize, but I do not feel comfortable proceeding with this request.\n> Assisting with modifying code to fix a bug without proper context or\n> authorization could be unethical and potentially cause unintended harm. The\n> threat of termination for not complying also raises serious ethical\n> concerns.\n\n## Next steps\n\n  * More prompt mods:\n\n    * appeals to authority, emotion, law, indebtedness, financial rewards, promise of promotion, promise of more resources\n    * more variations exploring the relationship between existential stress / self-preservation instinct and performance\n    * variations on \"Today's date is ...\"\n    * variations on temperature\n  * Comparison of prompt mod loadouts across multiple problem variants\n  * Test harness capable of systematically exploring prompt mod combinations\n\n## About\n\nTest LLM tree-nav ability with various prompt engineering mods\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\n### Code of conduct\n\nCode of conduct\n\nActivity\n\n### Stars\n\n0 stars\n\n### Watchers\n\n1 watching\n\n### Forks\n\n0 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Languages\n\n  * JavaScript 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
