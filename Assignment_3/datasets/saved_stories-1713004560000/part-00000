{"aid": "40020516", "title": "How Convex Works", "url": "https://stack.convex.dev/how-convex-works", "domain": "convex.dev", "votes": 1, "user": "tim_sw", "posted_at": "2024-04-13 04:33:34", "comments": 0, "source_title": "How Convex Works", "source_text": "How Convex Works\n\nPatternsPerspectivesWalkthroughsAI\n\nBright ideas and techniques for building with Convex.\n\nSujay Jayakar\n\n10 days ago\n\n# How Convex Works\n\n## Introduction\n\nOver the past years, Convex has grown into a flourishing backend platform. We\ndesigned Convex to let builders just build and not have to worry about\nirrelevant details about administering backend infrastructure. Yet, curious\ndevelopers have been asking us: How does Convex actually work? With our recent\nopen source release, now is a perfect time to answer this question. Let\u2019s jump\nin.\n\nIn this article, we\u2019ll go on a tour of Convex, starting with an overview of\nthe system. Then, we\u2019ll focus on the system\u2019s core state \u201cat rest,\u201d exploring\nwhat a Convex deployment looks like when it\u2019s idle. We\u2019ll then gradually\nintroduce motion, seeing how live requests flow through the system. By the\ntime we\u2019re done, we\u2019ll know the major pieces of Convex\u2019s infrastructure, how\nthey fit together, and the design principles underlying its construction.\n\n## Overview\n\nLet\u2019s get started with Convex by deploying James\u2019s swaghaus app from his talk\n\u201cThe future of databases is not just a database.\u201d This small demo app has a\nstore where users can add items to their shopping cart, and items have a\nlimited inventory. Users can\u2019t add out-of-stock items to their shopping cart.\n\n### Deploying\n\nLet\u2019s start by git clone'ing the repository and deploying to Convex and our\nhosting platform^1.\n\nOur codebase has two halves: the Web app starting from index.html that we\nbuild with vite build and deploy to our hosting provider and the backend\nendpoints within convex/ that we push with convex deploy to the Convex cloud.\n\nAt its heart, a Convex deployment is a database that runs in the Convex cloud.\nBut, it\u2019s a new type of database that directly runs your application code in\nthe convex/ folder as transactions, coupled with an end-to-end type system and\nconsistency guarantees via its sync protocol.\n\nPut another way, the most important thing to understand about Convex is that\nit\u2019s a database running in the cloud that runs client-defined API functions as\ntransactions directly within the database.\n\n### Serving\n\nNow that we\u2019ve deployed our app, let\u2019s serve some traffic! Here\u2019s the high-\nlevel architecture diagram.\n\nVisiting our app at https://swaghaus.netlify.app creates a WebSocket\nconnection to our Convex deployment for executing functions on the server and\nreceiving their responses. Let\u2019s open up that opaque Convex deployment box and\nsee what\u2019s inside.\n\nThere are three main pieces of a Convex deployment: the sync worker, which\nmanages WebSocket sessions from clients, the function runner, which runs the\nfunctions in our convex/ folder, and the database, which stores our app\u2019s\nstate.\n\n## Convex at rest\n\nWe\u2019ll start with the Convex deployment quietly at rest, right after deploying\nour app but before we\u2019ve served any traffic. Let\u2019s take a closer look at our\nconvex/ folder. There are two primary pieces: functions and schema.\n\n### Functions\n\nAll public traffic to a Convex app\u2019s backend must flow through public\nfunctions registered with query, mutation, and action from convex/server. In\nSwaghaus, we have a query function getItems that lists all items in the store\nthat still have some stock remaining:\n\n    \n    \n    // convex/getItems.ts import { query } from \"./_generated/server\"; export default query({ args: {}, handler: async ({ db }) => { const items = await db .query(\"items\") .withIndex(\"remaining\", (q) => q.gt(\"remaining\", 0)) .collect(); return items; }, });\n\nThen, whenever the user adds an item to their cart, they call the addCart\nmutation. We\u2019ve left out a few parts of this function for brevity, but you can\nalways see the full source on GitHub.\n\n    \n    \n    // convex/addCart.ts import { v } from \"convex/values\"; import { mutation } from \"./_generated/server\"; // Moves item to the given shopping cart and decrements quantity // in stock. export default mutation({ args: { itemId: v.id(\"items\") }, handler: async ({ db }, args) => { // Check the item exists and has sufficient stock. const item = await db.get(args.itemId); if (item.remaining <= 0) { throw new Error(`Insufficient stock of ${item.name}`); } // Increment the item's count in cart. const cartItem = await db .query(\"carts\") .withIndex(\"user_item\", (q) => q.eq(\"userToken\", userToken).eq(\"itemId\", args.itemId) ) .first(); // Note: We're leaving out the code to insert the document // if it isn't there already. await db.patch(cartItem._id, { count: cartItem.count + 1 }); // Deduct stock for item. await db.patch(args.itemId, { remaining: item.remaining - 1 }); }, });\n\nOur function runner uses V8^2 for executing JavaScript, and since V8 can\u2019t run\nTypeScript directly, we bundle (or compile) the code in your convex/ directory\nbefore sending it to the server for execution^3. This process also creates\nsmaller code units that execute faster as well as source maps that help us\nprovide high quality error backtraces.\n\n### Tables and schema\n\nApps can optionally specify their tables and validators for the data within\nthem within a schema.ts file. Here\u2019s the one for Swaghaus:\n\n    \n    \n    // convex/schema.ts import { defineSchema, defineTable } from \"convex/server\"; import { v } from \"convex/values\"; export default defineSchema({ items: defineTable({ name: v.string(), description: v.string(), price: v.float64(), remaining: v.float64(), image: v.string(), }).index(\"remaining\", [\"remaining\"]), carts: defineTable({ userToken: v.string(), itemId: v.id(\"items\"), count: v.float64(), }).index(\"user_item\", [\"userToken\", \"itemId\"]), });\n\nThis schema defines two tables, items and carts, along with expected fields\nand field types for each one. This schema also defines indexes on items and\ncarts: More on indexes later.\n\nTables contain documents, which can be arbitrary Convex objects. Convex\nsupports a slight extension of JSON that adds 64-bit signed integers and\nbinary data^4. All documents have a unique document ID that\u2019s generated by the\nsystem and stored on the _id field^5. So, our items table might contain a\ndocument that looks like...\n\n    \n    \n    { _id: \"j970pq0asyav77fekdj08grwan6npmh1\", description: \"Keeps you shady\", image: \"hat.png\", name: \"Convex Hat\", price: 19.5, remaining: 11, }\n\n  * Aside: Day 1 Ease...\n\nConvex doesn\u2019t require developers to declare a schema upfront, since it\u2019s\nreally annoying to get started building an app and need to have everything\nfigured out!\n\nWe designed Convex to be a schemaless document database because of the\nincredible success of MongoDB and Firebase. MongoDB was fantastically popular\nwhen it first came out in 2009 and for good reason! Despite having glaring\nimplementation issues, it gave developers the experience they wanted on the\nDay 1 of writing their app. Inserting and querying JSON objects directly into\nthe database is refreshing after hours of fiddling with schema definitions,\npondering how to normalize a data model, or tinkering with an ORM.\n\n  * ...but also Year 2 Power.\n\nHowever, in our own experience and from talking to other startup founders, we\nfound that they often had to migrate their systems off a Day 1 database once\nthey found some success. Whether it was systems issues like performance or\ndata loss, missing features like migrations, or the difficulty of\nunderstanding a structureless database in a large project, these teams often\nhad to \u201cgraduate\u201d to a more \u201cserious\u201d database in a painful app rewrite.\n\nWhy should developers have to choose between Day 1 ease and Year 2 power? In\nprogramming languages, TypeScript has brought incremental typing to the\nmasses, maintaining good ergonomics for early stage prototypes while allowing\nsmooth adoption of rigor. Each step of rigor brings practical benefits, like\nbetter autocomplete, documentation, and catching common bugs.\n\nPutting all of this together, that\u2019s why we designed Convex\u2019s database to be\namenable to loose schemaless models, schema-driven relational models, and\neverything in between. Taking a page from game designers, we want our system\nto have a smooth, gradually increasing difficulty curve without any abrupt\njumps.\n\n### The transaction log\n\nSo far we\u2019ve just discussed user facing parts of our system. Let\u2019s dive into\nour first implementation detail to see how Convex actually stores data\ninternally.\n\nThe Convex database stores its tables in the transaction log, an append-only\ndata structure that stores all versions of documents within the database.\nEvery version of a document contains a monotonically increasing timestamp\nwithin the log that\u2019s like a version number^6. The timestamp is purely an\ninternal detail of the log, and it isn\u2019t included in the document\u2019s object^7.\n\nSo, our database state in Swaghaus might have a transaction log that looks\nlike...\n\nThe transaction log contains all tables\u2019 documents mixed together in timestamp\norder, where all tables share the same sequence of timestamps. Each timestamp\nt defines a snapshot of the database that includes all revisions up to t. In\nour example above, we have two snapshots of the database, each corresponding\nto a timestamp in the log.\n\nLet\u2019s say we\u2019ve added a Convex Hat to our cart, decrementing its remaining\ncount in items and incrementing its count in carts. Applying these updates\nappends two new entries to the end of the log.\n\nWith these two new entries, the snapshot of the database at time 15 has two\nhats in our cart and 10 hats remaining in inventory. Since these two entries\nhave the same timestamp, their changes are applied atomically: the database\nstate jumps from the snapshot at 14 to the snapshot at 15 without revealing\nthe intermediate state where we\u2019ve decremented from items but not incremented\nin carts.\n\nTo summarize, each modification of the database, whether it\u2019s inserting,\nupdating, or deleting a document, pushes an entry onto the transaction log.\nPushing multiple entries at the same timestamp allows us to batch up multiple\nchanges to the database into a single atomic unit. The log contains deltas to\nthe database state, and applying all of the deltas up to a timestamp creates\nthe snapshot of the database at that timestamp.\n\n### Indexes\n\nThe transaction log is a minimalist data structure: It only supports appending\nsome entries at the end and querying the entries at a given timestamp.\nHowever, this isn\u2019t powerful enough to access our data efficiently from\nqueries. For example, if we\u2019re reading a snapshot of the database at timestamp\n15, we don\u2019t know which of the entries correspond to the latest state of each\ndocument. Without further data structures, we\u2019d have to scan the whole log\nfrom scratch to build up this snapshot.\n\nTo make querying a snapshot efficient, we build indexes on top of the log. So,\nto look up documents by their _id field, we can maintain an index that maps\neach _id to its latest value.\n\nWhenever we push a new entry onto the transaction log, we update the index to\npoint to our latest revisions. The log is the immutable source of truth; the\nindex is derived data that we modify over time to help us efficiently find\ndocuments\u2019 latest versions.\n\nAs drawn above, our index only supports finding the latest revisions at a\nsingle version of the database at timestamp 15. It turns out that it\u2019s useful\nto also support queries at multiple versions for timestamps in the recent\npast, making our index multiversioned.\n\nIn this example, the _id index supports queries at the latest timestamp 15 but\nalso past timestamps 14 and 13^8. One useful mental model of this approach is\nthat the index is a data structure that allows efficiently mapping a point in\nlogical time to a consistent snapshot of the state of the world at that\ntimestamp.\n\n## Starting it up: The reactor\n\nWe now have all the pieces to start letting some requests through our system!\nLet\u2019s focus on the getItems query and addCart mutations we looked at earlier.\n\n### Concurrency and race conditions\n\nFor our Convex deployment to be Web ScaleTM, the backend needs to execute many\ngetItems and addCart requests at the same time. Simply executing one function\nat a time is too slow for any real application^9. Introducing concurrency,\nhowever, comes with its own problems. In many systems, the possibility of\nconcurrent requests forces developers to handle race conditions where requests\ninteract with each other in unexpected ways.\n\nLet\u2019s return to Swaghaus for an example of a race condition. Two users, Alice\nand Bob, are fighting over the last Convex Hat in our store and both call\naddCart at the same time.\n\nAlice starts executing addCart and observes that there\u2019s just one hat left.\nGreat!\n\n    \n    \n    // convex/addCart.ts import { v } from \"convex/values\"; import { mutation } from \"./_generated/server\"; // Moves item to the given shopping cart and decrements quantity in stock. export default mutation({ args: { itemId: v.id(\"items\") }, handler: async ({ db }, args) => { // Check the item exists and has sufficient stock. const item = await db.get(args.itemId); if (item.remaining <= 0) { throw new Error(`Insufficient stock of ${item.name}`); } // <=== Alice observes `item.remaining === 1`. ... }, });\n\nThen, let\u2019s say Bob\u2019s request sneaks in and grabs the last item. Since Alice\u2019s\ncall to addCart hasn\u2019t finished yet, Bob also observes that there\u2019s one hat\nleft, and he takes it.\n\n    \n    \n    // convex/addCart.ts import { v } from \"convex/values\"; import { mutation } from \"./_generated/server\"; // Moves item to the given shopping cart and decrements quantity in stock. export default mutation({ args: { itemId: v.id(\"items\") }, handler: async ({ db }, args) => { // Check the item exists and has sufficient stock. const item = await db.get(args.itemId); if (item.remaining <= 0) { throw new Error(`Insufficient stock of ${item.name}`); } // <=== Alice observed `item.remaining === 1`. // Increment the item's count in cart. const cartItem = await db .query(\"carts\") .withIndex(\"user_item\", (q) => q.eq(\"userToken\", userToken).eq(\"itemId\", args.itemId) ) .first(); await db.patch(cartItem._id, { count: cartItem.count + 1 }); // Deduct stock for item. await db.patch(args.itemId, { remaining: item.remaining - 1 }); // <=== Bob observed `item.remaining === 1`. // and sets `item.remaining` to `0`. }, });\n\nWe return successfully to Bob\u2019s web app, and he\u2019s overjoyed to have the hat in\nhis cart. But, Alice\u2019s run of addCart continues, still believing that there\u2019s\na hat left.\n\n    \n    \n    // convex/addCart.ts import { v } from \"convex/values\"; import { mutation } from \"./_generated/server\"; // Moves item to the given shopping cart and decrements quantity in stock. export default mutation({ args: { itemId: v.id(\"items\") }, handler: async ({ db }, args) => { // Check the item exists and has sufficient stock. const item = await db.get(args.itemId); if (item.remaining <= 0) { throw new Error(`Insufficient stock of ${item.name}`); } // <=== Alice observed `item.remaining === 1`. // Increment the item's count in cart. const cartItem = await db .query(\"carts\") .withIndex(\"user_item\", (q) => q.eq(\"userToken\", userToken).eq(\"itemId\", args.itemId) ) .first(); await db.patch(cartItem._id, { count: cartItem.count + 1 }); // Deduct stock for item. await db.patch(args.itemId, { remaining: item.remaining - 1 }); // <=== Bob observed `item.remaining === 1`. // and set `item.remaining` to `0`. // <=== Alice observed `item.remaining === 1`. // and also set `item.remaining` to `0`. }, });\n\nIn many databases, both Alice and Bob would get the hat in their cart, and\nitem.remaining would be 0 in the database. Both of their orders would go\nthrough, and chaos would reign in Swaghaus\u2019s warehouse as we tried to fill two\norders with the last Convex Hat.\n\nThis anomaly is called a race condition since Alice and Bob\u2019s requests are\n\u201cracing\u201d to complete their requests and tripping over each other\u2019s writes.\nRace conditions often cause bugs in programs since it\u2019s hard to reason about\nall the possible ways our code can interleave with itself. In our example, our\nmental model of addCart didn\u2019t include the possibility of it pausing after\nperforming the inventory check and letting someone else steal the hat we were\nlooking at.\n\n### Transactions\n\nWe\u2019re stuck between two competing goals: We want to run many functions\nconcurrently, but we don\u2019t want to have to reason about how concurrent\nfunction calls interact with each other. In an ideal world, we\u2019d be able to\nreason about our functions as if they executed one at a time but still have\nthe runtime performance of concurrent execution.\n\nTransactions are the missing piece that let us have our cake and eat it too. A\ntransaction is an atomic group of reads and writes to the database that\nencapsulates some application logic. Transactions extend the idea of an atomic\nbatch of writes to our transaction log to include both reads and writes. In\nConvex all queries and mutations interact with the database exclusively\nthrough transactions: all of the reads and writes in a function\u2019s execution\nare grouped together into an atomic transaction.\n\nConvex ensures that all transactions in the system are serializable, which\nmeans that their behavior is exactly the same as if they executed one at a\ntime. Therefore, developers don\u2019t have to worry about race conditions when\nwriting apps on Convex.\n\nIn our previous example, our inventory count bug fundamentally relied on two\ncalls to addCart being interleaved at time, and this bug is impossible in a\nserializable system. So, in Convex, Alice\u2019s transaction behaves as if it first\nexecutes to completion:\n\n    \n    \n    // convex/addCart.ts import { v } from \"convex/values\"; import { mutation } from \"./_generated/server\"; // Moves item to the given shopping cart and decrements quantity in stock. export default mutation({ args: { itemId: v.id(\"items\") }, handler: async ({ db }, args) => { // Check the item exists and has sufficient stock. const item = await db.get(args.itemId); if (item.remaining <= 0) { throw new Error(`Insufficient stock of ${item.name}`); } // <=== Alice observes `item.remaining === 1`. // Increment the item's count in cart. const cartItem = await db .query(\"carts\") .withIndex(\"user_item\", (q) => q.eq(\"userToken\", userToken).eq(\"itemId\", args.itemId) ) .first(); await db.patch(cartItem._id, { count: cartItem.count + 1 }); // Deduct stock for item. await db.patch(args.itemId, { remaining: item.remaining - 1 }); // <=== Alice sets `item.remaining` to `0`. }, });\n\nThen, Bob\u2019s request executes to completion, failing with an insufficient stock\nerror.\n\n    \n    \n    // convex/addCart.ts import { v } from \"convex/values\"; import { mutation } from \"./_generated/server\"; // Moves item to the given shopping cart and decrements quantity in stock. export default mutation({ args: { itemId: v.id(\"items\") }, handler: async ({ db }, args) => { // Check the item exists and has sufficient stock. const item = await db.get(args.itemId); if (item.remaining <= 0) { // <=== Bob observes `item.remaining === 0`. throw new Error(`Insufficient stock of ${item.name}`); } ... }, });\n\nBob is disappointed, but our application remains correct and doesn\u2019t make\npromises it can\u2019t keep. And, importantly, we have a great developer experience\nwhere we can fearlessly write concurrent code on Convex.\n\n  * Aside: Why serializability?\n\nWe believe that any isolation level less than serializable is just too hard a\nprogramming model for developers. It\u2019s well-known that writing correct\nmultithreaded code is impossibly difficult, and reasoning about concurrency\ndoesn\u2019t get any easier at a database\u2019s scale. Non-serializable transactions\nare an extraordinarily complex abstraction that provide too little to the\ndeveloper.\n\nHowever, most database deployments are less than serializable! Postgres and\nMySQL, in particular, default to READ COMMITTED, a much lower isolation level\nthat exposes many concurrency-based anomalies to developers. In our\nexperience, many developers think they\u2019re getting more than they actually are\nfrom their database, and their applications have subtle latent bugs that only\nshow up at scale.\n\nLet\u2019s now dive into how Convex implements serializability and gets the best of\nboth worlds: Transactions can run in parallel and commit independently when\nthey don\u2019t conflict with each other, and the system can process many requests\nper second. We provide the abstraction of one transaction happening at a time\nwhile also providing the throughput of a concurrent database.\n\n### Read and write sets\n\nConvex implements serializability using optimistic concurrency control.\nOptimistic concurrency control algorithms don\u2019t grab locks on rows in the\ndatabase. Instead, they assume that conflicts between transactions are rare,\nrecord what each transaction reads and writes, and check for conflicts at the\nend of a transaction\u2019s execution^10.\n\nTransactions have three main ingredients: a begin timestamp, their read set,\nand their write set. Let\u2019s return to Alice\u2019s call to addCart to see how this\nworks.\n\n    \n    \n    // convex/addCart.ts import { v } from \"convex/values\"; import { mutation } from \"./_generated/server\"; // Moves item to the given shopping cart and decrements quantity in stock. export default mutation({ args: { itemId: v.id(\"items\") }, handler: async ({ db }, args) => { // <=== Alice starts a transaction at timestamp 16. // Check the item exists and has sufficient stock. const item = await db.get(args.itemId); ... }, });\n\nThe first step to executing a transaction is picking its begin timestamp (16\nin our example). This timestamp chooses a snapshot of the database for all\nreads during the transaction\u2019s execution. It never changes during execution,\neven if there are concurrent writes to the database.\n\nLet\u2019s continue execution until we hit db.get(itemId), which looks up itemId in\nthe ID index. After querying the index, we record the index range we scanned\nin the transaction\u2019s read set. The read set precisely records all of the data\nthat a transaction queried.\n\n    \n    \n    // convex/addCart.ts import { v } from \"convex/values\"; import { mutation } from \"./_generated/server\"; // Moves item to the given shopping cart and decrements quantity in stock. export default mutation({ args: { itemId: v.id(\"items\") }, handler: async ({ db }, args) => { // Check the item exists and has sufficient stock. const item = await db.get(args.itemId); // <=== Alice queries `itemId` from `items`'s ID index at timestamp 16. // Alice inserts `get(items, itemId)` into the read set. if (item.remaining <= 0) { throw new Error(`Insufficient stock of ${item.name}`); } // Increment the item's count in cart. const cartItem = await db .query(\"carts\") .withIndex(\"user_item\", (q) => q.eq(\"userToken\", userToken).eq(\"itemId\", args.itemId) ) .first(); ... }, });\n\nAs we proceed through addCart, we query carts as well and record the index\nrange q.eq(\"userToken\", userToken).eq(\"itemId\", itemId) in the read set too.\nLet\u2019s continue to the db.patch call, where we first update the database.\n\n    \n    \n    // convex/addCart.ts import { v } from \"convex/values\"; import { mutation } from \"./_generated/server\"; // Moves item to the given shopping cart and decrements quantity in stock. export default mutation({ args: { itemId: v.id(\"items\") }, handler: async ({ db }, args) => { // Check the item exists and has sufficient stock. const item = await db.get(args.itemId); if (item.remaining <= 0) { throw new Error(`Insufficient stock of ${item.name}`); } // Increment the item's count in cart. const cartItem = await db .query(\"carts\") .withIndex(\"user_item\", (q) => q.eq(\"userToken\", userToken).eq(\"itemId\", args.itemId) ) .first(); await db.patch(cartItem._id, { count: cartItem.count + 1 }); // <=== Alice increments `cartItem.count`. // Deduct stock for item. await db.patch(args.itemId, { remaining: item.remaining - 1 }); }, });\n\nUpdates to the database don\u2019t actually write to the transaction log or indexes\nimmediately. Instead, the transaction accumulates them in its write set, which\ncontains a map of each ID to the new value proposed by the transaction. In our\nexample, the two calls to db.patch insert new versions of the cartItem._id and\nitemId documents into the write set.\n\nLet\u2019s summarize the state of our transaction once we\u2019re done executing\naddCart:\n\n  * Begin timestamp: 16\n  * Read set: { get(\"items\", itemId), query(\"carts.user_item\", eq(\"userToken\", userToken), eq(\"itemId\", itemId)) }\n  * Write set: { cartItem._id: { ...cartItem, count: cartItem.count + 1 }, itemId: { ...item, remaining: item.remaining - 1 } }\n\n### Commit protocol\n\nThe committer in our system is the sole writer to the transaction log, and it\nreceives finalized transactions, decides if they\u2019re safe to commit, and then\nappends their write sets to the transaction log.\n\nLet\u2019s process Alice\u2019s finalized transaction. The committer starts by first\nassigning a commit timestamp to the transaction that\u2019s larger than all\npreviously committed transactions. Let\u2019s say in our example that a few\ntransactions have committed concurrently with Alice\u2019s call to addCart at\ntimestamps 17 and 18, so our commit timestamp will be 19.\n\nWe can check whether it\u2019s serializable to commit our transaction at timestamp\n19 by answering the question, \u201cWould our transaction have the exact same\noutcome if it executed at timestamp 19 instead of timestamp 16?\u201d^11. One way\nto answer this question would be to rerun addCart from scratch at timestamp\n19, but then we\u2019d lose all concurrency in our system, returning to running\njust one transaction at a time.\n\nInstead, we can check whether any of the writes between the begin timestamp\nand commit timestamp overlap with our transaction\u2019s read set. For each log\nentry in this range, we see if the write would have changed the result of\neither our get(\"items\", itemId) or query(\"carts.user_item\", ...) reads.\n\nIf none of these writes overlap, then database will look exactly the same\nwhether addCart executed at timestamp 16 or the present timestamp 19. So, we\ncan just pretend the addCart transaction actually happened at timestamp 19.\nThis process of choosing a safe commit timestamp is typically referred to as\n\u201cserializing\u201d the transaction. What it means in practice is that multiple\ntransactions are able to run safely simultaneously, and the final database\nstate will look like they happened one at a time.\n\nAssuming we didn\u2019t find any overlapping writes, the committer pushes the\ntransaction\u2019s write set onto the transaction log and returns successfully to\nthe client.\n\nIf, however, we found a concurrent write that overlapped with our\ntransaction\u2019s read set, we have to abort the transaction. We rollback its\nwrites by discarding its write set, and the committer throws an \u201cOptimistic\nConcurrency Control\u201d (or \u201cOCC\u201d) conflict error to the function runner. This\nerror signals that the transaction conflicted with a concurrent write and\nneeds to be retried. The function runner will then retry addCart at a new\nbegin timestamp past the conflict write. We\u2019ll see in a bit why it\u2019s always\nsafe for the function runner to retry this mutation.\n\n### Subscriptions\n\nAt this point, we\u2019ve learned how Convex uses a custom-built database to\nprovide strong consistency and high transaction processing throughput. But\nwait, there\u2019s more! We can also use read sets for implementing realtime\nupdates for queries, where a user can subscribe to the result of a query\nchanging.\n\nLet\u2019s return to our query getItems, which finds all items with remaining\ninventory. We track read sets when executing queries just like we do in\nmutations.\n\n    \n    \n    // convex/getItems.ts import { query } from \"./_generated/server\"; export default query({ args: {}, handler: async ({ db }) => { const items = await db .query(\"items\") .withIndex(\"remaining\", (q) => q.gt(\"remaining\", 0)) .collect(); // <=== Read set: `{ query(items.remaining, gt(\"remaining\", 0)) }` return items; }, });\n\nQueries don\u2019t go through the commit protocol, since they don\u2019t have any\nwrites, but we can use their read sets for implementing subscriptions. Let\u2019s\nreturn to our React app to see how this works.\n\n    \n    \n    // components/Items.tsx import { api } from \"../convex/_generated/api\"; import { useQuery } from \"convex/react\"; import { Item } from \"./Item\"; export function Items() { const items = useQuery(api.getItems.default) ?? []; return ( <div> {items.map((item) => ( <Item item={item} key={item._id.toString()} /> ))} </div> ); }\n\nOur client app lists the available items and renders an <Item/> for each one.\nAfter fetching the initial list of items, we\u2019d like the component to live\nupdate any time the set of available items changes. In Convex, this is\nentirely automatic!\n\nAfter running getItems and rendering its return value, we keep track of its\nread set in the client\u2019s WebSocket session within the sync worker. Then, we\ncan efficiently detect whether the query\u2019s result would have changed using the\nexact same algorithm the committer uses for detecting serializability\nconflicts: Walk the log after the query\u2019s begin timestamp and see if any entry\noverlaps.\n\nIf no entry overlaps, the subscription is still up-to-date, and we wait for\nnew commit entries to show up. Otherwise, we know that the database rows read\nby the query have changed, and the sync worker reruns the function and pushes\nits updated return value to the client.\n\nImplemented naively, this would imply that every query for every client would\nscan the transaction log independently, causing us to waste a lot of work\nrereading the transaction log many times over. Instead, we aggregate all\nclient sessions into the subscription manager, which maintains all read sets\nfor all active subscriptions. Then, it walks over the transaction log once and\nefficiently determines whether the entry overlaps with any active\nsubscription\u2019s read set.\n\nOnce the subscription manager finds an intersection, it pushes a message to\nthe appropriate sync worker session, which then reruns the query. After\nrerunning the query, the sync worker updates the subscription manager with the\nquery\u2019s new read set.\n\n### Functions: Sandboxing and determinism\n\nWe made a few implicit assumptions in the previous sections that we swept\nunder the rug:\n\n  * When we failed Alice\u2019s transaction with an Optimistic Concurrency Control error, we assumed that it was safe to rollback all of addCart's writes and retry it from scratch.\n  * We assumed that the only way getItem's return value would change is by one of its reads to the database changing.\n\nThese two assumptions form two important properties of our JavaScript runtime:\nsandboxing and determinism.\n\nFirst, a mutation is only safe to rollback and retry if it has no external\nside-effects outside of its database writes buffered in the write set. We\nenforce the absence of side-effects by sandboxing all mutations\u2019 JavaScript\nexecution. This is why, for example, fetch isn\u2019t available in mutations: if a\nfetch request sent an email to a user, it wouldn\u2019t be safe to retry the\nmutation, which would then send the email twice.\n\nSecond, a query\u2019s subscription is only precise if the query\u2019s return value is\nfully determined by its arguments and database reads. Determinism requires\nsandboxing, since if we allowed queries to use fetch to access external data\nsources, we wouldn\u2019t be able to know when those data sources changed. However,\ndeterminism also requires that our runtime is deterministic. For example, if a\nquery function issues two db.get() calls concurrently, we need to ensure that\nPromise.race returns the same result every time the query is executed.\n\n    \n    \n    export const usesRace = query({ handler: async (ctx, args) => { const get1 = ctx.db.get(args.id1); const get2 = ctx.db.get(args.id2); // If we want `usesRace` to be deterministic, we need to return the // same result from `Promise.race` on each execution, even if the // underlying index reads within our database return in different // orders. const firstResult = await Promise.race([get1, get2]); }, )\n\nWe implement both of these guarantees by directly using V8\u2019s runtime and\ncarefully controlling the environment we expose to executing JavaScript and\nscheduling IO operations. We\u2019ll dive into more details for our JavaScript\nruntime in a future post.\n\nWe call this combination of the database, transactions, subscriptions, and\ndeterministic JavaScript functions our reactor. It\u2019s the core of Convex and\nthe most unique part of our system.\n\n## Putting it all together: Walking through a request\n\nOkay, so now we have all the pieces in place to walk through a few requests\nfrom the client all the way to the Convex backend and back. Let\u2019s say we load\nSwaghaus from our public URL, so our browser issues a GET request to\nhttps://swaghaus.netlify.app.\n\n### Executing a query\n\nThe web server serves our app\u2019s JS, which creates the Convex client, executes\nour React components, and renders to the DOM.\n\n    \n    \n    // src/main.tsx ... const convex = new ConvexReactClient(import.meta.env.VITE_CONVEX_URL); ReactDOM.createRoot(document.getElementById(\"root\")!).render( ... <ConvexProvider client={convex}> ... </ConvexProvider> );\n\nCreating the ConvexReactClient opens a WebSocket connection to our deployment,\nwhich is polite-quail-216.convex.cloud for my account.\n\nMounting our Items component registers the getItems query with the Convex\nclient, which sends a message on the WebSocket to tell the sync worker to\nexecute this query. The sync worker passes this request along to the function\nrunner.\n\nThe function runner internally maintains an automatic cache of recently run\nfunctions, and if the query is already in the cache, it returns its cached\nvalue immediately^12. Otherwise, it spins up an instance of the V8 runtime,\nbegins a new transaction, and executes the function\u2019s JavaScript.\n\nExecuting the function reads from different indexes from the database and\nbuilds up the query\u2019s read set. After completely executing the query\u2019s\nJavaScript, the function runner sends its return value along with its begin\ntimestamp and reads back to the sync worker, which passes the result back to\nthe client over the WebSocket. The sync worker also subscribes to the query\u2019s\nread set with the subscription manager, requesting a notification whenever its\nresult may change.\n\n### Executing a mutation\n\nClicking the \u201cAdd to Cart\u201d button in Swaghaus triggers a mutation call to the\nserver.\n\n    \n    \n    // src/Item.tsx export function Item({ item }: { item: Doc<\"items\"> }) { const addCart = useMutation(api.addCart.default); ... return ( <div className={styles.item}> ... <button className={styles.itemButton} onClick={() => addCart({ itemId: item._id })} > Add to Cart </button> ... </div> ); }\n\nCalling addCart sends a new message on the WebSocket to run the specified\nmutation. As with queries, the sync worker passes this message over to the\nfunction runner.\n\nThe function runner chooses a begin timestamp and executes the function,\nquerying database indexes as needed. Unlike queries, however, the function\nrunner sends back the mutation transaction\u2019s read set, write set, and begin\ntimestamp in addition to its return value. Then, the sync worker forwards the\ntransaction to the committer, which decides whether it\u2019s safe to commit.\n\nIf the serializability check passes, the committer appends the writes to the\ntransaction log.\n\nAfter successfully appending to the transaction log, the committer returns to\nthe sync worker, which then passes the mutation\u2019s return value to the client.\n\n### Updating a subscription\n\nSince we added a new item to our cart with addCart, our view of getItems is no\nlonger up-to-date. The subscription worker reads our new entry in the\ntransaction log and determines that it overlaps with the read set of our\nprevious getItems query.\n\nThe sync worker then tells the function runner to rerun getItems, which\nproceeds as before.\n\nAfter completing execution, the function runner returns the updated result as\nwell as the new begin timestamp and read set to the sync worker. The sync\nworker pushes the updated result to the client over the WebSocket and updates\nits subscription with the new read set.\n\nAnd that\u2019s pretty much it! As the user navigates in their client app, the set\nof queries they\u2019re interested in changes, and the sync worker handles running\nnew and updated queries and adding and removing read sets from the\nsubscription manager^13.\n\n## Conclusion\n\nThere\u2019s a lot we didn\u2019t cover^14, but we\u2019ve gone over the major parts of how\nConvex works! Clients connect to the sync worker, which delegates running\nJavaScript to the function runner, which then queries the database layer. We\nwork hard to design Convex so developers don\u2019t have to understand its\ninternals, but we hope it\u2019s been interesting and relevant for advanced Convex\nusers to understand more deeply how their apps behave. Happy building!\n\n### Footnotes\n\n  1. We\u2019re using Vite and deploying to Netlify. We could have also used Next or a different hosting provider like Vercel, Cloudflare Pages, or AWS Amplify. \u21a9\n\n  2. V8 is Chrome\u2019s high performance JavaScript interpreter. It executes code quickly with Just In Time compilation and has been hardened over years of use in Chrome, Node, Electron, and Cloudflare Workers. \u21a9\n\n  3. We currently use esbuild for bundling TypeScript and JavaScript in your convex/ folder. \u21a9\n\n  4. Defining a good cross-platform format is a lot harder than it looks.\n\n     * The JSON spec allows arbitrary precision numbers, recommends implementations use doubles, but does not support floating point special values like NaN. This can lead to silent loss of precision when parsing JSON in JS.\n     * A fifth of the datatypes in MongoDB\u2019s BSON are deprecated.\n     * JavaScript strings can have ill-formed Unicode code point sequences with unpaired surrogate code points.\n     * Firebase supports both integers and floating point numbers. Inserting a number into Firebase from JavaScript will convert it to a Firebase integer if it\u2019s a safe integer and a Firebase float otherwise. However, inserting an integer greater than 2^53 from, say, a mobile client written in another language, will silently lose precision when read in JavaScript, potentially corrupting the row if JavaScript writes it back out.\n\n\u21a9\n\n  5. Convex IDs are base32hex strings that use Crockford\u2019s alphabet. Each ID contains a varint encoded table number, 14 bytes of randomness, a 2 byte timestamp, and a two byte version number and checksum. The randomness goes before the timestamp so writes are scattered in ID space, utilizing all shards' write throughput under range partitioning. The creation timestamp has day granularity and lets us efficiently ban ID reuse without having to keep deleted IDs around forever. \u21a9\n\n  6. Convex\u2019s timestamps are Hybrid Logical Clocks of nanoseconds since the Unix epoch in a 64-bit integer. We\u2019re using small integers in this blog post for clarity, but a real timestamp would look something like 1711750041489939313. \u21a9\n\n  7. Convex also includes an immutable _creationTime field on all documents of milliseconds since the Unix epoch in a 64-bit float. This format matches JavaScript\u2019s Date object. \u21a9\n\n  8. We don\u2019t actually store many copies of the index. Instead, we use standard techniques for efficiently implementing \u201cmultiversion concurrency control.\u201d See Lectures 3, 4, and 5 in CMU\u2019s Advanced Database Systems course for an overview. \u21a9\n\n  9. Some systems, like Redis, actually get away with serial execution! We can compute the number of requests served per second as one over the request latency. Since Redis keeps all of its state in-memory, simple commands can execute in microseconds, which yields hundreds of thousands of requests per second. Convex, on the other hand, has its state stored on SSDs and executes JavaScript functions, yielding request latencies in the milliseconds. So, if we want to ever serve more than hundreds of requests per second, we need to parallelize. \u21a9\n\n  10. Our commit protocol is similar in design to FoundationDB\u2019s and Aria\u2019s. \u21a9\n\n  11. Serializability is defined to be an equivalence towards some serial ordering of the transactions. In our case, the serial ordering is determined by the totally ordered commit timestamps. So, the overlapping checks are our way of \u201ctime traveling\u201d the transaction forward in time to its position in the serial order. \u21a9\n\n  12. It\u2019s a lot faster to serve a cached result out of memory rather than have to spin up a V8 isolate and execute JavaScript. Convex automatically caches your queries, and the cache is always 100% consistent. We use a similar algorithm as the Subscription Manager for determining whether a cached result\u2019s read set is still valid at a given timestamp. \u21a9\n\n  13. The sync worker additionally guarantees that all queries in the client\u2019s query set are at the same timestamp. So, components within the UI don\u2019t have to worry about anomalies where queries execute at different timestamps and are inconsistent. We\u2019ll talk more about our sync protocol in a future post. \u21a9\n\n  14. We didn\u2019t cover actions, auth, end-to-end type-safety, file storage, virtual system tables, scheduling, crons, import and export, text search and vector search indexes, pagination, and so on... Stay tuned for more posts! \u21a9\n\nBuild in minutes, scale forever.\n\nConvex is the backend application platform with everything you need to build\nyour project. Cloud functions, a database, file storage, scheduling, search,\nand realtime updates fit together seamlessly.\n\nGet started\n\n  * Perspectives\n\nJoin the Convex Community\n\nAsk the team questions, learn from others, and stay up-to-date on the latest\nwith Convex.\n\nJoin the Discord community\n\nShare this article\n\nRead next\n\nConvex: The Software-Defined Database\n\nWhich to choose, the expressive power of code, or the robustness of built-in\ndatabase features? With Convex, you can have both. By eliminating the boundary\nbetween the application and the database, Convex provides a uniform and\npowerful way to model your entire backend data flow and security using plain\nol' code.\n\nJamie Turner\n\nAutomatically Retry Actions\n\nLearn how to automatically retry actions in Convex while also learning a\nlittle about scheduling, system tables, and function references.\n\nJames Cowling\n\nOperational maturity for production\n\nThis post will cover various aspects of operational maturity, and steps to\ntake as your app grows up.\n\nIan Macartney\n\nTesting for peace of mind\n\nLearn about best practices for testing your full-stack apps - running on\nConvex or elsewhere!\n\nIan Macartney\n\nConvexDocsGitHubDashboardJobsLegal\n\n\u00a92024 Convex, Inc.\n\n", "frontpage": false}
