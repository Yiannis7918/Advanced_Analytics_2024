{"aid": "40103030", "title": "Dolphin-2.9-llama3-8B Released", "url": "https://huggingface.co/cognitivecomputations/dolphin-2.9-llama3-8b", "domain": "huggingface.co", "votes": 1, "user": "davidbarker", "posted_at": "2024-04-21 03:40:46", "comments": 0, "source_title": "cognitivecomputations/dolphin-2.9-llama3-8b \u00b7 Hugging Face", "source_text": "cognitivecomputations/dolphin-2.9-llama3-8b \u00b7 Hugging Face\n\nHugging Face\n\n#\n\ncognitivecomputations\n\n/\n\ndolphin-2.9-llama3-8b\n\nText Generation Transformers Safetensors\n\nllama generated_from_trainer conversational Inference Endpoints text-\ngeneration-inference\n\nModel card Files Files and versions Community\n\n4\n\nEdit model card\n\n# Dolphin 2.9 Llama 3 8b \ud83d\udc2c\n\nCurated and trained by Eric Hartford, Lucas Atkins, and Fernando Fernandes,\nand Cognitive Computations\n\nDiscord: https://discord.gg/8fbBeC7ZGx\n\nMy appreciation for the sponsors of Dolphin 2.9:\n\n  * Crusoe Cloud - provided excellent on-demand 10xL40S node\n\nThis model is based on Llama-3-8b, and is governed by META LLAMA 3 COMMUNITY\nLICENSE AGREEMENT\n\nThe base model has 8k context, and the full-weight fine-tuning was with 4k\nsequence length.\n\nIt took 2.5 days on 8x L40S provided by Crusoe Cloud\n\nThis model was trained FFT on all parameters, using ChatML prompt template\nformat.\n\nexample:\n\n    \n    \n    <|im_start|>system You are Dolphin, a helpful AI assistant.<|im_end|> <|im_start|>user {prompt}<|im_end|> <|im_start|>assistant\n\nDolphin-2.9 has a variety of instruction, conversational, and coding skills.\nIt also has initial agentic abilities and supports function calling.\n\nDolphin is uncensored. I have filtered the dataset to remove alignment and\nbias. This makes the model more compliant. You are advised to implement your\nown alignment layer before exposing the model as a service. It will be highly\ncompliant with any requests, even unethical ones. Please read my blog post\nabout uncensored models. https://erichartford.com/uncensored-models You are\nresponsible for any content you create using this model. Enjoy responsibly.\n\nDolphin is licensed according to Meta's Llama license. I grant permission for\nany use, including commercial, that falls within accordance with Meta's\nLlama-3 license. Dolphin was trained on data generated from GPT4, among other\nmodels.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n  * learning_rate: 2e-05\n  * train_batch_size: 3\n  * eval_batch_size: 3\n  * seed: 42\n  * distributed_type: multi-GPU\n  * num_devices: 8\n  * gradient_accumulation_steps: 4\n  * total_train_batch_size: 96\n  * total_eval_batch_size: 24\n  * optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n  * lr_scheduler_type: cosine\n  * lr_scheduler_warmup_steps: 7\n  * num_epochs: 3\n\n### Training results\n\nTraining Loss| Epoch| Step| Validation Loss  \n---|---|---|---  \n1.146| 0.0005| 1| 1.1064  \n0.6962| 0.2501| 555| 0.6636  \n0.6857| 0.5001| 1110| 0.6503  \n0.6592| 0.7502| 1665| 0.6419  \n0.6465| 1.0002| 2220| 0.6317  \n0.5295| 1.2395| 2775| 0.6408  \n0.5302| 1.4895| 3330| 0.6351  \n0.5188| 1.7396| 3885| 0.6227  \n0.521| 1.9896| 4440| 0.6168  \n0.3968| 2.2289| 4995| 0.6646  \n0.3776| 2.4789| 5550| 0.6619  \n0.3983| 2.7290| 6105| 0.6602  \n  \n### Framework versions\n\n  * Transformers 4.40.0\n  * Pytorch 2.2.2+cu121\n  * Datasets 2.18.0\n  * Tokenizers 0.19.1\n\nDownloads last month\n\n    0\n\nSafetensors\n\nModel size\n\n8.03B params\n\nTensor type\n\nBF16\n\n\u00b7\n\n##\n\nFinetuned from\n\n## Datasets used to train cognitivecomputations/dolphin-2.9-llama3-8b\n\n## Evaluation results\n\nMetadata error: specify a dataset to view leaderboard\n\n", "frontpage": false}
