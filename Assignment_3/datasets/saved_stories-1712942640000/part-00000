{"aid": "40011332", "title": "Tiny-autodiff: A tiny autograd library made for educational purposes in D", "url": "https://github.com/rillki/tiny-autodiff", "domain": "github.com/rillki", "votes": 1, "user": "teleforce", "posted_at": "2024-04-12 11:22:02", "comments": 0, "source_title": "GitHub - rillki/tiny-autodiff: A tiny autograd library made for educational purposes.", "source_text": "GitHub - rillki/tiny-autodiff: A tiny autograd library made for educational\npurposes.\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nrillki / tiny-autodiff Public\n\n  * Notifications\n  * Fork 2\n  * Star 6\n\nA tiny autograd library made for educational purposes.\n\n### License\n\nBSL-1.0 license\n\n6 stars 2 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# rillki/tiny-autodiff\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n3 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nrillkiTape added33bb822 \u00b7\n\n## History\n\n50 Commits  \n  \n### imgs\n\n|\n\n### imgs\n\n| README update and icon added  \n  \n### source\n\n|\n\n### source\n\n| Tape added  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| update  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit  \n  \n### README.md\n\n|\n\n### README.md\n\n| Tape added  \n  \n### dub.sdl\n\n|\n\n### dub.sdl\n\n| README update and rename T{GRAD => AUTODIFF}_USE_XXX  \n  \n### dub.selections.json\n\n|\n\n### dub.selections.json\n\n| update  \n  \n## Repository files navigation\n\n# Tiny AutoDiff\n\nA tiny autograd library. Implements backpropagation autodiff. It supports all\nyou need to build small neural networks.\n\n## Library\n\nAdd library to your project using DUB:\n\n    \n    \n    dub add tiny-autodiff\n\n## Precision\n\nUse the versions configuration to specify the precision:\n\n  * TAUTODIFF_USE_FLOAT\n  * TAUTODIFF_USE_DOUBLE\n  * TAUTODIFF_USE_REAL\n\n    \n    \n    // dub.sdl versions \"TAUTODIFF_USE_FLOAT\"\n    \n    \n    // dub.json versions: [\"TAUTODIFF_USE_FLOAT\"]\n\n## Example usage\n\n### Value\n\n    \n    \n    import rk.tautodiff; auto a = value(2); auto b = value(-3); auto c = value(10); auto f = value(-2); auto e = a * b; auto d = e + c; auto g = f * d; // backward g.backward(); // check grad after backward assert(g.grad == 1); assert(f.grad == 4); assert(d.grad == -2); assert(e.grad == -2); assert(c.grad == -2); assert(b.grad == -4); assert(a.grad == 6);\n\n### ChainSolver\n\nUse ChainSolver to solve equations step by step.\n\n    \n    \n    import rk.tautodiff; // create solver auto solver = ChainSolver(0); // 0 is initial value // operations using the produced result solver += 5; // 0 + 5 = 5 solver *= 2; // 3 * 2 = 6 // append new value and work with it solver ~= solver / value(2); assert(solver.data == 3); // backward solver.backward(); assert(solver.grad == 1); // zero grad solver.zeroGrad(); assert(solver.grad == 0); // reset solver.reset(); assert(solver.data == 0); assert(solver.grad == 0); // total length (allocated elements) assert(solver.values.length == 4);\n\n### Tape\n\nCreate tapes of equations and update the resulting value:\n\n    \n    \n    // init auto tape = new Tape(); assert(tape.values == []); assert(tape.values.length == 0); assert(tape.locked == false); assert(!tape.isLocked); // d = a * b - c auto a = 5.value; auto b = 10.value; auto c = 25.value; auto d = a * b; auto e = d - c; assert(e.data == 25); // push tape.pushBack(a); tape ~= b; tape ~= [c, d, e]; assert(tape.values == [a, b, c, d, e]); assert(tape.values.length == 5); assert(tape.lastValue.data == 25); // lock tape tape.lock(); // tape ~= 24.value; // assert error: reset the tape to push new values // modify value a.data = 6; // update tape tape.update(); assert(tape.lastValue.data == 35); // reset tape to push new values tape.reset(); tape ~= 35.value; // good\n\n### Multi-layer perceptron\n\n    \n    \n    import rk.tautodiff; import std.array : array; import std.stdio : writefln; import std.algorithm : map; // define data auto input = [ // binary [0, 0, 0, 0], // 0 [0, 0, 0, 1], // 1 [0, 0, 1, 0], // 2 [0, 0, 1, 1], // 3 [0, 1, 0, 0], // 4 [0, 1, 0, 1], // 5 [0, 1, 1, 0], // 6 [0, 1, 1, 1], // 7 [1, 0, 0, 0], // 8 [1, 0, 0, 1], // 9 [1, 0, 1, 0], // 10 [1, 0, 1, 1], // 11 [1, 1, 0, 0], // 12 [1, 1, 0, 1], // 13 [1, 1, 1, 0], // 14 [1, 1, 1, 1], // 15 ].map!(x => x.map!(y => y.value).array).array; auto target = [ // 1: even, 0: odd 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0 ].map!(x => x.value).array; // split train, test auto input_train = input[0 .. 12]; auto input_test = input[12 .. $]; // define model auto model = new MLP([4, 8, 1], &activateRelu, &activateSigmoid); // define loss function auto lossL2(Value[] preds) { import std.algorithm : reduce; // voldemort type struct L2Loss { Value loss; float accuracy; } // mse loss Value[] losses; foreach (i; 0..preds.length) losses ~= (preds[i] - target[i]) * (preds[i] - target[i]); auto dataLoss = losses.reduce!((a, b) => a + b) / preds.length; // accuracy float accuracy = 0.0; foreach (i; 0..preds.length) accuracy += ((preds[i].data > 0.5) == target[i].data); accuracy /= preds.length; // return voldemort type with cost and accuracy return L2Loss(dataLoss, accuracy); } // train enum lr = 0.05; enum epochs = 100; foreach (epoch; 0..epochs) { // forward Value[] preds; foreach (x; input_train) preds ~= model.forward(x); // loss auto l2 = lossL2(preds); // backward model.zeroGrad(); l2.loss.backward(); // update model.update(lr); // debug print if (epoch % 10 == 0) writefln(\"epoch %3s loss %.4f accuracy %.2f\", epoch, l2.loss.data, l2.accuracy); } // test foreach (i, x; input_test) { auto pred = model.forward(x)[0]; assert((pred.data > 0.5) == target[i].data); }\n\nOutput:\n\n    \n    \n    epoch 0 loss 1.9461 accuracy 0.50 epoch 10 loss 0.1177 accuracy 0.75 epoch 20 loss 0.0605 accuracy 1.00 epoch 30 loss 0.0395 accuracy 1.00 ... epoch 90 loss 0.0010 accuracy 1.00\n\n## References\n\n  * Golem (D)\n  * Grain (D)\n  * Micrograd (Py)\n  * Teenygrad (Py)\n  * Tinygrad (Py)\n\n## LICENSE\n\nAll code is licensed under the BSL license.\n\n## About\n\nA tiny autograd library made for educational purposes.\n\n### Topics\n\nautomatic-differentiation dlang autograd tensors autodiff autodifferentiation\nd-programming-language\n\n### Resources\n\nReadme\n\n### License\n\nBSL-1.0 license\n\nActivity\n\n### Stars\n\n6 stars\n\n### Watchers\n\n2 watching\n\n### Forks\n\n2 forks\n\nReport repository\n\n## Releases 3\n\nv1.0.2 Latest\n\nMar 26, 2024\n\n\\+ 2 releases\n\n## Packages 0\n\nNo packages published\n\n## Languages\n\n  * D 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
