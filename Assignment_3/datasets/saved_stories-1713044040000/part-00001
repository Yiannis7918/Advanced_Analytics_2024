{"aid": "40023756", "title": "A math puzzle and a better algorithm for top-k", "url": "https://quickwit.io/blog/top-k-complexity", "domain": "quickwit.io", "votes": 3, "user": "thunderbong", "posted_at": "2024-04-13 15:31:48", "comments": 0, "source_title": "A math puzzle and a better algorithm for top-k | Quickwit", "source_text": "A math puzzle and a better algorithm for top-k | Quickwit\n\nSkip to main content\n\nQuickwit 0.8 is out, enjoy faster queries!\n\n  * engineering\n\n# A math puzzle and a better algorithm for top-k\n\n  * Paul Masurel\n\nApril 12, 2024\n\n## A math brain teaser\n\nAn old friend is currently visiting Tokyo with his family. This weekend, we\nwent for a walk with our daughters in Yoyogi park. Knowing I have a sweet\ntooth for math puzzles, he told me about one he got from a coworker.\n\nWhile I had never exactly heard about it, something immediately felt familiar.\nI did not know the answer but suspiciously rushed in the right direction, and\nsolved it a little bit too rapidly. At first, I could not explain what this\nd\u00e9j\u00e0 vu feeling was about, so I pocketed the petty glory without telling him.\n\nLater, I understood the source of the d\u00e9j\u00e0 vu. This math problem is directly\nrelated to search, so I had probably derived the same calculation once in the\npast.\n\nHere is the problem...\n\nLet's put the world population (about 8 billion people) in a line. Each person\nis said to be the tallest-so-far if they are taller than all the people in\nfront of them.\n\nFor instance, we could have a line with people with the following heights:\n\n[175cm, 120cm, 172cm, 90cm, 156cm, 190cm, 192cm, 160cm, 185cm, 125cm, ...]\n\nThe bold numbers are the positions matching the tallest-so-far property.\n\nThe question is: in average, how many people in this 8 billions people line\nare tallest-so-far?\n\nIn mathy words, what is the expectancy of the total number of tallest-so-far\npeople in the line?\n\nLet me insert a rabbit to avoid any spoilers.\n\nThe solution relies on a common trick: express the problem as a sum of random\nvariables and rely on the linearity of expectancy.\n\nLet's consider the random variables (Xi)i=1..8 billion\n\nXi={10if the person in ith position is \u2019tallest-so-far\u2019else\n\nThen, we have our solution S equal to the sum of these 8 billion random\nvariables:\n\nS=E[i=1\u22118 billionXi]\n\nEven if our random variables are not independent, the expectancy is still\nlinear:\n\nS=i=1\u22118 billionE[Xi]\n\nThe probability for any given person to be the tallest out of the first i\npeople is i1. So finally, we get:\n\nS=i=1\u22118 billioni1\n\nWe recognize here the harmonic series Hn. It is equivalent to ln(n). We can\neven use the Euler-Maclaurin formula to get a very accurate approximation.\n\nS=H8 billion\u2248ln(8 billion)+\u03b3\u224823.4\n\nDid you have an intuition about the range of the result?\n\nIf you squint a little, this little problem is also applicable to the expected\nnumber world records in the absence of technical progress or dopping in sports\nfor instance.\n\nAnd if you squint harder, you will discover this problem is also related to\nsearch!\n\n# Top-K in search\n\nSo how is this problem related to search? Well, search engines usually work by\ncreating an iterator over the matching document IDs and their score.\n\nOnce the iterator is created, we are left with the need to iterate through the\ndocuments and keep track of the K documents with the highest score so far.\nThis is a famous algorithm problem called \"Top-K\".\n\nThe usual solution consists of maintaining a min-heap with the top K documents\nso far. At all times, the min-heap gives us the threshold above which a\ndocument should enter the top K and replace one of the elements of the heap.\n\nA textbook implementation could look like this:\n\n    \n    \n    pub struct Hit { pub score: u64, pub doc: u32, }\n    \n    impl Ord for Hit { fn cmp(&self, other: &Self) -> Ordering { self.score.cmp(&other.score).reverse() } }\n    \n    /* ... implementations of PartialOrd, Eq, PartialEq ... */\n    \n    pub fn heap_top_k(mut hits: impl Iterator<Item=Hit>, k: usize) -> Vec<Hit> { assert!(k>0); let mut top_k = BinaryHeap::with_capacity(k); top_k.extend((&mut hits).take(k)); let mut threshold = top_k.peek().unwrap().score;\n    \n    for hit in hits { if hit.score <= threshold { continue; } let mut head = top_k.peek_mut().unwrap(); *head = hit; drop(head); threshold = top_k.peek().unwrap().score; }\n    \n    top_k.into_sorted_vec() }\n\nThe worst-case complexity is obtained for a list that would be inversely\nsorted. In that case, heapreplace would be called for every single document,\nand we would have a complexity of \u0398(n ln(k)), where n is the number of\ndocuments and k the number of documents we want to keep.\n\nBut what is the average-case complexity? Is it really the same? I have yet to\nsee a text book mentionning it, and ChatGPT does not seem to know better.\n\n## The puzzle becomes handy\n\nOur puzzle is equivalent to computing the average complexity of a top-1\ncollector.\n\nThe property of being the tallest-so-far just means that we exceed the\nthreshold and need to update the heap.\n\nFor k=1, the computation of the average complexity of top-K is just a\ngeneralization of our problem. Let's count the number of calls to\nheapreplace(..)\n\nWe use the following random variable:\n\nXi={10if we have to mutate the heap at the ith elementelse\n\nThe number of times we will call heapreplace is the sum of these random\nvariables: \\\n\nS=E[i=k\u2211nXi]=i=k\u2211nE[Xi]\n\nFor all i > k, the probability for the ith element ith to be in the top K of\nthe first i elements ik. So we have:\n\nS=i=k\u2211nik=k(Hn\u2212Hk)=kln(n)\u2212kln(k)+\u0398(1)\n\nEach call to heapreplace itself has a complexity of ln(k), so we end up with a\ncomplexity in average of \u0398(n+kln(n)ln(k)).\n\n## A better algorithm\n\nTantivy actually uses a different solution with a complexity that is\n\u03b8(n+kln(k)), even in the worst case. It was taught to us by ChanMin, who was\nmy team mate at Indeed, and was implemented years later by Pascal.\n\nThe intuition is that we do not need to update the threshold every time we\ninsert a new element.\n\nInstead of maintaining a perfect threshold at all time using a Min-heap, we\ncan try and update it one out of k times. It can be done efficiently by to\nfilling a buffer of size 2k with the scored document IDs. When the buffer\nreaches its capacity, we can compute the new threshold using the median\nalgorithm and discard all elements below the median.\n\nWe can then rinse and repeat the operation kn times.\n\nIn rust the implementation looks like this:\n\n    \n    \n    fn top_k(mut hits: impl Iterator<Item=Hit>, k: usize) -> Vec<Hit> { assert!(k>0); let mut top_k = Vec::with_capacity(2 * k); top_k.extend((&mut hits).take(k));\n    \n    let mut threshold = 0u64; for hit in hits { if hit.score <= threshold { continue; } top_k.push(hit); if top_k.len() == 2*k { // The standard library does all of the heavy lifting here. let (_, median_el, _) = top_k.select_nth_unstable(k - 1); threshold = median_el.score; top_k.truncate(k); } } top_k.sort_unstable(); top_k.truncate(k); top_k }\n\nselect_nth_unstable is linear in k. Also, the number of flush operations is\nlinear is of kn. Finally, we still have to sort our k elements, hence the\nworst-case complexity:\n\n\u0398(k\u00d7kn+kln(k))=\u0398(n+kln(k))\n\n## Benchmarks\n\nOf course, the theory does not capture the characteristics of modern CPUs.\nPerformance is usually guided by the memory hierarchy and the branch\npredictor, so any decision should be validated by benchmarks.\n\nLet's have a look at the performance of the two algorithms for different\nvalues of k.\n\nThe code used for the benchmark is slightly different from the one in the\nbenchmark. You can find the code here.\n\nFirst, let's have a look at the results for average case, in which all of the\ndocs are randomly shuffled.\n\nHere is performance for different values of k, (n = 1_000_000).\n\n    \n    \n    topk fastest \u2502 slowest \u2502 median \u2502 mean \u2570\u2500 shuffled \u2502 \u2502 \u2502 \u251c\u2500 HeapTopK \u2502 \u2502 \u2502 \u2502 \u251c\u2500 1 296.1 \u03bcs \u2502 462.8 \u03bcs \u2502 298 \u03bcs \u2502 304.4 \u03bcs \u2502 \u251c\u2500 2 294.1 \u03bcs \u2502 773.4 \u03bcs \u2502 299.6 \u03bcs \u2502 312 \u03bcs \u2502 \u251c\u2500 4 297.2 \u03bcs \u2502 331.6 \u03bcs \u2502 299.9 \u03bcs \u2502 303.6 \u03bcs \u2502 \u251c\u2500 8 299.2 \u03bcs \u2502 347.4 \u03bcs \u2502 301.7 \u03bcs \u2502 304.2 \u03bcs \u2502 \u251c\u2500 16 301.9 \u03bcs \u2502 350.4 \u03bcs \u2502 304.2 \u03bcs \u2502 308 \u03bcs \u2502 \u251c\u2500 32 307.4 \u03bcs \u2502 355.4 \u03bcs \u2502 310.7 \u03bcs \u2502 313.6 \u03bcs \u2502 \u251c\u2500 64 318.4 \u03bcs \u2502 364.4 \u03bcs \u2502 322.5 \u03bcs \u2502 326.6 \u03bcs \u2502 \u251c\u2500 128 339.8 \u03bcs \u2502 401.6 \u03bcs \u2502 345.2 \u03bcs \u2502 347.9 \u03bcs \u2502 \u251c\u2500 256 384.3 \u03bcs \u2502 438 \u03bcs \u2502 389.3 \u03bcs \u2502 392.1 \u03bcs \u2502 \u251c\u2500 512 467.5 \u03bcs \u2502 525.8 \u03bcs \u2502 474.2 \u03bcs \u2502 478.2 \u03bcs \u2502 \u251c\u2500 1024 625.7 \u03bcs \u2502 705.3 \u03bcs \u2502 642.9 \u03bcs \u2502 647 \u03bcs \u2502 \u2570\u2500 2048 939.5 \u03bcs \u2502 1.061 ms \u2502 957.9 \u03bcs \u2502 961.9 \u03bcs \u2570\u2500 MedianTopK \u2502 \u2502 \u2502 \u251c\u2500 1 296 \u03bcs \u2502 329.2 \u03bcs \u2502 299.5 \u03bcs \u2502 302.9 \u03bcs \u251c\u2500 2 296.9 \u03bcs \u2502 335.7 \u03bcs \u2502 298.9 \u03bcs \u2502 301.5 \u03bcs \u251c\u2500 4 297.4 \u03bcs \u2502 348 \u03bcs \u2502 300 \u03bcs \u2502 303.1 \u03bcs \u251c\u2500 8 299.5 \u03bcs \u2502 335.6 \u03bcs \u2502 301.5 \u03bcs \u2502 305.2 \u03bcs \u251c\u2500 16 301.7 \u03bcs \u2502 354.9 \u03bcs \u2502 305.1 \u03bcs \u2502 308.9 \u03bcs \u251c\u2500 32 306.5 \u03bcs \u2502 346.2 \u03bcs \u2502 309.1 \u03bcs \u2502 312.5 \u03bcs \u251c\u2500 64 312.8 \u03bcs \u2502 355.5 \u03bcs \u2502 315.5 \u03bcs \u2502 319 \u03bcs \u251c\u2500 128 322.3 \u03bcs \u2502 369.3 \u03bcs \u2502 326.9 \u03bcs \u2502 330.4 \u03bcs \u251c\u2500 256 340.4 \u03bcs \u2502 374.6 \u03bcs \u2502 345.1 \u03bcs \u2502 347.2 \u03bcs \u251c\u2500 512 369.8 \u03bcs \u2502 426.6 \u03bcs \u2502 375.8 \u03bcs \u2502 379.3 \u03bcs \u251c\u2500 1024 415.8 \u03bcs \u2502 481.6 \u03bcs \u2502 423.3 \u03bcs \u2502 427.8 \u03bcs \u2570\u2500 2048 491.5 \u03bcs \u2502 552.9 \u03bcs \u2502 503.8 \u03bcs \u2502 510.2 \u03bcs\n\nWe see that both algorithm behave relatively nicely. The heap-based algorithm\nperformance degrades, but very slowly as k increases.\n\nOverall however, it is already a win for the median-based algorithm.\n\nNow here are the results for the worst case, in which values are already\nsorted:\n\n    \n    \n    topk fastest \u2502 slowest \u2502 median \u2502 mean \u2570\u2500 sorted \u2502 \u2502 \u2502 \u251c\u2500 HeapTopK \u2502 \u2502 \u2502 \u2502 \u251c\u2500 1 752.3 \u03bcs \u2502 833.6 \u03bcs \u2502 760.1 \u03bcs \u2502 765.8 \u03bcs \u2502 \u251c\u2500 2 1.677 ms \u2502 1.781 ms \u2502 1.684 ms \u2502 1.689 ms \u2502 \u251c\u2500 4 6.719 ms \u2502 7.372 ms \u2502 7.077 ms \u2502 7.072 ms \u2502 \u251c\u2500 8 11.24 ms \u2502 11.63 ms \u2502 11.35 ms \u2502 11.36 ms \u2502 \u251c\u2500 16 12.43 ms \u2502 12.88 ms \u2502 12.58 ms \u2502 12.59 ms \u2502 \u251c\u2500 32 13.98 ms \u2502 14.35 ms \u2502 14.11 ms \u2502 14.12 ms \u2502 \u251c\u2500 64 14.33 ms \u2502 28.94 ms \u2502 14.52 ms \u2502 14.71 ms \u2502 \u251c\u2500 128 14.33 ms \u2502 15.23 ms \u2502 14.51 ms \u2502 14.51 ms \u2502 \u251c\u2500 256 15.62 ms \u2502 16.22 ms \u2502 15.83 ms \u2502 15.84 ms \u2502 \u251c\u2500 512 18.24 ms \u2502 18.75 ms \u2502 18.44 ms \u2502 18.43 ms \u2502 \u251c\u2500 1024 21.84 ms \u2502 22.45 ms \u2502 22.07 ms \u2502 22.06 ms \u2502 \u2570\u2500 2048 24.7 ms \u2502 25.47 ms \u2502 25.03 ms \u2502 25.03 ms \u2570\u2500 MedianTopK \u2502 \u2502 \u2502 \u251c\u2500 1 4.255 ms \u2502 4.487 ms \u2502 4.283 ms \u2502 4.293 ms \u251c\u2500 2 3.557 ms \u2502 3.708 ms \u2502 3.576 ms \u2502 3.588 ms \u251c\u2500 4 4.192 ms \u2502 4.744 ms \u2502 4.208 ms \u2502 4.236 ms \u251c\u2500 8 2.88 ms \u2502 3.06 ms \u2502 2.897 ms \u2502 2.908 ms \u251c\u2500 16 2.353 ms \u2502 2.489 ms \u2502 2.363 ms \u2502 2.372 ms \u251c\u2500 32 2.237 ms \u2502 2.437 ms \u2502 2.247 ms \u2502 2.259 ms \u251c\u2500 64 2.13 ms \u2502 2.328 ms \u2502 2.15 ms \u2502 2.159 ms \u251c\u2500 128 2.31 ms \u2502 2.466 ms \u2502 2.327 ms \u2502 2.338 ms \u251c\u2500 256 2.257 ms \u2502 2.389 ms \u2502 2.266 ms \u2502 2.273 ms \u251c\u2500 512 2.22 ms \u2502 2.386 ms \u2502 2.23 ms \u2502 2.243 ms \u251c\u2500 1024 2.195 ms \u2502 2.32 ms \u2502 2.201 ms \u2502 2.21 ms \u2570\u2500 2048 2.196 ms \u2502 2.391 ms \u2502 2.206 ms \u2502 2.22 ms\n\nThis is where the linearity over K really delivers!\n\n### Related posts\n\n  * ### Grafana Plugin 0.4: QoL features ahead!\n\n  * A math brain teaser\n  * The puzzle becomes handy\n  * A better algorithm\n  * Benchmarks\n\nCopyright \u00a9 2024 Quickwit, Inc. Built with Docusaurus.\n\n", "frontpage": false}
