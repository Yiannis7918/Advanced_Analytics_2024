{"aid": "40030308", "title": "Travelling with Tailscale", "url": "https://mrkaran.dev/posts/travel-tailscale/", "domain": "mrkaran.dev", "votes": 1, "user": "nalgeon", "posted_at": "2024-04-14 11:12:45", "comments": 0, "source_title": "Travelling with Tailscale", "source_text": "Travelling with Tailscale | Karan Sharma\n\n## Karan Sharma\n\n# Travelling with Tailscale\n\n27 Mar, 2024\n\n6 minutes (1617 words)\n\nI have an upcoming trip to Europe, which I am quite excited about. I wanted to\nset up a Tailscale exit node to ensure that critical apps I depend on, such as\nbanking portals continue working from outside the country. Tailscale provides\na feature called \u201cExit nodes\u201d. These nodes can be setup to route all traffic\n(0.0.0.0/0, ::/0) through them.\n\nI deployed a tiny DigitalOcean droplet in BLR region and setup Tailscale as an\nexit node. The steps are quite simple and can be found here.\n\n    \n    \n    $ echo 'net.ipv4.ip_forward = 1' | sudo tee -a /etc/sysctl.d/99-tailscale.conf $ echo 'net.ipv6.conf.all.forwarding = 1' | sudo tee -a /etc/sysctl.d/99-tailscale.conf $ sudo sysctl -p /etc/sysctl.d/99-tailscale.conf $ sudo tailscale up --advertise-exit-node\n\nThe node is now advertised as an exit node, and we can confirm that from the\noutput of tailscale status:\n\n    \n    \n    $ sudo tailscale status 100.78.212.33 pop-os mr-karan@ linux - 100.75.180.88 homelab mr-karan@ linux - 100.100.191.57 iphone mr-karan@ iOS offline 100.123.189.14 karans-macbook-pro mr-karan@ macOS offline 100.104.67.7 lab mr-karan@ linux offline 100.108.220.87 tailscale-exit mr-karan@ linux active; exit node; direct 167.71.236.222:41641, tx 21540 rx 17356\n\nOn the client side, I was able to start Tailscale and configure it to send all\nthe traffic to the exit node with:\n\n    \n    \n    sudo tailscale up --exit-node=100.108.220.87\n\nWe can confirm that the traffic is going via the exit node by checking our\npublic IP from this device:\n\n    \n    \n    \u279c curl https://ipinfo.io { \"ip\": \"167.x.x.222\", \"city\": \"Doddaballapura\", \"region\": \"Karnataka\", \"country\": \"IN\", \"loc\": \"13.2257,77.5750\", \"org\": \"AS14061 DigitalOcean, LLC\", \"postal\": \"560100\", \"timezone\": \"Asia/Kolkata\", \"readme\": \"https://ipinfo.io/missingauth\" }\n\nHowever, I encountered a minor issue since I needed to bring my work laptop\nfor on-call duties, in case any critical production incidents required my\nattention during my travels. At my organization, we use Netbird as our VPN,\nwhich, like Tailscale, creates a P2P overlay network between different\ndevices.\n\nThe problem was that all 0.0.0.0 traffic was routed to the exit node, meaning\nthe internal traffic meant for Netbird to access internal sites on our private\nAWS VPC network was no longer routed via the Netbird interface.\n\nNetbird automatically propagates a bunch of IP routing rules when connected to\nthe system. These routes are to our internal AWS VPC infrastructure. For\nexample:\n\n    \n    \n    10.0.0.0/16 via 100.107.12.215 dev wt0\n\nHere, wt0 is the Netbird interface. So, for example, any IP like 10.0.1.100\nwill go via this interface. To verify this:\n\n    \n    \n    $ ip route get 10.0.1.100 10.0.1.100 dev wt0 src 100.107.12.215 uid 1000\n\nHowever, after connecting to the Tailscale exit node, this was no longer the\ncase. Now, even the private IP meant to be routed via Netbird was being routed\nthrough Tailscale:\n\n    \n    \n    $ ip route get 10.0.1.100 10.0.1.100 dev tailscale0 table 52 src 100.78.212.33 uid 1000\n\nAlthough Tailscale nodes allow for the selective whitelisting of CIDRs to\nroute only the designated network packets through them, my scenario was\ndifferent. I needed to selectively bypass certain CIDRs and route all other\ntraffic through the exit nodes. I came across a relevant GitHub issue, but\nunfortunately, it was closed due to limited demand.\n\nThis led me to dig deeper into understanding how Tailscale propagates IP\nroutes, to see if there was a way for me to add custom routes with a higher\npriority.\n\nInitially, I examined the IP routes for Tailscale. Typically, one can view the\nroute table list using ip route, which displays the routes in the default and\nmain tables. However, Tailscale uses routing table 52 for its routes, instead\nof the default or main table.\n\n    \n    \n    $ ip route show table 52 default dev tailscale0 100.75.180.88 dev tailscale0 ... others ... throw 127.0.0.0/8 192.168.29.0/24 dev tailscale0\n\nA few notes on the route table:\n\n  * default dev tailscale0 is the default route for this table. Traffic that doesn\u2019t match any other route in this table will be sent through the tailscale0 interface. This ensures that any traffic not destined for a more specific route will go through the Tailscale network.\n\n  * throw 127.0.0.0/8: This is a special route that tells the system to \u201cthrow\u201d away traffic destined for 127.0.0.0/8 (local host addresses) if it arrives at this table, effectively discarding it before it reaches the local routing table.\n\nWe can see the priority of these IP rules are evaluated using ip rule show:\n\n    \n    \n    \u279c ip rule show 0: from all lookup local 5210: from all fwmark 0x80000/0xff0000 lookup main 5230: from all fwmark 0x80000/0xff0000 lookup default 5250: from all fwmark 0x80000/0xff0000 unreachable 5270: from all lookup 52 32766: from all lookup main 32767: from all lookup default\n\nThis command lists all the current policy routing rules, including their\npriority (look for the pref or priority value). Each rule is associated with a\npriority, with lower numbers having higher priority.\n\nBy default, Linux uses three main routing tables:\n\n  * Local (priority 0)\n  * Main (priority 32766)\n  * Default (priority 32767)\n\nSince Netbird already propagates the IP routes in the main routing table, we\nonly need to add a higher priority rule to lookup in the main table before\nTailscale takes over.\n\n    \n    \n    $ sudo ip rule add to 10.0.0.0/16 pref 5000 lookup main\n\nNow, our ip rule looks like:\n\n    \n    \n    $ ip rule show 0: from all lookup local 5000: from all to 10.0.0.0/16 lookup main 5210: from all fwmark 0x80000/0xff0000 lookup main 5230: from all fwmark 0x80000/0xff0000 lookup default 5250: from all fwmark 0x80000/0xff0000 unreachable 5270: from all lookup 52 32766: from all lookup main 32767: from all lookup default\n\nTo confirm whether the packets for destination 10.0.0.0/16 get routed via wt0\ninstead of tailscale0, we can use the good ol\u2019 ip route get:\n\n    \n    \n    $ ip route get 10.0.1.100 10.0.1.100 dev wt0 src 100.107.12.215 uid 1000\n\nPerfect! This setup allows us to route all our public traffic via exit node\nand only the internal traffic meant for internal AWS VPCs get routed via\nNetbird VPN.\n\nSince, these rules are ephemeral and I wanted to add a bunch of similar\nnetwork routes, I created a small shell script to automate the process of\nadding/deleting rules:\n\n    \n    \n    #!/bin/bash # Function to add IP rules for specified CIDRs add() { echo \"Adding IP rules...\" sudo ip rule add to 10.0.0.0/16 pref 5000 lookup main # ... others ... } # Function to remove IP rules based on preference numbers remove() { echo \"Removing IP rules...\" sudo ip rule del pref 5000 # ... others .... } # Check the first argument to determine which function to call case $1 in add) add ;; remove) remove ;; *) echo \"Invalid argument: $1\" echo \"Usage: $0 add|remove\" exit 1 ;; esac\n\nFin!\n\nTags: #Linux #Devops #Networking\n\n\u00a9 2023 Karan Sharma\n\n", "frontpage": false}
