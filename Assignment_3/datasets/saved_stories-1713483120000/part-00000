{"aid": "40078623", "title": "Run Meta Llama 3 with an API", "url": "https://replicate.com/blog/run-llama-3-with-an-api", "domain": "replicate.com", "votes": 13, "user": "bfirsh", "posted_at": "2024-04-18 17:29:41", "comments": 0, "source_title": "Run Meta Llama 3 with an API", "source_text": "Run Meta Llama 3 with an API \u2013 Replicate\n\n# Run Meta Llama 3 with an API\n\nPosted today by @cbh123\n\nLlama 3 is the latest language model from Meta. It has state of the art\nperformance and a context window of 8000 tokens, double Llama 2's context\nwindow.\n\nWith Replicate, you can run Llama 3 in the cloud with one line of code.\n\n## Before you begin\n\nWe recommend signing in to Replicate before you get started. If you're new to\nReplicate, you can try us out for free. Once you have an account, you'll have\nan access token that will let you run Llama 3 in the cloud.\n\nJoin Replicate\n\n## Try Llama 3 in our API playground\n\nBefore you dive in, try Llama 3 in our API playground.\n\nTry tweaking the prompt and see how Llama 3 responds. Most models on Replicate\nhave an interactive API playground like this, available on the model page:\nhttps://replicate.com/meta/meta-llama-3-70b-instruct.\n\nThe API playground is a great way to get a feel for what a model can do, and\nprovides copyable code snippets in a variety of languages to help you get\nstarted.\n\nmeta/meta-llama-3-70b-instruct\n\nAPI Playground\n\n## Input\n\nRun this model in Node.js with one line of code:\n\nnpx create-replicate --model=meta/meta-llama-3-70b-instruct\n\nor set up a project from scratch\n\nRun meta/meta-llama-3-70b-instruct using Replicate\u2019s API. Check out the\nmodel's API reference for a detailed overview of the input/output schemas.\n\n    \n    \n    const input = { top_k: 50, top_p: 0.9, prompt: \"Work through this problem step by step:\\n\\nQ: Sarah has 7 llamas. Her friend gives her 3 more trucks of llamas. Each truck has 5 llamas. How many llamas does Sarah have in total?\", max_tokens: 512, min_tokens: 0, temperature: 0.6, prompt_template: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", presence_penalty: 1.15, frequency_penalty: 0.2 }; for await (const event of replicate.stream(\"meta/meta-llama-3-70b-instruct\", { input })) { process.stdout.write(event.toString()); };\n\nTo learn more, take a look at the guide on getting started with Node.js.\n\nRun meta/meta-llama-3-70b-instruct using Replicate\u2019s API. Check out the\nmodel's API reference for a detailed overview of the input/output schemas.\n\n    \n    \n    # The meta/meta-llama-3-70b-instruct model can stream output as it's running. for event in replicate.stream( \"meta/meta-llama-3-70b-instruct\", input={ \"top_k\": 50, \"top_p\": 0.9, \"prompt\": \"Work through this problem step by step:\\n\\nQ: Sarah has 7 llamas. Her friend gives her 3 more trucks of llamas. Each truck has 5 llamas. How many llamas does Sarah have in total?\", \"max_tokens\": 512, \"min_tokens\": 0, \"temperature\": 0.6, \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"presence_penalty\": 1.15, \"frequency_penalty\": 0.2 }, ): print(str(event), end=\"\")\n\nTo learn more, take a look at the guide on getting started with Python.\n\nRun meta/meta-llama-3-70b-instruct using Replicate\u2019s API. Check out the\nmodel's API reference for a detailed overview of the input/output schemas.\n\n    \n    \n    output = Replicate.run( \"meta/meta-llama-3-70b-instruct:fbfb20b472b2f3bdd101412a9f70a0ed4fc0ced78a77ff00970ee7a2383c575d\", input: %{ \"top_k\": 50, \"top_p\": 0.9, \"prompt\": \"Work through this problem step by step:\\n\\nQ: Sarah has 7 llamas. Her friend gives her 3 more trucks of llamas. Each truck has 5 llamas. How many llamas does Sarah have in total?\", \"max_tokens\": 512, \"min_tokens\": 0, \"temperature\": 0.6, \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"presence_penalty\": 1.15, \"frequency_penalty\": 0.2 } ) IO.inspect(output)\n\nTo learn more, take a look at the client library\u2019s readme.\n\nRun meta/meta-llama-3-70b-instruct using Replicate\u2019s API. Check out the\nmodel's API reference for a detailed overview of the input/output schemas.\n\n    \n    \n    curl -s -X POST \\ -H \"Authorization: Bearer $REPLICATE_API_TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d $'{ \"input\": { \"top_k\": 50, \"top_p\": 0.9, \"prompt\": \"Work through this problem step by step:\\\\n\\\\nQ: Sarah has 7 llamas. Her friend gives her 3 more trucks of llamas. Each truck has 5 llamas. How many llamas does Sarah have in total?\", \"max_tokens\": 512, \"min_tokens\": 0, \"temperature\": 0.6, \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\n\\\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\\\n\\\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\", \"presence_penalty\": 1.15, \"frequency_penalty\": 0.2 } }' \\ https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions\n\nTo learn more, take a look at Replicate\u2019s HTTP API reference docs.\n\nPull and run meta/meta-llama-3-70b-instruct using Cog (this will download the\nfull model and run it in your local environment):\n\n    \n    \n    cog predict r8.im/meta/meta-llama-3-70b-instruct@sha256:fbfb20b472b2f3bdd101412a9f70a0ed4fc0ced78a77ff00970ee7a2383c575d \\ -i 'top_k=50' \\ -i 'top_p=0.9' \\ -i $'prompt=\"Work through this problem step by step:\\\\n\\\\nQ: Sarah has 7 llamas. Her friend gives her 3 more trucks of llamas. Each truck has 5 llamas. How many llamas does Sarah have in total?\"' \\ -i 'max_tokens=512' \\ -i 'min_tokens=0' \\ -i 'temperature=0.6' \\ -i $'prompt_template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\n\\\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\\\n\\\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\"' \\ -i 'presence_penalty=1.15' \\ -i 'frequency_penalty=0.2'\n\nTo learn more, take a look at the Cog documentation.\n\nPull and run meta/meta-llama-3-70b-instruct using Docker (this will download\nthe full model and run it in your local environment):\n\n    \n    \n    docker run -d -p 5000:5000 r8.im/meta/meta-llama-3-70b-instruct@sha256:fbfb20b472b2f3bdd101412a9f70a0ed4fc0ced78a77ff00970ee7a2383c575d curl -s -X POST \\ -H \"Content-Type: application/json\" \\ -d $'{ \"input\": { \"top_k\": 50, \"top_p\": 0.9, \"prompt\": \"Work through this problem step by step:\\\\n\\\\nQ: Sarah has 7 llamas. Her friend gives her 3 more trucks of llamas. Each truck has 5 llamas. How many llamas does Sarah have in total?\", \"max_tokens\": 512, \"min_tokens\": 0, \"temperature\": 0.6, \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\n\\\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\\\n\\\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\", \"presence_penalty\": 1.15, \"frequency_penalty\": 0.2 } }' \\ http://localhost:5000/predictions\n\n## Output\n\nLet's break this problem down step by step. Step 1: Sarah already has 7\nllamas. Step 2: Her friend gives her 3 trucks of llamas. We need to find out\nhow many llamas are in these 3 trucks. Step 3: Each truck has 5 llamas, so we\nmultiply the number of trucks (3) by the number of llamas per truck (5): 3\ntrucks x 5 llamas/truck = 3 x 5 = 15 llamas Step 4: Sarah already had 7\nllamas, and now she gets 15 more llamas from her friend. To find the total\nnumber of llamas Sarah has, we add the two numbers together: 7 llamas (already\nhad) + 15 llamas (from her friend) = 22 llamas Therefore, Sarah has a total of\n22 llamas.\n\n    \n    \n    { \"completed_at\": \"2024-04-18T16:31:23Z\", \"created_at\": \"2024-04-18T16:31:19.530000Z\", \"error\": \"\", \"id\": \"7zr9g2asx9rgj0cey8698vm4d4\", \"input\": { \"top_p\": 0.9, \"prompt\": \"Work through this problem step by step:\\n\\nQ: Sarah has 7 llamas. Her friend gives her 3 more trucks of llamas. Each truck has 5 llamas. How many llamas does Sarah have in total?\", \"max_tokens\": 512, \"min_tokens\": 0, \"temperature\": 0.6, \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"presence_penalty\": 1.15, \"frequency_penalty\": 0.2 }, \"logs\": \"\", \"metrics\": { \"total_time\": 3.47, \"input_token_count\": 54, \"tokens_per_second\": 41.72998441835564, \"output_token_count\": 166, \"predict_time\": 4.045511 }, \"output\": [ \"Let\", \"'s\", \" break\", \" this\", \" problem\", \" down\", \" step\", \" by\", \" step\", \".\\n\\n\", \"Step\", \" \", \"1\", \":\", \" Sarah\", \" already\", \" has\", \" \", \"7\", \" ll\", \"amas\", \".\\n\\n\", \"Step\", \" \", \"2\", \":\", \" Her\", \" friend\", \" gives\", \" her\", \" \", \"3\", \" trucks\", \" of\", \" ll\", \"amas\", \".\", \" We\", \" need\", \" to\", \" find\", \" out\", \" how\", \" many\", \" ll\", \"amas\", \" are\", \" in\", \" these\", \" \", \"3\", \" trucks\", \".\\n\\n\", \"Step\", \" \", \"3\", \":\", \" Each\", \" truck\", \" has\", \" \", \"5\", \" ll\", \"amas\", \",\", \" so\", \" we\", \" multiply\", \" the\", \" number\", \" of\", \" trucks\", \" (\", \"3\", \")\", \" by\", \" the\", \" number\", \" of\", \" ll\", \"amas\", \" per\", \" truck\", \" (\", \"5\", \"):\\n\\n\", \"3\", \" trucks\", \" x\", \" \", \"5\", \" ll\", \"amas\", \"/tr\", \"uck\", \" =\", \" \", \"3\", \" x\", \" \", \"5\", \" =\", \" \", \"15\", \" ll\", \"amas\", \"\\n\\n\", \"Step\", \" \", \"4\", \":\", \" Sarah\", \" already\", \" had\", \" \", \"7\", \" ll\", \"amas\", \",\", \" and\", \" now\", \" she\", \" gets\", \" \", \"15\", \" more\", \" ll\", \"amas\", \" from\", \" her\", \" friend\", \".\", \" To\", \" find\", \" the\", \" total\", \" number\", \" of\", \" ll\", \"amas\", \" Sarah\", \" has\", \",\", \" we\", \" add\", \" the\", \" two\", \" numbers\", \" together\", \":\\n\\n\", \"7\", \" ll\", \"amas\", \" (\", \"already\", \" had\", \")\", \" +\", \" \", \"15\", \" ll\", \"amas\", \" (\", \"from\", \" her\", \" friend\", \")\", \" =\", \" \", \"22\", \" ll\", \"amas\", \"\\n\\n\", \"Therefore\", \",\", \" Sarah\", \" has\", \" a\", \" total\", \" of\", \" \", \"22\", \" ll\", \"amas\", \".\", \"\" ], \"started_at\": \"2024-04-18T16:31:19Z\", \"status\": \"succeeded\", \"urls\": { \"stream\": \"https://streaming-api.svc.us.c.replicate.net/v1/streams/cvg64spwkdzlpdltjkmv6jotmwn2cq5btgxirmsbejvsxx7xp2la\", \"get\": \"https://api.replicate.com/v1/predictions/7zr9g2asx9rgj0cey8698vm4d4\", \"cancel\": \"https://api.replicate.com/v1/predictions/7zr9g2asx9rgj0cey8698vm4d4/cancel\" }, \"version\": \"fbfb20b472b2f3bdd101412a9f70a0ed4fc0ced78a77ff00970ee7a2383c575d\" }\n\nLet's break this problem down step by step.\\n \\n Step 1: Sarah already has 7\nllamas.\\n \\n Step 2: Her friend gives her 3 trucks of llamas. We need to find\nout how many llamas are in these 3 trucks.\\n \\n Step 3: Each truck has 5\nllamas, so we multiply the number of trucks (3) by the number of llamas per\ntruck (5):\\n \\n 3 trucks x 5 llamas/truck = 3 x 5 = 15 llamas\\n \\n Step 4:\nSarah already had 7 llamas, and now she gets 15 more llamas from her friend.\nTo find the total number of llamas Sarah has, we add the two numbers\ntogether:\\n \\n 7 llamas (already had) + 15 llamas (from her friend) = 22\nllamas\\n \\n Therefore, Sarah has a total of 22 llamas.\n\nInput tokens\n\n54\n\nOutput tokens\n\n160\n\nTokens per second\n\n41.73 tokens / second\n\nTime to first token\n\n\u2013\n\nGenerated in 4.0 seconds\n\n## Running Llama 3 with JavaScript\n\nYou can run Llama 3 with our official JavaScript client:\n\nRun meta/meta-llama-3-70b-instruct using Replicate\u2019s API. Check out the\nmodel's API reference for a detailed overview of the input/output schemas.\n\n    \n    \n    const input = { prompt: \"Can you write a poem about open source machine learning?\" }; for await (const event of replicate.stream(\"meta/meta-llama-3-70b-instruct\", { input })) { process.stdout.write(event.toString()); };\n\nTo learn more, take a look at the guide on getting started with Node.js.\n\n## Running Llama 3 with Python\n\nYou can run Llama 3 with our official Python client:\n\nRun meta/meta-llama-3-70b-instruct using Replicate\u2019s API. Check out the\nmodel's API reference for a detailed overview of the input/output schemas.\n\n    \n    \n    # The meta/meta-llama-3-70b-instruct model can stream output as it's running. for event in replicate.stream( \"meta/meta-llama-3-70b-instruct\", input={ \"prompt\": \"Can you write a poem about open source machine learning?\" }, ): print(str(event), end=\"\")\n\nTo learn more, take a look at the guide on getting started with Python.\n\n## Running Llama 3 with cURL\n\nYour can call the HTTP API directly with tools like cURL:\n\nRun meta/meta-llama-3-70b-instruct using Replicate\u2019s API. Check out the\nmodel's API reference for a detailed overview of the input/output schemas.\n\n    \n    \n    curl -s -X POST \\ -H \"Authorization: Bearer $REPLICATE_API_TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d $'{ \"input\": { \"prompt\": \"Can you write a poem about open source machine learning?\" } }' \\ https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions\n\nTo learn more, take a look at Replicate\u2019s HTTP API reference docs.\n\nYou can also run Llama using other Replicate client libraries for Golang,\nSwift, Elixir, and others.\n\n## Choosing which model to use\n\nThere are four variant Llama 3 models on Replicate, each with their own\nstrengths. Llama 3 comes in two parameter sizes: 70 billion and 8 billion,\nwith both base and chat tuned models.\n\n  * meta/meta-llama-3-70b-instruct: 70 billion parameter model fine-tuned on chat completions. If you want to build a chat bot with the best accuracy, this is the one to use.\n  * meta/meta-llama-3-8b-instruct: 8 billion parameter model fine-tuned on chat completions. Use this if you\u2019re building a chat bot and would prefer it to be faster and cheaper at the expense of accuracy.\n  * meta/meta-llama-3-70b: 70 billion parameter base model. This is the 70 billion parameter model before the instruction tuning on chat completions.\n  * meta/meta-llama-3-8b: 8 billion parameter base model. This is the 8 billion parameter model before the instruction tuning on chat completions.\n\n## Example chat app\n\nIf you want a place to start, we\u2019ve built a demo chat app in Next.js that can\nbe deployed on Vercel:\n\nTry it out on llama3.replicate.dev. Take a look at the GitHub README to learn\nhow to customize and deploy it.\n\n## Keep up to speed\n\n  * Follow us on Twitter X to get the latest from the Llamaverse.\n  * Hop in our Discord to talk Llama.\n\nHappy hacking! \ud83e\udd99\n\nOn this page\n\n  * Try Llama 3 in our API playground\n  * Running Llama 3 with JavaScript\n  * Running Llama 3 with Python\n  * Running Llama 3 with cURL\n  * Choosing which model to use\n  * Example chat app\n  * Keep up to speed\n\nReplicate\n\nAbout Guides Terms Privacy Status GitHub X Discord Support\n\nCopy\n\nShow\n\nCopy\n\nCopy\n\nShow run API\n\nCopy\n\nCopy\n\nShow\n\nCopy\n\nCopy\n\nShow run API\n\nCopy\n\nCopy\n\nShow\n\nCopy\n\nCopy\n\nCopy\n\nShow\n\nCopy\n\nCopy\n\nCopy\n\nCopy\n\nCopy\n\nCopy\n\n# Logs (7zr9g2asx9rgj0cey8698vm4d4)\n\nSucceeded\n\nCopy\n\nShow\n\nCopy\n\nCopy\n\nShow run API\n\nCopy\n\nCopy\n\nShow\n\nCopy\n\nCopy\n\nShow run API\n\nCopy\n\nShow\n\nCopy\n\nCopy\n\n", "frontpage": true}
