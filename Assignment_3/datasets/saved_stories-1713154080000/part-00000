{"aid": "40034900", "title": "Kernel Density Estimation: A Primer", "url": "https://vvanirudh.github.io/blog/kde/", "domain": "vvanirudh.github.io", "votes": 3, "user": "vvanirudh", "posted_at": "2024-04-14 22:07:13", "comments": 0, "source_title": "A Gentle Primer for Nonparametric Density Estimation: Kernel Density Estimation | Anirudh Vemula", "source_text": "A Gentle Primer for Nonparametric Density Estimation: Kernel Density Estimation | Anirudh Vemula\n\n# A Gentle Primer for Nonparametric Density Estimation: Kernel Density\nEstimation\n\n## 2024/04/13\n\nReading time: 7 minutes.\n\nIn my previous post, we discussed a neat intuition for nonparametric density\nestimation and introduced a nonparametric method, histograms. This post\ndescribes another (very popular) density estimation method called Kernel\nDensity Estimation (KDE).\n\n### Revisit Intuition\n\nLet\u2019s revisit the intuition that we developed for nonparametric density\nestimation. Given a region R\u2282RDR\u2282RD of volume VV, and that contains KK points\nfrom a sampled dataset of size NN, we can estimate p(x)p(x) for any point\nx\u2208Rx\u2208R as\n\np(x)=KNVp(x)=KNV\n\nWe have shown that this intuition lends us two strategies for estimating\np(x)p(x):\n\n  1. We can fix VV and determine KK from data\n  2. We can fix KK and determine VV from data\n\nHistograms, as described in the previous post, use strategy 1 where we fix the\nbin volume and determine how many points fall within each bin from the data.\nKernel Density Estimation, as we will see, follows the same strategy with two\nmajor differences:\n\n  1. Regions RR (or the bins) are centered on data points\n  2. Use of kernel functions to represent density in region RR\n\n### Kernel Density Estimation\n\nExtending the binning idea of histograms, let\u2019s take the region RR to be a\nsmall hypercube of side \u0394\u0394 centered at any x\u2208RDx\u2208RD. Note that in histograms,\nthe regions were always centered at the discretization of RDRD space.\n\nUsing the same 1-D example as the previous post we can visualize how this\nhypercube would look like at an arbitrary xx\n\n    \n    \n    delta = 0.1 x = 1.0 ax.stem([x - delta/2, x + delta/2], [3, 3], markerfmt=' ', linefmt='k:', basefmt=' ')\n\nLooks exactly like a histogram bin, except it is defined at any arbitrary xx\n(and not the discretization points, like in histogram.) To count the number of\npoints (KK) falling within RR, we can define the function k(u)k(u),\n\nk(u)={1,if |ui|\u226412,\u2200i=1,\u22ef,D0,otherwise\n\nWe can now count the number of points K within R by noting that the quantity\nk(x\u2212xn\u0394) is 1 if data point xn is inside the hypercube R and 0 otherwise.\n\nLet\u2019s plot k(x\u2212xn\u0394) for all samples xn in the above example,\n\n    \n    \n    k = lambda u: 1 if abs(u) <= 0.5 else 0 sorted_samples = sorted(samples) plt.plot( sorted_samples, [k((x_n - x) / delta) for x_n in sorted_samples], label='k((x - x_n)/delta)', color='red' )\n\nLooks like a box, doesn\u2019t it? Now we can simply count the number of points K\ninside this box to estimate the density at x. Thus, we can estimate K as\n\nK=N\u2211i=1k(x\u2212xn\u0394)\n\nSubstituting the above equation into the nonparametric density estimation\nintuition equation, we get\n\np(x)=1N\u0394DN\u2211i=1k(x\u2212xn\u0394)\n\nwhere we used the fact that the volume of the D-dimensional hypercube of side\n\u0394 is V=\u0394D. Let\u2019s plot p(x) for our 1-D example,\n\n    \n    \n    K = lambda x: sum([k((x - x_n) / delta) for x_n in samples]) p = lambda x: K(x) / (N * delta) plt.plot(xaxis, list(map(p, xaxis)), label=\"p(x) estimate\", color='red')\n\nNot a bad estimate! Looks a little noisy but approximates the true density\nquite well.\n\nHowever, the interpretation of placing a region at every point x and counting\nnumber of points in the region sounds cumbersome. We can use a symmetry\nargument by noting that our function k is symmetric (i.e. k(u)=k(\u2212u)) to\nreinterpret our density estimate, not as a single hypercube centered at x but\nas the sum over N cubes centered on each of the N data points xn!\n\nThus, so far, the only difference from histograms was that the bin centers are\non the data points themselves! As a result of this, the bins can overlap and\nwe compute the density estimate at x as the sum of density estimates from all\noverlapping bins. This eliminates the need to discretize the input space, as\nneeded in histograms, and gives us a more smooth (and less discrete) density\nestimate than histograms.\n\n#### Kernel Functions\n\nThe density estimate from the above plot still looks \u201cchoppy\u201d (its actually\ndiscontinuous and they would be visible at inifinitesimally small resolution.)\nThis is because, the density estimate \u201cjumps\u201d at the hypercube boundaries as\nthe \u201cinfluence\u201d of a data point on the estimate begins/terminates.\n\nWe can fix this by changing the function k(u) that we used. k(u) is an example\nof a kernel function, specifically a box kernel. We can choose any kernel\nfunction k(u) to ensure that the resulting estimate is a valid probability\ndistribution as long as,\n\nk(u)\u22650\n\n\u222bk(u)du=1\n\nTo avoid the discontinuities, we can choose a smoother kernel function that\nhas \u201csoft\u201d boundaries, such as a Gaussian kernel\n\nk(u)=1(2\u03c0)D2exp(\u2212||u||22)\n\nLet\u2019s plot k(x\u2212xn\u0394) for all samples xn in our 1-D example at a specific x and\nvisualize it\n\n    \n    \n    x = 1.0 gaussian_k = lambda u: (1/(2*math.pi)**0.5) * math.exp(-0.5 * u**2) plt.plot( sorted_samples, [gaussian_k((x_n - x) / delta) for x_n in sorted_samples], label='k((x - x_n)/delta)', color='red' )\n\nLooks like a gaussian PDF as expected. Let's see how our estimate of p(x)\nlooks like if we used a Gaussian kernel instead of a box kernel.\n\n    \n    \n    K = lambda x: sum([gaussian_k((x - x_n) / delta) for x_n in samples]) p = lambda x: K(x) / (N * delta) plt.plot(xaxis, list(map(p, xaxis)), label=\"p(x) estimate\", color='red')\n\nVoila! We have a smooth estimate of the density function that approximates the\nunderlying density reasonably well. Thus, our density model is obtained by\nplacing a Gaussian over each of the N data points, adding up their\ncontributions over the entire dataset, and dividing by N to get a normalized\ndensity estimate.\n\nWe can make the approximation more accurate by tweaking \u0394, which in the\ncontext of a Gaussian kernel is the variance of the kernel. Using \u0394=0.05, we\nget the following plot which already looks very much like the true density\n\nWe see that similar to histograms, \u0394 plays the role of a smoothing parameter\nand we obtain over-smoothing for large values of \u0394 and sensitivity to noise in\nthe dataset for small values. Thus, the parameter \u0394 controls the complexity of\nthe density model. As an example, here\u2019s the estimate with \u0394=0.01 that shows\nsensitivity to noise.\n\nAnd that\u2019s Kernel Density Estimation. The only differences from histogram\nwere:\n\n  1. Using regions centered on data points\n  2. Flexibility to use any kernel function to represent the density in a region\n\nInterested readers can check out this super cool interactive blog post that\nallows you to play with different kernel functions and different values of \u0394\nto visualize resulting density estimates and the influence of data points on\nthe estimate.\n\n### Why (not) Kernel Density Estimation?\n\nSimilar to histograms, KDE is a nonparametric method that does not compute any\nparameters from data. While our post has explored two kernel functions: box\nand Gaussian kernel, there are a lot of kernel functions, and using them gives\nuser a wide range of modeling choices that is not available with histograms.\n\nThe greatest merit of KDE over histograms is that it scales well to higher\ndimensional density estimation, as the number of \u201cregions\u201d do not scale\nexponentially in input dimension, unlike histograms. Furthermore, there is\nvirtually no computation involved in the \u201ctraining\u201d phase for KDE as we simply\nrequire the training data to be stored to be used at inference time.\n\nHowever, this means that the computational complexity (and the number of\n\u201cregions\u201d) of KDE scales linearly with the data size, which makes it\nprohibitive to use for large datasets. This is compounded in high dimensions,\nwhere we require a large number of data points to reliably estimate the\ndensity, making KDE computationally more expensive as a result.\n\nIn the next post, we will finally introduce a nonparametric method that\nfollows strategy 2, nearest neighbors density estimation, where we fix K and\nestimate V from data.\n\nThe source code for this post can be found here.\n\n\u00a9 Anirudh Vemula 2024 | Github | LinkedIn | Google Scholar | Email\n\n", "frontpage": false}
