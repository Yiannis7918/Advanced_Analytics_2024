{"aid": "40076231", "title": "MLOps vs. Eng: Misaligned Incentives and Failure to Launch?", "url": "https://www.heavybit.com/library/article/machine-learning-engineering-ai-incentives", "domain": "heavybit.com", "votes": 1, "user": "chuckhend", "posted_at": "2024-04-18 13:50:22", "comments": 0, "source_title": "MLOps vs. Eng: Misaligned Incentives and Failure to Launch? | Heavybit", "source_text": "MLOps vs. Eng: Misaligned Incentives and Failure to Launch? | Heavybit\n\nApr 23 \ud83d\uddd3 DevGuild: AI Summit is returning\n\nRequest to Join\n\n  * About\n\n    * Team\n    * FAQ\n    * News\n    * Apply\n  * Portfolio\n\n    * Spotlights\n    * Jobs\n  * Community\n\n    * DevGuild\n  * Library\n\n    * Browse\n    * Podcast Network\n    * Reports\n    * DevToolsDigest\n\n  1. Library\n  2. MLOps vs. Eng: Misaligned Incentives and Failure to Launch?\n\n# MLOps vs. Eng: Misaligned Incentives and Failure to Launch?\n\n  * MLOps\n  * Generative AI\n  * AI\n  * Engineering Management\n  * Change Management\n  * Key Performance Indicator / KPI\n  * Machine Learning\n  * DevOps\n\nLight Mode\n\nFailure to Launch: The Challenges of Getting ML Models into Prod\n\nWhy Do So Few ML Projects Make it to Production?\n\nApproaching MLOps as a Learning Journey with Alexandra Johnson\n\nDiscussion: How MLOps, Dev, and Execs Map Models to Value\n\nML Experts With No Business Background vs. Business Leads With No ML\nBackground\n\nContextualizing ML Projects in Terms of Business Timeframes and Metrics\n\nMore Resources:\n\nImprove Communication (But Ship Something!) with Andrew Fong\n\nDiscussion: Balancing the Black Box vs. Shipped Products\n\nIs Change Management The Key Leadership Skill To Adopt AI Successfully?\n\nImplementing AI at the Pace of Your Product (and Buyers)\n\nMore Resources:\n\nMake the Metrics Make Sense with Stefan Krawczyk\n\nDiscussion: Put Change in the Roadmap (and Business Plan)\n\nGetting a Valuable MVP Running and Iterating\n\nClosing the Gap Between Executive Expectations and Ops Realities\n\nMore Resources:\n\nWill MLOps Go the Way of DevOps? with Adam Zimman\n\nDiscussion: Applying Software Dev Lessons to the Future\n\nWhy Experimentation Is Important Enough to Bake Into Business Incentives\n\nWhy ML in Production May Be Having Its \u201cDevOps\u201d Moment\n\nHow Operationalizing Could Lead to Common Ground\n\nMore Resources:\n\nConclusion\n\n  * Andrew ParkEditorial Lead Heavybit\n\nAPR 15, 2024\n\n37 min\n\n## Failure to Launch: The Challenges of Getting ML Models into Prod\n\nMachine learning is a subset of AI\u2013the practice of using complex algorithms to\nmodel human learning and cognition. The ML we typically speak of today focuses\non using models trained via supervised learning (on specific, known-quantity\ndata sets for input and output) and unsupervised learning (finding hidden\npatterns or insights in other data sets). After 20-30 years, you might expect\neveryone to have figured out how to get ML projects into production.\nUnfortunately, that isn\u2019t the case. This article will cover:\n\n  * The ongoing \u201cfailure to launch\u201d problem with ML models in production\n  * Misaligned incentives, skill sets, and cultural expectations between data science, engineering, and management teams\n  * Perspectives from veterans from both the ML and software development disciplines\n  * Specific recommendations for teams to build alignment around ML projects to not only get them into production, but drive results.\n\nHear more diverse perspectives on getting AI into production at the DevGuild\nAI Summit II event.\n\n## Why Do So Few ML Projects Make it to Production?\n\nStudies suggest that as few as 10% to 20% of all machine learning projects\never make it to production. More disturbingly, additional research suggests\nthat 91% of models degrade in performance over time\u2013so the few models that do\ngo live aren\u2019t \u201cset it and forget it\u201d projects. Ongoing performance issues for\nML in production means ML operations teams need to regularly perform triage.\n\nThere\u2019s a potential challenge happening at the organizational level\u2013that data\nscience teams and engineering teams may simply be too far out of sync with\nregards to their skill sets, incentives, day-to-day priorities, and culture.\nData scientists, in the interest of training models to be as robust as\npossible, anticipate occasional failures as learning opportunities to better\ntune future performance. Engineering teams build for consistent performance,\nand may be gun-shy about investing many cycles into ML systems that\nfundamentally aren\u2019t deterministic, and may fail for completely unanticipated\nreasons, sometimes catastrophically. Meanwhile, management teams that have\nalready invested heavily in expensive AI projects are increasingly feeling\npressure to deliver ROI.\n\nIf you find the state of affairs surprising, it\u2019s not just you. We at Heavybit\nfully expected machine learning to be the next great driver of change in\noperations. But MLOps seems to have fallen short of expectations. Why? And\nwhat can we\u2013as stakeholders, investors or founders\u2013do to help data science and\nengineering teams actually get AI projects into production and reach their\nfull potential? Below is a set of expert interviews on how to bridge the gap\nbetween data science and engineering\u2013and how to attack MLOps\u2019 failure-to-\nlaunch problem.\n\n## Approaching MLOps as a Learning Journey with Alexandra Johnson\n\nAlexandra Johnson is founder and CEO of Rubber Ducky Labs, building\noperational analytics for recommender systems. She has held product leadership\nand engineering positions at startups for more than a decade, including four\nyears at early MLOps startup SigOpt (acquired by Intel).\n\n  * Resolving Misalignments: Data science and engineering teams need better infrastructure, better processes, and better mutual understanding of each other\u2019s daily priorities\u2013and they need to avoid the pitfalls of being shortsighted.\n  * Starting from Business Goals and Value: Rather than focusing entirely on arcane data science or engineering KPIs, organizations may be better served by also including business value and successful implementation of business-focused use cases to benchmark the effectiveness of their ML program.\n  * Managing Executive Expectations: Teams can collaborate better with C-suite executives by using universally recognized business metrics such as specific timelines and budgeting to frame the need for ongoing learning, adaptation, and even setting expectations for model decay (and the occasional failure).\n\n## Discussion: How MLOps, Dev, and Execs Map Models to Value\n\nIn addition to offering the above suggestions, Johnson reflects on how, with\nrecent developments in the still-growing MLOps space, working with machine\nlearning models may be going through its own DevOps-like growing pains\u2013a\nsituation where not every party is keyed into the best possible technology or\nwhy the tech would be worth the investment. \u201cWhen we talk about the\nfundamental misalignment between what MLOps is trying to do versus what the\nbusiness is trying to do\u2013you can compare that to investments in\ninfrastructure. Which by now, is basically second nature. Obviously, if there\nwere a company right now building a static-hosted website, and it wanted to\nrun it on its own servers, that\u2019d be a very different conversation than it\nwould\u2019ve been 20 years ago. Today you would say, \u2018Why aren't you using a cloud\nhosting service?\u2019 We\u2019ve already built up the muscle for making those\ninvestments in DevOps and infrastructure when it comes to general software\nengineering. But we\u2019re just starting to learn how to build that muscle on the\nmachine learning side.\u201d\n\nOn the topic of reconciling different expectations with management teams,\nJohnson identifies a fundamental rift between business, dev, and ML teams,\nwho, in many of today\u2019s organizations, may not be speaking the same language.\nIn a worst-case scenario, teams stop trusting each other and develop a\nshortsighted focus on immediately measurable results\u2013when they should\npotentially be seeking a neutral third party that understands each side\u2019s\nobjectives. \u201cI was just talking to someone who works for a very large company,\nat which their business stakeholders had basically lost trust in their\ntechnical team.\u201d\n\n\u201cWhen that trust is gone, executives tend to start making very pointed\nrequests to the technical team. They start asking for very short-term value.\nAnd then the technical team is in a panic trying to deliver exactly what they\nwere asked for. And there isn't anyone in the organization who can unpack the\nsituation and point out the consequences of optimizing for short-term value.\nIdeally, you\u2019d be able to bring in people to the organization who can\nunderstand both sides and who can explain to executives that while machine\nlearning is very powerful, if you optimize for short-term value too much, you\ncan get into trouble\u2013and here are the ways that can happen. Avoid deep\nalgorithmic discussion and put it in plain-and-simple business terms.\u201d\n\n### ML Experts With No Business Background vs. Business Leads With No ML\nBackground\n\nJohnson suggests that the relative newness of AI/ML to business has led to a\nfamiliar disparity in standing and influence in the organization: Relatively\njunior technical people trying to explain arcane technical complexities to\ntenured business executives that don\u2019t share domain expertise. \u201cI think the\nindustry is so new that you have people at the business layer who don't\nnecessarily have an intuitive or deep enough understanding about how ML\nprojects are executed to understand exactly what they're asking for. And\nyou've got quite a few people in ML who haven't been in the industry as long,\nor have the seniority or comparable experience to their counterparts in the\nbusiness layer. I think we need to see more investment in bringing up some of\nthe technical folks to the executive level; and it wouldn\u2019t hurt to also take\nthe \u2018business people\u2019 and give them an ML bootcamp. Not even about how the\nalgorithms work, but about how ML projects are executed.\u201d\n\nJohnson also suggests that one aspect of the chasm between business goals and\ndata science might be the \u2018science\u2019 part. \u201cI was having a conversation with\nsomeone on this topic who kept interjecting the phrase, \u2018as a scientist.\u2019 This\nperson would say, \u2018Well, as a scientist, I see this.\u2019 But science is not a\ndiscipline in which we always expect concrete results. Experimentation is not\na discipline in which we always expect to succeed. So you have some periods of\nthings like data collection, development, experimentation, and some periods of\nfailure. And that can be really scary for a business to bring in something\nthat seems \u2018risky\u2019 in this way, even in recommender systems. You're taking\nthis area\u2013where you used to have full control over the products that you're\nshowing your users\u2013and turning it over to a system that you don't understand.\nThat system could potentially give you much better results, but it could also\npotentially give you much worse results.\u201d\n\n### Contextualizing ML Projects in Terms of Business Timeframes and Metrics\n\n\u201cSo, I think that one thing that ties the experimentation of ML back to\nbusiness goals is timelines\u2013which can be long. It\u2019s important to understand,\nfrom a project management perspective, what's going on with the timeline of\nthe project, and whether it\u2019s actually on track, because these projects do\nalso get off track.\u201d Johnson points out a particularly noticeable similarity\nto the world of software development. \u201cAs a software developer, it's very hard\nto estimate how long it's going to take to do something. In ML, it's even\nharder. So from a project management perspective, folks need to get started by\nbeing able ask, \u2018Hey, is this project even on track for what we originally\nwanted to learn?\u2019\u201d\n\n\u201cAnd once we can verify that, yes, it is on track, and yes, we are launching\nthings to production, it\u2019s important to understand how to ask the important\nquestions: Are we seeing the performance that we want to see? Are we learning\nwhat we wanted to learn? What are the risks of poor performance of our model?\nAnd then, once you can assess some of those things, you can, from an executive\nperspective, make the decisions you need to make. For example, when working on\na recommendation engine project, if someone can very clearly say, \u2018We are\nbehind on this project because we have a risk of showing poor recommendations\nto users and this could damage our brand,\u2019 then executives can make a better\ndecision. A much clearer insight than arcane metrics such as AUC being 80% or\nNDCG being something else.\u201d\n\n## More Resources:\n\n  * Video: Understanding Business Analytics with Sean Byrne\n  * Video: Startup Planning and Prioritization with Craig Kerstiens\n\n## Improve Communication (But Ship Something!) with Andrew Fong\n\nAndrew Fong is CEO and cofounder of Prodvana, a platform that streamlines and\naccelerates software delivery. As a developer and engineering lead with 25\nyears of experience shipping products and leading software teams, he has\nserved tours of duty at Vise, Dropbox, and YouTube. His takeaways include:\n\n  * The Ideal State is Better Communication Driven By Mutual Interest: In a perfect world, devs, data scientists, and execs all make the effort to improve channels of communication because they\u2019re genuinely curious about each others\u2019 goals.\n  * The Search for Common Ground Starts From the Top Down: It\u2019s incumbent on leadership to improve understanding between themselves, devs, and data scientists.\n  * The Challenge Is Balancing Non-Deterministic AI and Business Need for Predictable Outcomes: Ultimately, dev teams need to deliver a product or service people consistently want to buy\u2013which means maximizing impact and minimizing risk.\n\n## Discussion: Balancing the Black Box vs. Shipped Products\n\nBeyond the above suggestions, Fong suggests that in order to resolve\nmisaligned incentives, different groups are going to have to get past the\n\u201cthree different people telling three different stories\u201d phase. \u201cFor example,\nhistorically, the DevOps side of the world will say \u2018No one understands\nreliability.\u2019 Well, that's still going to be the problem if you're now\nintroducing something a little bit more non-deterministic, like an AI model,\non top of that.\u201d\n\nFong draws the analogy to the initial resistance to the DevOps movement\u2013which\nwas as arguably much of a cultural conflict as it was a technical one. \u201cI\ndon't think this really has anything to do with \u2018AI versus not AI.\u2019 This is\njust about being able to communicate what outcomes you\u2019re looking for, from\nall three parties. I don't think it's all that different from what we saw (or\nstill see) from the site reliability community with regard to software\nengineering versus executives. Maybe the tools need to be slightly different\nfor the personas, but the actual organizational incentives, I think, are\nexactly the same.\u201d\n\n\u201cI would hope that no matter how much AI research goes into your product, you\nend up with a predictable business outcome. Whatever AI model or thing you\u2019re\nbuilding still needs to generate something that somebody wants to buy on a\ncontinuous basis, so that had better be predictable on some level. We can\nabsolutely talk about the efficacy of a model or how you test it, and all of\nthose things. But to me, all of that goes back to saying, \u2018OK, if we do this\ncorrectly, it's still going to produce something predictable.\u2019 Predictably\nwhat someone wants to buy. And I think in a conversation about incentives not\nbeing aligned, that might be the point that\u2019s getting lost.\u201d\n\n### Is Change Management The Key Leadership Skill To Adopt AI Successfully?\n\nFong suggests that change management might be the biggest challenge standing\nbetween misaligned leadership, engineers, and data scientists and that ideal\nfuture where everyone is finally on the same page. \u201cExecutives that can rise\nto the challenge of change management\u2013and people within an organization that\ncan handle change management and help teams work through it? They're probably\nthe ones that are \u2018at a premium\u2019 right now. I think the ability to build\nsoftware is not that uncommon. I think the ability to fully understand the AI\nside may be more uncommon, but a researcher who can't help manage change\nwithin an organization over the course of a year is probably way less valuable\nthan one who can.\u201d\n\n\u201cFor my team, this stuff has admittedly been fascinating. I think it was\nshortly after the release of GPT-4 in Spring of 2023, and our startup was\nrelatively young at the time. We had to think carefully about where we would\nplace our attention and resources. We were gaining traction in the market and\nonboarding customers, but we also had some serious discussions about whether,\nand to what extent, we would consider working these new AI products into what\nwe do. As a startup that focuses on deployments, we had to ask ourselves from\na first principles perspective: Will people accept non-deterministic\ndeployments? Probably not.\u201d\n\n### Implementing AI at the Pace of Your Product (and Buyers)\n\n\u201cSome months later, my co-founder and I started spending some time looking\nmore seriously at what was available in the world of generative AI and what\nmight be possible. We were doing a lot of prototyping\u2013really just to see what\nwas doable at the chat prompt level. We realized that what was possible could\nbe way bigger than what we thought, so we decided to spend some time thinking\nthrough what we could do with these types of systems.\u201d\n\n\u201cAnd we found ourselves asking ourselves the same questions many other\ntechnical teams have probably asked themselves: Where does this fit in the\nproduct? What\u2019s the right use case for it? We attempted to work on something\nreally small initially\u2013generating release labels correctly from commit\nmessages. As we worked through it, we realized that we were going to have to\ndo this\u2013it became a must-do project. But we also knew we weren\u2019t necessarily\nAI experts. So we decided to work at a pace that would make sense for our\nproduct space and for our buyer.\u201d\n\nFong reflects on his own professional history and empathizes with developers\u2019\nconcerns about risk. \u201cI have spent 25+ years in infrastructure. And I think\nthat I know infrastructure people fairly well, and they're usually pretty\nrisk-averse to things that are not deterministic. We knew we couldn\u2019t just\ndrop [generative AI] into the product and \u2018go with it.\u2019 We had to build an\nactual thoughtful strategy around what buyers might actually want from such a\nthing. We had to ask ourselves: Where will buyers accept [generative AI]?\nWhere can it create leverage for them? We had to think beyond AI being cool\ntech and interrogate where we could create real value for our customers.\u201d\n\n## More Resources:\n\n  * Article: Empowering Your Team for Growth by Kiersten Gaffney\n  * Video: Executive Communication with Michael Dearing\n\n## Make the Metrics Make Sense with Stefan Krawczyk\n\nStefan Krawczyk is the CEO and founder of DAGWorks, an open-core platform that\nprovides observability, lineage, and catalog for ML and LLM workflows from\ndevelopment to production. A machine learning & data science veteran with 15+\nyears of industry experience, he has held engineering and research positions\nat Stitch Fix, Nextdoor, LinkedIn, and Stanford University. Stefan suggests:\n\n  * Designing for Change: The LLMOps space, and AI in general, is evolving so quickly that it doesn\u2019t make sense to build entire businesses around industry conditions that might not be around in six months.\n  * Putting AI into Production Means an Evolving SDLC: It will definitely behoove teams to take a systems thinking approach toward implementing AI\u2013and understanding the downstream effects of any tweaks or changes you implement.\n  * Drawing Up Metrics That Acknowledge the Need for Experimentation: Businesses need metrics to understand what\u2019s working and what isn\u2019t, and will be best served by also taking into account the need for iterative experimentation.\n\n## Discussion: Put Change in the Roadmap (and Business Plan)\n\nKrawczyk expands on the above points by noting that the AI and ML spaces are\nevolving so quickly that teams should consider change as a constant part of\ntheir plans. \u201cIt\u2019s exciting and terrifying. There's a lot of momentum and\nspeed when you look at how different the space is from just a few months\nago\u2013so if you're going to get into the MLOps space, you need to design for\nchange. But to manage change well, you also need the ability to reliably\nevaluate your outputs.\u201d\n\n\u201cIf you're really serious about putting something into production, you should\nbe able to ensure that you can actually change those pieces out when the need\narises\u2013and that you haven't made too many assumptions about them, which may\nrequire the right tools, among other things. I\u2019m admittedly a bit biased, as\nI\u2019ve been working on the open-source project Hamilton, which I think is a\ngreat way to model dataflows that are modular and easy to modify. For example,\nwhen what you\u2019re working on has changed or has been simplified, like larger\ncontext windows, better LLMs, and so on...How quickly can you adopt it, and\nhow quickly can you evaluate what changes with it so you can move with\nconfidence?\u201d\n\n\u201cUnderstanding what's going on in the system and how things have changed post-\nlaunch is going to be pretty critical to your software development life cycle.\nIn other words, let\u2019s say you change a prompt and it seems to work. The space\nof inputs for an LLM tends to be much larger than a standard unit test, so you\ncan never essentially have 100% evaluation coverage unless you build out a\ngiant suite of tests to confirm that you\u2019ll get your expected output from\nwhichever type of input you\u2019re using.\u201d\n\n### Getting a Valuable MVP Running and Iterating\n\nReflecting on the question of whether the future will belong to large-scale\nfoundation models, open models, or smaller, local models, Krawczyk suggests\nthat teams will be better served by starting from their intended outcomes and\nunderstanding how upstream changes will affect their AI program going forward.\n\u201cSome people have suggested that the future will belong to complementary tools\nthat close the gaps in popular foundation models, but if the likes of OpenAI,\nAnthropic, or Google update their models and fix those issues, your business\nis gone. Another reason to design for change.\u201d\n\n\u201cFor machine learning programs, you\u2019d usually start with heuristics and rules,\nand you\u2019d build your way up. Now, these major foundation models, in contrast,\nseem very \u2018general purpose.\u2019 I think you will potentially see reasons to move\naway or try something else\u2013reasons like hallucinations and controllability,\nbut also cost curves and other business considerations.\u201d\n\n\u201cFor example, maybe you\u2019ve built an MVP on ChatGPT. But maybe you realize that\nyou can get away with using a much smaller model. Then that means the hardware\nto run it gets cheaper as the model requires less memory, and so on. But\nbefore investing in that, you first need to prove the business value. I think\nthe life cycle will look something like this: Get something running, prove\nthat it works, prove that it's valuable, and then refine it with something\nlike fine tuning. So I think if anything, there's going to be more tooling\naround the software development life cycle. How do you develop? How do you\nthen change something that's running? You'll have different APIs or\nfoundational models for different parts or different calls.\u201d\n\n\u201cFrom there, how do you evaluate that? How do you change it with confidence?\nMonitoring could be a potentially huge challenge, for instance. If you have a\nlot of these LLM API calls back to back, some small perturbation up the top\nmight really impact things down the line. How do you easily debug, trace, and\nunderstand what went wrong? I think these are problems that have existed\nbefore, but just at a different kind of scale and rate of change. That\u2019s why\nI\u2019m also building another open source framework called Burr.\u201d\n\n### Closing the Gap Between Executive Expectations and Ops Realities\n\nKrawczyk points out the inherent conflict in trying to fit inherently non-\ndeterministic systems into traditionally rigid units of measurement.\n\u201cAdmittedly, you\u2019re dealing with something that\u2019s part of a hype cycle. But\nyou want to manage expectations so that execs don\u2019t think you've over-promised\nand under-delivered, right? We want our project to be up and running, and\nstable. We\u2019d naturally try to define some sort of metrics or boundary,\nsomething to prove some sort of baseline. But given my time in research work\nwith ML and related disciplines, I think that some of this stuff just doesn't\nalways fall nicely into a sprint.\u201d\n\n\u201cIn the very short term, you might look at metrics like iteration speed\u2013define\nsome boundaries of what success is. If your immediate goal is making execs\nhappy, you can start with things that are inherently measurable, and narrow\nthe scope, such as down to a specific use case. But with regards to how\nincentives are aligned between data scientists, developers, and executives,\nit\u2019s ultimately a tricky question, because you have to ask: Who's developing\nand who's productionizing? How do you actually get things out? Who is\nultimately responsible?\u201d\n\n\u201cIf speed is critical, you\u2019d want the same person or team doing both. How\nwould that team be measured? There\u2019s organizational stuff to think\nabout\u2013something that more people are going to encounter over time because\nback-end and front-end devs can now spin up these applications themselves. If\neng teams promise the world, or are expected to deliver the world, and they\ncan\u2019t, they\u2019ll need someone with an ML or data science background to set\neverything up. This is why it\u2019s so important to be able to properly measure\nand evaluate, something that those with a ML and/or data science background\nshould know how to do.\u201d\n\n## More Resources:\n\n  * Podcast: Practical Product on Roadmaps and Product Direction\n  * Video: How Iteration and Feedback Launched GitHub Copilot with Alyss Noland\n\n## Will MLOps Go the Way of DevOps? with Adam Zimman\n\nAdam Zimman is an angel investor and strategic advisor to early stage VC firms\nand software startups. He has served as a professional developer, an\nengineering executive, and a go-to-market executive at organizations such as\nDell (EMC), VMWare, GitHub, and LaunchDarkly. Adam offers the following\nobservations:\n\n  * Risk-Taking and Experimentation Need to Become Part of Your Culture: Google\u2019s SRE team adopted the concept of error budgets to stress-test systems. Organizations that adopt non-deterministic models should do the same.\n  * Incentives Need to Align With Experimentation: The orgs that make important discoveries faster will be those that don\u2019t penalize taking risks.\n  * Operational Excellence Will Come from Integrating MLOps into the Dev Life Cycle: Orgs will get past the bottlenecks of requests getting thrown over the fence by rethinking the development life cycle to include what MLOps teams need.\n  * The Lines Between Software Devs, Data Engineers, and Data Scientists Must Blur: Orgs will see the greatest efficiencies with common lines between data science and dev. And given how few ML PhDs there are, the data engineers of the future may increasingly come from traditional software dev backgrounds.\n\n## Discussion: Applying Software Dev Lessons to the Future\n\nTo contextualize his observations, Zimman refers to some of his recent\nconversations with startups he advises\u2013and how they rhyme with the rise of\nDevOps. \u201cI've talked to a number of startups that are looking to establish\nthis notion of MLOps pipelines and set up some standards around it. It seems\nlike there aren\u2019t too many who have quite nailed it, at this point. There's\nstill a huge bifurcation between the academic community, which has been very\nparticipatory in the building of ML models, and the engineering-plus-\noperations community, which has been looking to support them operationally.\nThere's still a sense that data scientists are still very much viewed as an\n\u2018other,\u2019 as opposed to just a regular member of the engineering team. Right\nnow, one of the biggest challenges is a cultural one, not a tooling one.\u201d\n\n\u201cIt\u2019s kind of like when developers didn't consider themselves to be part of\noperations. This was part of the motivation behind the DevOps movement. It was\ngetting to that realization: \u2018For the work that the two of you [Dev and Ops]\nare doing, neither one of you exists without the other.\u2019 Data scientists still\nseem to sit outside of that, until the situation changes culturally\u2013until some\ncompanies start looking at how to incorporate data scientists as, for\ninstance, a different developer persona \u2013 similar to front end versus back end\ndevelopers. From there, you have to figure out\u2013how do you get them to use the\nsame tools? How do you get them to use the same processes that are going to\ndrive that unification, not just functionally, but culturally? You need to get\nthem to realize: \u2018Oh yeah. I guess we are all working together.\u2019\u201d\n\n### Why Experimentation Is Important Enough to Bake Into Business Incentives\n\nZimman reflects on how the current risk-averse environment is eerily similar\nto the software startup scene 10 years ago. \u201cThis whole conversation reminds\nme of the early days at a startup where I worked. Basically, the idea of ML\nmodels being this \u2018adjustable algorithm you still need to refine\u2019 requires a\nculture and mindset that encourages experimentation, like the Google SRE error\nbudgets. Really thinking about how you can actually encourage people to break\nthings because you know that will ultimately make you better, faster, and more\nstable.\u201d\n\n\u201cFor the vast majority of organizations, that is still a huge cultural change.\nIn those earlier days, we had people agree that experimentation sounded like a\ngreat idea in theory, analysts claimed it was valuable, and there was data to\nback it up. But in reality, very few organizations have been able to adopt a\nrisk-taking culture with any kind of success. And it was because the\nincentives for advancement, for compensation\u2013for avoiding getting calls in the\nmiddle of night\u2013were all completely misaligned with this idea of failing on\npurpose for the sake of learning.\u201d\n\n### Why ML in Production May Be Having Its \u201cDevOps\u201d Moment\n\nWhile the \u201cbefore\u201d picture for how MLOps, engineers, and executives get on the\nsame page remains uncertain, Zimman is confident the \u201cafter\u201d picture will look\na lot like modern DevOps, both culturally and organizationally. \u201cI\u2019ve heard\nboth James Governor and Charity Majors use the term operational excellence,\nwhich leads me to think of things in terms of maturity models. Teams are going\nto get to a point where operations are part of your development backlog and\nlifecycle. You\u2019ll need to start thinking about how your operations team can\nactually make requests back to engineering or changes to systems and services\nand applications that will help them unblock bottlenecks and just make\nthemselves more efficient.\u201d\n\n\u201cFrankly, that\u2019s something of an anomaly, even among Fortune 2000\ncompanies\u2013I\u2019d argue that less than 10% of them have reached that point of\nmaturity from an operations perspective.\u201d But Zimman cautions against siloing\nacross different teams, which can jeopardize alignment and slow things down.\n\u201cMy big concern is that the picture starts looking like DevSecOps\u2013where all of\na sudden, you have this notion of a \u2018third team.\u2019 To me, there really\nshouldn\u2019t be.\u201d\n\n\u201cIf you want to think about the context of DevOps, then at its core you\u2019re\ndoing one of two things: You\u2019re building a thing, or you're running a thing.\nIf you're on the build side, you're on the build side and that's it. You are\npart of being able to write code, deploy new code, and continue to iterate and\ndevelop. If you're on the run side, then you're on the run side. Your\nresponsibilities are only to keep things up and keep things running and look\nfor opportunities to be able to do improvement. But the reality is that you\nshould be working closely with those two organizations to make sure that\nyou're iterating quickly and intelligently to tackle the things that matter\nmost, first.\u201d\n\n### How Operationalizing Could Lead to Common Ground\n\nZimman suggests that while data, engineering, and operations will converge,\ndifferences will necessarily remain. \u201cPersonally, I don't believe that large-\nscale DevOps teams are actually \u2018a thing.\u2019 I think at scale, DevOps is just a\nchange that only affects process, or technology, or alignment, or culture.\nIt's not a single team structure. While it can roll up to a chief development\nofficer or CIO, the reality is that you still have individuals whose roles are\neither developer or operator. When orgs try to claim that its developers are\noperators, that just doesn't work at scale in my experience.\u201d\n\n\u201cSo, operations teams are still going to be operations teams. And I think that\nthey're going to need the ability to own and control model deployment and\ndelivery. Similarly, I think that on the developer side, the role of data\nengineer may end up going to developers. And so, they're going to need to make\nsure they're working within the kind of guidelines you\u2019d expect within the\ndevelopment community. Now some of those guidelines may need to shift and\nexpand so that they incorporate different tools or augmented processes that\nare needed by a data scientist. But ultimately, I think the healthy teams are\ngoing to find ways to be able to look at consolidating that and collapsing\nthat down so that there's less of a distinction.\u201d\n\n\u201cI've been in organizations where initially there's a need from the marketing\nteam to be able to have developers be able to build the website. And\nultimately, the most successful ones I've seen that have been the ones where\nthat\u2019s inevitably a function that gets moved out of marketing and back into\nR&D, because there's a closer alignment to being able to encourage developers\nwith a career track by putting them in an org with other developers. This\nisn\u2019t strictly an org structure thing\u2013you can still have reporting back into\nmarketing, maybe with a dotted line back to engineering, but there's got to be\na greater relationship and connection between those types of functions.\u201d\n\n\u201cIt's going to happen with data science. What I think will really change is\nthat as having models in production and the need for data science expertise\nbecomes more mainstream, more developers will need to take on the role of data\nengineer because there just aren't enough data scientists to do it. You're\njust going to see fewer folks coming from academia as there are only so many\nPhDs. And while some PhDs may continue to be enamored with \u2018the perfect,\u2019 an\nincreasing population of software-devs-turned-data-engineers will recognize\nthat perfect is the enemy of good, and that ultimately, we have to ship\nsomething. There needs to be an approach of a Progressive Delivery model that\nprioritizes shipping, testing in production, operational excellence, and\nconsistent incremental improvement.\u201d\n\n## More Resources:\n\n  * Article: IR and DevOps in the Age of Generative AI\n  * Video: Building Companies that DevOps Teams Love with Jesse Robbins\n\n## Conclusion\n\nOver time, as more teams come to understand the complexities of machine\nlearning models in production, more organizations will hopefully be able to\nset sensible goals that allow for experimentation (and the occasional\nunexpected model hiccup), especially if software developers, data scientists,\nand executives can close the gap of specialized knowledge through more\neducation, tooling, and culture that encourages transparent knowledge sharing\nand learning by improvement.\n\nFor more discussion on the challenges of getting AI into production, join the\nDevGuild AI Summit II event.\n\n### Content from the Library\n\nVisit library\n\nJul 18, 2023\n\nArticle\n\n### Digging Deeper into Building on LLMs, AI Coding Assistants, and\nObservability\n\nAre Prompting and ChatGPT Programming the Future? If you follow the headlines,\nAI coding assistants such as GitHub Copilot and...\n\nFeb 5, 2024\n\nVideo\n\n### From Labs to Launch: Stories from GitHub Copilot\n\nDiscover GitHub Copilot's launch strategies and actionable insights to apply\nto your product, and navigate the ever-evolving...\n\nDec 12, 2023\n\nPodcast\n\n### Generationship Ep. #2, Putting LLMs to Work with Liz Fong-Jones and\nPhillip Carter of Honeycomb\n\nIn episode 2 of Generationship, Rachel Chalmers speaks with Liz Fong-Jones and\nPhillip Carter of Honeycomb. Together they explore...\n\nStay in the loop with the latest news from the Heavybit team, community, event\ninvitations, and portfolio company job openings.\n\n  * Portfolio\n  * Team\n  * News\n  * Apply\n\n  * DevGuild\n  * Jobs\n  * Inclusion\n\n  * Browse\n  * Podcast Network\n  * DevToolsDigest\n\nHeavybit is the leading fund for developer and enterprise startups.\n\n\u00a9 2024 Heavybit\n\n", "frontpage": false}
