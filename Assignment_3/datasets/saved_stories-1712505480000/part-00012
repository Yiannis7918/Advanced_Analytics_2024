{"aid": "39958236", "title": "Dark Visitors: Robots.txts API", "url": "https://darkvisitors.com/docs/robots-txts-api", "domain": "darkvisitors.com", "votes": 1, "user": "cdme", "posted_at": "2024-04-07 04:44:12", "comments": 0, "source_title": "Robots.txts API | Dark Visitors", "source_text": "Robots.txts API | Dark Visitors\n\n# Robots.txts API\n\nThe robots.txts API lets you to generate robots.txts using the agent list. The\nagent list is updated continuously, so regenerating your robots.txt\nperiodically will keep it up to date.\n\n## Generate a Robots.txt\n\n### The Request\n\nURL| https://api.darkvisitors.com/robots-txts  \n---|---  \nMethod| POST  \n  \n#### Headers\n\nAuthorization| A bearer token with your project's access token (e.g. Bearer\n48d7dcbd-fc44-4b30-916b-2a5955c8ee42). After you sign up and create a new\nproject, you can copy your access token from the project settings page.  \n---|---  \n  \n#### Body\n\nagent_types| An array of agent types. Agent types include AI Assistant, AI\nData Scraper, and AI Search Crawler.  \n---|---  \ndisallow| A string specifying which URLs are disallowed. Defaults to / to\ndisallow all URLs.  \n  \n#### Example\n\nThis example would generate a robots.txt that blocks all known AI data\nscrapers from all URLs.\n\n    \n    \n    const robotsTXT = await fetch(\"https://api.darkvisitors.com/robots-txts\", { method: \"POST\", headers: { \"Authorization\": \"Bearer \" + ACCESS_TOKEN }, body: { \"agent_types\": [ \"AI Data Scraper\" ], \"disallow\": \"/\" } })\n\n### The Response\n\nThe response is a robots.txt in text/plain format. You can use this as is, or\nappend additional lines to include things like sitemap directives.\n\nServe the resulting text file as your website's robots.txt. Regenerate it with\nthe API periodically (e.g. once per day) to stay up to date with the latest\nknown agents.\n\n\u00a9 2023 Bit Flip LLC\n\nContact Terms Privacy\n\n", "frontpage": false}
