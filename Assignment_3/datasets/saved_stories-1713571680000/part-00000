{"aid": "40090108", "title": "TorchFix \u2013 a linter for PyTorch-using code with autofix support", "url": "https://github.com/pytorch-labs/torchfix", "domain": "github.com/pytorch-labs", "votes": 2, "user": "callmekit", "posted_at": "2024-04-19 18:06:24", "comments": 0, "source_title": "GitHub - pytorch-labs/torchfix: TorchFix - a linter for PyTorch-using code with autofix support", "source_text": "GitHub - pytorch-labs/torchfix: TorchFix - a linter for PyTorch-using code\nwith autofix support\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\npytorch-labs / torchfix Public\n\n  * Notifications\n  * Fork 6\n  * Star 49\n\nTorchFix - a linter for PyTorch-using code with autofix support\n\n### License\n\nView license\n\n49 stars 6 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# pytorch-labs/torchfix\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n6 Branches\n\n4 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nkit1980Bump version to 0.5.0 (#40)Apr 18, 202455da3d0 \u00b7 Apr 18, 2024Apr 18,\n2024\n\n## History\n\n69 Commits  \n  \n### .github/workflows\n\n|\n\n### .github/workflows\n\n| Update to checkout action v4 (#35)| Mar 18, 2024  \n  \n### tests\n\n|\n\n### tests\n\n| Add codemod for TorchNonPublicAliasVisitor (#36)| Mar 19, 2024  \n  \n### torchfix\n\n|\n\n### torchfix\n\n| Bump version to 0.5.0 (#40)| Apr 18, 2024  \n  \n### .flake8\n\n|\n\n### .flake8\n\n| Rename flake8-torch to torchfix (#4247)| Jun 2, 2023  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Rename flake8-torch to torchfix (#4247)| Jun 2, 2023  \n  \n### CODE_OF_CONDUCT.md\n\n|\n\n### CODE_OF_CONDUCT.md\n\n| Adopt contributing files from test-infra| Nov 23, 2023  \n  \n### CONTRIBUTING.md\n\n|\n\n### CONTRIBUTING.md\n\n| Adopt contributing files from test-infra| Nov 23, 2023  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Adopt contributing files from test-infra| Nov 23, 2023  \n  \n### README.md\n\n|\n\n### README.md\n\n| Use Alerts markdown in Readme (#30)| Mar 13, 2024  \n  \n### pyproject.toml\n\n|\n\n### pyproject.toml\n\n| Require at least Python 3.9 (#20)| Jan 31, 2024  \n  \n### requirements-dev.txt\n\n|\n\n### requirements-dev.txt\n\n| Update mypy to 1.7.0 (#18)| Jan 31, 2024  \n  \n## Repository files navigation\n\n# TorchFix - a linter for PyTorch-using code with autofix support\n\nTorchFix is a Python code static analysis tool - a linter with autofix\ncapabilities - for users of PyTorch. It can be used to find and fix issues\nlike usage of deprecated PyTorch functions and non-public symbols, and to\nadopt PyTorch best practices in general.\n\nTorchFix is built upon https://github.com/Instagram/LibCST - a library to\nmanipulate Python concrete syntax trees. LibCST enables \"codemods\" (autofixes)\nin addition to reporting issues.\n\nTorchFix can be used as a Flake8 plugin (linting only) or as a standalone\nprogram (with autofix available for a subset of the lint violations).\n\nWarning\n\nCurrently TorchFix is in a beta version stage, so there are still a lot of\nrough edges and many things can and will change.\n\n## Installation\n\nTo install the latest code from GitHub, clone/download\nhttps://github.com/pytorch-labs/torchfix and run pip install . inside the\ndirectory.\n\nTo install a release version from PyPI, run pip install torchfix.\n\n## Usage\n\nAfter the installation, TorchFix will be available as a Flake8 plugin, so\nrunning Flake8 normally will run the TorchFix linter.\n\nTo see only TorchFix warnings without the rest of the Flake8 linters, you can\nrun flake8 --isolated --select=TOR0,TOR1,TOR2\n\nTorchFix can also be run as a standalone program: torchfix . Add --fix\nparameter to try to autofix some of the issues (the files will be\noverwritten!) To see some additional debug info, add --show-stderr parameter.\n\nCaution\n\nPlease keep in mind that autofix is a best-effort mechanism. Given the dynamic\nnature of Python, and especially the beta version status of TorchFix, it's\nvery difficult to have certainty when making changes to code, even for the\nseemingly trivial fixes.\n\nWarnings for issues with codes starting with TOR0, TOR1, and TOR2 are enabled\nby default. Warnings with other codes may be too noisy, so not enabled by\ndefault. To enable them, use standard flake8 configuration options for the\nplugin mode or use torchfix --select=ALL . for the standalone mode.\n\n## Reporting problems\n\nIf you encounter a bug or some other problem with TorchFix, please file an\nissue on https://github.com/pytorch-labs/torchfix/issues.\n\n## Rules\n\n### TOR001 Use of removed function\n\n#### torch.solve\n\nThis function was deprecated since PyTorch version 1.9 and is now removed.\n\ntorch.solve is deprecated in favor of torch.linalg.solve. torch.linalg.solve\nhas its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see torch.lu, which can be used with\ntorch.lu_solve or torch.lu_unpack.\n\nX = torch.solve(B, A).solution should be replaced with X =\ntorch.linalg.solve(A, B).\n\n### TOR002 Likely typo require_grad in assignment. Did you mean requires_grad?\n\nThis is a common misspelling that can lead to silent performance issues.\n\n### TOR003 Please pass use_reentrant explicitly to checkpoint\n\nThe default value of the use_reentrant parameter in torch.utils.checkpoint is\nbeing changed from True to False. In the meantime, the value needs to be\npassed explicitly.\n\nSee this forum post for details.\n\n### TOR101 Use of deprecated function\n\n#### torch.nn.utils.weight_norm\n\nThis function is deprecated. Use torch.nn.utils.parametrizations.weight_norm\nwhich uses the modern parametrization API. The new weight_norm is compatible\nwith state_dict generated from old weight_norm.\n\nMigration guide:\n\n  * The magnitude (weight_g) and direction (weight_v) are now expressed as parametrizations.weight.original0 and parametrizations.weight.original1 respectively.\n\n  * To remove the weight normalization reparametrization, use torch.nn.utils.parametrize.remove_parametrizations.\n\n  * The weight is no longer recomputed once at module forward; instead, it will be recomputed on every access. To restore the old behavior, use torch.nn.utils.parametrize.cached before invoking the module in question.\n\n## License\n\nTorchFix is BSD License licensed, as found in the LICENSE file.\n\n## About\n\nTorchFix - a linter for PyTorch-using code with autofix support\n\n### Topics\n\npython static-code-analysis linter static-analysis pytorch flake8\nflake8-plugin\n\n### Resources\n\nReadme\n\n### License\n\nView license\n\n### Code of conduct\n\nCode of conduct\n\nActivity\n\nCustom properties\n\n### Stars\n\n49 stars\n\n### Watchers\n\n8 watching\n\n### Forks\n\n6 forks\n\nReport repository\n\n## Releases 4\n\nTorchFix 0.5.0 Latest\n\nApr 18, 2024\n\n\\+ 3 releases\n\n## Contributors 7\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
