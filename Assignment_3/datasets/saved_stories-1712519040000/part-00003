{"aid": "39960728", "title": "Native RAG on Apple Sillicon Mac with MLX", "url": "https://github.com/qnguyen3/chat-with-mlx", "domain": "github.com/qnguyen3", "votes": 5, "user": "nikivi", "posted_at": "2024-04-07 13:43:07", "comments": 0, "source_title": "GitHub - qnguyen3/chat-with-mlx: Chat with your data natively on Apple Silicon using MLX Framework.", "source_text": "GitHub - qnguyen3/chat-with-mlx: Chat with your data natively on Apple Silicon\nusing MLX Framework.\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nqnguyen3 / chat-with-mlx Public\n\n  * Notifications\n  * Fork 96\n  * Star 973\n\nChat with your data natively on Apple Silicon using MLX Framework.\n\ntwitter.com/stablequan\n\n### License\n\nMIT license\n\n973 stars 96 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# qnguyen3/chat-with-mlx\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n11 Branches\n\n6 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nonuralpszrMerge pull request #74 from qnguyen3/dependabot/pip/mlx-\nlm-0.4.0c4c06db \u00b7\n\n## History\n\n97 Commits  \n  \n### .github\n\n|\n\n### .github\n\n| ci: \ud83d\udc77 python installation tests on apple silicon runner for make sure...  \n  \n### assets\n\n|\n\n### assets\n\n| update gif  \n  \n### chat_with_mlx\n\n|\n\n### chat_with_mlx\n\n| feat(cli): \u2728 port and share params added  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| ci: \ud83d\udc77 pre-commits configs are added and formatting complete  \n  \n### .pre-commit-config.yaml\n\n|\n\n### .pre-commit-config.yaml\n\n| ci: \ud83d\udc77 pre-commits configs are added and formatting complete  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit  \n  \n### MANIFEST.in\n\n|\n\n### MANIFEST.in\n\n| ci: \ud83d\udc77 pre-commits configs are added and formatting complete  \n  \n### README.md\n\n|\n\n### README.md\n\n| fixing the typo  \n  \n### pyproject.toml\n\n|\n\n### pyproject.toml\n\n| Merge pull request #74 from qnguyen3/dependabot/pip/mlx-lm-0.4.0  \n  \n## Repository files navigation\n\n# Native RAG on Apple Sillicon Mac with MLX \ud83e\uddd1\ud83d\udcbb\n\nThis repository showcases a Retrieval-augmented Generation (RAG) chat\ninterface with support for multiple open-source models.\n\n## Features\n\n  * Chat with your Data: doc(x), pdf, txt and YouTube video via URL.\n  * Multilingual: Chinese \ud83c\udde8\ud83c\uddf3, English\ud83c\udff4, French\ud83c\uddeb\ud83c\uddf7, German\ud83c\udde9\ud83c\uddea, Hindi\ud83c\uddee\ud83c\uddf3, Italian\ud83c\uddee\ud83c\uddf9, Japanese\ud83c\uddef\ud83c\uddf5,Korean\ud83c\uddf0\ud83c\uddf7, Spanish\ud83c\uddea\ud83c\uddf8, Turkish\ud83c\uddf9\ud83c\uddf7 and Vietnamese\ud83c\uddfb\ud83c\uddf3\n  * Easy Integration: Easy integrate any HuggingFace and MLX Compatible Open-Source Model.\n\n## Installation and Usage\n\n### Easy Setup\n\n  * Install Pip\n  * Install: pip install chat-with-mlx\n  * Note: Setting up this way is really hard if you want to add your own model (which I will let you add later in the UI), but it is a fast way to test the app.\n\n### Manual Pip Installation\n\n    \n    \n    git clone https://github.com/qnguyen3/chat-with-mlx.git cd chat-with-mlx python -m venv .venv source .venv/bin/activate pip install -e .\n\n#### Manual Conda Installation\n\n    \n    \n    git clone https://github.com/qnguyen3/chat-with-mlx.git cd chat-with-mlx conda create -n mlx-chat python=3.11 conda activate mlx-chat pip install -e .\n\n#### Usage\n\n  * Start the app: chat-with-mlx\n\n## Supported Models\n\n  * Google Gemma-7b-it, Gemma-2b-it\n  * Mistral-7B-Instruct, OpenHermes-2.5-Mistral-7B, NousHermes-2-Mistral-7B-DPO\n  * Mixtral-8x7B-Instruct-v0.1, Nous-Hermes-2-Mixtral-8x7B-DPO\n  * Quyen-SE (0.5B), Quyen (4B)\n  * StableLM 2 Zephyr (1.6B)\n  * Vistral-7B-Chat, VBD-Llama2-7b-chat, vinallama-7b-chat\n\n## Add Your Own Models\n\n### Solution 1\n\nThis solution only requires you to add your own model with a simple .yaml\nconfig file in chat_with_mlx/models/configs\n\nexamlple.yaml:\n\n    \n    \n    original_repo: google/gemma-2b-it # The original HuggingFace Repo, this helps with displaying mlx-repo: mlx-community/quantized-gemma-2b-it # The MLX models Repo, most are available through `mlx-community` quantize: 4bit # Optional: [4bit, 8bit] default_language: multi # Optional: [en, es, zh, vi, multi]\n\nAfter adding the .yaml config, you can go and load the model inside the app\n(for now you need to keep track the download through your Terminal/CLI)\n\n### Solution 2\n\nDo the same as Solution 1. Sometimes, the download_snapshot method that is\nused to download the models are slow, and you would like to download it by\nyour own.\n\nAfter the adding the .yaml config, you can download the repo by yourself and\nadd it to chat_with_mlx/models/download. The folder name MUST be the same as\nthe orginal repo name without the username (so google/gemma-2b-it ->\ngemma-2b-it).\n\nA complete model should have the following files:\n\n  * model.safetensors\n  * config.json\n  * merges.txt\n  * model.safetensors.index.json\n  * special_tokens_map.json - this is optional by model\n  * tokenizer_config.json\n  * tokenizer.json\n  * vocab.json\n\n## Known Issues\n\n  * You HAVE TO unload a model before loading in a new model. Otherwise, you would need to restart the app to use a new model, it would stuck at the old one.\n  * When the model is downloading by Solution 1, the only way to stop it is to hit control + C on your Terminal.\n  * If you want to switch the file, you have to manually hit STOP INDEXING. Otherwise, the vector database would add the second document to the current database.\n  * You have to choose a dataset mode (Document or YouTube) in order for it to work.\n\n## WHY MLX?\n\nMLX is an array framework for machine learning research on Apple silicon,\nbrought to you by Apple machine learning research.\n\nSome key features of MLX include:\n\n  * Familiar APIs: MLX has a Python API that closely follows NumPy. MLX also has fully featured C++, C, and Swift APIs, which closely mirror the Python API. MLX has higher-level packages like mlx.nn and mlx.optimizers with APIs that closely follow PyTorch to simplify building more complex models.\n\n  * Composable function transformations: MLX supports composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization.\n\n  * Lazy computation: Computations in MLX are lazy. Arrays are only materialized when needed.\n\n  * Dynamic graph construction: Computation graphs in MLX are constructed dynamically. Changing the shapes of function arguments does not trigger slow compilations, and debugging is simple and intuitive.\n\n  * Multi-device: Operations can run on any of the supported devices (currently the CPU and the GPU).\n\n  * Unified memory: A notable difference from MLX and other frameworks is the unified memory model. Arrays in MLX live in shared memory. Operations on MLX arrays can be performed on any of the supported device types without transferring data.\n\n## Acknowledgement\n\nI would like to send my many thanks to:\n\n  * The Apple Machine Learning Research team for the amazing MLX library.\n  * LangChain and ChromaDB for such easy RAG Implementation\n  * People from Nous, VinBigData and Qwen team that helped me during the implementation.\n\n## Star History\n\n## About\n\nChat with your data natively on Apple Silicon using MLX Framework.\n\ntwitter.com/stablequan\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\n### Stars\n\n973 stars\n\n### Watchers\n\n11 watching\n\n### Forks\n\n96 forks\n\nReport repository\n\n## Releases\n\n6 tags\n\n## Packages 0\n\nNo packages published\n\n## Contributors 4\n\n  * onuralpszr Onuralp SEZER\n  * qnguyen3 Quan Nguyen\n  * dependabot[bot]\n  * amrrs amrrs\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
