{"aid": "39959777", "title": "4 Papers that build on Devin, the AI software engineer", "url": "https://tomhipwell.co/blog/march_24_roundup/", "domain": "tomhipwell.co", "votes": 2, "user": "pongogogo", "posted_at": "2024-04-07 10:21:41", "comments": 0, "source_title": "March '24 Roundup", "source_text": "March '24 Roundup | Tom Hipwell\n\n# March '24 Roundup\n\nMarch was the month we got Grok, OpenAI confirmed their strategy and we no\nlonger needed to run on vibes alone as gpt-4 was displaced at the top of the\nleaderboards. An experiment was also kicked off to learn about the pricing\npower of the major LLM providers.\n\nOne of the things I most enjoyed this month was the explosion of interest in\nLLM agents with the launch of Devin, the AI software engineer. So this month\nI\u2019ve pulled out 4 papers which expand on agent based workflows and show how\nthings might evolve over the little while.\n\n### Design2Code: How Far Are We From Automating Front-End Engineering?\n\nLet\u2019s start by looking at the limits of LLMs in a zero-shot (i.e. single\nprompt) context. Based on this paper I would say we\u2019re quite a long way from\nautomating front-end engineering (Betteridge\u2019s law applies again). The paper\noutlines the results of getting a bunch of today\u2019s top models to generate\nhtml/css (note no js) to imitate websites based on an input image of the\nwebsite and a small prompt (actually two prompts, there\u2019s a review step as\nwell). The researchers tested the outputs and supposedly the GPT-4V generated\noutput in particular was preferred >50% of the time. I would say this is a\nlong way from automating the work of frontend devs but I can see the power for\nsome basic site building use cases. I suspect an agent based approach would\nshow a lot more promise here - what makes a big difference to the end result\nis how you zoom in on the individual components of the page and iteratively\nrefine. More on this shortly.\n\nA slight aside - I had a lot of fun playing around with the prompts from the\npaper, mostly because the no js constraint made me try and get some animations\nworking in CSS which I hadn\u2019t tried before. A nice example of the type of fun\nside quest that LLMs enable. I find working with LLMs helps me get into that\noptimal fast-feedback-with-learning, rewarding, game-like state pretty quickly\nand this is one of the joys I get from using them - so I\u2019d like more tools\nthat enable me to drop into that cycle quickly that feel less instrusive.\n\n### MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution\n\nThe Magis paper describes a Multi-Agent setup which gets similar performance\nto Devin on SWE Benchmark tasks (it resolves about 14% of provided Github\nIssues). It\u2019s a multi-agent model as there\u2019s different roles being played here\n- that of manager (responsible for forming the plan and farming the tasks out\nto the workers), repo custodian (finds the right files in the repo), developer\n(makes the changes) and QA (performs code review for developer changes).\n\nI do wonder if one of the things we\u2019ll get from all this is a better\nunderstanding of our craft - for example the agent struggling is positively\ncorrelated with files and functions touched, but lines of code changed has\nless of an impact. Intuitive perhaps but if we can start to better define what\nmakes a changeset complex I think that would be useful feedback when\ndeveloping. I also think a better understanding of change complexity will\nenable us to apply agents to the right problems earlier as we adopt these\ntools as part of our flow.\n\nThe limits of the technology today are working across 2 files and 190 lines in\nthe changeset (this is the maximum size of a successful issue resolution that\nthe agent was able to perform) and the paper tries to expand on what it is\nabout the harder problems that makes them difficult for the agent framework to\nsolve. It\u2019s not spelt out in the paper but I suspect the bottleneck on raising\nthese limits is the planning step as this needs a bit more reasoning power. If\nI grokked it correctly the framework works with two phases: plan -> deliver.\nThis doesn\u2019t really reflect how software gets crafted, so I wonder if you got\na better feedback loop into the planning step (i.e. get multiple agents to\ndebate the plan or use a prompting technique like self-reflection - or hey,\njust allow spikes) you\u2019d get better results.\n\n### More Agents Is All You Need\n\nLet\u2019s look at agent based models of reasoning in a bit more detail to build on\nthis point. This paper sets out to explore if we just brute force a problem by\nadding more and more agents to attempt a solution, do we get a better results?\nWe\u2019re looking for a general phenomenon here - a scaling property of the \u201craw\u201d\napplication of additional agents that enables us to have a better chance of\nsolving any problem. The paper explores this question by tackling reasoning\nproblems using a majority voting approach. First, a prompt is iteratively fed\ninto a single LLM (or a multi-agent collaboration framework) repeatedly until\nwe get a sample set of responses (the paper also explores using different LLMs\nin an ensemble style at this point). We then remove the dupes and use majority\nvoting to select a winner.\n\nThe paper finds that it does work, at least across three domains general\nreasoning (5-11% improvement), coding (4-9%) and arithmetic reasoning\n(12-24%). Also, smaller LLMs can outperform larger ones by just scaling up the\nensemble size (Llama2-13B model achieves 59% accuracy on the GSM8K dataset,\noutperforming the Llama2-70B model, which scores 54%). What\u2019s nice is that the\npaper then keeps expanding this analysis, breaking apart the different\nreasoning problems and isolating what makes each reasoning task difficult to\ntry and find where the performance gain is coming from. It looks at few\ndifferent factors - inherent difficulty (where we see constant gains until we\nhit a ceiling in the reasoning ability of LLMs), number of steps (where we see\ngains increasing with the number of steps [the limit in the paper is 8 steps])\nand problem decomposition (so using hierarchical voting to break harder\nreasoning problems into simpler ones that with a strong prior probablilty that\nthe LLM would solve by applying sample-and-vote all the way down).\n\nSo, if we put all this together, I\u2019d say it looks likely that we can probably\nget low single digit improvements on the Devin/Magis performance with the\ncurrent generation of LLMs. At least for classes of problem which don\u2019t hit\nthe inherent difficulty ceiling - joining the two papers together I\u2019d say in a\nsoftware engineering context that\u2019s probably number of files/functions in the\nchangeset (whereas number of lines of code changed/number of steps needed is\nunlikely to impact the agent).\n\n### AIOS: LLM Agent Operating System\n\nIf we\u2019re going to have all these agents knocking about, something probably\nneeds to change at the OS layer. This paper describes an attempt to build an\noperating system \u201cwith soul\u201d, basically putting an LLM at it\u2019s heart and\ncharging it with agent resource allocation, context switching, concurrency,\naccess controls and toolchain. The splash that Devin caused this month tells\nus the agentic workflows could be here to stay, and this paper outlines an\narchitecture for an Agent OS - at a high level, there\u2019s an LLM specific kernel\nwhich has a few components like a scheduler, context manager (which supports\nsnap-shotting for pause/resume) a memory manager (handy for lazily managing\nthe context window size required by each agent) and storage, tool and access\nmanagers. Agent developers then deal with the OS through an SDK and this heavy\nlifting is handed off to the LLM OS layer. There\u2019s a level of indirection as\nwell, as there\u2019s a separate OS Kernel for managing everything else, and the\nLLM Kernel can only go through this layer to interact with hardware. Neat.\nNote that this is a development of an idea that Andrey Karpathy shared in\nNovember of last year.\n\n### Stealing Part of a Production Language Model\n\nA bonus paper on AI security to finish. Deepmind set out how to reverse\nengineer the hidden dimensionality and final output projection matrix of a\nlanguage model. Hidden dimensionality gives you the size of each model layer,\nthis tells you how much capacity for processing information each layer has.\nThe final projection matrix describes how the output of all model layers is\ncombined and transformed into a useful output for end users. The exploit works\nby generating a large number of random prompts and then analysing the\nstatistical properties of the model responses (specifically, this is done\nusing singular value decomposition). The result is important, as if you\nunderstand how the final projection works then your chances of, say, prompt\ninjection or a jailbreak may increase. It\u2019s also the first time that a paper\nhas been published which describes how to steal a single layer from a\nproduction LLM.\n\n\u00a9 2024 Tom Hipwell. Built with Hugo.\n\n", "frontpage": false}
