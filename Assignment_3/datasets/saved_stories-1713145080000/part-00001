{"aid": "40033743", "title": "Digital Astronomy with Cellular Automata", "url": "https://kylehovey.github.io/blog/automata-nebula", "domain": "kylehovey.github.io", "votes": 4, "user": "marvinborner", "posted_at": "2024-04-14 19:37:01", "comments": 0, "source_title": "Digital Astronomy with Cellular Automata", "source_text": "Digital Astronomy with Cellular Automata \u00b7 Kyle Hovey\n\nHere you will find occasional musings I have on math, science, and other\nthings. Portfolio Mathstodon\n\n\u00a9 2024. All rights reserved.\n\n### Kyle Hovey\n\n# Digital Astronomy with Cellular Automata\n\nIn my last post, I shared my journey through understanding the link between\nentropy, thermodynamics, evolution, computation, and mathematics. At the end,\nI shared some preliminary research on using entropy/complexity to classify the\nbehavior of Cellular Automata (CA) and perhaps pave a road to finding more\nuniversal CA (those capable of computation). At that time, I only had a\nhandful of samples which, albeit showing promise, fell short of demonstrating\nconcrete results.\n\nI am incredibly excited to share that I have now run my simulations on every\npossible Life-Like Cellular Automaton rule (a total of 262,144 rules), and it\nshows some great potential in classifying every rule based on its emergent\nbehavior. Not only that, but this method establishes what appears to be a\nstrong metric for finding \u201cislands\u201d of rules that have similar behavior.\n\nThis is exciting news, because past classifications of even elementary CA such\nas the semi-totalistic Moore neighborhood variety (called the Life-Like CA)\nhave either required generalizations that are computationally intractable to\nascertain, or required a great deal of manual filtering and edge-case handling\nin order to separate sets of rules into classes.\n\n## Abstract\n\nStephen Wolfram (one of the biggest researchers in CA) proposed a four-level\nclassification scheme for one dimensional cellular automata. He later extended\nthese definitions to include two-dimensional cellular automata like the Life-\nLike CA we are looking at here. The classifications are:\n\n  1. Evolution leads to a homogeneous state.\n  2. Evolution leads to a set of separated simple stable or periodic structures.\n  3. Evolution leads to a chaotic pattern.\n  4. Evolution leads to complex localized structures, sometimes long-lived.\n\nBut, as mentioned in this post regarding some caveats about these\nclassifications, gliders have been found in all four of these classes. This is\nproblematic because gliders are one of the most essential parts of data\ntransmission in machines built inside of CA, so the four classes may not be\nenough to identify the presence or absence of a universal CA. Also, it has\nbeen shown that, given a rule, finding which class a CA belongs to is an\nundecidable problem (for one-dimensional CA at least, but I would imagine the\nargument abstracts well to any Cartesian dimension).\n\nMy goal here was to focus on dynamic classification of the emergent properties\nof any given CA given its rules. By not subscribing to manually generated\nlabels on classification, we can instead focus on developing a metric of\nsimilarity. In this sense, each rule becomes its own \u201cclass\u201d and you can find\nrules that are sufficiently close in behavior to be considered the same class.\nGeometrically, this would be an analysis of CA by way of clustering.\n\nThe difficult part, of course, is developing a representation of a given rule\nthat would allow for clustering. I settled on producing a curve of the\nKolmogorov complexity across the generations of the automaton\u2019s universe. My\ninspiration for this approach came from a few core concepts. First, that\nentropy and complexity looked like valid metrics to measure the emergent\nbehavior of a system and its potential for self-organized criticality. My\nreasoning behind this intuition is that structure typically implies order, and\norder implies either repeating patterns or extension of structure that can be\nderived from existing structure. Kolmogorov complexity would capture the\namount of bits required to express this structure. I later learned that this\nview is not a new one, as Wolfram took a similar approach in examining the\nprocession of spacial and temporal entropy in his research on one-dimensional\nCA.\n\nIn a addition to the convenient dualistic simplicity of studying life-or-death\nCA, the grid of an automaton can be interpreted as a bitmap image. Image\ncompression is (unsurprisingly) adept at finding something close to the\nsmallest possible representation of an image, and PNG compression does it\nwithout loss of information. Image compression asymptotically approximates\nKolmogorov Complexity up to a constant dependent on the compression algorithm.\nTherefore, as the compression algorithm is the same for each measurement, we\nhave a viable pipeline for estimating the Kolmogorov complexity of each state\nof the CA universes we encounter. If we wish to relate all of this back to\nentropy, we can do so. Entropy is the expected value of Kolmogorov Complexity\nin this context, so this data will be useful for that as well.\n\nIn order to get an amortized generalization of each rule, I started from\nrandom initial states of the universe with each cell having an equal\nprobability of starting in any of the possible states. I then ran simulations\nfor hundreds of generations with multiple random initial conditions and found\nthe average complexity at each step. Other researchers looking into the\ngeneral behavior of CA have taken this approach of random initial state and it\nseems to be a valid way to capture their behavior.\n\nLastly, I chose to study only Life-Like CA. These are the semi-totalistic CA\nrules that only depend on the Moore neighborhood of each cell. This made the\nsearch space something that I could simulate in reasonable time, given that it\nonly had around a quarter million possible rules (even though it still took\ntwo weeks to generate all of the data).\n\nThe result of these simulations were 262,144 records of the average complexity\nin bytes of the board of all possible Life-Like CA. Each record had 256\nsamples, each record was averaged from 10 runs, and each rule was run with a\nboard size of 100x100 cells.\n\n## Using UMAP, a Digital Telescope\n\nObviously, no one has the time to go through the graphs of over a quarter\nmillion samples, so I needed to find a way to classify the results. Recently I\nhave been infatuated with the UMAP algorithm. It has the ability to compress\ndata with thousands of dimensions into a lower dimensional space (in this case\n2D or 3D) while still preserving structures/features in the data. It is a\nremarkable feat of algebraic topology that deserves more awareness of in the\nscientific community.\n\nWhen first learning about dimensionality reduction algorithms such as UMAP or\ntSNE, I was extremely skeptical of their efficacy. It seemed impossible to\nretain structure when losing that many dimensions. What made their usage click\nfor me was the knowledge that, even if your data lives in a space that has\nthousands of dimensions (called the ambient space), there is a very good\nchance that the local dimension of real-world data is of much lower dimension\nthan this. The goal, then, of UMAP is to preserve the structure found in the\ndata by finding a good manifold to embed it into. For further understanding on\nthis topic, check out the presentation that Leland McInnes (the creator of\nUMAP) gave on his algorithm.\n\nIn a sense, UMAP is a digital telescope that lets us look at constellations of\nhigh-dimensional data that we have never had the ability to visualize before.\nAlgorithms like tSNE have worked in similar ways in the past, but UMAP is the\nfirst algorithm to be efficient enough to run on data with thousands of\ndimensions using something as prosaic as a laptop and a dream. This is to say\nthat UMAP scales incredibly well, especially when compared to what is already\nout there.\n\nArmed with UMAP, I fed the algorithm all 262,144 vectors (each with 256\ndimensions, one for each complexity snapshot) and patiently waited for the\nembedding to complete. After fifteen minutes of my laptop revving up my fans,\nI had my first snapshot of the overarching structure of the Life-Like CA\n(points are colored by the average forward difference between each complexity\nsnapshot):\n\n(High-Res Version)\n\nThere it was, the massive Hertzspring-Russelesque serpent hiding in the\nstructure of emergent complexity in automata. It is important to note that\ncompressing dimensions can make parts of the data look separate in the\nembedding, even though they are connected in the ambient space they came from.\nIt would be reasonable to assume that the serpent is one continuous entity,\nand the \u201cjump\u201d in the center was a result of the embedding.\n\nWhile beautiful, this representation would not mean much if it did not\naccomplish the goal we set out to achieve: a metric for classification of\nrules that behave in similar ways to a given starting rule. Starting from the\nGame of Life, I began examining nearby rules and found that the metric did\nindeed yield other rules that produced uncanny behavior.\n\nRules Close to the Game of Life (B3/S23)\n\nB3/S23| B3/S013| B38/S013| B38/S238  \n---|---|---|---  \n  \nRules Close to Day and Night (B3678/S34678)\n\nB3678/S34678| B36/S01456| B3678/S01456| B3567-S01478  \n---|---|---|---  \n  \nRules Close to Anneal (B4678/S35678)\n\nB4678/S35678| B468/S035678| B0123578/S0124| B46/S035678  \n---|---|---|---  \n  \nRules Close to Maze-Finder (B138/S12357)\n\nB138/S12357| B124/S123467| B0124/S0123467| B038/S012358  \n---|---|---|---  \n  \nWhat is fascinating about this embedding is that it extends the idea of\nStephen Wolfram\u2019s four-level classification of CA to a continuum that can be\nembedded in as many dimensions as you see fit. CA classically known for\nsupporting persistent structures and gliders such as Game of Life, Day and\nNight, and High Life exist in the middle of the serpent where the average\ndifference is on the edge of chaos. CA that burn through complexity at a\nhigher rate such as Morley, Anneal, and Diamoeba are far out on the tail of\nthe serpent, along with many rules that result in universes that either die\nout quickly or fill the whole board with live cells (two low-complexity\nattractors). Meanwhile, rules like Replicator (which duplicates existing\nstructure) exist in the head of the serpent where complexity stays roughly the\nsame throughout the generations. Rules at the head seem to tend very quickly\ntowards chaos, an apt opposite to the rules found in the tail.\n\n## Caveats and Room for Improvement\n\nYou might notice that for the Anneal CA that there was an example that behaved\nlike Anneal but oscillated between black and white states every generation.\nThis was one of the most fascinating parts about this structure for me. Rules\nthat normally would not be classified together clearly had similar behavior,\neven though they had different ways of expressing it.\n\nThis didn\u2019t always work out for the best though, and there were cases of\n\u201cclose\u201d rules that had obviously different behavior. I think this shows that\nthis complexity metric either requires more resolution in the samples, or that\nsome types of behavior are not adequately described by the procession of\ncomplexity alone.\n\nOne major improvement that I could see benefiting this model would be a\ntransformation on the data that would be resilient to translations in the\ncomplexity curves. For instance, perhaps one CA immediately takes a dive in\ncomplexity following one behavior, and another with similar behavior is just\nslightly slower to hit that tipping point. Both curves would look nearly\nidentical, save for the latter one having the sigmoid-like decrease in\ncomplexity occur later in the curve. If you were to translate the first curve\nforward, or the second curve backward in time you would have a better metric\nfor joining complexity like that.\n\nAnother augmentation that could help refine this metric is examining the\nforward differences of each complexity curve instead of the raw data itself. I\nactually tried this and got another promising embedding:\n\nUltimately, I chose to spend the most time studying the embedding of the raw\ndata because I did not want to impose my own nuanced constructions on the\ndata. There is certainly much more that could be done to pre-process this data\nbefore embedding, and I am excited for what results that may yield.\n\n## Reproducibility\n\nAn important consideration is that UMAP is a non-deterministic algorithm. That\nis to say that each run of UMAP will most likely produce slightly different\nembeddings. I can verify that after running it around 50 times, the structure\nremained the same, but the orientation would sometimes differ.\n\n## Source Code and Data Explorer\n\nI used various languages to generate and analyze this data. The automata\nsimulator was written in C++, and the program to assemble histories of the\ncomplexity snapshots was written in Bash. PNG compression was done with\nImagemagick via conversion from ASCII PPM (the simple output of the C++\nsimulation) to PNG. The Bash script saves the complexity histories as separate\nrows (one per each run) in a CSV file (one for each rule).\n\nThen for the data analysis, I used Python to read in all of the CSV data and\nsave it as a Numpy ndarray, while also averaging each of the ten trials I had\ngenerated for each of the rules. For each of the types of analysis I wanted to\ndo, I made a Jupyter notebook that had access to Python 3 with all of the\nnecessary dependencies for UMAP and displaying the results of the embeddings.\nThe GitHub repo does not have the full data committed to the repository as the\nGun-zipped tar-ball is just over half a gigabyte in size.\n\nLastly, I wanted a more natural way to explore the results and verify the\nstructure of the embedding. I created a web-app using React and TerraJS that\nlets you select points in the serpent nebula and see what sort of automaton\nresults from that point in the embedding. There is a zoomed view that shows\nneighboring points within a certain radius of the one that has been chosen. I\nalso added the ability to enter rules and see where they are located in the\nserpent.\n\nHere is a live demo of the app. Both the view of the full nebula and the\nzoomed portion are clickable, it just takes a second to find the closest rule.\nPlease note that the page will take a few seconds to load as initializing the\ndata for all quarter million rules is a processor-heavy task. As a result, I\ndon\u2019t expect mobile performance to hold up (or even work). I\u2019m open to PR\u2019s to\nimprove the app.\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-\nShareAlike 4.0 International License.\n\n", "frontpage": false}
