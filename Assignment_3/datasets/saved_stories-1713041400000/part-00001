{"aid": "40023497", "title": "Multi Needle in a Haystack", "url": "https://blog.langchain.dev/multi-needle-in-a-haystack/", "domain": "langchain.dev", "votes": 1, "user": "zaking17", "posted_at": "2024-04-13 14:48:19", "comments": 0, "source_title": "Multi Needle in a Haystack", "source_text": "Multi Needle in a Haystack\n\nSkip to content\n\nSign in Subscribe\n\n# Multi Needle in a Haystack\n\n6 min read Mar 13, 2024\n\n## Key Links\n\n  * Video\n  * Code\n\n## Overview\n\nInterest in long context LLMs is surging as context windows expand to 1M\ntokens. One of the most popular and cited benchmarks for long context LLM\nretrieval is Greg Kamradt's Needle in A Haystack: a fact (needle) is injected\ninto a (haystack) of context (e.g., Paul Graham essays) and the LLM is asked a\nquestion related to this fact. This explores retrieval across context length\nand document placement.\n\nBut, this isn't fully reflective of many retrieval augmented generation (RAG)\napplications; RAG is often focused on retrieving multiple facts (from an\nindex) and then reasoning over them. We present a new benchmark that tests\nexactly this. In our Multi-Needle + Reasoning benchmark we show two new\nresults:\n\n  1. Performance degrades as you ask LLMs to retrieve more facts\n  2. Performance degrades when the LLM has to reason about retrieved facts\n\nSee below plot for a summary of the results: as the number of needles\nincreases, retrieval decreases; and reasoning over those needles is worse than\njust retrieval.\n\nWe also show (similar to previous benchmarks) that performance decreases as\nmore and more context is passed in. However, we additionally investigate not\njust overall performance but why performance drops when retrieving multiple\nneedles. Looking at the heatmap of results below, we can see that when\nretrieving multiple needles GPT-4 consistently retrieves needles towards the\nend while ignoring needles at the beginning, similar to the single needle\nstudies.\n\nBelow we'll walk through benchmark usage and discuss results on GPT-4.\n\n## Usage\n\nTo perform a Multi-Needle + Reasoning evaluation, a user only needs three\nthings: (1) A question that requires multiple needles to answer, (2) an answer\nderived from the needles, and (3) list of needles to be inserted into the\ncontext.\n\nWe extended Greg Kamradt's LLMTest_NeedleInAHaystack repo to support multi-\nneedle evaluation and LangSmith as an valuator. Using LangSmith for\nevaluation, we create a LangSmith eval set with items (1) question and (2)\nanswer above.\n\nAs an example, lets use this case study where the needle was a combination of\npizza ingredients. We create a new LangSmith eval set (here) named multi-\nneedle-eval-pizza-3 with our question and answer:\n\n    \n    \n    question: What are the secret ingredients needed to build the perfect pizza? answer: The secret ingredients needed to build the perfect pizza are figs, prosciutto, and goat cheese.\n\nQuestion, Answer pairs for LangSmith multi-needle-eval-pizza-3 eval set\n\nOnce we've created a dataset, we with few flags:\n\n  * document_depth_percent_min - the depth of the first needle. The remaining needles are inserted at roughly equally spaced intervals after the first.\n  * multi_needle - flag to run multi-needle evaluation\n  * needles - the full list of needles to inject into the context\n  * evaluator - choose langsmith\n  * eval_set - choose the eval set we created multi-needle-eval-pizza-3\n  * context_lengths_num_intervals - number of context lengths to test\n  * context_lengths_min (and max) - context length bounds to test\n\nWe can run this to execute the evaluation:\n\n    \n    \n    python main.py --evaluator langsmith --context_lengths_num_intervals 6 --document_depth_percent_min 5 --document_depth_percent_intervals 1 --provider openai --model_name \"gpt-4-0125-preview\" --multi_needle True --eval_set multi-needle-eval-pizza-3 --needles '[ \" Figs are one of the secret ingredients needed to build the perfect pizza. \", \" Prosciutto is one of the secret ingredients needed to build the perfect pizza. \", \" Goat cheese is one of the secret ingredients needed to build the perfect pizza. \"]' --context_lengths_min 1000 --context_lengths_max 120000\n\nCommand to run multi-needle evaluation using LangSmith\n\nThis will kick off a workflow below. It will insert the needles into the\nhaystack, prompt the LLM to generate a response to the question using the\ncontext with the inserted needles, and evaluate whether the generation\ncorrectly retrieved the needles using the ground truth answer and the logged\nneedles that were inserted.\n\nWorkflow for Multi-Needle + Reasoning evaluation\n\n## GPT-4 Retrieval Results\n\nTo test multi-needle retrieval for GPT-4, we built three LangSmith eval sets:\n\n  * multi-needle-eval-pizza-1 here - Insert a single needle\n  * multi-needle-eval-pizza-3 here - Insert three needles\n  * multi-needle-eval-pizza-10 here - Insert ten needles\n\nWe evaluate the ability of GPT4 (128k token context length) to retrieve 1, 3,\nor 10 needles in a single turn for small (1000 token) and large (120,000\ntoken) context lengths. All commands run are here. All resulting generations\nwith public links to LangSmith traces are here. Here is a summary figure of\nour results:\n\nThere are clear observations:\n\n  * Performance degrades at the number of needles increases from 1 to 10\n  * Performance degrades as the context increases from 1000 to 120,000 tokens\n\nTo explore and validate these results, we can drill into LangSmith traces:\nhere is one LangSmith trace where we inserted 10 needles. Here is the GPT-4\ngeneration:\n\n    \n    \n    The secret ingredients needed to build the perfect pizza include espresso-soaked dates, gorgonzola dolce, candied walnuts, and pear slices.\n\nGPT-4 generation for replicate 1 for 10 needles, 24,800 token context\n\nOnly four of the secret ingredients are in the generation. Based on the trace,\nwe verify that all 10 needles are in the context and we log the inserted\nneedle order:\n\n    \n    \n    * Figs * Prosciutto * Smoked applewood bacon * Lemon * Goat cheese * Truffle honey * Pear slices * Espresso-soaked dates * Gorgonzola dolce * Candied walnuts\n\nOrder of the 10 needles placed in the context\n\nFrom this we can confirm that the four secret ingredients in the generation\nare the last four needles placed in our context. This provokes an interesting\npoint about where retrieval fails. Greg's single needle analysis showed GPT-4\nretrieval failure when the needle is place towards the start of the document.\n\nBecause we log the placement of each needle, we can explore this too: the\nbelow heatmap shows 10 needle retrieval with respect to context length. Each\ncolumn is a single experiment when we ask GPT-4 to retrieve 10 needles in the\ncontext.\n\nAs the context length grows, we also see retrieval failure towards the start\nof the document. The effect appears to start earlier in the multi-needle case\n(around 25k tokens) than the single needle case (which started around 73k\ntokens for GPT-4).\n\n## GPT-4 Retrieval & Reasoning\n\nRAG is often focused on retrieving multiple facts (from an indexed corpus of\ndocuments) and then reasoning over them. To test this, we build 3 datasets\nthat build on the above by asking for the first letter of all secret\ningredients. This requires retrieval of ingredients and reasoning about them\nto answer the question.\n\n  * multi-needle-eval-pizza-reasoning-1 - here\n  * multi-needle-eval-pizza-reasoning-3 - here\n  * multi-needle-eval-pizza-reasoning-10- here\n\nNote that this is an extremely simple form of reasoning. For future\nbenchmarks, we want to include different levels of reasoning.\n\nWe compared the fraction of correct answers for 3 replicates between retrieval\nand retrieval + reasoning. All data with traces is here. Retrieval and\nreasoning both degrade as the context length increases, reasoning lags\nretrieval. This suggests that retrieval may set an upper bound on reasoning\nperformance, as expected.\n\n## Conclusion\n\nThe emergence of long context LLMs is extremely promising. In order to use\nthem with or in place of external retrieval systems, it is critical to\nunderstand their limitations. The Multi-Needle + Reasoning benchmark can\ncharacterize the performance of long context retrieval relative to using a\ntraditional RAG approach.\n\nWe can draw a few general insights, but further testing is needed:\n\n  * No retrieval guarantees - Multiple facts are not guaranteed to be retrieved, especially as the number of needles and context size increases.\n  * Different patterns of retrieval failure - GPT-4 fails to retrieve needles towards the start of documents as context length increases.\n  * Prompting matters - Following insights mentioned here and here, specific prompt formulations may be needed to improve recall with certain LLMs.\n  * Retrieval vs reasoning - Performance degrades when the LLM is asked to reason about the retrieved facts.\n\n### Join our newsletter\n\nUpdates from the LangChain team and community\n\n\u00a9 LangChain Blog 2024\n\n", "frontpage": false}
