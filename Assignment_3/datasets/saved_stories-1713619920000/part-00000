{"aid": "40095523", "title": "Cameras and Lenses (2020)", "url": "https://ciechanow.ski/cameras-and-lenses/", "domain": "ciechanow.ski", "votes": 2, "user": "jusgu", "posted_at": "2024-04-20 07:30:08", "comments": 0, "source_title": "Cameras and Lenses \u2013 Bartosz Ciechanowski", "source_text": "Cameras and Lenses \u2013 Bartosz Ciechanowski\n\nBartosz Ciechanowski\n\nBlog Archives\n\nPatreon\n\nX / Twitter\n\nInstagram\n\ne-mail\n\nRSS\n\nDecember 7, 2020\n\n# Cameras and Lenses\n\nPictures have always been a meaningful part of the human experience. From the\nfirst cave drawings, to sketches and paintings, to modern photography, we\u2019ve\nmastered the art of recording what we see.\n\nCameras and the lenses inside them may seem a little mystifying. In this blog\npost I\u2019d like to explain not only how they work, but also how adjusting a few\ntunable parameters can produce fairly different results:\n\nOver the course of this article we\u2019ll build a simple camera from first\nprinciples. Our first steps will be very modest \u2013 we\u2019ll simply try to take any\npicture. To do that we need to have a sensor capable of detecting and\nmeasuring light that shines onto it.\n\n# Recording Light\n\nBefore the dawn of the digital era, photographs were taken on a piece of film\ncovered in crystals of silver halide. Those compounds are light-sensitive and\nwhen exposed to light they form a speck of metallic silver that can later be\ndeveloped with further chemical processes.\n\nFor better or for worse, I\u2019m not going to discuss analog devices \u2013 these days\nmost cameras are digital. Before we continue the discussion relating to light\nwe\u2019ll use the classic trick of turning the illumination off. Don\u2019t worry\nthough, we\u2019re not going to stay in darkness for too long.\n\nThe image sensor of a digital camera consists of a grid of photodetectors. A\nphotodetector converts photons into electric current that can be measured \u2013\nthe more photons hitting the detector the higher the signal.\n\nIn the demonstration below you can observe how photons fall onto the\narrangement of detectors represented by small squares. After some processing,\nthe value read by each detector is converted to the brightness of the\nresulting image pixels which you can see on the right side. I\u2019m also\nsymbolically showing which photosite was hit with a short highlight. The\nslider below controls the flow of time:\n\nThe longer the time of collection of photons the more of them are hitting the\ndetectors and the brighter the resulting pixels in the image. When we don\u2019t\ngather enough photons the image is underexposed, but if we allow the photon\ncollection to run for too long the image will be overexposed.\n\nWhile the photons have the \u201ccolor\u201d of their wavelength, the photodetectors\ndon\u2019t see that hue \u2013 they only measure the total intensity which results in a\nblack and white image. To record the color information we need to separate the\nincoming photons into distinct groups. We can put tiny color filters on top of\nthe detectors so that they will only accept, more or less, red, green, or blue\nlight:\n\nThis color filter array can be arranged in many different formations. One of\nthe simplest is a Bayer filter which uses one red, one blue, and two green\nfilters arranged in a 2x2 grid:\n\nA Bayer filter uses two green filters because light in green part of the\nspectrum heavily correlates with perceived brightness. If we now repeat this\npattern across the entire sensor we\u2019re able to collect color information. For\nthe next demo we will also double the resolution to an astonishing 1 kilopixel\narranged in a 32x32 grid:\n\nNote that the individual sensors themselves still only see the intensity, and\nnot the color, but knowing the arrangement of the filters we can recreate the\ncolored intensity of each sensor, as shown on the right side of the\nsimulation.\n\nThe final step of obtaining a normal image is called demosaicing. During\ndemosaicing we want to reconstruct the full color information by filling in\nthe gaps in the captured RGB values. One of the simplest way to do it is to\njust linearly interpolate the values between the existing neighbors. I\u2019m not\ngoing to focus on the details of many other available demosaicing algorithms\nand I\u2019ll just present the resulting image created by the process:\n\nNotice that yet again the overall brightness of the image depends on the\nlength of time for which we let the photons through. That duration is known as\nshutter speed or exposure time. For most of this presentation I will ignore\nthe time component and we will simply assume that the shutter speed has been\nset just right so that the image is well exposed.\n\nThe examples we\u2019ve discussed so far were very convenient \u2013 we were surrounded\nby complete darkness with the photons neatly hitting the pixels to form a\ncoherent image. Unfortunately, we can\u2019t count on the photon paths to be as\nfavorable in real environments, so let\u2019s see how the sensor performs in more\nrealistic scenarios.\n\nOver the course of this article we will be taking pictures of this simple\nscene. The almost white background of this website is also a part of the\nscenery \u2013 it represents a bright overcast sky. You can drag around the demo to\nsee it from other directions:\n\nLet\u2019s try to see what sort of picture would be taken by a sensor that is\nplaced near the objects without any enclosure. I\u2019ll also significantly\nincrease the sensor\u2019s resolution to make the pixels of the final image align\nwith the pixels of your display. In the demonstration below the left side\nrepresents a view of the scene with the small greenish sensor present, while\nthe right one shows the taken picture:\n\nThis is not a mistake. As you can see, the obtained image doesn\u2019t really\nresemble anything. To understand why this happens let\u2019s first look at the\nlight radiated from the scene.\n\nIf you had a chance to explore how surfaces reflect light, you may recall that\nmost matte surfaces scatter the incoming light in every direction. While I\u2019m\nonly showing a few examples, every point on every surface of this scene\nreflects the photons it receives from the whiteish background light source all\naround itself:\n\nThe red sphere ends up radiating red light, the green sphere radiates green\nlight, and the gray checkerboard floor reflects white light of lesser\nintensity. Most importantly, however, the light emitted from the background is\nalso visible to the sensor.\n\nThe problem with our current approach to taking pictures is that every pixel\nof the sensor is exposed to the entire environment. Light radiated from every\npoint of the scene and the white background hits every point of the sensor. In\nthe simulation below you can witness how light from different directions hits\none point on the surface of the sensor:\n\nClearly, to obtain a discernible image we have to limit the range of\ndirections that affect a given pixel on the sensor. With that in mind, let\u2019s\nput the sensor in a box that has a small hole in it. The first slider controls\nthe diameter of the hole, while the second one controls the distance between\nthe opening and the sensor:\n\nWhile not shown here, the inner sides of the walls are all black so that no\nlight is reflected inside the box. I also put the sensor on the back wall so\nthat the light from the hole shines onto it. We\u2019ve just built a pinhole\ncamera, let\u2019s see how it performs. Observe what happens to the taken image as\nwe tweak the diameter of the hole with the first slider, or change the\ndistance between the opening and the sensor with the second one:\n\nThere are so many interesting things happening here! The most pronounced\neffect is that the image is inverted. To understand why this happens let\u2019s\nlook at the schematic view of the scene that shows the light rays radiated\nfrom the objects, going through the hole, and hitting the sensor:\n\nAs you can see the rays cross over in the hole and the formed image is a\nhorizontal and a vertical reflection of the actual scene. Those two flips end\nup forming a 180\u00b0 rotation. Since rotated images aren\u2019t convenient to look at,\nall cameras automatically rotate the image for presentation and for the rest\nof this article I will do so as well.\n\nWhen we change the distance between the hole and the sensor the viewing angle\nchanges drastically. If we trace the rays falling on the corner pixels of the\nsensor we can see that they define the extent of the visible section of the\nscene:\n\nRays of light coming from outside of that shape still go through the pinhole,\nbut they land outside of the sensor and aren\u2019t recorded. As the hole moves\nfurther away from the sensor, the angle, and thus the field of view visible to\nthe sensor gets smaller. We can see this in a top-down view of the camera:\n\nCoincidentally, this diagram also helps us explain two other effects. Firstly,\nin the photograph the red sphere looks almost as big as the green one, even\nthough the scene view shows the latter is much larger. However, both spheres\nend up occupying roughly the same span on the sensor and their size in the\npicture is similar. It\u2019s also worth noting that the spheres seem to grow when\nthe field of view gets narrower because their light covers larger part of the\nsensor.\n\nSecondly, notice that different pixels of the sensor have different distance\nand relative orientation to the hole. The pixels right in the center of the\nsensor see the pinhole straight on, but pixels positioned at an angle to the\nmain axis see a distorted pinhole that is further away. The ellipse in the\nbottom right corner of the demonstration below shows how a pixel positioned at\nthe blue point sees the pinhole:\n\nThis change in the visible area of the hole causes the darkening we see in the\ncorners of the photograph. The value of the cosine of the angle I\u2019ve marked\nwith a yellow color is quite important as it contributes to the reduction of\nvisible light in four different ways:\n\n  * Two cosine factors from the increased distance to the hole, it\u2019s essentially the inverse square law\n  * A cosine factor from the side squeeze of the circular hole seen at an angle\n  * A cosine factor from the relative tilt of the receptor\n\nThese four factors conspire together to reduce the illumination by a factor of\ncos^4(\u03b1) in what is known as cosine-fourth-power law, also described as\nnatural vignetting.\n\nSince we know the relative geometry of the camera and the opening we can\ncorrect for this effect by simply dividing by the falloff factor and from this\npoint on I will make sure that the images don\u2019t have darkened corners.\n\nThe final effect we can observe is that when the hole gets smaller the image\ngets sharper. Let\u2019s see how the light radiated from two points of the scene\nends up going through the camera depending on the diameter of the pinhole:\n\nWe can already see that larger hole size ends up creating a bigger spread on\nthe sensor. Let\u2019s see this situation up close on a simple grid of detecting\ncells. Notice what happens to the size of the final circle hitting the sensor\nas that diameter of the hole changes:\n\nWhen the hole is small enough rays from the source only manage to hit one\npixel on the sensor. However, at larger radii the light spreads onto other\npixels and a tiny point in the scene is no longer represented by a single\npixel causing the image to no longer be sharp.\n\nIt\u2019s worth pointing out that sharpness is ultimately arbitrary \u2013 it depends on\nthe size at which the final image is seen, viewing conditions, and visual\nacuity of the observer. The same photograph that looks sharp on a postage\nstamp may in fact be very blurry when seen on a big display.\n\nBy reducing the size of the cone of light we can make sure that the source\nlight affects a limited number of pixels. Here, however, lays the problem. The\nsensor we\u2019ve been using so far has been an idealized detector capable of\nflawless adjustment of its sensitivity to the lighting conditions. If we\ninstead were to fix the sensor sensitivity adjustment, the captured image\nwould look more like this:\n\nAs the relative size of the hole visible to the pixels of the sensor gets\nsmaller, be it due to reduced diameter or increased distance, fewer photons\nhit the surface and the image gets dimmer.\n\nTo increase the number of photons we capture we could extend the duration of\ncollection, but increasing the exposure time comes with its own problems \u2013 if\nthe photographed object moves or the camera isn\u2019t held steady we risk\nintroducing some motion blur.\n\nAlternatively, we could increase the sensitivity of the sensor which is\ndescribed using the ISO rating. However, boosting the ISO may introduce a\nhigher level of noise. Even with these problems solved an actual image\nobtained by smaller and smaller holes would actually start getting blurry\nagain due to diffraction effects of light.\n\nIf you recall how diffuse surfaces reflect light you may also realize how\nincredibly inefficient a pinhole camera is. A single point on the surface of\nan object radiates light into its surrounding hemisphere, however, the pinhole\ncaptures only a tiny portion of that light.\n\nMore importantly, however, a pinhole camera gives us minimal artistic control\nover which parts of the picture are blurry. In the demonstration below you can\nwitness how changing which object is in focus heavily affects what is the\nprimary target of attention of the photograph:\n\nLet\u2019s try to build an optical device that would solve both of these problems:\nwe want to find a way to harness a bigger part of the energy radiated by the\nobjects and also control what is blurry and how blurry it is. For the objects\nin the scene that are supposed to be sharp we want to collect a big chunk of\ntheir light and make it converge to the smallest possible point. In essence,\nwe\u2019re looking for an instrument that will do something like this:\n\nWe could then put the sensor at the focus point and obtain a sharp image.\nNaturally, the contraption we\u2019ll try to create has to be transparent so that\nthe light can pass through it and get to the sensor, so let\u2019s begin the\ninvestigation by looking at a piece of glass.\n\n# Glass\n\nIn the demonstration below I put a red stick behind a pane of glass. You can\nadjust the thickness of this pane with the gray slider below:\n\nWhen you look at the stick through the surface of a thick glass straight on,\neverything looks normal. However, as your viewing direction changes the stick\nseen through the glass seems out of place. The thicker the glass and the\nsteeper the viewing angle the bigger the offset.\n\nLet\u2019s focus on one point on the surface of the stick and see how the rays of\nlight radiated from its surface propagate through the subsection of the glass.\nThe red slider controls the position of the source and the gray slider\ncontrols the thickness. You can drag the demo around to see it from different\nviewpoints:\n\nFor some reason the rays passing through glass at an angle are deflected off\ntheir paths. The change of direction happens whenever the ray enters or leaves\nthe glass.\n\nTo understand why the light changes direction we have to peek under the covers\nof classical electromagnetism and talk a bit more about waves.\n\n# Waves\n\nIt\u2019s impossible to talk about wave propagation without involving the time\ncomponent, so the simulations in this section are animated \u2013 you can play and\npause them by clickingtapping on the button in their bottom left corner.\n\nBy default all animations are enabled, but if you find them distracting, or if\nyou want to save power, you can globally pause all the following\ndemonstrations.disabled, but if you\u2019d prefer to have things moving as you read\nyou can globally unpause them and see all the waves oscillating.\n\nLet\u2019s begin by introducing the simplest sinusoidal wave:\n\nA wave like this can be characterized by two components. Wavelength \u03bb is the\ndistance over which the shape of the wave repeats. Period T defines how much\ntime a full cycle takes.\n\nFrequency f, is just a reciprocal of period and it\u2019s more commonly used \u2013 it\ndefines how many waves per second have passed over some fixed point.\nWavelength and frequency define phase velocity v_p which describes how quickly\na point on a wave, e.g. a peak, moves:\n\nv_p = \u03bb \u00b7 f\n\nThe sinusoidal wave is the building block of a polarized electromagnetic plane\nwave. As the name implies electromagnetic radiation is an interplay of\noscillations of electric field E and magnetic field B:\n\nIn an electromagnetic wave the magnetic field is tied to the electric field so\nI\u2019m going to hide the former and just visualize the latter. Observe what\nhappens to the electric component of the field as it passes through a block of\nglass. I need to note that dimensions of wavelengths are not to scale:\n\nNotice that the wave remains continuous at the boundary and inside the glass\nthe frequency of the passing wave remains constant, However, the wavelength\nand thus the phase velocity are reduced \u2013 you can see it clearly from the\nside.\n\nThe microscopic reason for the phase velocity change is quite complicated, but\nit can be quantified using the index of refraction n, which is the ratio of\nthe speed of light c to the phase velocity v_p of lightwave in that medium:\n\nn = c / v_p\n\nThe higher the index of refraction the slower light propagates through the\nmedium. In the table below I\u2019ve presented a few different indices of\nrefraction for some materials:\n\nvacuum| 1.00  \n---|---  \nair| 1.0003  \nwater| 1.33  \nglass| 1.53  \ndiamond| 2.43  \n  \nLight traveling through air barely slows down, but in a diamond it\u2019s over\ntwice as slow. Now that we understand how index of refraction affects the\nwavelength in the glass, let\u2019s see what happens when we change the direction\nof the incoming wave:\n\nThe wave in the glass has a shorter wavelength, but it still has to match the\npositions of its peaks and valleys across the boundary. As such, the direction\nof propagation must change to ensure that continuity.\n\nI need to note that the previous two demonstrations presented a two\ndimensional wave since that allowed me to show the sinusoidal component\noscillating into the third dimension. In real world the lightwaves are three\ndimensional and I can\u2019t really visualize the sinusoidal component without\nusing the fourth dimension which has its own set of complications.\n\nThe alternative way of presenting waves is to use wavefronts. Wavefronts\nconnect the points of the same phase of the wave, e.g. all the peaks or\nvalleys. In two dimensions wavefronts are represented by lines:\n\nIn three dimensions the wavefronts are represented by surfaces. In the\ndemonstration below a single source emits a spherical wave, points of the same\nphase in the wave are represented by the moving shells:\n\nBy drawing lines that are perpendicular to the surface of the wavefront we\ncreate the familiar rays. In this interpretation rays simply show the local\ndirection of wave propagation which can be seen in this example of a section\nof a spherical 3D wave:\n\nI will continue to use the ray analogy to quantify the change in direction of\nlight passing through materials. The relation between the angle of incidence\n\u03b8_1 and angle of refraction \u03b8_2 can be formalized with the equation known as\nSnell\u2019s law:\n\nn_1 \u00b7 sin(\u03b8_1) = n_2 \u00b7 sin(\u03b8_2)\n\nIt describes how a ray of light changes direction relative to the surface\nnormal on the border between two different media. Let\u2019s see it in action:\n\nWhen traveling from a less to more refractive material the ray bends towards\nthe normal, but when the ray exits the object with higher index of refraction\nit bends away from the normal.\n\nNotice that in some configurations the refracted ray completely disappears,\nhowever, this doesn\u2019t paint a full picture because we\u2019re currently completely\nignoring reflections.\n\nAll transparent objects reflect some amount of light. You may have noticed\nthat reflection on a surface of a calm lake or even on the other side of the\nglass demonstration at the beginning of the previous section. The intensity of\nthat reflection depends on the index of refraction of the material and the\nangle of the incident ray. Here\u2019s a more realistic demonstration of how light\nwould get refracted and reflected between two media:\n\nThe relation between transmittance and reflectance is determined by Fresnel\nequations. Observe that the curious case of missing light that we saw\npreviously no longer occurs \u2013 that light is actually reflected. The transition\nfrom partial reflection and refraction to the complete reflection is\ncontinuous, but near the end it\u2019s very rapid and at some point the refraction\ncompletely disappears in the effect known as total internal reflection.\n\nThe critical angle at which the total internal reflection starts to happen\ndepends on the indices of refraction of the boundary materials. Since that\ncoefficient is low for air, but very high for diamond a proper cut of the\nfaces makes diamonds very shiny.\n\nWhile interesting on its own, reflection in glass isn\u2019t very relevant to our\ndiscussion and for the rest of this article we\u2019re not going to pay much\nattention to it. Instead, we\u2019ll simply assume that the materials we\u2019re using\nare covered with high quality anti-reflective coating.\n\n# Manipulating Rays\n\nLet\u2019s go back to the example that started the discussion of light and glass.\nWhen both sides of a piece of glass are parallel, the ray is shifted, but it\nstill travels in the same direction. Observe what happens to the ray when we\nchange the relative angle of the surfaces of the glass.\n\nWhen we make two surfaces of the glass not parallel we gain the ability to\nchange the direction of the rays. Recall, that we\u2019re trying to make the rays\nhitting the optical device converge at a certain point. To do that we have to\nbend the rays in the upper part down and, conversely, bend the rays in the\nlower part up.\n\nLet\u2019s see what happens if we shape the glass to have different angles between\nits walls at different height. In the demonstration below you can control how\nmany distinct segments a piece of glass is shaped to:\n\n3\n\n5\n\n7\n\n\u221e\n\nAs the number of segments approaches infinity we end up with a continuous\nsurface without any edges. If we look at the crossover point from the side you\nmay notice that we\u2019ve managed to converge the rays across one axis, but the\ntop-down view reveals that we\u2019re not done yet. To focus all the rays we need\nto replicate that smooth shape across all possible directions \u2013 we need\nrotational symmetry:\n\nWe\u2019ve created a convex thin lens. This lens is idealized, in the later part of\nthe article we\u2019ll discuss how real lenses aren\u2019t as perfect, but for now it\nwill serve us very well. Let\u2019s see what happens to the focus point when we\nchange the position of the red source:\n\nWhen the source is positioned very far away the incoming rays become parallel\nand after passing through lens they converge at a certain distance away from\nthe center. That distance is known as focal length.\n\nThe previous demonstration also shows two more general distances: s_o which is\nthe distance between the object, or source, and the lens, as well as s_i which\nis the distance between the image and the lens. These two values and the focal\nlength f are related by the thin lens equation:\n\n1 / s_o + 1 / s_i = 1 / f\n\nFocal length of a lens depends on both the index of refraction of the material\nfrom which the lens is made and its shape:\n\nNow that we understand how a simple convex lens works we\u2019re ready to mount it\ninto the hole of our camera. We will still control the distance between the\nsensor and the lens, but instead of controlling the diameter of the lens we\u2019ll\ninstead control its focal length:\n\nWhen you look at the lens from the side you may observe how the focal length\nchange is tied to the shape of the lens. Let\u2019s see how this new camera works\nin action:\n\nOnce again, a lot of things are going on here! Firstly, let\u2019s try to\nunderstand how the image is formed in the first place. The demonstration below\nshows paths of rays from two separate points in the scene. After going through\nthe lens they end up hitting the sensor:\n\nNaturally, this process happens for every single point in the scene which\ncreates the final image. Similarly to a pinhole a convex lens creates an\ninverted picture \u2013 I\u2019m still correcting for this by showing you a rotated\nphotograph.\n\nSecondly, notice that the distance between the lens and the sensor still\ncontrols the field of view. As a reminder, the focal length of a lens simply\ndefines the distance from the lens at which the rays coming from infinity\nconverge. To achieve a sharp image, the sensor has to be placed at the\nlocation where the rays focus and that\u2019s what\u2019s causing the field of view to\nchange.\n\nIn the demonstration below I\u2019ve visualized how rays from a very far object\nfocus through a lens of adjustable focal length, notice that to obtain a sharp\nimage we must change the distance between the lens and the sensor which in\nturn causes the field of view to change:\n\nIf we want to change the object on which a camera with a lens of a fixed focal\nlength is focused, we have to move the image plane closer or further away from\nthe lens which affects the angle of view. This effect is called focus\nbreathing:\n\nA lens with a fixed focal length like the one above is often called a prime\nlens, while lenses with adjustable focal length are called zoom lenses. While\nthe lenses in our eyes do dynamically adjust their focal lengths by changing\ntheir shape, rigid glass can\u2019t do that so zoom lenses use a system of multiple\nglass elements that change their relative position to achieve this effect.\n\nIn the simulation above notice the difference in sharpness between the red and\ngreen spheres. To understand why this happens let\u2019s analyze the rays emitted\nfrom two points on the surface of the spheres. In the demonstration below the\nright side shows the light seen by the sensor just from the two marked points\non the spheres:\n\nThe light from the point in focus converges to a point, while the light from\nan out-of-focus point spreads onto a circle. For larger objects the multitude\nof overlapping out-of-focus circles creates a smooth blur called bokeh. With\ntiny and bright light sources that circle itself is often visible, you may\nhave seen effects like the one in the demonstration below in some photographs\ncaptured in darker environments:\n\nNotice that the circular shape is visible for lights both in front of and\nbehind the focused distance. As the object is positioned closer or further\naway from the lens the image plane \u201cslices\u201d the cone of light at different\nlocation:\n\nThat circular spot is called a circle of confusion. While in many\ncircumstances the blurriness of the background or the foreground looks very\nappealing, it would be very useful to control how much blur there is.\n\nUnfortunately, we don\u2019t have total freedom here \u2013 we still want the primary\nphotographed object to remain in focus so its light has to converge to a\npoint. We just want to change the size of the circle of out-of-focus objects\nwithout moving the central point. We can accomplish that by changing the angle\nof the cone of light:\n\nThere are two methods we can use to modify that angle. Firstly, we can change\nthe focal length of the lens \u2013 you may recall that with longer focal lengths\nthe cone of light also gets longer. However, changing the focal length and\nkeeping the primary object in focus requires moving the image plane which in\nturn changes how the picture is framed.\n\nThe alternative way of reducing the angle of the cone of light is to simply\nignore some of the \u201couter\u201d rays. We can achieve that by introducing a stop\nwith a hole in the path of light:\n\nThis hole is called an aperture. In fact, even the hole in which the lens is\nmounted is an aperture of some sort, but what we\u2019re introducing is an\nadjustable aperture:\n\nLet\u2019s try to see how an aperture affects the photographs taken with our\ncamera:\n\nIn real camera lenses an adjustable aperture is often constructed from a set\nof overlapping blades that constitute an iris. The movement of those blades\nchanges the size of the aperture:\n\nThe shape of the aperture also defines the shape of bokeh. This is the reason\nwhy bokeh sometimes has a polygonal shape \u2013 it\u2019s simply the shape of the\n\u201ccone\u201d of light after passing through the blades of the aperture. Next time\nyou watch a movie pay a close attention to the shape of out-of-focus\nhighlights, they\u2019re often polygonal:\n\nAs the aperture diameter decreases, larger and larger areas of the\nphotographed scene remain sharp. The term depth of field is used to define the\nlength of the region over which the objects are acceptably sharp. When\ndescribing the depth of field we\u2019re trying to conceptually demark those two\nboundary planes and see how far apart they are from each other.\n\nLet\u2019s see the depth of field in action. The black slider controls the\naperture, the blue slider controls the focal length, and the red slider\nchanges the position of the object relative to the camera. The green dot shows\nthe place of perfect focus, while the dark blue dots show the limits, or the\ndepth, of positions between which the image of the red light source will be\nreasonably sharp, as shown by a single outlined pixel on the sensor:\n\nNotice that the larger the diameter of aperture and the shorter the focal\nlength the shorter the distance between the dark blue dots and thus the\nshallower the depth of field becomes. If you recall our discussion of\nsharpness this demonstration should make it easier to understand why reducing\nthe angle of the cone increases the depth of field.\n\nIf you don\u2019t have perfect vision you may have noticed that squinting your eyes\nmake you see things a little better. Your eyelids covering some part of your\niris simply act as an aperture that decreases the angle of the cone of light\nfalling into your eyes making things sightly less blurry on your retina.\n\nAn interesting observation is that aperture defines the diameter of the base\nof the captured cone of light that is emitted from the object. Twice as large\naperture diameter captures roughly four times more light due to increased\nsolid angle. In practice, the actual size of the aperture as seen from the\npoint of view of the scene, or the entrance pupil, depends on all the lenses\nin front of it as the shaped glass may scale the perceived size of the\naperture.\n\nOn the other hand, when a lens is focused correctly, the focal length defines\nhow large a source object is in the picture. By doubling the focal length we\ndouble the width and the height of the object on the sensor thus increasing\nthe area by the factor of four. The light from the source is more spread out\nand each individual pixel receives less light.\n\nThe total amount of light hitting each pixel is proportional to the ratio\nbetween the focal length f and the diameter of the entrance pupil D. This\nratio is known as the f-number:\n\nN = f / D\n\nA lens with a focal length of 50 mm and the entrance pupil of 25 mm would have\nN equal to 2 and the f-number would be known as f/2. Since the amount of light\ngetting to each pixel of the sensor increases with the diameter of the\naperture and decreases with the focal length, the f-number controls the\nbrightness of the projected image.\n\nThe f-number with which commercial lenses are marked usually defines the\nmaximum aperture a lens can achieve and the smaller the f-number the more\nlight the lens passes through. Bigger amount of incoming light allows\nreduction of exposure time, so the smaller the f-number the faster the lens\nis. By reducing the size of the aperture we can modify the f-number with which\na picture is taken.\n\nThe f-numbers are often multiples of 1.4 which is an approximation of 2.\nScaling the diameter of an adjustable aperture by 2 scales its area by 2 which\nis a convenient factor to use. Increasing the f-number by a so-called stop\nhalves the amount of received light. The demonstration below shows the\nrelatives sizes of the aperture through which light is being seen:\n\nf/1.4\n\nf/2\n\nf/2.8\n\nf/4\n\nf/5.6\n\nf/8\n\nTo maintain the overall brightness of the image when stopping down we\u2019d have\nto either increase the exposure time or the sensitivity of the sensor.\n\nWhile aperture settings let us easily control the depth of field, that change\ncomes at a cost. When the f-number increases and the aperture diameter gets\nsmaller we effectively start approaching a pinhole camera with all its related\ncomplications.\n\nIn the final part of this article we will discuss the entire spectrum of\nanother class of problems that we\u2019ve been conveniently avoiding all this time.\n\n# Aberrations\n\nIn our examples so far we\u2019ve been using a perfect idealized lens that did\nexactly what we want and in all the demonstrations I\u2019ve relied on a certain\nsimplification known as the paraxial approximation. However, the physical\nworld is a bit more complicated.\n\nThe most common types of lenses are spherical lenses \u2013 their curved surfaces\nare sections of spheres of different radii. These types of lenses are easier\nto manufacture, however, they actually don\u2019t perfectly converge the rays of\nincoming light. In the demonstration below you can observe how fuzzy the focus\npoint is for various lens radii:\n\nThis imperfection is known as spherical aberration. This specific flaw can be\ncorrected with aspheric lenses, but unfortunately there are other types of\nproblems that may not be easily solved by a single lens. In general, for\nmonochromatic light there are five primary types of aberrations: spherical\naberration, coma, astigmatism, field curvature, and distortion.\n\nWe\u2019re still not out of the woods even if we manage to minimize these problems.\nIn normal environments light is very non-monochromatic and nature sets another\nhurdle into optical system design. Let\u2019s quickly go back to the dark\nenvironment as we\u2019ll be discussing a single beam of white light.\n\nObserve what happens to that beam when it hits a piece of glass. You can make\nthe sides non-parallel by using the slider:\n\nWhat we perceive as white light is a combination of lights of different\nwavelengths. In fact, the index of refraction of materials depends on the\nwavelength of the light. This phenomena called dispersion splits what seems to\nbe a uniform beam of white light into a fan of color bands. The very same\nmechanism that we see here is also responsible for a rainbow.\n\nIn a lens this causes different wavelengths of light to focus at different\noffsets \u2013 the effect known as chromatic aberration. We can easily visualize\nthe axial chromatic aberration even on a lens with spherical aberration fixed.\nI\u2019ll only use red, green, and blue dispersed rays to make things less crowded,\nbut remember that other colors of the spectrum are present in between. Using\nthe slider you can control the amount of dispersion the lens material\nintroduces:\n\nChromatic aberration may be corrected with an achromatic lens, usually in the\nform of a doublet with two different types of glass fused together.\n\nTo minimize the impact of the aberrations, camera lenses use more than one\noptical element on their pathways. In this article I\u2019ve only shown you simple\nlens systems, but a high-end camera lens may consist of a lot of elements that\nwere carefully designed to balance the optical performance, weight, and cost.\n\nWhile we, in our world of computer simulations on this website, can maintain\nthe illusion of simple and perfect systems devoid of aberrations, vignetting,\nand lens flares, real cameras and lenses have to deal with all these problems\nto make the final pictures look good.\n\n# Further Watching and Reading\n\nOver on YouTube Filmmaker IQ channel has a lot of great content related to\nlenses and movie making. Two videos especially fitting here are The History\nand Science of Lenses and Focusing on Depth of Field and Lens Equivalents.\n\nWhat Makes Cinema Lenses So Special!? on Potato Jet channel is a great\ninterview with Art Adams from ARRI. The video goes over many interesting\ndetails of high-end cinema lens design, for example, how the lenses compensate\nfor focus breathing, or how much attention is paid to the quality of bokeh.\n\nFor a deeper dive on bokeh itself Jakub Tr\u00e1vn\u00edk\u2019s On Bokeh is a great article\non the subject. The author explains how aberrations may cause bokeh of non\nuniform intensity and shows many photographs of real cameras and lenses.\n\nIn this article I\u2019ve mostly been using geometrical optics with some soft\ntouches of electromagnetism. For a more modern look at the nature of light and\nits interaction with matter I recommend Richard Feynman\u2019s QED: The Strange\nTheory of Light and Matter. The book is written in a very approachable style\nsuited for general audience, but it still lets Feynman\u2019s wits and brilliance\nshine right through.\n\n# Final Words\n\nWe\u2019ve barely scratched the surface of optics and camera lens design, but even\nthe most complex systems end up serving the same purpose: to tell light where\nto go. In some sense optical engineering is all about taming the nature of\nlight.\n\nThe simple act of pressing the shutter button in a camera app on a smartphone\nor on the body of a high-end DSLR is effortless, but it\u2019s at this moment when,\nthrough carefully guided rays hitting an array of photodetectors, we\nimmortalize reality by painting with light.\n\nIf you enjoy these articles, consider supporting on Patreon.\n\nCopyright \u00a9 2024 Bartosz Ciechanowski\n\n", "frontpage": false}
