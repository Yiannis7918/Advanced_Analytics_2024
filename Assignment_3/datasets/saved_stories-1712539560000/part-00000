{"aid": "39963094", "title": "Using Metrics to Measure Individual Developer Performance", "url": "https://lauratacho.com/blog/using-metrics-to-measure-individual-developer-performance", "domain": "lauratacho.com", "votes": 1, "user": "kiyanwang", "posted_at": "2024-04-07 19:25:45", "comments": 0, "source_title": "Using Metrics to Measure Individual Developer Performance", "source_text": "Using Metrics to Measure Individual Developer Performance \u2014 Laura Tacho\n\n0\n\nSkip to Content\n\nGET IN TOUCH\n\nGET IN TOUCH\n\n# Using Metrics to Measure Individual Developer Performance\n\nFeb 28\n\nWritten By Laura Tacho\n\n\u201cWhat metrics should leaders use to measure the individual performance of\ndevelopers on their teams?\u201d\n\nI get asked this question a lot. I\u2019ve asked myself this question before, too \u2013\nboth as a developer and then later as a leader.\n\nA lot of research and \u201cbest practice\u201d will tell us that metrics like lines of\ncode, story points completed, or deployment frequency are not appropriate to\nmeasure individual performance. This is true. These metrics came out of\nresearch and studies to measure different things, like devops maturity,\nsoftware delivery capability, and overall delivery performance. Applying them\nto individuals is unfair at best.\n\nBut here\u2019s the question we don\u2019t talk about enough:\n\n\u201cWhat data are you going to use to evaluate my performance?\u201d\n\nThis is the question that really counts \u2014 coming directly from your team\nmembers. There\u2019s a better answer than \u201cbest practice says not to use metrics,\u201d\nwhich doesn\u2019t spark a lot of trust in a fair performance review.\n\nSo if not metrics like PRs and commits, then what? I\u2019ll break down arguments\nagainst using common metrics to measure performance of individuals, and then\nwalk you through how I approach developing an evidence-based performance\nmanagement system.\n\n#### Software is written by teams, not individuals\n\nEven if a feature or component is owned by an individual, it\u2019s likely that\ntheir code depends on systems written by others. This is the main reason why\napplying team-level metrics to an individual is unfair. An individual cannot\nfully control their performance within a system where they are just one\ncontributor. If the metrics aren\u2019t satisfactory, they need to be addressed on\na team or system level, not an individual level.\n\nAdditionally, team practices can vary widely. Lines of code, story points\ncompleted, and deployment frequency are examples of team or system performance\nmetrics that are sometimes applied to individuals. And here\u2019s another layer of\ntrickiness: how can you fairly and effectively measure individual performance\nwhen teams have different estimation practices? For example, what might be a\n3-point story to one team is a 5-point story to another. So is the individual\non the team who calls it a 5-point story a better performer, because they\nclose down more points?\n\nAlong with this, we often measure the wrong metrics altogether; that is,\nmetrics that we think will give us a strong indication of performance, but\nhave low correlation to impact and outcomes. Abi Noda, CEO and co-founder of\nDX, talks about the \u201cFlawed Five\u201d metrics that will lead you astray, both on a\nteam level but especially on an individual level: The elusive quest to measure\ndeveloper productivity - GitHub Universe 2019\n\n#### Wrong metrics mean losing trust\n\nBut is it harmful to use these metrics in order to get a \u201cclose enough\u201d\nunderstanding of individual performance?\n\nYes.\n\nA fast way to reassure your team that you don\u2019t understand their job is to\npick what they perceive as arbitrary and unfair metrics to measure their\nperformance:\n\n> A friend\u2019s org (at bigco) has decided to track these metrics to track IC\n> productivity:\n>\n> \\- number of jira tickets closed - number of changes pushed - number of\n> comments on code review\n>\n> I predict a round of promotions for some very simple python scripts\n>\n> \u2014 Adam Leventhal (@ahl) January 24, 2021\n\nAside from losing trust in you, your team will be concerned about the\nimplications of these metrics. If I pair with someone and therefore don\u2019t\ncommit the code myself, am I penalised? What if I\u2019m working on a design\ndocument or coordinating a release? Responsibilities of developers often go\nbeyond data that can be scraped from GitHub. You don\u2019t want metrics that\nencourage the wrong behaviours.\n\n#### Gaming the system?\n\nDo you know about New York subway dogs? The NY metro banned dogs from the\nsubway, unless they can fit in a bag. It\u2019s not hard to imagine what happened\nnext.\n\n> MTA only allows dogs on Subway trains if they fit in bags. \ud83d\udc36\ud83d\ude00\ud83d\ude00\ud83d\ude00\n> pic.twitter.com/sIIBBc9CmD\n>\n> \u2014 Tamara Torres (@tamaratorresnyc) September 16, 2019\n\nHumans are wired to maximise incentive, and also we are pretty creative. I\u2019m\nnot suggesting that your teams will intentionally start to game the system\nwhen it comes to improving these metrics. But it might happen, both as a\nfunction of self-preservation, but also because you picked the wrong metrics\nto begin with.\n\nGoodhart\u2019s law states that when a measure becomes a target, it ceases to be a\ngood measure. If you measure a factory based on weight output, expect heavy\nproducts. If you measure it based on the number of items produced, expect\ntiny, tiny products. A practical example: if the number of commits is a target\nfor individual performance, expect to see some very dirty git histories. You\u2019d\nprobably do the same thing.\n\nAnother danger comes from metrics that encourage behaviours that have a\nnegative impact on your business. They punish desired behaviour while\nincentivising damaging behaviour.\n\n  * Code coverage is a target, so development hours are spent on writing more tests, but the Change Failure Rate stays the same\n\n  * More story points are pushed out, but maintainability suffers\n\n  * Your team hits aggressive deadlines, but 1\u20443 of the team resigns within 2 months\n\n  * More PRs are closed, but you\u2019re not acquiring new customers\n\nIn this case, not only can your team lose trust in your leadership\ncapabilities, but it\u2019s likely that your own leadership team will lose trust in\nyour judgement, as well.\n\n#### Evidence doesn\u2019t have to mean activity data\n\nSo, what to do instead?\n\nIt\u2019s reasonable for an individual contributor to ask about the metrics which\nwill be used to evaluate their performance. And it is important to have a\ntransparent answer to this question. But, it doesn\u2019t have to involve activity\ndata from tools like GitHub and JIRA alone. They may tell one part of the\nstory, but it\u2019s unlikely that activity data alone can give you a clear picture\nof performance across all competencies that you expect from your team.\n\nEvidence doesn\u2019t have to mean activity data.\n\nInstead of looking for a list of metrics to determine how you measure\nperformance, figure out how you want to measure performance and then find\nmetrics that help you measure the stuff that\u2019s important to your company.\nWhile engineering roles do share common traits and objectives across\ncompanies, there\u2019s not really a one-size-fits-all approach that will\ndefinitely fit your company\u2019s objectives.\n\n#### Work backwards\n\nTime for a practical example. Here\u2019s a job posting for a Senior Ruby on Rails\nEngineer at Treatwell.\n\n(Side note \u2013 a list of over 1300 of companies that are still hiring.)\n\nLooking at the responsibilities listed in the job description, I\u2019ll work\nthrough how I arrive at a list of metrics, and other sources of evidence, to\nevaluate performance.\n\n  * You\u2019ll work as part of a cross-functional squad, collaborating to deliver incremental, meaningful changes to our customers.\n\nMost software engineering roles have this type of delivery objective as part\nof its core performance expectations, but each role has different expectations\nof what\u2019s being delivered. First, we need to break this down into smaller\nobjectives that are more easily supported with evidence. Since this is both\nthe top responsibility, and also the most complex one, I\u2019ll spend more time\nbreaking it down. I\u2019m going to focus on some keywords here:\n\n  * Cross-functional, collaborating: working as part of a cross-functional delivery team implies that this role is responsible for more than just writing code; they are responsible for making decisions and delivering business results as an equal partner to product management and design.\n\n  * Incremental: The team should deliver small changes at a rapid pace. Given the cross-functional nature of the team, competencies like estimation and prioritisation are just as important as pure execution skills.\n\n  * Meaningful: Simply put, the software should perform its business function. This aligns closely to the Performance (P) category in the SPACE framework, which covers criteria like user adoption but also quality and stability.\n\nConsidering only output-based metrics from GitHub and JIRA just isn\u2019t\nappropriate for the full scope of this role.\n\nSo instead, my rough list might start to look like this:\n\n  * Project on-time delivery, measured by % of projects that are delivered +/- 1 week of forecasted deadline\n\n  * Satisfaction with engineering partnership, measured by feedback from cross-functional partners.\n\n  * Quality and reliability, measured by incident, bugs, or even customer support ticket volume.\n\n  * Business performance of features, measured by user adoption and other team-defined usage metrics.\n\nWhat I\u2019m not measuring is also important.\n\nI\u2019ve made a choice here not to directly measure things like PR count, commits,\nor even story points, though delivery is part of the role. What this role\ndescription emphasised is value delivered to the user. If the value is not\nthere, I might debug why that is with specific activity metrics. I might also\nlook into them if I receive peer feedback that this person is not able to keep\nup with the pace of development.\n\nFor the other areas in the role description, I\u2019ll go through the same process.\nMy brief notes below:\n\n  * You\u2019ll help your team in designing the system architecture for large scale applications.\n\n    * Participation in architecture decisions, with consideration for the number of decisions with direct responsibility. This would be a sum measurement, where I just count the number of times it happened.\n\n    * Outcomes of these decisions, measured by quality of software and ability to deliver on-time (at this point, we start seeing the interconnectedness of some of these performance criteria)\n\n    * Communication and collaboration, measured by feedback from the engineering team as well as cross-functional partners.\n\n  * You\u2019ll support and mentor junior team members, helping them create well thought out and robust solutions.\n\n    * Quality of junior team members\u2019 outcomes, measured by quality of software as outlined above, but filtering by projects where this person played a large role in mentoring and guiding junior team members\n\n    * Satisfaction with learning opportunities, measured by gathering feedback from junior team members\n\n  * You\u2019ll help your team identify opportunities to improve their ability to deliver all kinds of changes to their users.\n\n    * Leadership and participation in retros, post-mortems, and other continuous improvement processes, measured by instances of participation.\n\n  * You\u2019ll help with the running and maintenance of your team's applications in production.\n\n    * Operational stability, measured by the quality metrics mentioned above, and also other appropriate team-defined metrics.\n\nThis list is already getting a bit long, and this doesn\u2019t include evidence\nfrom my own observations yet.\n\nWith so many objectives and sources of information, it\u2019s likely that some\nperformance cycles won\u2019t touch on every single one. That\u2019s fine \u2013 as long as\nyou plan for it, and make expectations clear about what happens when that\u2019s\nnot the case. Some things might be fine to drop off (like architectural\nleadership, if there were no large architecture projects during the evaluation\nperiod) but not operational stability or project on-time delivery.\n\n#### Metrics for senior vs. junior roles\n\nThe more senior a role is, like the one used in the example above, the more\nlikely it is that the role\u2019s responsibilities focus on strategic outcomes\nrather than task output.\n\nWhereas a junior engineer will have duties on the task level, a staff engineer\nis responsible for building systems of software that enable other teams to\nexecute effectively. It may be perfectly reasonable to look at task-level\nmetrics for a junior engineer, but not for that staff engineer.\n\nTreatwell doesn\u2019t have a published career ladder that I can reference here,\nbut chances are that you\u2019ll be looking at a career ladder along side a job\nexpectations document (and if not, you can find a lot of them for reference on\nprogression.fyi).\n\nThe next step with these metrics would be to double-check that they\u2019re also\naligned to the role\u2019s seniority and scope. Sometimes it can be the case where\njob descriptions, career ladders, and performance management processes don\u2019t\nactually align to each other, but they should all be reinforcing the same\nthings.\n\nFor example, looking at Medium\u2019s career ladder for the mobile and web\nengineering tracks, we see a big difference in scope between criteria for\nthose on Track 1 vs Track 4.\n\nTrack 1 examples\n\n  * Delivers features requiring simple local modifications\n\n  * Adds simple actions that call server endpoints\n\n  * Uses CSS appropriately, following style guide\n\n  * Reuses existing components appropriately\n\nThe scope of Track 1 is at the task level, and working within well-defined\nsystems.\n\nBy Track 4, the problems are far beyond the task level.\n\n  * Makes architectural decisions that eliminate entire classes of bugs\n\n  * Designed and pioneered proto-based model storage\n\n  * Migrated Android persistance layer to reactive programming\n\nA senior role is responsible for managing whole projects and strategy, while a\njunior engineer is responsible for managing their tasks. As expectations\nchange, so should the metrics for evaluating performance.\n\nIf I were a Track 4 engineer, I would find it a bit silly/annoying if my\ncommit history was taken into consideration for my performance, as long as I\nwas hitting the objectives laid out in my role description.\n\n#### Using system-level metrics to measure performance of managers and senior\ntechnical leaders\n\nBut as a Track 4 engineer at Medium, I don\u2019t think it\u2019s unreasonable to use\nthe team\u2019s total number of bugs, or total amount of time spent on bugs, as a\nsuccess metric for my performance.\n\nAnd that takes me to one exception: using team metrics to measure individual\nperformance is generally unfair unless it\u2019s the explicit role of the\nindividual to influence the system performance metrics.\n\nUsually, this happens at a staff+ or management level. If your role\u2019s main\nobjective is to support teams by improving performance of CI/CD systems, and\nyou\u2019ve been given resources, time, and autonomy to do so, it\u2019s reasonable that\nmetrics like Change Failure Rate or Build Time would be used to evaluate how\neffective you\u2019ve performed your role.\n\n#### No shortcuts\n\nIf you\u2019ve read this far, hoping for a list of metrics that you can grab and\nstart using on your teams, you won\u2019t find one. There\u2019s no shortcut here, but\nyou can eliminate some trial and error by following these principles:\n\n  * Instead of looking for a list of metrics to determine how you measure performance, figure out how you want to measure performance and then find metrics that help you measure the stuff that\u2019s important to your company.\n\n  * Focus on outcomes, not output, but you might use output metrics like activity from GitHub and JIRA to debug why outcomes were missed.\n\n  * It might be appropriate to use team-level metrics to evaluate the performance of senior technical leaders and managers, depending on their scope of responsibility.\n\n  * Watch out for Goodhart\u2019s Law, or other cases where you metrics may encourage the wrong behaviour.\n\n  * Metrics are one thing, but not everything. You still need to do the hard work of active performance management, setting expectations, giving regular feedback, and supporting your team.\n\nLaura Tacho\n\nPrevious\n\nPrevious\n\n## 1 Tip to Help You Stay Out of \u201cSolution Mode\u201d During Coaching Conversations\n\nNext\n\nNext\n\n## Democratic Technical Decisions\n\nLaura Tacho Consulting\n\nlaura@lauratacho.com | Austria, Central European Time\n\nImpressum | Data and Privacy Policy\n\n", "frontpage": false}
