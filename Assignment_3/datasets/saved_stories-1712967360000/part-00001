{"aid": "40015953", "title": "I accidentally built a meme search engine", "url": "https://harper.blog/2024/04/12/i-accidentally-built-a-meme-search-engine/", "domain": "harper.blog", "votes": 1, "user": "EamonLeonard", "posted_at": "2024-04-12 18:13:54", "comments": 0, "source_title": "I accidentally built a meme search engine", "source_text": "I accidentally built a meme search engine | Harper Reed's Blog\n\n## Harper Reed's Blog\n\nMusing from a normal person doing normal things\n\n12 Apr 2024\n\n# I accidentally built a meme search engine\n\n## Or: how to learn about clip/siglip and vector encoding images\n\ntl;dr: I built a meme search engine using siglip/CLIP and vector encoding\nimages. It was fun and I learned a lot.\n\nI have been building a lot of applied AI tools for a while. One of the\ncomponents that always seemed the most magical has always been vector\nembeddings. Word2Vec and the like have straight blown my mind. It is like\nmagic.\n\nI saw a simple app on hacker news that was super impressive. Someone crawled a\nbunch of Tumblr images and used siglip to get the embeddings and then made a\nsimple \u201cclick the image and see similar images\u201d app. It was like magic. I had\nno idea how to achieve this, but it seemed accessible.\n\nI decided to use my sudden motivation as an opportunity to learn how \u201call this\nworks.\u201d\n\n## wut\n\nIf you have never ran into vector embeddings, clip/siglip, vector databases,\nand the like - never fear.\n\nBefore I saw the hack on hn I really didn\u2019t think much about vector\nembeddings, multi modal embeddings or vector datastores. I had used faiss\n(facebooks simple vector store), and Pinecone ($$) for some hacks, but didn\u2019t\nreally dig in. Just got it to work and then was like \u201cyep. Tests pass.\u201d\n\nI still barely know what vectors are. Lol. Before I dug in and built this, I\nreally didn\u2019t understand how I would use it outside of RAG or another LLM\nprocess.\n\nI learn by building. It helps if the results are really intriguing, and in\nthis case kind of magical.\n\n### WTF terms\n\nI had a few friends read this over before publishing and a couple were like\n\u201cwtf is X?\u201d Here is a short list of terms that were largely new to me:\n\n  * Vector Embeddings - Vector embeddings convert your text of images into numerical representations, allowing you to find similar pics and search your library effectively.\n  * Vector Database - A vector database is a way to store and search through encoded items, enabling you to find similar items.\n  * Word2Vec - Word2Vec is a groundbreaking technique that converts words into numerical vectors, enabling you to perform tasks like finding similar words and exploring relationships between them.\n  * CLIP - CLIP is OpenAI\u2019s model that encodes images and text into numerical vectors.\n  * OpenCLIP - OpenCLIP is an open-source implementation of OpenAI\u2019s CLIP model, allowing anyone to use and build upon this powerful image and text encoding technology without the need for special access or permissions.\n  * FAISS - FAISS is an efficient library for managing and searching through large collections of image vectors, making it fast and easy to find the images you\u2019re looking for.\n  * ChromaDB - ChromaDB is a database that stores and retrieves your image and text vectors, quickly returning similar results for your searches.\n\n## Keep it simple, harper.\n\nThis is a pretty straight forward hack. I am just fucking around so I wasn\u2019t\nsuper interested in making it scalable. I did have an interest in making it\nreplicable. I wanted to make something that you could run without a lot of\nwork.\n\nOne of my goals was to make sure everything runs locally to my laptop. We have\nthese fancy Mac GPUs - let\u2019s heat them up.\n\nThe first step was building out a simple crawler that would crawl a directory\nof images. I use Apple Photos, so I didn\u2019t have a directory full of my photos\nlaying around. I did, however, have a giant bucket of memes from my precious\nand very secret meme chat group (don\u2019t tell anyone). I exported the chat,\nmoved the images to a directory and BAM - I had my test image set.\n\n### The Crawler\n\nI created the world\u2019s worst crawler. Well. I should be honest: Claude created\nthe world\u2019s worst crawler with my instructions.\n\nIt is a bit complicated but here are the steps:\n\n  1. It gets the file list of the target directory\n  2. It stores the list in a msgpack file\n  3. I reference the msgpack file and then iterate through every image and store it in a sqlite db. Grabing some metadata about the file\n\n     * hash\n     * filesize\n     * location\n  4. I iterate through that sqlite db and then use CLIP to get the vector encoding of every image.\n  5. Then I store those vectors back in the sqlite db\n  6. Then I iterate through the sqlite db and insert the vectors and image path into chroma vector db\n  7. Then we are done\n\nThis is a lot of wasted work. You could iterate through the images, grab the\nembeddings and slam it into chroma (I chose chroma cuz it is easy, free, and\nno infra).\n\nI have built it this way because:\n\n  * After the memes, I crawled 140k images and wanted it to be resilient to crashing.\n  * I needed it to be able to resume building out the databases in case it crashed, power went out, etc\n  * I really like loops\n\nRegardless of the extra complexity, it worked flawlessly. I have crawled over\n200k images without a blip.\n\n### An embedding system\n\nEncoding the images was fun.\n\nI started with siglip and created a simple web service where we could upload\nthe image and get the vectors back. This ran on one of our GPU boxes at the\nstudio and worked well. It wasn\u2019t fast, but it was way faster than running\nopen clip locally.\n\nI still wanted to run it locally. I remembered that the ml-explore repo from\napple had some neat examples that could help. And BAM they had a clip\nimplementation that was fast af. Even using the larger model, it was faster\nthan the 4090. Wildstyle.\n\nI just needed to make it easy to use in my script.\n\n### MLX_CLIP\n\nClaude and I were able to coerce the example script from apple into a fun lil\npython class that you can use locally on any of your machines. It will\ndownload the models if they don\u2019t exist, convert them, and then use them in\nflight with your script.\n\nYou can check it out here: https://github.com/harperreed/mlx_clip\n\nI am pretty chuffed with how well it turned out. I know most people know this,\nbut the apple silicon is fast af.\n\nIt turned out to be rather simple to use:\n\n    \n    \n    import mlx_clip # Initialize the mlx_clip model with the given model name. clip = mlx_clip.mlx_clip(\"openai/clip-vit-base-patch32\") # Encode the image from the specified file path and obtain the image embeddings. image_embeddings = clip.image_encoder(\"assets/cat.jpeg\") # Print the image embeddings to the console. print(image_embeddings) # Encode the text description and obtain the text embeddings. text_embeddings = clip.text_encoder(\"a photo of a cat\") # Print the text embeddings to the console. print(text_embeddings)\n\nI would love to get this to work with siglip, as I prefer that model (it is\nway better than CLIP). However, this is a POC more than a product I want to\nmaintain. If anyone has any hints on how to get it working with siglip - hmu.\nI don\u2019t want to reinvent open clip - which should theoretically run well on\napple silicon, and is very good.\n\n### Now what\n\nNow that we had all the image vectors slammed into the vector datastore we\ncould get started with the interface. I used the built in query functionality\nof chromadb to show similar images.\n\nGrab the vectors of the image you are starting with. Query those vectors with\nchromadb. Chromed returns a list of image ids that are similar in declining\nsimilarity.\n\nI then wrapped it all up in a tailwind/flask app. This was incredible.\n\nI can\u2019t imagine the amount of work we would have done in 2015 to build this. I\nspent maybe 10 hours total on this and it was trivial.\n\nThe results are akin to magic.\n\n### Memes concept search\n\nNow remember, I used memes as my initial set of images. I had 12000 memes to\nsearch through.\n\nStart with this:\n\nSo true\n\nEncode it, pass it to chroma to return similar results.\n\nAnd then similar images that return are like this:\n\nAnother example:\n\nGives you results like:\n\nIt is really fun to click around.\n\n### Namespaces?\n\nThe magic isn\u2019t clicking on an image and getting a similar image. That is\ncool, but wasn\u2019t \u201choly shit\u201d for me.\n\nWhat blew my mind was using the same model to encode the search text into\nvectors and finding images similar to the text.\n\nFor whatever reason, this screws up my brain. It is one thing to have a neat\nsemantic like search for images based on another images. Being able to have a\nnice multi modal interface really made it like a magic trick.\n\nHere are some examples:\n\nSearching for money. I grab the encoding for money and pass the vectors to\nchroma. The results for money are:\n\nSearching for AI\n\nSearching for red (a dozy! Is it a color? Is a lifestyle? Is it Russia?)\n\nSo on and so forth. Forever. It is magical. You can find all sorts of gems you\nforgot about. Oh shit I need a meme about writing a blog post:\n\n(I am self aware, I just don\u2019t care - lol)\n\n### How does it work with a photo library?\n\nIt works super well.\n\nI highly recommend running this against your photo library. To get started, I\ndownloaded my google photos takeout archive. Extracted it onto an external\ndisk. I had to run a few scripts against it to make it usable (Whoever\ndesigned the google photos takeout is very excited about duplicate data). I\nthen pointed the script at that directory instead of my memes folder and let\n\u2019er rip.\n\nI had about 140k photos and it took about 6 hours to run through. Not so bad.\nThe results are incredible.\n\n#### Here are some fun examples:\n\nObviously these are similar (I also have a dupe problem in google photos)\n\nWe have had a lot of poodles. Here are some\n\nYou can search for landmarks. I had no idea I had taken a photo of fuji-san\nfrom a plane!\n\nAnd then find similar images of Mt Fuji.\n\nIt is pretty easy to search for places.\n\nOr emotions. I am apparently surprising so I have a lot of surprised photos.\n\nAlso niche things like low riders. (These are from Shibuya!)\n\nAnd you can use it to find things that are not easy to find or search for.\nLike bokeh.\n\nIt\u2019s wonderful, because I can click through and find great images I had\nforgotten about. Like this great photo of Baratunde that I took in 2017:\n\n### This will be everywhere\n\nI imagine that we will see this tech rolled into all the various photo apps\nshortly. Google Photos probably already does this, but they have googled it so\nmuch that nobody notices.\n\nThis is too good to not roll into whatever photo app you use. If I had any\nlarge scale product that used photos or images, I would immediately set up a\npipeline to start encoding the images to see what kind of weird features this\nunlocks.\n\n## YOU CAN USE THIS FOR THE LOW PRICE OF FREE\n\nI put the source here: harperreed/photo-similarity-search.\n\nPlease check it out.\n\nIt is pretty straight forward to get going. It is a bit hacky. lol.\n\nI would use conda or something similar to keep things clean. The interface is\nsimple tailwind. The web is flask. The code is python. I am your host, harper\nreed.\n\n## My challenge for you!\n\nPlease build an app that I can use to catalog my photo library in a nice way.\nI don\u2019t want to upload to another destination. I want to have a simple Mac app\nthat I can point to my photo library and say \u201ccrawl this.\u201d I imagine a lot of\nneat stuff could be added:\n\n  * Llava/Moondream auto captioning\n  * Keywords / Tags\n  * Vector similarity\n  * etc\n\nIt should run locally. Be a native app. Be simple, and effective. Maybe plug\ninto Lightroom, capture one, or apple photos.\n\nI want this. Build it. Let\u2019s discover all the amazing photos we have taken\nthrough the magic of AI.\n\n## Extra Credit: Light Room Preview JPEG Recovery\n\nMy hacking buddy Ivan was around while I was building this. He immediately saw\nthe magic of what a person could discover by using this on their photo\nlibrary. He wanted to use it immediately.\n\nHis photo catalog is on an external hard drive - but he had his Lightroom\npreview file locally. He wrote a quick script to extract the thumbnails and\nmetadata from the preview file and save it to an external disk.\n\nWe then ran the image vector crawler and BAM - he could see similar images and\nwhat not. Worked perfectly.\n\n#### Recover your Lightroom photos. Or at least the thumbnails.\n\nIvan\u2019s simple script to extract the images from the preview file is really\nawesome. If you have ever lost your real photo library (corrupt harddrive, or\nwhatever) and you still have the lrpreview file - this script can help you\nextract at least the lower res version.\n\nA super handy script to keep around.\n\nYou can check it out here: LR Preview JPEG Extractor.\n\n## Thanks for reading.\n\nAs always, hmu and let\u2019s hang out. I am thinking a lot about AI, Ecommerce,\nphotos, hifi, hacking and other shit.\n\nIf you are in Chicago come hang out.\n\nCopyright \u00a9 Harper Reed \u00b7 Contact Harper Reed \u00b7 Generated @ Apr 12, 2024\n\n", "frontpage": false}
