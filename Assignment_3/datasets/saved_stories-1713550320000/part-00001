{"aid": "40085794", "title": "Improving Facial Expression Synthesis Through GAN-Based Frontalization", "url": "https://blog.metaphysic.ai/improving-facial-expression-synthesis-through-gan-based-frontalization/", "domain": "metaphysic.ai", "votes": 1, "user": "Hard_Space", "posted_at": "2024-04-19 12:11:24", "comments": 0, "source_title": "Improving Facial Expression Synthesis Through Gan-Based Frontalization", "source_text": "Improving Facial Expression Synthesis Through Gan-Based Frontalization -\nMetaphysic.ai\n\nSkip to content\n\n## Home\n\n# Improving Facial Expression Synthesis Through Gan-Based Frontalization\n\n  * April 19, 2024\n  * 12:07 pm\n\n### About the author\n\n#### Martin Anderson\n\nI'm Martin Anderson, a writer occupied exclusively with machine learning,\nartificial intelligence, big data, and closely-related topics, with an\nemphasis on image synthesis, computer vision, and NLP.\n\n  * Author Website\n  * Author Archive\n\n## Share This Post\n\nThough we have written quite extensively about the difficulties of generating\nextreme angles of people in facial synthesis systems when that type of data is\nlacking in the dataset (it usually is), there is a strand of research in\ncomputer vision which scopes itself in the opposite direction: frontalization\nis a technique that attempts to produce passport-style, \u2018straight-on\u2019 images\nfrom source material where the subject is not directly facing the camera.\n\nFrontalization attempts to depict subjects from a straightforward POV, based\non elliptical views. Source: https://core.ac.uk/download/pdf/199220278.pdf\n\nKnown also as Frontal View Synthesis (FVS), this is a relatively well-funded\npursuit, since it largely benefits security operations \u2013 for instance, where\nthe authorities may have an in-the-wild oblique image of someone taken from a\nsecurity camera, which is not ideal for witness verification, and would be\nbetter served by the kind of simple mug-shot normally only ever available from\nATM cameras (where the customer is forced into a frontal pose by the design of\nthe machine).\n\nThe trouble with trying to make guesses in either direction (oblique>passport,\npassport>oblique) is that whatever facial expression is depicted in the source\nimage is likely to be coming along for the ride when the neural conversion\ntakes place.\n\nIn details from the above-cited project, we see that the system in question\ncannot 'neutralize' or alter the expression in the source image.\n\nThe field of Facial Expression Recognition (FER), stuck as it currently is\nwith the very limited number of facial expressions available in the dominant\nparadigm, the Facial Action Coding System (FACS), is already challenged to\nrecognize expressions in a passport-style canonical view, i.e., a mugshot.\n\nEvery degree of facial pose-angle away from that mugshot-pose effectively\nbecomes a domain of its own, which means that for recognition and synthesis\nsystems dealing with arbitrary and extreme facial poses, each expression\nrepresents, effectively, a type of sub-identity that must be contended with:\n\nRelating extreme facial poses to default or canonical reference emotions.\nSource: https://www.sciencedirect.com/science/article/pii/S1110016823000327\n\nAny work that advances the state-of-the-art in facial expression inference at\nextreme angles and most especially which provides improved ways of either\nfrontalizing or de-frontalizing source images, is something that\u2019s going to be\nof interest to the growing neural VFX community.\n\n## eMotion-GAN\n\nOne such paper has just emerged from France, which uses a motion-based\nGenerative Adversarial Network (GAN) to map expressions from extreme angles\nback down to a canonical mugshot pose.\n\nLarge variations for frontal view synthesis are handled effectively in the new\nsystem. Source: https://arxiv.org/pdf/2404.09940.pdf\n\nIn terms of subsequent categorization, where effective, this technique can\nhelp to associate recognized emotions in a mugshot viewpoint with the same\nemotion from an oblique angle, building up a cohesive domain.\n\nProjecting this line of research into the future, this kind of approach could\nhelp film and TV directors to tweak facial expressions in post-production in a\nsemantic and systematic way, instead of an artisanal way where latent codes\nare pushed around until the face \u2018looks right\u2019.\n\nAdditionally, it would facilitate workflows where more agile and flexible\nexpression recognition was possible, making datasets more searchable for\napposite material for any particular shot or model training.\n\nThe paper states:\n\n\u2018Considering the motion induced by head variation as noise and the motion\ninduced by facial expression as the relevant information, our model is trained\nto filter out the noisy motion in order to retain only the motion related to\nfacial expression.\n\n\u2018The filtered motion is then mapped onto a neutral frontal face to generate\nthe corresponding expressive frontal face. We conducted extensive evaluations\nusing several widely recognized dynamic FER datasets, which encompass\nsequences exhibiting various degrees of head pose variations in both intensity\nand orientation.\n\n\u2018Our results demonstrate the effectiveness of our approach in significantly\nreducing the FER performance gap between frontal and non-frontal faces.\nSpecifically, we achieved a FER improvement of up to +5% for small pose\nvariations and up to +20% improvement for larger pose variations.\u2019\n\nNotable in the new system is that it does not rely on facial landmarks as an\nanchor reference for processing (which can smuggle in unwanted ID data), which\nis uncommon in similar literature; and that it can also transfer expressions\nbetween subjects (and, the authors claim, to a variety of other face\ncategories including animals and drawings, though this is not extensively\ndealt with in the new work).\n\nThe new paper is titled eMotion-GAN: A Motion-based GAN for Photorealistic and\nFacial Expression Preserving Frontal View Synthesis, and comes (unusually)\nwith a full code release. The four IEEE members contributing to the work come\nfrom IMT Nord Europe, and the Centre de Recherche en Informatique Signal et\nAutomatique de Lille, both in Lille, France.\n\n## Method\n\nThe method essentially treats head pose variation as part of a motion domain,\nand centers on extracting facial motion while disregarding head pose motion.\nThe authors contend that this approach allows for the extraction of generic\n(rather than ID-locked) expression data, which can be used across a variety of\nindividuals.\n\n(However, it should be noted that the 6-8 popular facial expressions in FACS\nremain in themselves a limitation of any system which adopts FACS \u2013 but we\nhave to start somewhere!)\n\nFrom the new paper, examples of how eMotion-GAN estimates motion from\nvariation in head poses, and retains only motion from the facial musculature,\ntransposing the result to a synthesized image in frontal plane view\n\nThe process consists of two primary phases: motion frontalization, and motion\nwarping:\n\nSchema for eMotion-GAN.\n\nIn the first phase, visualized in the schema above, motion frontalization\nconverts optical flow data and filters the motion related to the head pose,\ntransposing the motion from the facial musculature into a frontal view. The\nexpression discriminator works in concert with various reconstruction loss\nfunctions.\n\nIn the second phase, motion warping, the neutralized and frontalized face\ngenerates the desired expression, with a pre-trained classifier handling\nexpression recognition.\n\nIn the first phase, given two consecutive face images, the face is detected\nand cropped before optical flow is calculated, and high resolution images are\nused to preserve identity information, though the images will eventually be\nresized to match the far lower resolutions that can pass through the\nprocessing capabilities of the machine training the model.\n\nThe architecture at work in the first module begins with a flow generator\ncomprising an encoding and decoding network with the input and output set to\n128x128px.\n\nThe subsequent PatchGAN discriminator is inspired by the Pix2Pix GAN\ndiscriminator, a Convolutional Neural Network (CNN) which guesses whether a\nslice of the generated output is real or fake, building up a comprehensive\nprediction iteratively.\n\nNext, the expression discriminator serves as an additional CNN-based\nclassifier for FER purposes, using optical flow data as input, and then\nencoding the facial motion of the identity under study.\n\nThe motion warping module distorts the frontalized facial motion processed by\nthe previous module into a neutral expression, by applying the calculated\nmotion fields that have been extracted up to this point. The expressions\nsubsequently produced are evaluated by an FER detector (denoted as eFER),\nwhich ensures fidelity of facial expression preservation.\n\nThe aforementioned image generator includes a motion encoder, an identity\nencoder, and a joint decoder for rendering the facial image.\n\nThe eFER functionality makes use of the pretrained Chinese DMUE model, which\nis intended to combat annotation ambiguity. In accordance with the combative\nnature of the generator/discriminator model in GAN systems, it also provides\nblind feedback as to whether the previous submitted face was warped correctly\nor not (\u2018blind\u2019 in the sense that it doesn\u2019t tell the generator in what way it\nfailed, only that it did, to ensure that the generator does not overfit to a\nfeedback trend).\n\nImage-based predictions from three batches under DMUE, used in the new system.\nSource: https://arxiv.org/pdf/2104.00232.pdf\n\nIn terms of loss functions within eMotion-GAN, the authors note that GANs\noften suffer from artifacts, and they have therefore added additional\npenalized losses to mitigate this. These consist of Mean Absolute Error (MAE),\nVGG-based perceptual loss, and image-based facial expression less.\n\nFor the flow generator, Optical Flow\u2019s End Point Error (EPE) and Charbonnier\nlosses are employed. End Point Loss is an optical flow-specific metric that\nmeasures the Euclidean distance between an estimated optical flow and the\nunderlying ground truth; Charbonnier loss, instead, is a type of L1 loss\nthat\u2019s particularly suited for depth estimation.\n\nBesides these additional losses, the standard GAN losses apply. Sigmoid cross-\nentropy is used for the initial discriminator, and categorical cross-entropy\nfor the emotion discriminator.\n\n## Data and Tests\n\nTo test the system, experiments were conducted across various datasets that\ncontain a wide variety of facial expressions from the same individuals, and\nfrom other individuals. Whether generated for the purpose, or found in a\ndataset, paired images are necessary for these purposes.\n\nThe researchers based the majority of experiments on the SNaP-2DFe dataset,\nbeing the sole open source dataset featuring paired images with facial\nexpressions acquired simultaneously using mugshot and oblique viewpoints.\n\nThe paper states:\n\n\u2018Simultaneous Natural and Posed 2D Facial expressions dataset [(SNaP-2DFe)] is\nan image dataset that meets the requirements requested for the motion\nfrontalization phase. It contains paired images of frontal and non-frontal\nfaces for different subjects and facial expressions.\n\n\u2018More specifically, it provides images of subjects with expressive faces\nacquired simultaneously with head pose variation (unconstrained recording) and\nwithout head pose variation (constrained recording) from 15 different subjects\nwith 6 categories of head poses (yaw, pitch, roll, diagonal, nothing and Tx)\nand 7 different facial expressions most commonly used in the literature\n(disgust, happiness, anger, surprise, neutral, fear and sadness).\u2019\n\nSamples from the SNaP-2DFe dataset. Source*: https://archive.ph/P39WF\n\nFor training, common FER datasets were employed: CK+; ADFES; MMI; and Oulu-\nCASIA.\n\nExamples from the MMI dataset, used in tests for eMotion-GAN. Source:\nhttps://ibug.doc.ic.ac.uk/research/mmi-database/\n\nThe researchers state that these latter datasets were included primarily to\nbolster the system\u2019s ability to derive facial motion patterns. For those\ndatasets that did not include paired mugshot/oblique couplets, the authors\ngenerated apposite pair-matches, to obtain couplets.\n\nTo perform meaningful tests, the authors needed to compare results from an FER\nclassifier in both the motion and image domains, with no frontalization\napplied, and with frontalization^\u2020.\n\nFacial motion was extracted with the Farneback method, which obtains optical\nflow direction from grayscale imagery, and which the authors report is best-\nsuited for FER.\n\nOptical flow directional indicators on the right, using the Farneback method.\nSource: https://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf\n\nFor the learning rate for the motion frontalization module, 1\u00d710^-5 was used\nunder the Adam optimizer for the flow generator, the same but at 1\u00d710^-4 for\nthe patches discriminator and for Stochastic Gradient Descent (SGD) loss, and\nalso for the expression discriminator. A learning rate schedule was adopted\nfrom the DCGAN project, for the patches discriminator.\n\nFor the motion warping module, Adam was again used, with a learning rate of\n10^-5 for the image generator.\n\nThe model was trained for 15 epochs (an epoch being a complete presentation of\nthe data to the training system) at a batch size of 4, and PyTorch was used\nfor this end-to-end model.\n\nInitial tests were for Frontal View Synthesis, and were facilitated by SIFT\nflow for deep learning prior approaches, and Complete Face Recovery GAN (CFR-\nGAN) and pixel2style2pixel (pSp) for image-based methods. For optical flow-\nbased methods, Global-Local Universal Network (GLU-Net) was employed.\n\nAll experiments were performed under identical settings, with a baseline\nconsisting of face detection and facial cropping, without frontalization.\nResults were compared in both motion and image domains.\n\nThough the authors describe the initial test as a \u2018qualitative comparison\u2019, it\ndoes actually seem to qualify more as a quantitative comparison, since it is\nnot the object of a user-study or of casual observation, but rather makes use\nof three common metrics: Structural Similarity Index (SSIM); Root Mean Squared\nError (RMSE); and the aforementioned EPE.\n\nMean and standard deviations of EPE, RMSE and SSIM across the validation\ndatasets.\n\nOf these results, the authors comment:\n\n\u2018[The results table] shows the superiority of our method in both motion and\nimage over the other FVS methods.\n\n\u2018Indeed, a lower EPE indicates that our frontalization model succeeds in\nfrontalizing motion while preserving facial expression patterns, which is\nmainly due to the fact that our method frontalizes motion rather than image,\nallowing it to take advantage of the similarity of expression patterns between\ndifferent subjects, and a higher SSIM indicates that the warping model\nsuccessfully warps the frontalized motion in the image, which is mainly driven\nby the perceptual loss included in the warping model that allows it to\ngenerate consistent expressive faces while preserving the facial identity of\nthe subject.\u2019\n\nNext came qualitative experiments for image warping, the second module in the\ncore schema, where the diverse FVS methods mentioned earlier were trialed\nagainst eMotion-GAN:\n\nQualitative comparisons against diverse frameworks for image warping.\n\nHere the authors state:\n\n\u2018In terms of image reconstruction, our method manages to warp the frontalized\nflow into the neutral face to recover the original facial expression. Unlike\nother methods, our method does not suffer from artifacts and distortion of\nexpression patterns. Thus, allowing the facial expression to be recovered\nwithout loss of identity of the subject.\u2019\n\nFER comparisons were next on the slate. Here the datasets used were ADFES,\nMMI, CASIA, SNAP-F and SNAP-NF (frontalized and non-frontalized variants from\nthe SNaP-2DFe project, and CK+. Rival systems were Ad-Corre, DAN, HSE, DMUE,\nagainst the authors own classifier, the flow-based Flow-CNN.\n\nQuantitative comparison against rival methods. Please refer to the source\npaper for better resolution.\n\nOf these results, the paper states:\n\n\u2018In [the] image domain, the baseline seems to achieve the best results given\nthat recent FER approaches are trained to analyze frontal and less constrained\nfaces, thanks to augmentation methods.\n\n\u2018Image-based frontalization tends to introduce excessive deformation, altering\nboth facial structure and expression. Our motion-based approach applies a loss\nthat preserves face structure and ensures consistent textural rendering,\nalbeit with a slight attenuation of [expression].\n\n\u2018Nevertheless, our method is still competitive on most datasets as it nearly\nachieves the results given by recent FER methods in the literature.\n\n\u2018However, in motion field, the results clearly demonstrate that our proposed\nmethod outperforms other FVS methods across datasets.\u2019\n\nFinally, besides the ablation studies that we are not covering here, the\nresearchers tested cross-subject facial motion transfer, which seeks to swap\nexpressions across subjects, and even across sub-domains:\n\nCross-subject motion transfer tests the ability of eMotion-GAN to swap\nexpressions across apparently incompatible types of image. Please refer to\nsource paper for improved resolution.\n\nThe authors comment:\n\n\u2018Our motion warping model can be used for different interesting applications,\nincluding: 1) data augmentation by generating a variety of expressive faces\ngiven a neutral face. Indeed, along with our warping model, we can build a\nconditioned GAN to generate facial expressions motion conditioning on the\nfacial expression and warp the generated facial motion to neutral faces to\ngenerate additional data. 2) class balancing: to balance number of samples per\nexpression in a non-balanced FER dataset. 3) intensification or attenuation of\nfacial expressions: to transform micro-expressions into macro-expressions and\nvice versa. 4) category balancing: as in many datasets, categories such as\ngender, age and ethnicity are not balanced, our warping model can be used to\novercome such constraint.\u2019\n\n## Conclusion\n\nFER is a thorny challenge in facial synthesis systems that cannot always\ndepend on input that comes pre-frontalized. The ability to create a canonical\nor default pose for a variety of expressions could lead in time to the same\nkind of divergence-based flexibility that CGI-based systems such as 3D\nMorphable Models (3DMMs) and FLAME employ, i.e., by defining features based on\ntheir divergence from a \u2018resting\u2019 canonical template.\n\nThe ability to process such a default position without the aid of mesh-based\nancillary systems such as 3DMM could be a useful step away from the growing\ninterdependence of CGI and neural techniques.\n\n* Link is too long to include, available in snapshot\n\n^\u2020 Please refer to the source paper for very extensive detail of the minutiae\nof the testing methodology, which far exceeds the scope of this coverage.\n\nRETURN TO METAPHYSIC BLOG HOME\n\nPrevPreviousDealing with Unconventional Facial Expressions in Neural Synthesis\n\n## More To Explore\n\nAI ML DL\n\n### Improving Facial Expression Synthesis Through Gan-Based Frontalization\n\nIt is difficult enough for modern Facial Expression Recognition (FER) systems\nto accurately identify even the minimal 6-8 emotions defined by FACS, from\nfront-facing mugshots; when the person is depicted at a hard angle, the\nproblem is severely compounded. Though this is a problem most addressed by\nsecurity researchers, it has implications also for the VFX sector, which is\nincreasingly being asked to provide expression-editing tools for faces, no\nmatter where the faces are pointing. A new paper from French researchers\nclaims to have made a breakthrough in this regard, with a novel method of\n\u2018frontalizing\u2019 obscure face angles.\n\nMartin Anderson April 19, 2024\n\nAI ML DL\n\n### Dealing with Unconventional Facial Expressions in Neural Synthesis\n\nThe much-used Facial Action Coding System (FACS) has a very narrow range of\ncore expressions \u2013 6-8, depending on which version you\u2019re using. However, film\nand TV directors interested in using neural tools to adjust facial\nperformances in post are going to need a more precise toolkit, which falls\noutside the FACS paradigm. Now, researchers from Greece and Germany are at\nleast offering an expression recognition and synthesis system, titled SMIRK,\nwhich can address a wider range of facial expressions, and hopefully advance\nthe state of the art in this under-served aspect of human synthesis.\n\nMartin Anderson April 15, 2024\n\n\u201c\n\n## It is the mark of an educated mind to be able to entertain a thought\nwithout accepting it.\n\nAristotle\n\nCopyright \u00a9 2023. All rights reserved. Privacy Policy\n\n### Quick Links\n\n  * Home\n  * Every Anyone\n  * Synthetic Futures\n\n### Connect with us\n\n  * Discord\n  * Tiktok\n  * Twitter\n  * Youtube\n  * Instagram\n  * Github\n  * Linkedin\n\n### Contact Info\n\n  * info@metaphysic.ai\n  * press@metaphysic.ai\n\n", "frontpage": false}
