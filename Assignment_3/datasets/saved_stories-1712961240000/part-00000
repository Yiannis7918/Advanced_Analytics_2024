{"aid": "40014762", "title": "A visual proof that neural nets can compute any function (2015)", "url": "http://neuralnetworksanddeeplearning.com/chap4.html", "domain": "neuralnetworksanddeeplearning.com", "votes": 1, "user": "Tomte", "posted_at": "2024-04-12 16:31:32", "comments": 0, "source_title": "Neural networks and deep learning", "source_text": "Neural networks and deep learning\n\ns\u2299t\n\n# CHAPTER 4\n\n# A visual proof that neural nets can compute any function\n\nNeural Networks and Deep Learning\n\nWhat this book is about\n\nOn the exercises and problems\n\nUsing neural nets to recognize handwritten digits\n\n  * Perceptrons\n  * Sigmoid neurons\n  * The architecture of neural networks\n  * A simple network to classify handwritten digits\n  * Learning with gradient descent\n  * Implementing our network to classify digits\n  * Toward deep learning\n\nHow the backpropagation algorithm works\n\n  * Warm up: a fast matrix-based approach to computing the output from a neural network\n  * The two assumptions we need about the cost function\n  * The Hadamard product, s\u2299t\n  * The four fundamental equations behind backpropagation\n  * Proof of the four fundamental equations (optional)\n  * The backpropagation algorithm\n  * The code for backpropagation\n  * In what sense is backpropagation a fast algorithm?\n  * Backpropagation: the big picture\n\nImproving the way neural networks learn\n\n  * The cross-entropy cost function\n  * Overfitting and regularization\n  * Weight initialization\n  * Handwriting recognition revisited: the code\n  * How to choose a neural network's hyper-parameters?\n  * Other techniques\n\nA visual proof that neural nets can compute any function\n\n  * Two caveats\n  * Universality with one input and one output\n  * Many input variables\n  * Extension beyond sigmoid neurons\n  * Fixing up the step functions\n  * Conclusion\n\nWhy are deep neural networks hard to train?\n\n  * The vanishing gradient problem\n  * What's causing the vanishing gradient problem? Unstable gradients in deep neural nets\n  * Unstable gradients in more complex networks\n  * Other obstacles to deep learning\n\nDeep learning\n\n  * Introducing convolutional networks\n  * Convolutional neural networks in practice\n  * The code for our convolutional networks\n  * Recent progress in image recognition\n  * Other approaches to deep neural nets\n  * On the future of neural networks\n\nAppendix: Is there a simple algorithm for intelligence?\n\nAcknowledgements\n\nFrequently Asked Questions\n\nIf you benefit from the book, please make a small donation. I suggest $5, but\nyou can choose the amount.\n\nAlternately, you can make a donation by sending me Bitcoin, at address\n1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAx\n\nSponsors\n\nDeep Learning Workstations, Servers, and Laptops\n\nThanks to all the supporters who made the book possible, with especial thanks\nto Pavel Dudrenov. Thanks also to all the contributors to the Bugfinder Hall\nof Fame.\n\nResources\n\nMichael Nielsen on Twitter\n\nBook FAQ\n\nCode repository\n\nMichael Nielsen's project announcement mailing list\n\nDeep Learning, book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\n\ncognitivemedium.com\n\nBy Michael Nielsen / Dec 2019\n\nOne of the most striking facts about neural networks is that they can compute\nany function at all. That is, suppose someone hands you some complicated,\nwiggly function, f(x)f(x):\n\nNo matter what the function, there is guaranteed to be a neural network so\nthat for every possible input, xx, the value f(x)f(x) (or some close\napproximation) is output from the network, e.g.:\n\nThis result holds even if the function has many inputs,\nf=f(x1,...,xm)f=f(x1,...,xm), and many outputs. For instance, here's a network\ncomputing a function with m=3m=3 inputs and n=2n=2 outputs:\n\nThis result tells us that neural networks have a kind of universality. No\nmatter what function we want to compute, we know that there is a neural\nnetwork which can do the job.\n\nWhat's more, this universality theorem holds even if we restrict our networks\nto have just a single layer intermediate between the input and the output\nneurons - a so-called single hidden layer. So even very simple network\narchitectures can be extremely powerful.\n\nThe universality theorem is well known by people who use neural networks. But\nwhy it's true is not so widely understood. Most of the explanations available\nare quite technical. For instance, one of the original papers proving the\nresult* *Approximation by superpositions of a sigmoidal function, by George\nCybenko (1989). The result was very much in the air at the time, and several\ngroups proved closely related results. Cybenko's paper contains a useful\ndiscussion of much of that work. Another important early paper is Multilayer\nfeedforward networks are universal approximators, by Kurt Hornik, Maxwell\nStinchcombe, and Halbert White (1989). This paper uses the Stone-Weierstrass\ntheorem to arrive at similar results. did so using the Hahn-Banach theorem,\nthe Riesz Representation theorem, and some Fourier analysis. If you're a\nmathematician the argument is not difficult to follow, but it's not so easy\nfor most people. That's a pity, since the underlying reasons for universality\nare simple and beautiful.\n\nIn this chapter I give a simple and mostly visual explanation of the\nuniversality theorem. We'll go step by step through the underlying ideas.\nYou'll understand why it's true that neural networks can compute any function.\nYou'll understand some of the limitations of the result. And you'll understand\nhow the result relates to deep neural networks.\n\nTo follow the material in the chapter, you do not need to have read earlier\nchapters in this book. Instead, the chapter is structured to be enjoyable as a\nself-contained essay. Provided you have just a little basic familiarity with\nneural networks, you should be able to follow the explanation. I will,\nhowever, provide occasional links to earlier material, to help fill in any\ngaps in your knowledge.\n\nUniversality theorems are a commonplace in computer science, so much so that\nwe sometimes forget how astonishing they are. But it's worth reminding\nourselves: the ability to compute an arbitrary function is truly remarkable.\nAlmost any process you can imagine can be thought of as function computation.\nConsider the problem of naming a piece of music based on a short sample of the\npiece. That can be thought of as computing a function. Or consider the problem\nof translating a Chinese text into English. Again, that can be thought of as\ncomputing a function* *Actually, computing one of many functions, since there\nare often many acceptable translations of a given piece of text.. Or consider\nthe problem of taking an mp4 movie file and generating a description of the\nplot of the movie, and a discussion of the quality of the acting. Again, that\ncan be thought of as a kind of function computation* *Ditto the remark about\ntranslation and there being many possible functions.. Universality means that,\nin principle, neural networks can do all these things and many more.\n\nOf course, just because we know a neural network exists that can (say)\ntranslate Chinese text into English, that doesn't mean we have good techniques\nfor constructing or even recognizing such a network. This limitation applies\nalso to traditional universality theorems for models such as Boolean circuits.\nBut, as we've seen earlier in the book, neural networks have powerful\nalgorithms for learning functions. That combination of learning algorithms +\nuniversality is an attractive mix. Up to now, the book has focused on the\nlearning algorithms. In this chapter, we focus on universality, and what it\nmeans.\n\n### Two caveats\n\nBefore explaining why the universality theorem is true, I want to mention two\ncaveats to the informal statement \"a neural network can compute any function\".\n\nFirst, this doesn't mean that a network can be used to exactly compute any\nfunction. Rather, we can get an approximation that is as good as we want. By\nincreasing the number of hidden neurons we can improve the approximation. For\ninstance, earlier I illustrated a network computing some function f(x)f(x)\nusing three hidden neurons. For most functions only a low-quality\napproximation will be possible using three hidden neurons. By increasing the\nnumber of hidden neurons (say, to five) we can typically get a better\napproximation:\n\nAnd we can do still better by further increasing the number of hidden neurons.\n\nTo make this statement more precise, suppose we're given a function f(x)f(x)\nwhich we'd like to compute to within some desired accuracy \u03b5>0\u03b5>0\\. The\nguarantee is that by using enough hidden neurons we can always find a neural\nnetwork whose output g(x)g(x) satisfies |g(x)\u2212f(x)|<\u03b5|g(x)\u2212f(x)|<\u03b5, for all\ninputs xx. In other words, the approximation will be good to within the\ndesired accuracy for every possible input.\n\nThe second caveat is that the class of functions which can be approximated in\nthe way described are the continuous functions. If a function is\ndiscontinuous, i.e., makes sudden, sharp jumps, then it won't in general be\npossible to approximate using a neural net. This is not surprising, since our\nneural networks compute continuous functions of their input. However, even if\nthe function we'd really like to compute is discontinuous, it's often the case\nthat a continuous approximation is good enough. If that's so, then we can use\na neural network. In practice, this is not usually an important limitation.\n\nSumming up, a more precise statement of the universality theorem is that\nneural networks with a single hidden layer can be used to approximate any\ncontinuous function to any desired precision. In this chapter we'll actually\nprove a slightly weaker version of this result, using two hidden layers\ninstead of one. In the problems I'll briefly outline how the explanation can,\nwith a few tweaks, be adapted to give a proof which uses only a single hidden\nlayer.\n\n### Universality with one input and one output\n\nTo understand why the universality theorem is true, let's start by\nunderstanding how to construct a neural network which approximates a function\nwith just one input and one output:\n\nIt turns out that this is the core of the problem of universality. Once we've\nunderstood this special case it's actually pretty easy to extend to functions\nwith many inputs and many outputs.\n\nTo build insight into how to construct a network to compute ff, let's start\nwith a network containing just a single hidden layer, with two hidden neurons,\nand an output layer containing a single output neuron:\n\nTo get a feel for how components in the network work, let's focus on the top\nhidden neuron. In the diagram below, click on the weight, ww, and drag the\nmouse a little ways to the right to increase ww. You can immediately see how\nthe function computed by the top hidden neuron changes:\n\nAs we learnt earlier in the book, what's being computed by the hidden neuron\nis \u03c3(wx+b)\u03c3(wx+b), where \u03c3(z)\u22611/(1+e\u2212z)\u03c3(z)\u22611/(1+e\u2212z) is the sigmoid function.\nUp to now, we've made frequent use of this algebraic form. But for the proof\nof universality we will obtain more insight by ignoring the algebra entirely,\nand instead manipulating and observing the shape shown in the graph. This\nwon't just give us a better feel for what's going on, it will also give us a\nproof* *Strictly speaking, the visual approach I'm taking isn't what's\ntraditionally thought of as a proof. But I believe the visual approach gives\nmore insight into why the result is true than a traditional proof. And, of\ncourse, that kind of insight is the real purpose behind a proof. Occasionally,\nthere will be small gaps in the reasoning I present: places where I make a\nvisual argument that is plausible, but not quite rigorous. If this bothers\nyou, then consider it a challenge to fill in the missing steps. But don't lose\nsight of the real purpose: to understand why the universality theorem is true.\nof universality that applies to activation functions other than the sigmoid\nfunction.\n\nTo get started on this proof, try clicking on the bias, bb, in the diagram\nabove, and dragging to the right to increase it. You'll see that as the bias\nincreases the graph moves to the left, but its shape doesn't change.\n\nNext, click and drag to the left in order to decrease the bias. You'll see\nthat as the bias decreases the graph moves to the right, but, again, its shape\ndoesn't change.\n\nNext, decrease the weight to around 22 or 33. You'll see that as you decrease\nthe weight, the curve broadens out. You might need to change the bias as well,\nin order to keep the curve in-frame.\n\nFinally, increase the weight up past w=100w=100. As you do, the curve gets\nsteeper, until eventually it begins to look like a step function. Try to\nadjust the bias so the step occurs near x=0.3x=0.3. The following short clip\nshows what your result should look like. Click on the play button to play (or\nreplay) the video:\n\nWe can simplify our analysis quite a bit by increasing the weight so much that\nthe output really is a step function, to a very good approximation. Below I've\nplotted the output from the top hidden neuron when the weight is w=999w=999.\nNote that this plot is static, and you can't change parameters such as the\nweight.\n\nIt's actually quite a bit easier to work with step functions than general\nsigmoid functions. The reason is that in the output layer we add up\ncontributions from all the hidden neurons. It's easy to analyze the sum of a\nbunch of step functions, but rather more difficult to reason about what\nhappens when you add up a bunch of sigmoid shaped curves. And so it makes\nthings much easier to assume that our hidden neurons are outputting step\nfunctions. More concretely, we do this by fixing the weight ww to be some very\nlarge value, and then setting the position of the step by modifying the bias.\nOf course, treating the output as a step function is an approximation, but\nit's a very good approximation, and for now we'll treat it as exact. I'll come\nback later to discuss the impact of deviations from this approximation.\n\nAt what value of xx does the step occur? Put another way, how does the\nposition of the step depend upon the weight and bias?\n\nTo answer this question, try modifying the weight and bias in the diagram\nabove (you may need to scroll back a bit). Can you figure out how the position\nof the step depends on ww and bb? With a little work you should be able to\nconvince yourself that the position of the step is proportional to bb, and\ninversely proportional to ww.\n\nIn fact, the step is at position s=\u2212b/ws=\u2212b/w, as you can see by modifying the\nweight and bias in the following diagram:\n\nIt will greatly simplify our lives to describe hidden neurons using just a\nsingle parameter, ss, which is the step position, s=\u2212b/ws=\u2212b/w. Try modifying\nss in the following diagram, in order to get used to the new parameterization:\n\nAs noted above, we've implicitly set the weight ww on the input to be some\nlarge value - big enough that the step function is a very good approximation.\nWe can easily convert a neuron parameterized in this way back into the\nconventional model, by choosing the bias b=\u2212wsb=\u2212ws.\n\nUp to now we've been focusing on the output from just the top hidden neuron.\nLet's take a look at the behavior of the entire network. In particular, we'll\nsuppose the hidden neurons are computing step functions parameterized by step\npoints s1s1 (top neuron) and s2s2 (bottom neuron). And they'll have respective\noutput weights w1w1 and w2w2. Here's the network:\n\nWhat's being plotted on the right is the weighted output w1a1+w2a2w1a1+w2a2\nfrom the hidden layer. Here, a1a1 and a2a2 are the outputs from the top and\nbottom hidden neurons, respectively* *Note, by the way, that the output from\nthe whole network is \u03c3(w1a1+w2a2+b)\u03c3(w1a1+w2a2+b), where bb is the bias on the\noutput neuron. Obviously, this isn't the same as the weighted output from the\nhidden layer, which is what we're plotting here. We're going to focus on the\nweighted output from the hidden layer right now, and only later will we think\nabout how that relates to the output from the whole network.. These outputs\nare denoted with aas because they're often known as the neurons' activations.\n\nTry increasing and decreasing the step point s1s1 of the top hidden neuron.\nGet a feel for how this changes the weighted output from the hidden layer.\nIt's particularly worth understanding what happens when s1s1 goes past s2s2.\nYou'll see that the graph changes shape when this happens, since we have moved\nfrom a situation where the top hidden neuron is the first to be activated to a\nsituation where the bottom hidden neuron is the first to be activated.\n\nSimilarly, try manipulating the step point s2s2 of the bottom hidden neuron,\nand get a feel for how this changes the combined output from the hidden\nneurons.\n\nTry increasing and decreasing each of the output weights. Notice how this\nrescales the contribution from the respective hidden neurons. What happens\nwhen one of the weights is zero?\n\nFinally, try setting w1w1 to be 0.80.8 and w2w2 to be \u22120.8\u22120.8. You get a\n\"bump\" function, which starts at point s1s1, ends at point s2s2, and has\nheight 0.80.8. For instance, the weighted output might look like this:\n\nOf course, we can rescale the bump to have any height at all. Let's use a\nsingle parameter, hh, to denote the height. To reduce clutter I'll also remove\nthe \"s1=...s1=...\" and \"w1=...w1=...\" notations.\n\nTry changing the value of hh up and down, to see how the height of the bump\nchanges. Try changing the height so it's negative, and observe what happens.\nAnd try changing the step points to see how that changes the shape of the\nbump.\n\nYou'll notice, by the way, that we're using our neurons in a way that can be\nthought of not just in graphical terms, but in more conventional programming\nterms, as a kind of if-then-else statement, e.g.:\n\n    \n    \n    if input >= step point: add 1 to the weighted output else: add 0 to the weighted output\n\nFor the most part I'm going to stick with the graphical point of view. But in\nwhat follows you may sometimes find it helpful to switch points of view, and\nthink about things in terms of if-then-else.\n\nWe can use our bump-making trick to get two bumps, by gluing two pairs of\nhidden neurons together into the same network:\n\nI've suppressed the weights here, simply writing the hh values for each pair\nof hidden neurons. Try increasing and decreasing both hh values, and observe\nhow it changes the graph. Move the bumps around by changing the step points.\n\nMore generally, we can use this idea to get as many peaks as we want, of any\nheight. In particular, we can divide the interval [0,1][0,1] up into a large\nnumber, NN, of subintervals, and use NN pairs of hidden neurons to set up\npeaks of any desired height. Let's see how this works for N=5N=5. That's quite\na few neurons, so I'm going to pack things in a bit. Apologies for the\ncomplexity of the diagram: I could hide the complexity by abstracting away\nfurther, but I think it's worth putting up with a little complexity, for the\nsake of getting a more concrete feel for how these networks work.\n\nYou can see that there are five pairs of hidden neurons. The step points for\nthe respective pairs of neurons are 0,1/50,1/5, then 1/5,2/51/5,2/5, and so\non, out to 4/5,5/54/5,5/5. These values are fixed - they make it so we get\nfive evenly spaced bumps on the graph.\n\nEach pair of neurons has a value of hh associated to it. Remember, the\nconnections output from the neurons have weights hh and \u2212h\u2212h (not marked).\nClick on one of the hh values, and drag the mouse to the right or left to\nchange the value. As you do so, watch the function change. By changing the\noutput weights we're actually designing the function!\n\nContrariwise, try clicking on the graph, and dragging up or down to change the\nheight of any of the bump functions. As you change the heights, you can see\nthe corresponding change in hh values. And, although it's not shown, there is\nalso a change in the corresponding output weights, which are +h+h and \u2212h\u2212h.\n\nIn other words, we can directly manipulate the function appearing in the graph\non the right, and see that reflected in the hh values on the left. A fun thing\nto do is to hold the mouse button down and drag the mouse from one side of the\ngraph to the other. As you do this you draw out a function, and get to watch\nthe parameters in the neural network adapt.\n\nTime for a challenge.\n\nLet's think back to the function I plotted at the beginning of the chapter:\n\nI didn't say it at the time, but what I plotted is actually the function\nf(x)=0.2+0.4x2+0.3xsin(15x)+0.05cos(50x),\n\nf(x)=0.2+0.4x2+0.3xsin(15x)+0.05cos(50x),(113)\n\nplotted over x from 0 to 1, and with the y axis taking values from 0 to 1.\n\nThat's obviously not a trivial function.\n\nYou're going to figure out how to compute it using a neural network.\n\nIn our networks above we've been analyzing the weighted combination \u2211jwjaj\noutput from the hidden neurons. We now know how to get a lot of control over\nthis quantity. But, as I noted earlier, this quantity is not what's output\nfrom the network. What's output from the network is \u03c3(\u2211jwjaj+b) where b is the\nbias on the output neuron. Is there some way we can achieve control over the\nactual output from the network?\n\nThe solution is to design a neural network whose hidden layer has a weighted\noutput given by \u03c3\u22121\u2218f(x), where \u03c3\u22121 is just the inverse of the \u03c3 function.\nThat is, we want the weighted output from the hidden layer to be:\n\nIf we can do this, then the output from the network as a whole will be a good\napproximation to f(x)* *Note that I have set the bias on the output neuron to\n0..\n\nYour challenge, then, is to design a neural network to approximate the goal\nfunction shown just above. To learn as much as possible, I want you to solve\nthe problem twice. The first time, please click on the graph, directly\nadjusting the heights of the different bump functions. You should find it\nfairly easy to get a good match to the goal function. How well you're doing is\nmeasured by the average deviation between the goal function and the function\nthe network is actually computing. Your challenge is to drive the average\ndeviation as low as possible. You complete the challenge when you drive the\naverage deviation to 0.40 or below.\n\nOnce you've done that, click on \"Reset\" to randomly re-initialize the bumps.\nThe second time you solve the problem, resist the urge to click on the graph.\nInstead, modify the h values on the left-hand side, and again attempt to drive\nthe average deviation to 0.40 or below.\n\nYou've now figured out all the elements necessary for the network to\napproximately compute the function f(x)! It's only a coarse approximation, but\nwe could easily do much better, merely by increasing the number of pairs of\nhidden neurons, allowing more bumps.\n\nIn particular, it's easy to convert all the data we have found back into the\nstandard parameterization used for neural networks. Let me just recap quickly\nhow that works.\n\nThe first layer of weights all have some large, constant value, say w=1000.\n\nThe biases on the hidden neurons are just b=\u2212ws. So, for instance, for the\nsecond hidden neuron s=0.2 becomes b=\u22121000\u00d70.2=\u2212200.\n\nThe final layer of weights are determined by the h values. So, for instance,\nthe value you've chosen above for the first h, h= 0.3, means that the output\nweights from the top two hidden neurons are 0.3 and -0.3, respectively. And so\non, for the entire layer of output weights.\n\nFinally, the bias on the output neuron is 0.\n\nThat's everything: we now have a complete description of a neural network\nwhich does a pretty good job computing our original goal function. And we\nunderstand how to improve the quality of the approximation by improving the\nnumber of hidden neurons.\n\nWhat's more, there was nothing special about our original goal function,\nf(x)=0.2+0.4x2+0.3sin(15x)+0.05cos(50x). We could have used this procedure for\nany continuous function from [0,1] to [0,1]. In essence, we're using our\nsingle-layer neural networks to build a lookup table for the function. And\nwe'll be able to build on this idea to provide a general proof of\nuniversality.\n\n### Many input variables\n\nLet's extend our results to the case of many input variables. This sounds\ncomplicated, but all the ideas we need can be understood in the case of just\ntwo inputs. So let's address the two-input case.\n\nWe'll start by considering what happens when we have two inputs to a neuron:\n\nHere, we have inputs x and y, with corresponding weights w1 and w2, and a bias\nb on the neuron. Let's set the weight w2 to 0, and then play around with the\nfirst weight, w1, and the bias, b, to see how they affect the output from the\nneuron:\n\nx=1y=1Output\n\nAs you can see, with w2=0 the input y makes no difference to the output from\nthe neuron. It's as though x is the only input.\n\nGiven this, what do you think happens when we increase the weight w1 to\nw1=100, with w2 remaining 0? If you don't immediately see the answer, ponder\nthe question for a bit, and see if you can figure out what happens. Then try\nit out and see if you're right. I've shown what happens in the following\nmovie:\n\nJust as in our earlier discussion, as the input weight gets larger the output\napproaches a step function. The difference is that now the step function is in\nthree dimensions. Also as before, we can move the location of the step point\naround by modifying the bias. The actual location of the step point is\nsx\u2261\u2212b/w1.\n\nLet's redo the above using the position of the step as the parameter:\n\nx=1y=1Output\n\nHere, we assume the weight on the x input has some large value - I've used\nw1=1000 - and the weight w2=0. The number on the neuron is the step point, and\nthe little x above the number reminds us that the step is in the x direction.\nOf course, it's also possible to get a step function in the y direction, by\nmaking the weight on the y input very large (say, w2=1000), and the weight on\nthe x equal to 0, i.e., w1=0:\n\nx=1y=1Output\n\nThe number on the neuron is again the step point, and in this case the little\ny above the number reminds us that the step is in the y direction. I could\nhave explicitly marked the weights on the x and y inputs, but decided not to,\nsince it would make the diagram rather cluttered. But do keep in mind that the\nlittle y marker implicitly tells us that the y weight is large, and the x\nweight is 0.\n\nWe can use the step functions we've just constructed to compute a three-\ndimensional bump function. To do this, we use two neurons, each computing a\nstep function in the x direction. Then we combine those step functions with\nweight h and \u2212h, respectively, where h is the desired height of the bump. It's\nall illustrated in the following diagram:\n\nx=1y=1Weighted output from hidden layer\n\nTry changing the value of the height, h. Observe how it relates to the weights\nin the network. And see how it changes the height of the bump function on the\nright.\n\nAlso, try changing the step point 0.30 associated to the top hidden neuron.\nWitness how it changes the shape of the bump. What happens when you move it\npast the step point 0.70 associated to the bottom hidden neuron?\n\nWe've figured out how to make a bump function in the x direction. Of course,\nwe can easily make a bump function in the y direction, by using two step\nfunctions in the y direction. Recall that we do this by making the weight\nlarge on the y input, and the weight 0 on the x input. Here's the result:\n\nx=1y=1Weighted output from hidden layer\n\nThis looks nearly identical to the earlier network! The only thing explicitly\nshown as changing is that there's now little y markers on our hidden neurons.\nThat reminds us that they're producing y step functions, not x step functions,\nand so the weight is very large on the y input, and zero on the x input, not\nvice versa. As before, I decided not to show this explicitly, in order to\navoid clutter.\n\nLet's consider what happens when we add up two bump functions, one in the x\ndirection, the other in the y direction, both of height h:\n\nx=1y=1Weighted output from hidden layer\n\nTo simplify the diagram I've dropped the connections with zero weight. For\nnow, I've left in the little x and y markers on the hidden neurons, to remind\nyou in what directions the bump functions are being computed. We'll drop even\nthose markers later, since they're implied by the input variable.\n\nTry varying the parameter h. As you can see, this causes the output weights to\nchange, and also the heights of both the x and y bump functions.\n\nWhat we've built looks a little like a tower function:\n\nx=1y=1Tower function\n\nIf we could build such tower functions, then we could use them to approximate\narbitrary functions, just by adding up many towers of different heights, and\nin different locations:\n\nx=1y=1Many towers\n\nOf course, we haven't yet figured out how to build a tower function. What we\nhave constructed looks like a central tower, of height 2h, with a surrounding\nplateau, of height h.\n\nBut we can make a tower function. Remember that earlier we saw neurons can be\nused to implement a type of if-then-else statement:\n\n    \n    \n    if input >= threshold: output 1 else: output 0\n\nThat was for a neuron with just a single input. What we want is to apply a\nsimilar idea to the combined output from the hidden neurons:\n\n    \n    \n    if combined output from hidden neurons >= threshold: output 1 else: output 0\n\nIf we choose the threshold appropriately - say, a value of 3h/2, which is\nsandwiched between the height of the plateau and the height of the central\ntower - we could squash the plateau down to zero, and leave just the tower\nstanding.\n\nCan you see how to do this? Try experimenting with the following network to\nfigure it out. Note that we're now plotting the output from the entire\nnetwork, not just the weighted output from the hidden layer. This means we add\na bias term to the weighted output from the hidden layer, and apply the sigma\nfunction. Can you find values for h and b which produce a tower? This is a bit\ntricky, so if you think about this for a while and remain stuck, here's two\nhints: (1) To get the output neuron to show the right kind of if-then-else\nbehaviour, we need the input weights (all h or \u2212h) to be large; and (2) the\nvalue of b determines the scale of the if-then-else threshold.\n\nx=1y=1Output\n\nWith our initial parameters, the output looks like a flattened version of the\nearlier diagram, with its tower and plateau. To get the desired behaviour, we\nincrease the parameter h until it becomes large. That gives the if-then-else\nthresholding behaviour. Second, to get the threshold right, we'll choose\nb\u2248\u22123h/2. Try it, and see how it works!\n\nHere's what it looks like, when we use h=10:\n\nEven for this relatively modest value of h, we get a pretty good tower\nfunction. And, of course, we can make it as good as we want by increasing h\nstill further, and keeping the bias as b=\u22123h/2.\n\nLet's try gluing two such networks together, in order to compute two different\ntower functions. To make the respective roles of the two sub-networks clear\nI've put them in separate boxes, below: each box computes a tower function,\nusing the technique described above. The graph on the right shows the weighted\noutput from the second hidden layer, that is, it's a weighted combination of\ntower functions.\n\nx=1y=1Weighted output\n\nIn particular, you can see that by modifying the weights in the final layer\nyou can change the height of the output towers.\n\nThe same idea can be used to compute as many towers as we like. We can also\nmake them as thin as we like, and whatever height we like. As a result, we can\nensure that the weighted output from the second hidden layer approximates any\ndesired function of two variables:\n\nx=1y=1Many towers\n\nIn particular, by making the weighted output from the second hidden layer a\ngood approximation to \u03c3\u22121\u2218f, we ensure the output from our network will be a\ngood approximation to any desired function, f.\n\nWhat about functions of more than two variables?\n\nLet's try three variables x1,x2,x3. The following network can be used to\ncompute a tower function in four dimensions:\n\nHere, the x1,x2,x3 denote inputs to the network. The s1,t1 and so on are step\npoints for neurons - that is, all the weights in the first layer are large,\nand the biases are set to give the step points s1,t1,s2,.... The weights in\nthe second layer alternate +h,\u2212h, where h is some very large number. And the\noutput bias is \u22125h/2.\n\nThis network computes a function which is 1 provided three conditions are met:\nx1 is between s1 and t1; x2 is between s2 and t2; and x3 is between s3 and t3.\nThe network is 0 everywhere else. That is, it's a kind of tower which is 1 in\na little region of input space, and 0 everywhere else.\n\nBy gluing together many such networks we can get as many towers as we want,\nand so approximate an arbitrary function of three variables. Exactly the same\nidea works in m dimensions. The only change needed is to make the output bias\n(\u2212m+1/2)h, in order to get the right kind of sandwiching behavior to level the\nplateau.\n\nOkay, so we now know how to use neural networks to approximate a real-valued\nfunction of many variables. What about vector-valued functions\nf(x1,...,xm)\u2208Rn? Of course, such a function can be regarded as just n separate\nreal-valued functions, f1(x1,...,xm),f2(x1,...,xm), and so on. So we create a\nnetwork approximating f1, another network for f2, and so on. And then we\nsimply glue all the networks together. So that's also easy to cope with.\n\n#### Problem\n\n  * We've seen how to use networks with two hidden layers to approximate an arbitrary function. Can you find a proof showing that it's possible with just a single hidden layer? As a hint, try working in the case of just two input variables, and showing that: (a) it's possible to get step functions not just in the x or y directions, but in an arbitrary direction; (b) by adding up many of the constructions from part (a) it's possible to approximate a tower function which is circular in shape, rather than rectangular; (c) using these circular towers, it's possible to approximate an arbitrary function. To do part (c) it may help to use ideas from a bit later in this chapter.\n\n### Extension beyond sigmoid neurons\n\nWe've proved that networks made up of sigmoid neurons can compute any\nfunction. Recall that in a sigmoid neuron the inputs x1,x2,... result in the\noutput \u03c3(\u2211jwjxj+b), where wj are the weights, b is the bias, and \u03c3 is the\nsigmoid function:\n\nWhat if we consider a different type of neuron, one using some other\nactivation function, s(z):\n\nThat is, we'll assume that if our neurons has inputs x1,x2,..., weights\nw1,w2,... and bias b, then the output is s(\u2211jwjxj+b).\n\nWe can use this activation function to get a step function, just as we did\nwith the sigmoid. Try ramping up the weight in the following, say to w=100:\n\nJust as with the sigmoid, this causes the activation function to contract, and\nultimately it becomes a very good approximation to a step function. Try\nchanging the bias, and you'll see that we can set the position of the step to\nbe wherever we choose. And so we can use all the same tricks as before to\ncompute any desired function.\n\nWhat properties does s(z) need to satisfy in order for this to work? We do\nneed to assume that s(z) is well-defined as z\u2192\u2212\u221e and z\u2192\u221e. These two limits are\nthe two values taken on by our step function. We also need to assume that\nthese limits are different from one another. If they weren't, there'd be no\nstep, simply a flat graph! But provided the activation function s(z) satisfies\nthese properties, neurons based on such an activation function are universal\nfor computation.\n\n#### Problems\n\n  * Earlier in the book we met another type of neuron known as a rectified linear unit. Explain why such neurons don't satisfy the conditions just given for universality. Find a proof of universality showing that rectified linear units are universal for computation.\n\n  * Suppose we consider linear neurons, i.e., neurons with the activation function s(z)=z. Explain why linear neurons don't satisfy the conditions just given for universality. Show that such neurons can't be used to do universal computation.\n\n### Fixing up the step functions\n\nUp to now, we've been assuming that our neurons can produce step functions\nexactly. That's a pretty good approximation, but it is only an approximation.\nIn fact, there will be a narrow window of failure, illustrated in the\nfollowing graph, in which the function behaves very differently from a step\nfunction:\n\nIn these windows of failure the explanation I've given for universality will\nfail.\n\nNow, it's not a terrible failure. By making the weights input to the neurons\nbig enough we can make these windows of failure as small as we like.\nCertainly, we can make the window much narrower than I've shown above -\nnarrower, indeed, than our eye could see. So perhaps we might not worry too\nmuch about this problem.\n\nNonetheless, it'd be nice to have some way of addressing the problem.\n\nIn fact, the problem turns out to be easy to fix. Let's look at the fix for\nneural networks computing functions with just one input and one output. The\nsame ideas work also to address the problem when there are more inputs and\noutputs.\n\nIn particular, suppose we want our network to compute some function, f. As\nbefore, we do this by trying to design our network so that the weighted output\nfrom our hidden layer of neurons is \u03c3\u22121\u2218f(x):\n\nIf we were to do this using the technique described earlier, we'd use the\nhidden neurons to produce a sequence of bump functions:\n\nAgain, I've exaggerated the size of the windows of failure, in order to make\nthem easier to see. It should be pretty clear that if we add all these bump\nfunctions up we'll end up with a reasonable approximation to \u03c3\u22121\u2218f(x), except\nwithin the windows of failure.\n\nSuppose that instead of using the approximation just described, we use a set\nof hidden neurons to compute an approximation to half our original goal\nfunction, i.e., to \u03c3\u22121\u2218f(x)/2. Of course, this looks just like a scaled down\nversion of the last graph:\n\nAnd suppose we use another set of hidden neurons to compute an approximation\nto \u03c3\u22121\u2218f(x)/2, but with the bases of the bumps shifted by half the width of a\nbump:\n\nNow we have two different approximations to \u03c3\u22121\u2218f(x)/2. If we add up the two\napproximations we'll get an overall approximation to \u03c3\u22121\u2218f(x). That overall\napproximation will still have failures in small windows. But the problem will\nbe much less than before. The reason is that points in a failure window for\none approximation won't be in a failure window for the other. And so the\napproximation will be a factor roughly 2 better in those windows.\n\nWe could do even better by adding up a large number, M, of overlapping\napproximations to the function \u03c3\u22121\u2218f(x)/M. Provided the windows of failure are\nnarrow enough, a point will only ever be in one window of failure. And\nprovided we're using a large enough number M of overlapping approximations,\nthe result will be an excellent overall approximation.\n\n### Conclusion\n\nThe explanation for universality we've discussed is certainly not a practical\nprescription for how to compute using neural networks! In this, it's much like\nproofs of universality for NAND gates and the like. For this reason, I've\nfocused mostly on trying to make the construction clear and easy to follow,\nand not on optimizing the details of the construction. However, you may find\nit a fun and instructive exercise to see if you can improve the construction.\n\nAlthough the result isn't directly useful in constructing networks, it's\nimportant because it takes off the table the question of whether any\nparticular function is computable using a neural network. The answer to that\nquestion is always \"yes\". So the right question to ask is not whether any\nparticular function is computable, but rather what's a good way to compute the\nfunction.\n\nThe universality construction we've developed uses just two hidden layers to\ncompute an arbitrary function. Furthermore, as we've discussed, it's possible\nto get the same result with just a single hidden layer. Given this, you might\nwonder why we would ever be interested in deep networks, i.e., networks with\nmany hidden layers. Can't we simply replace those networks with shallow,\nsingle hidden layer networks?\n\nChapter acknowledgments: Thanks to Jen Dodd and Chris Olah for many\ndiscussions about universality in neural networks. My thanks, in particular,\nto Chris for suggesting the use of a lookup table to prove universality. The\ninteractive visual form of the chapter is inspired by the work of people such\nas Mike Bostock, Amit Patel, Bret Victor, and Steven Wittens.\n\nWhile in principle that's possible, there are good practical reasons to use\ndeep networks. As argued in Chapter 1, deep networks have a hierarchical\nstructure which makes them particularly well adapted to learn the hierarchies\nof knowledge that seem to be useful in solving real-world problems. Put more\nconcretely, when attacking problems such as image recognition, it helps to use\na system that understands not just individual pixels, but also increasingly\nmore complex concepts: from edges to simple geometric shapes, all the way up\nthrough complex, multi-object scenes. In later chapters, we'll see evidence\nsuggesting that deep networks do a better job than shallow networks at\nlearning such hierarchies of knowledge. To sum up: universality tells us that\nneural networks can compute any function; and empirical evidence suggests that\ndeep networks are the networks best adapted to learn the functions useful in\nsolving many real-world problems.\n\n. .\n\nIn academic work, please cite this book as: Michael A. Nielsen, \"Neural\nNetworks and Deep Learning\", Determination Press, 2015\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial 3.0\nUnported License. This means you're free to copy, share, and build on this\nbook, but not to sell it. If you're interested in commercial use, please\ncontact me. Last update: Thu Dec 26 15:26:33 2019\n\n", "frontpage": false}
