{"aid": "40023095", "title": "Losing the Imitation Game", "url": "https://jenniferplusplus.com/losing-the-imitation-game/", "domain": "jenniferplusplus.com", "votes": 1, "user": "ColinWright", "posted_at": "2024-04-13 13:44:34", "comments": 0, "source_title": "Losing the imitation game", "source_text": "Losing the imitation game\n\nJennifer++\n\nArtificial Intelligence Featured\n\n# Losing the imitation game\n\nAI cannot develop software for you, but that's not going to stop people from\ntrying to make it happen anyway. And that is going to turn all of the easy\nsoftware development problems into hard problems.\n\n#### Jennifer Moore\n\nApr 9, 2023 \u2022 10 min read\n\nIf you've been anywhere near major news or social media in the last few\nmonths, you've probably heard repeatedly about so-called AI, ChatGPT, and\nlarge language models (LLMs). The hype surrounding these topics has been\nintense. And the rhetoric has been manipulative, to say the least. Proponents\nhave claimed that their models are or soon will be generally intelligent, in\nthe way we mean humans are intelligent. They're not. They've claimed that\ntheir AI will eliminate whole categories of jobs. And they've claimed that\ndeveloping these systems further and faster is both necessary and urgent,\njustified by science fiction dressed up as arguments for some sort of \"safety\"\nthat I find to be incoherent.\n\nThe outer layer of hype surrounding AI\u2014and LLM chatbots in particular\u2014is that\nthey will become indispensable tools of daily work, and entirely replace\npeople in numerous categories of jobs. These claims have included the fields\nof medicine, law, and education, among others. I think it's nonsense. They\nimagine self-teaching classrooms and self-diagnosing fitness gadgets. These\nthings will probably not even work as well as self-driving cars, which is to\nsay: only well enough to be dangerous. Of course, that's not stopping people\nfrom pushing these fantasies, anyway. But these fields are not my area of\nexpertise. My expertise is in software engineering. We should know better, but\nsoftware developers are falling victim to the same kind of AI fantasies.\n\n> A computer can never be held accountable. Therefore, a computer must never\n> make a management decision.\n\nWhile the capabilities are fantasy, the dangers are real. These tools have\ndenied people jobs, housing, and welfare. All erroneously. They have denied\npeople bail and parole, in such a racist way it would be comical if it wasn't\nreal. And the actual function of AI in all of these situations is to obscure\nliability for the harm these decisions cause.\n\n## So-Called AI\n\nArtificial Intelligence is an unhelpful term. It serves as a vehicle for\npeople's invalid assumptions. It hand-waves an enormous amount of complexity\nregarding what \"intelligence\" even is or means. And it encourages people\nconfuse concepts like cognition, agency, autonomy, sentience, consciousness,\nand a host of related ideas. However, AI is the vernacular term for this whole\nconcept, so it's the one I'll use. I'll let other people push that boulder,\nI'm here to push a different one.\n\nThose concepts are not simple ideas, either. Describing them gets into hard\nquestions of psychology, neurology, anthropology, and philosophy. At least.\nGiven that these are domains that the tech field has routinely dismissed as\nunimportant for decades, maybe it shouldn't be surprising that techies as a\ngroup are now completely unprepared to take a critical view of claims about\nAI.\n\n### The Turing Test\n\nCertainly part of how we got here is the Turing test. That is, the pop science\nreduction of Alan Turing's imitation game. The actual proposal is more\nsubstantial. And taking it seriously produces some interesting reading. But\nthe common notion is something like a computer is intelligent if it can\nreliably pass as human in conversation. I hope seeing it spelled out like that\nmakes it clear how dramatically that overreaches. Still, it's the framework\nthat people have, and it informs our situation. I think the bit that is\nparticularly informative is the focus on natural, conversational language. And\nalso, the deception inherent in the imitation game scenario, but I'll come\nback to that.\n\nOur understanding of intelligence is a moving target. We only have one\nmeaningful fixed point to work from. We assert that humans are intelligent.\nWhether anything else is, is not certain. What intelligence itself is, is not\ncertain. Not too long ago, a lot of theory rested on our ability to create and\nuse tools. But then that ability turned out to be not as rare as we thought,\nand the consensus about the boundaries of intelligence shifted. Lately, it has\nfallen to our use of abstract language. That brings us back to AI chatbots. We\nsuddenly find ourselves confronted with machines that seem to have a command\nof the English language that rivals our own. This is unfamiliar territory, and\nat some level it's reasonable that people will reach for explanations and come\nup with pop science notions like the Turing test.\n\n> Language: any system of formalized symbols, signs, sounds, gestures, or the\n> like used or conceived as a means of communicating thought, emotion, etc.\n\n## Language Models\n\nChatGPT and the like are powered by large language models. Linguistics is\ncertainly an interesting field, and we can learn a lot about ourselves and\neach other by studying it. But language itself is probably less than you think\nit is. Language is not comprehension, for example. It's not feeling, or\nintent, or awareness. It's just a system for communication. Our common lived\nexperiences give us lots of examples that anything which can respond to and\nproduce common language in a sensible-enough way must be intelligent. But\nthat's because only other people have ever been able to do that before. It's\nactually an incredible leap to assume, based on nothing else, that a machine\nwhich does the same thing is also intelligent. It's much more reasonable to\nquestion whether the link we assume exists between language and intelligence\nactually exists. Certainly, we should wonder if the two are as tightly coupled\nas we thought.\n\nThat coupling seems even more improbable when you consider what a language\nmodel does, and\u2014more importantly\u2014doesn't consist of. A language model is a\nstatistical model of probability relationships between linguistic tokens. It's\nnot quite this simple, but those tokens can be thought of as words. They might\nalso be multi-word constructs, like names or idioms. You might find \"raining\ncats and dogs\" in a large language model, for instance. But you also might\nnot. The model might reproduce that idiom based on probability factors\ninstead. The relationships between these tokens span a large number of\nparameters. In fact, that's much of what's being referenced when we call a\nmodel large. Those parameters represent grammar rules, stylistic patterns, and\nliterally millions of other things.\n\nWhat those parameters don't represent is anything like knowledge or\nunderstanding. That's just not what LLMs do. The model doesn't know what those\ntokens mean. I want to say it only knows how they're used, but even that is\nover stating the case, because it doesn't know things. It models how those\ntokens are used. When the model works on a token like \"Jennifer\", there are\nparameters and classifications that capture what we would recognize as things\nlike the fact that it's a name, it has a degree of formality, it's feminine\ncoded, it's common, and so on. But the model doesn't know, or understand, or\ncomprehend anything about that data any more than a spreadsheet containing the\nsame information would understand it.\n\n## Mental Models\n\nSo, a language model can reproduce patterns of language. And there's no\nparticular reason it would need to be constrained to natural, conversational\nlanguage, either. Anything that's included in the set of training data is fair\ngame. And it turns out that there's been a lot of digital ink spent on writing\nsoftware and talking about writing software. Which means those linguistic\npatterns and relationships can be captured and modeled just like any other.\nAnd sure, there are some programming tasks where just a probabilistic assembly\nof linguistic tokens will produce a result you want. If you prompt ChatGPT to\nwrite a python function that fetches a file from S3 and records something\nabout it in DynamoDB, I would bet that it just does, and that the result\nbasically works. But then, if you prompt ChatGPT to write an authorization\nrule for a new role in your application's proprietary RBAC system, I bet that\nit again just does, and that the result is useless, or worse.\n\n### Programming as Theory Building\n\nNon-trivial software changes over time. The requirements evolve, flaws need to\nbe corrected, the world itself changes and violates assumptions we made in the\npast, or it just takes longer than one working session to finish. And all the\nwhile, that software is running in the real world. All of the design choices\ntaken and not taken throughout development; all of the tradeoffs; all of the\nassumptions; all of the expected and unexpected situations the software\nencounters form a hugely complex system that includes both the software itself\nand the people building it. And that system is continuously changing.\n\nThe fundamental task of software development is not writing out the syntax\nthat will execute a program. The task is to build a mental model of that\ncomplex system, make sense of it, and manage it over time.\n\nTo circle back to AI like ChatGPT, recall what it actually does and doesn't\ndo. It doesn't know things. It doesn't learn, or understand, or reason about\nthings. What it does is probabilistically generate text in response to a\nprompt. That can work well enough if the context you need to describe the goal\nis so simple that you can write it down and include it with the prompt. But\nthat's a very small class of essentially trivial problems. What's worse is\nthere's no clear boundary between software development problems that are\ntrivial enough for an LLM to be helpful vs being unhelpful. The LLM doesn't\nknow the difference, either. In fact, the LLM doesn't know the difference\nbetween being tasked to write javascript or a haiku, beyond the different\nparameters each prompt would activate. And it will readily do a bad job of\nresponding to either prompt, with no notion that there even is such a thing as\na good or bad response.\n\nSoftware development is complex, for any non-trivial project. But complexity\nis hard. Overwhelmingly, when we in the software field talk about developing\nsoftware, we've dealt with that complexity by ignoring it. We write code\nsamples that fit in a tweet. We reduce interviews to trivia challenges about\nalgorithmic minutia. When we're feeling really ambitious, we break out the\ntodo app. These are contrivances that we make to collapse technical\ndiscussions into an amount of context that we can share in the few minutes we\nhave available. But there seem to be a lot of people who either don't\nunderstand that or choose to ignore it. They frame the entire process of\nsoftware development as being equivalent to writing the toy problems and code\nsamples we use among general audiences.\n\n## Automating the Easy Part\n\nThe intersection of AI hype with that elision of complexity seems to have\nproduced a kind of AI booster fanboy, and they're making personal brands out\nof convincing people to use AI to automate programming. This is an incredibly\nbad idea. The hard part of programming is building and maintaining a useful\nmental model of a complex system. The easy part is writing code. They're\npositioning this tool as a universal solution, but it's only capable of doing\nthe easy part. And even then, it's not able to do that part reliably. Human\nengineers will still have to evaluate and review the code that an AI writes.\nBut they'll now have to do it without the benefit of having anyone who\nunderstands it. No one can explain it. No one can explain what they were\nthinking when they wrote it. No one can explain what they expect it to do.\nEvery choice made in writing software is a choice not to do things in a\ndifferent way. And there will be no one who can explain why they made this\nchoice, and not those others. In part because it wasn't even a decision that\nwas made. It was a probability that was realized.\n\n> [A programmer's] education has to emphasize the exercise of theory building,\n> side by side with the acquisition of knowledge of data processing and\n> notations.\n\nBut it's worse than AI being merely inadequate for software development.\nDeveloping that mental model requires learning about the system. We do that by\nexploring it. We have to interact with it. We manipulate and change the\nsystem, then observe how it responds. We do that by performing the easy,\nsimple programing tasks. Delegating that learning work to machines is the tech\nequivalent of eating our seed corn. That holds true beyond the scope of any\nteam, or project, or even company. Building those mental models is itself a\nskill that has to be learned. We do that by doing it, there's not another way.\nAs people, and as a profession, we need the early career jobs so that we can\nlearn how to do the later career ones. Giving those learning opportunities to\ncomputers instead of people is profoundly myopic.\n\n## Imitation Game\n\nIf this is the first time you're hearing or reading these sentiments, that's\nnot too surprising. The marketing hype surrounding AI in recent months has\nbeen intense, pervasive, and deceptive. AI is usually cast as being hyper\ncompetent, and superhuman. To hear the capitalists who are developing it, AI\nis powerful, mysterious, dangerous, and inevitable. In reality, it's almost\nnone of those things. I'll grant that AI can be dangerous, but not for the\nreasons they claim. AI is complicated and misunderstood, and this is by\ndesign. They cloak it in rhetoric that's reminiscent of the development of\natomic weapons, and they literally treat the research like an arms race.\n\nI'm sure there are many reasons they do this. But one of the effects it has is\nto obscure the very mundane, serious, and real harms that their AI models are\ncurrently perpetuating. Moderating the output of these models depends on\narmies of low paid and precariously employed human reviewers, mostly in Kenya.\nThey're subjected to the raw, unfiltered linguistic sewage that is the result\nof training a language model on uncurated text found on the public internet.\nIf ChatGPT doesn't wantonly repeat the very worst of the things you can find\non reddit, 4chan, or kiwi farms, that is because it's being dumped on Kenyan\ngig workers instead.\n\nThat's all to say nothing of the violations of intellectual property and basic\nconsent that was required to train the models in the first place. The scale of\nthe theft and exploitation required to build the data sets these models train\nwith is almost inconceivable. And the energy consumption and e-waste produced\nby these systems is staggering.\n\nAll of this is done to automate the creation of writing or media that is\ndesigned to deceive people. It's intended to seem like people, or like work\ndone by people. The deception, from both the creators and the AI models\nthemselves, is pervasive. There may be real, productive uses for these kinds\nof tools. There may be ways to build and deploy them ethically and\nsustainably. But that's not the situation with the instances we have. AI, as\nit's been built today, is a tool to sell out our collective futures in order\nto enrich already wealthy people. They like to frame it as being akin to\nnuclear science. But we should really see it as being more like fossil fuels.\n\nCover photo by Helena Jankovi\u010dov\u00e1 Kov\u00e1\u010dov\u00e1\n\n## The free software commons\n\nFree and open source software has become a modern commons, but now it's\nvulnerable. Freedom isn't sufficient to secure it for the future.\n\nApr 5, 2024 10 min read\n\n## Letterbook - No universal translators\n\nIt turns out that federating is really hard. But, I managed it. Now I would\nlove to have your help with everything that comes after that.\n\nDec 25, 2023 6 min read\n\nFeatured\n\n## Letterbook\n\nWe build tools, but we are also shaped by the affordances of those tools. I'm\nbuilding Letterbook. I would like it if you join me.\n\nAug 16, 2023 5 min read\n\nJennifer++ \u00a9 2024\n\nPowered by Ghost\n\n", "frontpage": false}
