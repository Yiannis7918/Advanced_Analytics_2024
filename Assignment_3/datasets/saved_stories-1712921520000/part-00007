{"aid": "40008938", "title": "Farhadsakhodi1379 Gmail.cok", "url": "https://blog.cloudflare.com/making-full-stack-easier-d1-ga-hyperdrive-queues", "domain": "cloudflare.com", "votes": 1, "user": "Farhadsakhodi", "posted_at": "2024-04-12 02:45:35", "comments": 0, "source_title": "Making state easy with D1 GA, Hyperdrive, Queues and Workers Analytics Engine updates", "source_text": "Making state easy with D1 GA, Hyperdrive, Queues and Workers Analytics Engine\nupdates\n\nGet Started Free|Contact Sales|\n\n## The Cloudflare Blog\n\nSubscribe to receive notifications of new posts:\n\n# Making state easy with D1 GA, Hyperdrive, Queues and Workers Analytics\nEngine updates\n\n04/01/2024\n\n  * Rita Kozlov\n\n  * Matt Silverlock\n\n9 min read\n\nThis post is also available in \u7e41\u9ad4\u4e2d\u6587, \u7b80\u4f53\u4e2d\u6587, \u65e5\u672c\u8a9e, \ud55c\uad6d\uc5b4, Deutsch, Fran\u00e7ais and\nEspa\u00f1ol.\n\n### Making full-stack easier\n\nToday might be April Fools, and while we like to have fun as much as anyone\nelse, we like to use this day for serious announcements. In fact, as of today,\nthere are over 2 million developers building on top of Cloudflare\u2019s platform \u2014\nthat\u2019s no joke!\n\nTo kick off this Developer Week, we\u2019re flipping the big \u201cproduction ready\u201d\nswitch on three products: D1, our serverless SQL database; Hyperdrive, which\nmakes your existing databases feel like they\u2019re distributed (and faster!); and\nWorkers Analytics Engine, our time-series database.\n\nWe\u2019ve been on a mission to allow developers to bring their entire stack to\nCloudflare for some time, but what might an application built on Cloudflare\nlook like?\n\nThe diagram itself shouldn\u2019t look too different from the tools you\u2019re already\nfamiliar with: you want a database for your core user data. Object storage for\nassets and user content. Maybe a queue for background tasks, like email or\nupload processing. A fast key-value store for runtime configuration. Maybe\neven a time-series database for aggregating user events and/or performance\ndata. And that\u2019s before we get to AI, which is increasingly becoming a core\npart of many applications in search, recommendation and/or image analysis\ntasks (at the very least!).\n\nYet, without having to think about it, this architecture runs on Region:\nEarth, which means it\u2019s scalable, reliable and fast \u2014 all out of the box.\n\n### D1 GA: Production Ready\n\nYour core database is one of the most critical pieces of your infrastructure.\nIt needs to be ultra-reliable. It can\u2019t lose data. It needs to scale. And so\nwe\u2019ve been heads down over the last year getting the pieces into place to make\nsure D1 is production-ready, and we\u2019re extremely excited to say that D1 \u2014 our\nglobal, serverless SQL database \u2014 is now Generally Available.\n\nThe GA for D1 lands some of the most asked-for features, including:\n\n  * Support for 10GB databases \u2014 and 50,000 databases per account;\n  * New data export capabilities; and\n  * Enhanced query debugging (we call it \u201cD1 Insights\u201d) \u2014 that allows you to understand what queries are consuming the most time, cost, or that are just plain inefficient...\n\n... to empower developers to build production-ready applications with D1 to\nmeet all their relational SQL needs. And importantly, in an era where the\nconcept of a \u201cfree plan\u201d or \u201chobby plan\u201d is seemingly at risk, we have no\nintention of removing the free tier for D1 or reducing the 25 billion row\nreads included in the $5/mo Workers Paid plan:\n\nPlan| Rows Read| Rows Written| Storage  \n---|---|---|---  \nWorkers Paid| First 25 billion / month included\\+ $0.001 / million rows| First\n50 million / month included\\+ $1.00 / million rows| First 5 GB included\\+\n$0.75 / GB-mo  \nWorkers Free| 5 million / day| 100,000 / day| 5 GB (total)  \n  \nFor those who\u2019ve been following D1 since the start: this is the same pricing\nwe announced at open beta\n\nBut things don\u2019t just stop at GA: we have some major new features lined up for\nD1, including global read replication, even larger databases, more Time Travel\ncapabilities that will allow you to branch your database, and new APIs for\ndynamically querying and/or creating new databases-on-the-fly from within a\nWorker.\n\nD1\u2019s read replication will automatically deploy read replicas as needed to get\ndata closer to your users: and without you having to spin up, manage scaling,\nor run into consistency (replication lag) issues. Here\u2019s a sneak preview of\nwhat D1\u2019s upcoming Replication API looks like:\n\n    \n    \n    export default { async fetch(request: Request, env: Env) { const {pathname} = new URL(request.url); let resp = null; let session = env.DB.withSession(token); // An optional commit token or mode // Handle requests within the session. if (pathname === \"/api/orders/list\") { // This statement is a read query, so it will work against any // replica that has a commit equal or later than `token`. const { results } = await session.prepare(\"SELECT * FROM Orders\"); resp = Response.json(results); } else if (pathname === \"/api/orders/add\") { order = await request.json(); // This statement is a write query, so D1 will send the query to // the primary, which always has the latest commit token. await session.prepare(\"INSERT INTO Orders VALUES (?, ?, ?)\") .bind(order.orderName, order.customer, order.value); .run(); // In order for the application to be correct, this SELECT // statement must see the results of the INSERT statement above. // // D1's new Session API keeps track of commit tokens for queries // within the session and will ensure that we won't execute this // query until whatever replica we're using has seen the results // of the INSERT. const { results } = await session.prepare(\"SELECT COUNT(*) FROM Orders\") .run(); resp = Response.json(results); } // Set the token so we can continue the session in another request. resp.headers.set(\"x-d1-token\", session.latestCommitToken); return resp; } }\n\nImportantly, we will give developers the ability to maintain session-based\nconsistency, so that users still see their own changes reflected, whilst still\nbenefiting from the performance and latency gains that replication can bring.\n\nYou can learn more about how D1\u2019s read replication works under the hood in our\ndeep-dive post, and if you want to start building on D1 today, head to our\ndeveloper docs to create your first database.\n\n### Hyperdrive: GA\n\nWe launched Hyperdrive into open beta last September during Birthday Week, and\nit\u2019s now Generally Available \u2014 or in other words, battle-tested and\nproduction-ready.\n\nIf you\u2019re not caught up on what Hyperdrive is, it\u2019s designed to make the\ncentralized databases you already have feel like they\u2019re global. We use our\nglobal network to get faster routes to your database, keep connection pools\nprimed, and cache your most frequently run queries as close to users as\npossible.\n\nImportantly, Hyperdrive supports the most popular drivers and ORM (Object\nRelational Mapper) libraries out of the box, so you don\u2019t have to re-learn or\nre-write your queries:\n\n    \n    \n    // Use the popular 'pg' driver? Easy. Hyperdrive just exposes a connection string // to your Worker. const client = new Client({ connectionString: env.HYPERDRIVE.connectionString }); await client.connect(); // Prefer using an ORM like Drizzle? Use it with Hyperdrive too. // https://orm.drizzle.team/docs/get-started-postgresql#node-postgres const client = new Client({ connectionString: env.HYPERDRIVE.connectionString }); await client.connect(); const db = drizzle(client);\n\nBut the work on Hyperdrive doesn\u2019t stop just because it\u2019s now \u201cGA\u201d. Over the\nnext few months, we\u2019ll be bringing support for the other most widely deployed\ndatabase engine there is: MySQL. We\u2019ll also be bringing support for connecting\nto databases inside private networks (including cloud VPC networks) via\nCloudflare Tunnel and Magic WAN On top of that, we plan to bring more\nconfigurability around invalidation and caching strategies, so that you can\nmake more fine-grained decisions around performance vs. data freshness.\n\nAs we thought about how we wanted to price Hyperdrive, we realized that it\njust didn\u2019t seem right to charge for it. After all, the performance benefits\nfrom Hyperdrive are not only significant, but essential to connecting to\ntraditional database engines. Without Hyperdrive, paying the latency overhead\nof 6+ round-trips to connect & query your database per request just isn\u2019t\nright.\n\nAnd so we\u2019re happy to announce that for any developer on a Workers Paid plan,\nHyperdrive is free. That includes both query caching and connection pooling,\nas well as the ability to create multiple Hyperdrives \u2014 to separate different\napplications, prod vs. staging, or to provide different configurations (cached\nvs. uncached, for example).\n\nPlan| Price per query| Connection Pooling  \n---|---|---  \nWorkers Paid| $0| $0  \n  \nTo get started with Hyperdrive, head over to the docs to learn how to connect\nyour existing database and start querying it from your Workers.\n\n### Queues: Pull From Anywhere\n\nThe task queue is an increasingly critical part of building a modern, full-\nstack application, and this is what we had in mind when we originally\nannounced the open beta of Queues. We\u2019ve since been working on several major\nQueues features, and we\u2019re launching two of them this week: pull-based\nconsumers and new message delivery controls.\n\nAny HTTP-speaking client can now pull messages from a queue: call the new\n/pull endpoint on a queue to request a batch of messages, and call the /ack\nendpoint to acknowledge each message (or batch of messages) as you\nsuccessfully process them:\n\n    \n    \n    // Pull and acknowledge messages from a Queue using any HTTP client $ curl \"https://api.cloudflare.com/client/v4/accounts/${CF_ACCOUNT_ID}/queues/${QUEUE_ID}/messages/pull\" -X POST --data '{\"visibilityTimeout\":10000,\"batchSize\":100}}' \\ -H \"Authorization: Bearer ${QUEUES_TOKEN}\" \\ -H \"Content-Type:application/json\" // Ack the messages you processed successfully; mark others to be retried. $ curl \"https://api.cloudflare.com/client/v4/accounts/${CF_ACCOUNT_ID}/queues/${QUEUE_ID}/messages/ack\" -X POST --data '{\"acks\":[\"lease-id-1\", \"lease-id-2\"],\"retries\":[\"lease-id-100\"]}' \\ -H \"Authorization: Bearer ${QUEUES_TOKEN}\" \\ -H \"Content-Type:application/json\"\n\nA pull-based consumer can run anywhere, allowing you to run queue consumers\nalongside your existing legacy cloud infrastructure. Teams inside Cloudflare\nadopted this early on, with one use-case focused on writing device telemetry\nto a queue from our 310+ data centers and consuming within some of our back-\nof-house infrastructure running on Kubernetes. Importantly, our globally\ndistributed queue infrastructure means that messages are retained within the\nqueue until the consumer is ready to process them.\n\nQueues also now supports delaying messages, both when sending to a queue, as\nwell as when marking a message for retry. This can be useful to queue (pun\nintended) tasks for the future, as well apply a backoff mechanism if an\nupstream API or infrastructure has rate limits that require you to pace how\nquickly you are processing messages.\n\n    \n    \n    // Apply a delay to a message when sending it await env.YOUR_QUEUE.send(msg, { delaySeconds: 3600 }) // Delay a message (or a batch of messages) when marking it for retry for (const msg of batch.messages) { msg.retry({delaySeconds: 300}) }\n\nWe\u2019ll also be bringing substantially increased per-queue throughput over the\ncoming months on the path to getting Queues to GA. It\u2019s important to us that\nQueues is extremely reliable: lost or dropped messages means that a user\ndoesn\u2019t get their order confirmation email, that password reset notification,\nand/or their uploads processed \u2014 each of those are user-impacting and hard to\nrecover from.\n\n### Workers Analytics Engine is GA\n\nWorkers Analytics Engine provides unlimited-cardinality analytics at scale,\nvia a built-in API to write data points from Workers, and a SQL API to query\nthat data.\n\nWorkers Analytics Engine is backed by the same ClickHouse-based system we have\ndepended on for years at Cloudflare. We use it ourselves to observe the health\nof our own services, to capture product usage data for billing, and to answer\nquestions about specific customers\u2019 usage patterns. At least one data point is\nwritten to this system on nearly every request to Cloudflare\u2019s network.\nWorkers Analytics Engine lets you build your own custom analytics using this\nsame infrastructure, while we manage the hard parts for you.\n\nSince launching in beta, developers have started depending on Workers\nAnalytics Engine for these same use cases and more, from large enterprises to\nopen-source projects like Counterscale. Workers Analytics Engine has been\noperating at production scale with mission-critical workloads for years \u2014 but\nwe hadn\u2019t shared anything about pricing, until today.\n\nWe are keeping Workers Analytics Engine pricing simple, and based on two\nmetrics:\n\n  1. Data points written \u2014 every time you call writeDataPoint() in a Worker, this counts as one data point written. Every data point costs the same amount \u2014 unlike other platforms, there is no penalty for adding dimensions or cardinality, and no need to predict what the size and cost of a compressed data point might be.\n  2. Read queries \u2014 every time you post to the Workers Analytics Engine SQL API, this counts as one read query. Every query costs the same amount \u2014 unlike other platforms, there is no penalty for query complexity, and no need to reason about the number of rows of data that will be read by each query.\n\nBoth the Workers Free and Workers Paid plans will include an allocation of\ndata points written and read queries, with pricing for additional usage as\nfollows:\n\nPlan| Data points written| Read queries  \n---|---|---  \nWorkers Paid| 10 million included per month+$0.25 per additional million| 1\nmillion included per month+$1.00 per additional million  \nWorkers Free| 100,000 included per day| 10,000 included per day  \n  \nWith this pricing, you can answer, \u201chow much will Workers Analytics Engine\ncost me?\u201d by counting the number of times you call a function in your Worker,\nand how many times you make a request to a HTTP API endpoint. Napkin math,\nrather than spreadsheet math.\n\nThis pricing will be made available to everyone in coming months. Between now\nand then, Workers Analytics Engine continues to be available at no cost. You\ncan start writing data points from your Worker today \u2014 it takes just a few\nminutes and less than 10 lines of code to start capturing data. We\u2019d love to\nhear what you think.\n\n### The week is just getting started\n\nTune in to what we have in store for you tomorrow on our second day of\nDeveloper Week. If you have questions or want to show off something cool you\nalready built, please join our developer Discord.\n\nWe protect entire corporate networks, help customers build Internet-scale\napplications efficiently, accelerate any website or Internet application, ward\noff DDoS attacks, keep hackers at bay, and can help you on your journey to\nZero Trust.\n\nVisit 1.1.1.1 from any device to get started with our free app that makes your\nInternet faster and safer.\n\nTo learn more about our mission to help build a better Internet, start here.\nIf you're looking for a new career direction, check out our open positions.\n\nDiscuss on Hacker News\n\nDeveloper WeekDevelopersDeveloper PlatformD1HyperdriveQueuesCloudflare Workers\n\nFollow on X\n\nRita Kozlov|@ritakozlov_\n\nMatt Silverlock|@elithrar\n\nCloudflare|@cloudflare\n\nRelated posts\n\nApril 08, 2024 1:00 PM\n\n## Developer Week 2024 wrap-up\n\nDeveloper Week 2024 has officially come to a close. Here\u2019s a quick recap of\nthe announcements and in-depth technical explorations that went out last\nweek...\n\nBy\n\n  * Phillip Jones\n\nDeveloper Week, Developers, Product News, Cloudflare Workers, Cloudflare\nPages, Rate Limiting, API, R2 Storage, D1\n\nApril 05, 2024 3:50 PM\n\n## Cloudflare acquires Baselime to expand serverless application observability\ncapabilities\n\nToday, we\u2019re thrilled to announce that Cloudflare has acquired Baselime, a\nserverless observability company...\n\nBy\n\n  * Boris Tane,\n\n  * Rita Kozlov\n\nDeveloper Week, Developers, Developer Platform, Product News, Cloudflare\nWorkers, Observability, Acquisitions\n\nApril 05, 2024 1:01 PM\n\n## Browser Rendering API GA, rolling out Cloudflare Snippets, SWR, and\nbringing Workers for Platforms to all users\n\nBrowser Rendering API is now available to all paid Workers customers with\nimproved session management...\n\nBy\n\n  * Tanushree Sharma,\n\n  * Celso Martinho,\n\n  * Nikita Cano,\n\n  * Matt Bullock,\n\n  * Tim Kornhammar\n\nDeveloper Week, Developers, Developer Platform, Turnstile, Application\nServices, Product News, General Availability, Cloudflare Workers\n\nApril 05, 2024 1:00 PM\n\n## Cloudflare acquires PartyKit to allow developers to build real-time multi-\nuser applications\n\nWe're thrilled to announce that PartyKit, a trailblazer in enabling developers\nto craft ambitious real-time, collaborative, multiplayer applications, is now\na part of Cloudflare...\n\nBy\n\n  * Sunil Pai,\n\n  * Rita Kozlov\n\nDeveloper Week, Acquisitions, Cloudflare Workers, AI, Durable Objects\n\n  * Sales\n  * Enterprise Sales\n  * Become a Partner\n\nContact Sales:\n\n+1 (888) 993-5273\n\n  * Getting Started\n  * Pricing\n  * Case Studies\n  * White Papers\n  * Webinars\n  * Learning Center\n\n  * Community\n  * Community Hub\n  * Project Galileo\n  * Athenian Project\n  * Cloudflare TV\n\n  * Developers\n  * Developer Hub\n  * Developers Discord\n  * Cloudflare Workers\n  * Integrations\n\n  * Tools\n  * Cloudflare Radar\n  * Speed Test\n  * Is BGP Safe Yet?\n  * RPKI Toolkit\n  * Certificate Transparency\n\n  * Support\n  * Support\n  * Cloudflare Status\n  * Compliance\n  * GDPR\n\n  * Company\n  * About Cloudflare\n  * Our Team\n  * Press\n  * Analysts\n  * Careers\n  * Logo\n  * Network Map\n\n\u00a9 2024 Cloudflare, Inc. | Privacy Policy | Terms of Use |Cookie Preferences| Trust & Safety | Trademark\n\n## Our site uses cookies\n\nLike most websites, we use cookies to make our site work the way you expect it\nto, improve your experience on our site, analyze site usage, and assist in our\nmarketing efforts. By choosing \"Accept All Cookies\", you agree to the storing\nof all categories of cookies on your device. If you wish to accept or reject\nsome categories of cookies, please click \u201cCookie Preferences.\u201d\n\n## Your Cookie Options\n\nCloudflare uses four types of cookies as described below. You can decide which\ncategories of cookies you wish to accept to improve your experience on our\nwebsite. To learn more about the cookies we use on our site, please read our\nCookie Policy. Cloudflare's Cookie Policy\n\n### Manage Consent Preferences\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nStrictly Necessary cookies are essential to our website functioning as\nexpected. You cannot turn off Strictly Necessary cookies because they are\nrequired to deliver security, enable core site functionality, and help you use\nour site's features and services as you would expect (including remembering\nyour cookie consent preferences). Cloudflare does not use these cookies to\ntrack individuals across websites.\n\n#### Functional Cookies\n\nFunctional cookies allow us to remember choices you make about the kind of\nexperience you want on our site and to provide you with a more personalized\nexperience. For example, a functional cookie is required to remember which\nlanguage you prefer.\n\n#### Performance Cookies\n\nPerformance cookies help us learn how you use our website to help improve its\nperformance and design. These cookies provide us with aggregated statistical\ninformation such as number of page visits, page load speeds, how long a user\nspends on a particular page, and the types of browsers or devices used to\naccess our site.\n\n#### Targeting Cookies\n\nWe use Targeting cookies to deliver advertisements relevant to you and your\ninterests when you visit other websites that host advertisements.\n\n### Cookie List\n\nlabel\n\nConsent Leg.Interest\n\nlabel\n\nlabel\n\nlabel\n\n", "frontpage": false}
