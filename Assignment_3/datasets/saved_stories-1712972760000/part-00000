{"aid": "40016877", "title": "Ring Attention Explained \u2013 Unlocking Near Infinite Context Window", "url": "https://coconut-mode.com/posts/ring-attention/", "domain": "coconut-mode.com", "votes": 8, "user": "simonguozirui", "posted_at": "2024-04-12 19:44:00", "comments": 0, "source_title": "Ring Attention Explained", "source_text": "Ring Attention Explained | Coconut Mode\n\n# Ring Attention Explained\n\n10 Apr 2024\n\nBy Kilian Haefeli, Simon Zirui Guo, Bonnie Li\n\nContext length in Large Language Models has expanded rapidly over the last few\nyears. From GPT 3.5\u2019s 16k tokens, to Claude 2\u2019s 200k tokens, and recently\nGemini 1.5 Pro\u2019s 1 million tokens. The longer the context window, the more\ninformation the model can incorporate and reason about, unlocking many\nexciting use cases!\n\nHowever, increasing the context length has posed significant technical\nchallenges, constrained by GPU memory capacity. What if we we could use\nmultiple devices to scale to a near infinite context window? Ring Attention is\na promising approach to do so, and we will dive into the tricks and details in\nthis blog.\n\nIt took us a few tries to understand how Ring Attention works beneath its\nmagic. Several tricks have to click in your head to really grasp it! In this\nblog, we will go through them step by step:\n\n  * We will build up why scaling long context across GPU is so challenging.\n  * Understand how we can divide up attention calculation across the sequence dimension into chunks.\n  * See how we can map the divided up attention calculation on multiple GPUs and orchestrate the computation in a way that adds minimal overhead, by cleverly overlapping communication and computation.\n\nNotably the techniques we present here leading up to Ring Attention are\ngeneral tricks to split computation of Attention into parallel parts. These\ntechniques have been discovered by different amazing researchers and used in\nother transformer optimizations such as Self-attention does not need O(n2)\nmemory. and Flash Attention. Note that these optimizations and ring attention\nare orthogonal concepts, in that they can be used independently from each\nother.\n\n## Attention and Memory\n\nWhy is long context so hard? Let\u2019s look at computing attention in the\ntransformer. We have query, key, value - each is a matrix of shape s\u00d7d -\nsequence length times model dimension. We need to compute O=softmax(QKT)V,\nthis looks like\n\nNotice that the Score Matrix S=QKT and the Attention Matrix A=softmax(QKT) are\nboth of size s\u00d7s, so the memory complexity of naive attention is quadratic\nwith sequence length! Even using advanced optimizations like Flash Attention,\nthe memory complexity is still linear in sequence length. Thus for long\ncontexts, scaling the sequence length is limited by the memory capacity.\n\nThe goal of splitting the computation into parts is to split the memory cost,\ndominated by the sequence length, into parts which each require only a\nfraction of the total memory complexity. For N GPU\u2019s, we want to split\ncomputation into parts each requiring 1/N\u2019th of the whole memory cost. In\norder to achieve that, we need to split both Q and K, V along the sequence\ndimension.\n\nHowever, to compute the final attention result, we still need to access all\nthese matrices that are now split up across GPUs. Naively doing so could add a\nlot of communciation overhead especially as these matrices grow linearly with\nsequence length!\n\nRing Attention presents a way to cleverly to address this! A sneak peak of how\nthis is going to play out: We will rotate between devices to parallelize all\ncomputation and hide the communication overhead completely. It looks something\nlike:\n\nGPUs are arranged in a ring, each holding a portion of the query. During the\nRing Attention process, the GPUs will pass along blocks of K, V to each other;\neventually each device will have seen all the blocks needed to compute\nattention output! We will show why and how this works in the rest of blog.\n\n### Splitting Q\n\nHow can computation be split along the sequence length of Q? If you want to\nfind out for yourself, follow the orange colours in the Attention\nvisualization below. You will see that computation of each output row only\ndepends on a single query row in Q. Assume we split Q into BQ chunks of along\nits rows. Each Qi chunk is of the shape CQ\u00d7d, where CQ is the chunk size and\nCQ\u00d7BQ=s. Let\u2019s assign one such Query-chunk to each available GPU, what would\neach GPU need to compute attention? It still needs all of K and V! So far we\ndid split the memory of Q into BQ chunks, and the corresponding Score and\nAttention matrices are also split into BQ chunks. Now we need to split the\nkeys and values.\n\n### Splitting K and V\n\nHow can we split up the sequence length for keys and values? Hint: It is not\nas trivial as splitting queries was!\n\nLet\u2019s look at the computation for a single Query chunk in the illustration\nabove. What would happen if we additionally split the Keys along its rows\n(indicated by the blue colour)? Importantly note how the matrix multiplication\nS=QKT is now split along both rows and columns! The problem is that the\nSoftmax operation needs to be computed over full rows of S at a time.\n\nsoftmax(si)=\u2211j=1Nexp(sj)exp(si)\n\nDon\u2019t worry, we will see how we can work around this challenge. First we need\nto understand why the Softmax seems to require access to the whole row at\nonce: Looking at the softmax equation, it is clear that the upper part of the\nfraction (numerator) does not need access to the whole row at once as it\nsimply takes the exponent of each individual entry. The lower part of the\nfraction however requires you to compute the sum of ALL elements in the row -\nat least that is how it seems upon first inspection ;)\n\nWe will later see how we can handle the lower part (denominator) of softmax in\na parallel fashion, but for now imagine that the lower part simply does not\nexist.\n\nWe already know that we can easily partition computation along the rows\n(sequence dimension) of Q. Hence, here we partition the computation of a\nsingle Qi,Ai chunk into BKV independent sub-parts involving only a single\nKj,Vj, j\u22081,...,BKV chunk at each step. The computation of a single Query,\nOutput chunk Qi,Ai is:\n\nThis directly leads to our desired result! Computation is now split into an\nouter loop over BQ chunks of Q and over an inner loop over BKV chunks of K and\nV. At each step we compute only exp(QiKjT)\u22c5Vj\u2208Rh\u00d7cQ which requires only single\nchunks of Kj, Vj and Qi at a time, successfully dividing memory cost over the\nsequence length (Setting the number of chunks to N (BKV=BQ=N) would result in\n1/Nth of the memory complexity of stoing the full K,V,Q).\n\nPut it in a Ring: We have the outer loop (parallel over Query chunks) and\ninner loop (parallel over Key and Value chunks). But how are the computation\nsteps of this loop allocated to the devices?\n\nNote how the outer loop can be computed completely independently, whereas the\ninner loop calculates a sum of its results. We partition computation to\ndevices in the following way: Each device gets one Qi chunk (one outer loop\nindex) and calculates the inner loop iteratively for each Kj,Vj. Each device\nat a given step j only needs to keep track of the cumulative sum Ai of shape\nh\u00d7cQ and only needs a single Vj,Kj block at a time, along with its Qi.\nTherefore the memory requirement for each device is successfully split, even\nfor K and V.\n\n### Online softmax\n\nNow let\u2019s look at the normalization constant, how can we compute normalization\nconstant on each device with just local results?\n\nThere is a simple solution! We accumulate the partial sum\nlj=lj\u22121+\u2211kt\u2208Kjexp(QiktT), every time we receive a key and value chunk. By the\nend of the inner loop, the device would have accumulated\nl=lBKV=\u2211j=1sexp(QikjT), which is the normalization constant we need. Note that\nthe order of normalizing and multiplying with the value matrix V does not make\na difference, which is why we can accumulate the sum and execute the actual\nnormalization after everything else.\n\nTherefore each device i (holding Qi), additionally to its cumulative sum\nAj=Aj\u22121+exp(QiKjT)Vj, also keeps track of its current lj\u2208RBQ, while executing\nthe inner loop. At the end of the inner loop, each device finishes by dividing\nits computed unnormalized Attention by the normalization constant ABKV/lBKV.\n\n### Safe softmax\n\nThe exponential operation can easily grow out of bounds, leading to numerical\nissues and overflow. Typically to compute softmax, we subtract the maximum\nfrom every element. That is\n\nsoftmax(s1:N)=\u2211iexp(si)exp(s1:N)\u22c5exp(\u2212smax)exp(\u2212smax)=\u2211iexp(si\u2212smax)exp(s1:N\u2212smax)\n\nThis is called the safe softmax. How can we subtract the maximum when we are\ncomputing the softmax in blocks? We can keep track of the maximum so far.\nLet\u2019s say our current sum is Aj and current maximum is mj, we receive a new\nkey Kj+1 and value Vj+1, we compute QiKj+1T and get a new maximum\nmj+1=max(mj,max(QiKj+1T)), we can update our result as:\n\nAj+1=Aj\u22c5exp(mj\u2212mj+1)+exp(QiKj+1T\u2212mj+1)\u22c5Vj\n\nlj+1=lj\u22c5exp(mj\u2212mj+1)+exp(QiKj+1T\u2212mj+1)\n\nTo conclude: For an inner step j+1, before computing the cumulative sum Aj+1\nand the normalization constant lj+1 we first compute the current maximum mj+1,\nthen renormalize the previous Aj,lj using our newfound maximum and finally\ncompute the updated Aj+1,lj+1.\n\n## Putting it together\n\nFinally, we have assembled all the tools we need to construct ring attention:\n\n  1. Splitting along the sequence length of Q into an independent outer loop.\n  2. Applying online safe softmax in order to split along the sequence length of K and V resulting in an inner loop computing the attention cumulatively.\n\nAs hinted on before the way this is parallelized is by assigning each of the N\ndevices available one chunk Qi of Q. Therefore we need to split Q into N equal\nparts (BQ=N). Each device will individually compute its output block\nOutput(Qi,K,V)=softmax(QiKT)V iteratively by performing the inner loop over\nthe blocks of Keys and Values. The challenge is that the devices are not able\nto store the full K and V matrices at a time.\n\nFor example if we have 4 GPUs, we will split Query into 4 blocks along\nsequence dimension for each device. Each device would then compute output\nusing local Q and the whole K, V. The final output would be concatenation of\nthese local outputs along row-dimension\n\nRemember how we showed that we can further split K, V as the inner loop? K and\nV are now split into BKV=BQ=N blocks and initialize the devices so that each\ndevice holds a single Qi block and a single Key Kj block and Value Vj block.\nFor simplicity we can assume that device i holds Qi,Kj=i,Vj=i in the\nbeginning.\n\nAfter the devices have computed one inner loop step corresponding to their\ncurrent Vj,Kj, each device needs to receive a next Key and Value block to\ncontinue the inner loop. Sending and waiting for these matrices is a big\noverhead that we do not want to wait for! This is where the ring in Ring\nAttention comes into play: We lay out the N devices in a ring, where device i\ncan send data to device i+1 and so on as illustrated:\n\nObserve that for GPU1, while it is computing output using Q1 (its local query)\nand K1, V1 (the local K,V blocks that it currently has), it is also receiving\nK4, V4 from GPU4 (previous host int the ring) and sending Q1, V1 to GPU2 (next\nhost in the ring). The network communication are illustrated by the blinking\narrows. If we select the block size correctly, by the time GPU1 has computed\noutput using Q1, K1, V1, it has received the block K4, V4 to compute the\noutput in the next iteration!\n\nComputing a step of the inner loop on device i: Qi,Vj,Kj takes a certain\namount of time. If during that time the device i can also send its current\nVj,Kj to device i+1 and simultaneously receive Vj\u22121,Kj\u22121 from device i\u22121, then\nthe latency from sending and receiving the Key and Value chunks is hidden\nbehind executing the actual computation, as long as the time to transmit is\nlower than the time it takes to compute. We now can completely hide the\ncommunication overhead!\n\nHere we illustrate for N devices, it will take N iterations to finish the\nwhole output comptutation. Watch for each iteration on each device, different\npartial sum of the output is computed with the K,V block it currently has, and\nit eventually sees all the K,V blocks and has all the partial sum for the\noutput!\n\n## Memory and Arithmetic Complexity\n\nFor this analysis we will use the bfloat16 format that is commonly used in\ndeep learning applications. Parallel processing accelerators such as GPU\u2019s or\nTPU\u2019s are usually measured by their FLOPs:=F, which is the number of floating\npoint operations the device can theoretically execute per second. In practice,\nwe never really see full utilization but for the sake of this analysis we will\nassume that. Furthermore, the connections between the different devices we\nassume to have bandwidth :=BsecBytes.\n\nMemory Complexity: In order to receive send and compute at the same time we\nneed to have memor for receiving the new Key Value blocks. Storing the current\nKey Value blocks requires 2\u22c5d\u22c5c floats or 4\u22c5d\u22c5c Bytes. The memory for\nreceiving the new Key Value blocks is also of size 2\u22c5d\u22c5c floats or 4\u22c5d\u22c5c\nBytes. Now assuming that the computation itself does not require more memory\n(this would require a more in depth discussion, but there are ways to do this\nlike flash attention or blockwise attention) computing the output of the\ncurrent step requires d\u22c5c floats or 2\u22c5d\u22c5c Bytes. Furthermore each device needs\nto store its Qi block which also takes d\u2217c floats or 2\u22c5d\u22c5c Bytes. In total we\nrequire 6\u22c5d\u22c5c floating points or 12\u22c5d\u22c5c Bytes of memory.\n\n  * A note for people familiar with Flash Attention: Ring Attention is an orthogonal concept to things like flash attention and could be used together (Flash attention is actually used in the inner loop of Ring Attention). These latter methods have the goal of not materializing the full Attention matrix and thus getting linear memory complexity in sequence length vs quadratical like the naive implementation would. Ring Attention manages to split memory complexity for both the naive and the flash attention method by at least N times using N devices, because it splits everything into at least N or more parts (Splits Keys, Queries and Values into N parts, and splits the Attention Matrix into N2 parts)! No matter whether the memory complexity is dominated by the Keys, Queries and Values or by Attention Matrix, Ring Attention manages to split memory cost by at least N times.\n\nCommunication Complexity: During a single step, each device needs to send\n2\u22c5cQ\u22c5d floating point values, from Kj,Vj\u2208RcQ\u22c5d, to the next device over a\nchannel with bandwidth B. Each floating point value consists of two Bytes.\nThus the time it takes to transmit both is approximately 4\u22c5c\u22c5d/B\n\nArithmetic Complexity: Computing an inner loop step requires 2\u22c5d\u22c5c2 for\ncomputing QiKjT, 2\u22c5c\u22c5d for computing the softmax along with the lij,mi(j)\nnormalization and safety parameters, and 2\u22c5d\u22c5c2 for computing Aij\u22c5Vj. Assuming\nthat the device operates at the maximum possible FLOPs (in reality we would\nuse the achieved average FLOPs) the time it takes to compute is roughly\n\u22484\u22c5d\u22c5c2/F\n\nIn order to effectively overlap the communication and computation (aka hiding\nthe commmunication overhead), we need the time of transmission of K, V blocks\nequal to the time it takes to compute the local Q, K, V :\n4\u22c5c\u22c5d/B\u22644\u22c5d\u22c5c2/F\u27faB\u2265F/c\u27fas/N\u2265F/B\n\n## Further optimizations\n\nOne interesting case of Ring Attention is when used for causal transformer\nmodels, recall the triangular mask is used for attention calculation. This\nimplies that some GPUs won\u2019t need to compute over the whole sequence, which\nresults in them being idle for the most part. An extension of Ring Attention,\nStriped Attention addresses this constraint and provides a scheme to\ndistribute computation more even and hence making Ring Attention even faster!\n\nBesides technniques like Ring Attention and Flash Attention to enable the\nstandard Transformer architecture to have longer context length, there are\nalso attempts to experiment with model architecture such as state space models\n(SSMs) with linear attention such as Mamba, but that is a deepdive for another\nday.\n\n## Summary\n\nSo let\u2019s review how we arrived at this magical zero-overhead solution:\n\n  1. Attention requires quadratic memory (or linear if optimized) in the sequence length. Scaling transformers to any desired sequence length therefore requires splitting memory cost over several devices, and we want do that in a low-overhead manner.\n  2. We want to parallelize the attention calculation, as well as the shard the Q,K,V matrices across devices because they grow linearly with the sequence length.\n  3. One particular way is to shard on the sequence dimension for Query as blocks, but each sharded query each still needs to compute with all the K,V\u2019s (which can be huge)!\n  4. We show that it is possible to further parallelize softmax calculation by creating an inner loop over K,V, with the correct normalization to recover the final softmax! This leverages the fact that the operations are commutative and all we need to do is cumulatively sum up the normalization, whilst also keeping track and renormalizing with the current maximum.\n  5. Ring Attention builds on this technique of parallelizing Attention calculation by distributing the outer loop over Q blocks to the individual GPU\u2019s, while letting the inner loop be computed in a ring-reduce fashion. Instead of waiting for new Key, Value blocks we effectively hide transmition overhead by \u201crotating\u201d the Key, Value blocks in a ring around the devices to compute the partial sums. With the correct block size, we can overlap communication with computation and fully hide overhead induced by communication.\n\n## Acknowledgement\n\nWe like to thank Jay Alammar and Bowen Yang for their insights! We also like\nto thank Daanish Shabbir, Yuka Ikarashi, Ani Nrusimha, Anne Ouyang, for their\nhelpful feedback!\n\n\u00a9 2024 . Powered by Hugo blog awesome.\n\n", "frontpage": false}
