{"aid": "39963792", "title": "Sled theoretical performance guide \u2013 sled-rs.github.io", "url": "http://sled.rs/perf.html", "domain": "sled.rs", "votes": 1, "user": "jasim", "posted_at": "2024-04-07 20:51:12", "comments": 0, "source_title": "sled theoretical performance guide", "source_text": "sled theoretical performance guide | sled-rs.github.io\n\nsled github repo introduction documentation motivating experiences book shop\nsupport sled\n\nblog API considerations error handling in Rust jepsen-proof engineering\ntheoretical performance guide\n\n# sled theoretical performance guide\n\nThis guide covers timeless ideas that are helpful to keep in mind while\nworking with systems where performance matters. Many of these ideas are fairly\n\u201cdurable\u201d and will apply regardless of what hardware, programming language,\noperating system, or decade you are working in.\n\nStay tuned for follow-up articles that will drill into hardware and more Rust-\nspecific techniques.\n\nSkip to the table of contents if you don\u2019t like intros.\n\nThere\u2019s some foul language in here. Enjoy!\n\n## executive summary\n\n  * make it work, then make it pretty, then make it fast\n  * only do things that are necessary for correctness and safety\n  * your engineers will burn out, leave your team, and relentlessly shit talk you if they don\u2019t make their code pretty and low-friction to work with\n  * align all latency-throughput queuing positions for everything in the serial dependency graph, or you will get worst-of-all-worlds latency-throughput behavior, increasing latency and lowering throughput\n  * if something necessary can be done in the background without blocking, queue it up for eventual batch processing to improve cache performance\n  * look at allocation lifetime using DHAT, avoid short-lived allocations and ensure that long-lived allocations employ time-space trade-offs that consume plentiful resources instead of scarce ones where possible\n  * seriously, it\u2019s always your fault if your engineers quit.\n  * performance happens naturally when engineers love the codebase and they are aware of which parts of the system can be sped up\n  * before wasting time optimizing things that don\u2019t matter, we can easily test whether optimizing could be useful at all by adding delays or deleting relevant code to see what the impact of infinite optimization would be\n  * by default your machine will randomly change your cpu frequency, yank processes across cores and sockets, and generally make measurements deceptive and non-reproducible. we can control this behavior. if you don\u2019t account for this behavior, your measurements are not sound justifications for code changes\n  * wait, why does an executive care about performance, anyway?\n\n## non-executive introduction\n\nOk, now that the executives are gone, it is time to party.\n\nLet\u2019s put on some nice music.\n\nTake a deep breath.\n\nLet it out.\n\nThe year is 2020. and we are utterly fucked.\n\nSupposedly we are here reading this article because we \u201clike computers\u201d or\nsomething but let\u2019s be clear about one thing - you\u2019re not here because of\nthat. I know. You know. We all know. You couldn\u2019t give two fried pennies about\ncomputers if they didn\u2019t help you feel the things you so desperately need to\nfeel.\n\nYou\u2019re here, ultimately, because of control. Your entire life is entombed in\nan elaborate, entirely abstract labyrinth of machines that define so many\naspects of every moment of your waking life. You\u2019re here to better understand\nthe prison you find yourself within. You\u2019re here to sympathize with the\nmachine.\n\nWhile you will only seep ever more deeply into this morass of gold and glass,\nyou have the power to turn your captors into familiar friends. You have the\npower to make the machines into the only friends you will ever need. You are\nhere to abandon the organic and the analogue. There is nothing for you on the\nother side of your window. As you lounge (please fix your posture now) in your\ndesignated quarantine pit, gazing listlessly into the LED mesh while biding\nyour time, I invite you to join me - your newest simulated human experience -\non a once-in-a-lifetime adventure into the bowels of the machine through which\nyou experience joy and place orders for more toilet paper.\n\nIt begins today.\n\nThis guide contains basic information for getting started with performance-\nsensitive engineering. I think most folks will learn something new. I know I\nhave. And, part of why I wrote this is to have a single place with a memorable\nURL where I can return to when I forget most of this stuff. More importantly,\nwhen I unfurl fiery screeds in random internet comments I want to have an\nintimidating reference to which I can mercilessly link.\n\nI initially wrote this guide for the Rust ecosystem, where many people are now\ntrying their hands at optimization for the first time. But nearly all of this\ndocument applies to general programming, with a few of the hardware effects\nmentioned being specific to x86 and ARM circa 2020. Linux is assumed, because\nthat\u2019s what the overwhelming majority of server workloads run today.\n\nThis guide brings together ideas from systems engineering, systems theory,\npsychology, hardware, queuing theory, statistics, economics, distributed\nsystems, concurrency and philosophy of science with the goals of helping you\nto be less of an asshole and write code that tends to improve the metrics you\ncare about.\n\nPerformance is about being thoughtful about the metrics that matter to us and\nallowing ourselves to be aware of them while making decisions.\n\nThese materials are based on Tyler Neely\u2019s Rust workshop content, and have\nbeen inspired by the writings of Dmitry Vyukov, Mark Callaghan, Brendan Gregg,\nMartin Thompson, Pedro Ramalhete and others.\n\n## shameless, emotionally annihilating begging\n\nMy workshops have been the primary means of supporting sled development costs.\nUnfortunately, the workshops are now on hold due to coronavirus concerns. If\nyou feel like this information is useful, please consider supporting my\nefforts to share knowledge and productionize cutting edge database research\nwith implementations in Rust :)\n\nI love you.\n\n# contents\n\nLet\u2019s kick this shit up! Here\u2019s what it\u2019s gonna look like...\n\n  * principles\n  * e-prime and precise language\n  * metrics: latency, throughput, utilization and saturation\n\n    * latency vs throughput\n    * measuring latency\n    * productivity\n    * case study: sled\n  * experimental design\n\n    * experiment checklist\n  * concurrency and parallelism\n  * amdahl\u2019s law\n  * universal scalability law\n  * trade-offs\n\n    * the RUM conjecture\n    * memory pressure vs contention\n    * speculation\n    * scheduling\n  * scouting ahead\n\n    * flamegraphs\n    * deletion profiling\n    * causal-profiling\n\n# IT BEGINS\n\n## principles\n\n    \n    \n    You are not a Bayesian homunculus whose reasoning is \u201ccorrupted\u201d by cognitive biases. You just are cognitive biases.\n\n\\- Luke Muehlhauser, via R:AZ.\n\nThe first thing to consider is that our minds are pure shit and everything we\nknow is wrong. We must accept our fallibility before embarking down the path\nto fast-as-fuck machinery.\n\nWe build towers of assumptions that are bound to specific contexts, and when\nthe conditions that caused us to form these beliefs change, we tend not to\nrevisit the now-invalidated beliefs. Cache invalidation is hard when we are so\nrarely aware of the dependency graphs of what we believe.\n\nSo, we measure. Even when we\u2019re convinced that we\u2019re right. Because we are\nalways out of sync with reality, and we are fundamentally incapable of\naltering this fact. But we can be responsible in the face of that.\n\nCorollary: allow yourself to be wrong. Allowing yourself to be wrong with\nyourself, your collaborators, and in public is a key optimization for learning\nfaster and building better things with less effort and in less time.\n\nLuckily for us, machines tend to be somewhat amenable to measurement. We built\nthem that way. Indeed, constructing them to be somewhat measurable in the\nfirst place was the only way we were able to assemble them at all despite our\ninnumerable shortcomings. We took the predecessor to your current machine,\nchose some metrics to improve, made a huge number of mistakes while continuing\nto measure, and occasionally we got lucky: the metrics we cared about improved\nenough to alter production lines, crystallizing our occasionally successful\nresults into new production processes that eventually put your machine in\nfront of you.\n\nWe must measure.\n\nThe only thing that matters is that real programs on real hardware see real\nimprovements in relevant metrics like total cost of ownership, responsiveness,\netc... If a metric doesn\u2019t help a human, it\u2019s just a vanity pursuit that may\nmake the important metrics worse due to under-investment.\n\nWe must select our measurements with care. Our time is precious.\n\nBy making decisions that are considerate of available data, we are able to cut\nthrough so much bullshit. We have so many ideas that our experience tells us\nshould cause our code to improve, but it simply isn\u2019t true when we actually\nmeasure. Code changes that made your Java, C++, Haskell or JS programs faster\nmay not make Rust any faster. Don\u2019t make Rust code ugly just because some\nprogram you wrote in the 90s worked better with manual loop unrolling.\nCompilers change, in many ways for the better.\n\nDon\u2019t be macho. Make decisions while having real data on-hand and limit the\ndamage of hubris.\n\n## e-prime and precise language\n\nSo many aspects of performance-critical engineering can be highly contextual\nand may vary wildly from one machine to another. It helps to avoid the verb\n\u201cto be\u201d and its conjugations \u201cis\u201d, \u201care\u201d etc... when describing observations.\n\nWhen we say something \u201cis\u201d something else, we are casually equating two\nsimilar things for convenience purposes, and we are lying to some extent. By\navoiding false equivalences (usually easily spotted through use of \u201cis\u201d, \u201care\u201d\netc...) we can both communicate more precisely and we can allow ourselves to\nreason about complexity far more effectively. Situations that may seem\ncompletely ambiguous when described using \u201cto be\u201d phrases are often quite\nunambiguous when taking care to avoid false equivalences. This general form of\nspeech is known as E-Prime.\n\nDon\u2019t say \u201clock-free queues are faster than mutex-backed queues\u201d, say \u201con\nhardware H with T threads running in tight loops performing operations O, our\nspecific lock-free queue has been measured to achieve a latency distribution\nof X1 and our specific mutex-backed queue has been measured to achieve a\nlatency distribution of X2.\u201d It\u2019s likely your lock-free queue will sometimes\nperform worse than a well-made mutex-backed queue given certain hardware,\ncontention and many other factors. What does \u201cperform worse\u201d even mean?\nDifferent use cases will have a different mix of available resources. A\nparticular lock-free implementation that is higher throughput under high\ncontention may use more memory, have a slower median latency, be buggier, and\ntake longer to create than a mutex-backed structure that does not need to\nallocate or perform RCU. Is this ever going to be high contention, anyway? Are\nyou sure you\u2019re writing this for the benefit of your system, or because you\nwant to be macho at work?\n\nWhen we speak about comparative metrics, it is also important to avoid saying\ncommonly misunderstood things like \u201cworkload A is 15% slower than workload B\u201d.\nInstead of saying \u201cfaster\u201d it is helpful to speak in terms of latency or\nthroughput, because both may be used to describe \u201cspeed\u201d but they are in\ndirect opposition to each other. Speaking in terms of relative percentages is\noften misleading. What does A (90) is 10% lower than B (100) mean if we don\u2019t\nknow their actual values? Many people would think that B is 1.1 * A, but in\nthis case, 1.1 * 90 = 99. It is generally better to describe comparative\nmeasurements in terms of ratios rather than relative percentages.\n\nThe phrase workload A is 20% slower than workload B can be more clearly stated\nas workload A was measured to have a throughput of 4:5 that of workload B.\nEven though many people will see that and immediately translate it to \u201c80%\u201d in\ntheir heads, the chances of improperly reasoning about the difference are\nlower.\n\nWhen we make our communications less ambiguous, we have more brain cycles\navailable for speeding things up based on clarified mental models. The time we\nspend can be better spent because it can be more precisely targeted.\n\n## metrics\n\nPerformance metrics come in many shapes and sizes. Workloads will have a few\nmetrics that matter far more than others. Every workload has its own set of\npriorities and available resources.\n\nIt\u2019s at this point that I\u2019m obligated to bash benchmarketing, but honestly\nit\u2019s often an important tool for projects to see success - you just need to be\nclear about what your metrics actually are. Don\u2019t trick people. Give people\nthe means to reproduce your findings.\n\nMany systems performance metrics boil down to these:\n\n  * latency - the time that an operation takes\n  * throughput - how many operations can be performed in some unit of time\n  * utilization - the proportion of time that a system (server, disk, hashmap, etc...) is busy handling requests, as opposed to waiting for the next request to arrive.\n  * saturation - the extent to which requests must queue before being handled by the system, usually measured in terms of queue depth (length).\n  * space - whoah.\n\nAt higher scales, these metrics become factors in major concerns like:\n\n  * total cost of ownership\n\n    * how many servers do I need to pay for to get my shit done?\n    * how many hours do engineers spend taking care of this shit?\n    * how much power does this shit draw?\n\nQuick aside - as of 2020, cloud providers tend to under-charge for compute\n(ec2, lambda, etc...) in an effort to increase your reliance on their more\nexpensive storage products. A concept of \u201cdata gravity\u201d exists where your\ncompute must stay where the storage is. You can\u2019t move your compute to a\ndifferent cloud provider if your data is stuck in the old one. And egress\ntraffic is taxed heavily to make it more painful to get out. And where your\ncompute exists, more data is likely to be produced, increasing the\ngravitational field. Make sure your back-of-the-napkin calculations for\nrunning infrastructure somewhere take account of the storage costs primarily.\nEngineers love using the fancy hosted databases like Spanner, but the cost per\nbyte stored is astronomical. BE AWARE.\n\nIn trying to determine how many servers do I need to pay for to get my shit\ndone, we need to consider latency, throughput and required space (memory and\nstorage).\n\n### latency vs throughput\n\nLatency and throughput considerations are often in direct contradiction with\neach other. If we want to optimize the throughput of a server, we want to\nincrease the chance that when a server is finished processing one request that\nit already has another one lined up and ready to go. 100% utilization means\nthat the server is always doing useful work. If there is not work already\nwaiting to be served when the previous item completes, the utilization drops,\nalong with the throughput. Having things waiting to go in a queue is a common\nway to increase throughput.\n\nBut waiting (saturation) is bad for latency. All other things being equal,\nsending more requests to a system will cause latency to suffer because the\nchance that a request will have to wait in line before being served will\nincrease as well. If we want to minimize the latency of a server, we want to\nincrease the chance that there is an empty queue leading into it, because\nwaiting in that queue will slow down each request.\n\nLatency vs throughput is a fundamental relationship that has tremendous\nconsequences for performance-sensitive engineering. We are constantly faced\nwith decisions about whether we want our requests to be fast, or if we want\nthe system to generally handle many requests per second, with some being quite\nslow due to waiting so long in the queue.\n\nIf you want to improve both latency and throughput, you need to make the unit\nof work cheaper to perform.\n\nDifferent systems will have different relationships between utilization and\nsaturation. Network adapters are often designed to be able to keep receiving\nmore and more work and avoid saturation until relatively high utilization.\nOther devices, like spinning disks, will start saturating quite quickly,\nbecause the work causes other work to get slower by needing to drag the disk\nspindle to another physical location before it\u2019s able to handle the request.\nHere\u2019s a place where smart scheduling can make a huge difference for the\nrelationship between utilization and saturation. Most modern storage devices\nbased on flash memory are essentially distributed databases where every 32mb\nis a different shard, so you can get a lot of throughput without immediate\nnegative saturation by keeping the queue depths pretty deep, so more of the\ndistributed chips in the device can do work at once.\n\nA critical insight is that if you have a system that you wish to optimize for\nlow latency, it is helpful if the subsystems that it depends on are also\noptimized for latency. If you are serving user requests where you want to\nminimize response time, you probably want to avoid having that response depend\non an analytical database tuned for batch performance instead of low latency.\nIf you want to process trillions of records in some low amount of time without\ncaring much about the time to process a particular record, you probably want\nto rely on systems that have been tuned for throughput at the expense of\nlatency.\n\nAll systems have specific latency-throughput trade-offs. When your system\ndepends on subsystems in the critical path where different latency-throughput\ntrade-offs were made, your overall system will behave worse. If you force a\nlatency-bound user-facing response to go through a kafka queue before their\nweb browser loads the result, you are introducing a throughput-bound\ndependency into the critical path of a latency-bound workload, and the result\nwill be a worse experience. We should use low-latency dependencies in the\ncritical paths of low-latency workloads. We should use high-throughput\ndependencies in the critical paths of high-throughput workloads. Mixing and\nmatching systems in our critical paths without evaluating their queuing\ncharacteristics is likely to result in terrible performance.\n\nMeasure what\u2019s happening and keep your queuing behaviors aligned and your\nsystem will fly.\n\nUnder light load, throughput-bound systems can sometimes scale down to reduce\nbuffering and achieve lower latency at lower loads. Andrea Lattuada mentioned\na great example of an auto-tuning system to me: a throughput-oriented system\ncan take the entire queue of new requests, process them in a batch, and\ngenerally keep picking the queue length as the batch size where possible. This\nauto-tunes the batch size to be low (reducing latency) when there is a small\nrequest volume coming in. This yields nice low latency requests when the\nvolume of requests is low, but allows the system to scale up and take\nadvantage of batch optimizations as the request rate increases. Latency\nsuffers as load increases, because in this case, latency is not a high\npriority.\n\nAn auto-tuning low-latency system must scale something other than the queue\nlength when throughput increases if it is going to keep maintaining low\nlatency. Low-latency is dependent on keeping the the amount of time waiting in\na queue beneath a desired threshold. So, instead of increasing the batch size,\nwe must increase the amount of parallelism by using more cores, more servers,\netc... However, as Amdahl\u2019s Law and USL show, we can only parallelize a\nprogram by a certain amount, and the very act of parallelization will impose\nadditional costs which could negate any possible gains from parallelization.\n\nIt is important when building a system to be aware of whether your goal is to\nkeep requests processing at low-latencies or at high-throughputs, because this\nshould have a huge impact on your design, your dependencies, your scaling\ntechniques, etc...\n\nBe aware of where your system stands on this spectrum.\n\nFurther reading:\n\n  * http://www.brendangregg.com/usemethod.html\n  * Systems Performance: Enterprise and the Cloud by Brendan Gregg (buy the book just to read chapter 2: Methodology)\n  * Quantitative Analysis of Computer Systems by Clement Leung - awesome intro to queue theory.\n\n### measuring latency\n\nIn the process of satisfying a request of some sort, a system will often rely\non other systems. A database relies on a kernel to schedule its threads onto\nCPUs and provide access to storage. We can learn about what causes a database\nto be slow by learning about what causes its subcomponents and interactions\nwith other systems to be slow.\n\nIf you\u2019re measuring latency for a large number of requests, there are a number\nof ways that you can derive meaning from measurements. People often rely on\naverages to make sense of a large number of measurements. But the average is\nnot very interesting for our highly discrete computer systems because it hides\nthe impact of outliers and gives us no insight into the distribution of data.\nThings like normal curves and t-tests do not apply for data which do not\nfollow normal distributions. Determining what our distribution looks like at\nall is a vital part of our work.\n\nWe usually use histograms so that we can understand the distribution of our\ndata. The 0th percentile is the minimum measurement. The 100th percentile is\nthe maximum measurement. The 50th percentile is the median, where half of all\nmeasurements are beneath and half of all measurements are above. The 90th\npercentile is the latency that 90% of all measured latencies are less than or\nequal to. It\u2019s pretty cheap to measure histograms by using logarithmic\nbucketing to index into an array of buckets that are sized to be within 1% of\nthe true observed values. The historian crate was extracted from sled to\nassist with these measurements in a super cheap manner.\n\nImagine this scenario:\n\n  * a front-end system sends 100 requests to a back-end system\n  * the front-end system is able to send each request in parallel\n  * the latency distribution for the back-end system is a steady 1ms until the 99th percentile where it jumps to 1s.\n  * the front-end system must wait for the slowest response before it can respond to the user\n\nHow long does the front-end system need to wait for?\n\nThe probability of needing to wait 1 second for a single request is 1% (99th\npercentile is 1s). Pretending the distributions are independent, the\nprobability of needing to wait 1 second for 2 requests is 1.9% (1 - (0.99 ^\n2)). Intuition: if we sent 1,000,000 requests, the percentage would not become\n1,000,000 * 1%, or 10,000%, because 100% is the max probability an event can\nhave. For this example, everything between the 99th and 100th percentiles is\nexactly 1 second. All of our slowest 1% of requests take 1 second.\n\nThe probability of needing to wait 1 second for 100 requests is 1 - (0.99 ^\n100), or 63%. Even though the event only happens 1% of the time, our front-end\nsystem will have to wait 1 second in 63% of all cases, due to needing to send\nmultiple requests.\n\nOur systems are full of subcomponents that are accessed many times to satisfy\na higher-level request. The more often something happens, the higher the\npercentile we should care about is. For many workloads, looking at the 100th\npercentile (max measurement) is quite helpful, even though it only happened\nonce, because it can help to motivate capacity planning for other systems that\ndepend on it.\n\nFurther reading:\n\n  * The Tail at Scale by Jeff Dean\n\n### productivity\n\n    \n    \n    The computer is so powerful and so useful because it has eliminated many of the physical constraints of electromechanical devices. This is both its blessing and its curse: We do not have to worry about the physical realization of our software designs, but we also no longer have physical laws that limit the complexity of these designs\u2014the latter could be called the curse of flexibility - Nancy Leveson, 1995\n\nOne of the most frequently overlooked performance metrics is the cognitive\ncomplexity of a codebase. If engineers experience high friction when trying to\nchange a codebase, all efforts to make the code faster will be dramatically\nhindered. A codebase that is a joy for engineers to work with is a codebase\nthat will see the most long-term optimizations. Codebases that burn people out\nwill not see long-term success unless they receive tons of funding to replace\npeople who flee the project after short periods of activity. Organizational\ninstability is a high-quality predictive metric for the bugginess of a\ncodebase.\n\nPutting energy into reducing the complexity of your code will often make it:\n\n  * easier for humans to read (hence faster for them to optimize over time)\n  * easier for compilers to optimize due to being smaller\n  * faster to compile at all, resulting in a more responsive edit-measure loop, resulting in more optimizations per human time unit spent\n  * have less machine code, improving instruction cache at runtime (especially when running outside of microbenchmarks that conceal realistic cache effects)\n\n\u201cExperts write baby code.\u201d - Zarko Milosevic\n\nSo, we must pick our meaningful metrics, measure them after considerate\nexperimental design, make decisions while having these results at hand, and\nrepeat.\n\nOur unmeasured assumptions are incorrect. Optimizing without measuring is how\nyou end up with unmaintainable codebases that have been inflicted by many\ndisplays of \u201cperformative-optimization\u201d written with the metric of\n\u201cdemonstrates intellectual superiority\u201d over metrics like \u201clong-term\ncollaborator happiness\u201d.\n\nLet\u2019s strive to be clear about our metrics, at the very least.\n\n### sled case study\n\nHere are some other metrics that are interesting for sled:\n\n  * Single operation worst case latency: this is our primary metric because we are prioritizing transactional workloads above analytical workloads. We want users to have reliably responsive access to their data. We pay particular attention to the very worst case latency because it is fairly important from an operational perspective. We sometimes choose to sacrifice better average latency in order to improve the latency at high percentiles, to increase the overall dependability of the system.\n  * Peak memory utilization: we want a high fraction of all allocated memory to be made up of user data that is likely to be accessed. This lets us keep our cache hit rates higher given the available memory, reducing the latency of more operations.\n  * Recovery latency. How long does it take to start the database after crashing?\n  * Peak memory throughput: we want to avoid short-lived allocations that may be more efficiently stored on the stack. This also allows us to have more predictable latency as our memory usage grows, because most allocators start to degrade in various ways as they are pushed harder.\n  * Bulk-loading throughput: we want users to be able to quickly insert large amounts of data into sled quickly so they can start using it.\n  * Peak disk space utilization: we don\u2019t want sled to use 10x the space that user data requires. It\u2019s normal for databases to use 1-2x the actual data size because of various defragmenting efforts, but we reduce the number of deployment possibilities when this \u201cspace amplification\u201d is high.\n  * Peak disk throughput: there is a trade-off between new data that can be written and the amount of disk throughput we spend rewriting old data to defragment the storage file and use less total space. If we are careful about minimizing the amount of data that we write at all, we can increase our range of choice between smaller files and higher write throughput.\n  * Disk durability: the more we write data at all, the sooner our drives will die. We should avoid moving data around too much. A huge amount of the work of building a high quality storage engine boils down to treating the disk kindly, often at the expense of write throughput.\n\nIn sled, we measure histograms using code that was extracted into the\nhistorian crate. We also output a table of performance-related information\nwhen configured to do so. Having a profiler built-in makes finding bottlenecks\nquite easy, and in a quick glance it\u2019s easy to see where optimization effort\nmay be well spent.\n\n## experimental design\n\nExperimental design is about selecting meaningful workloads and metrics,\navoiding bias and achieving reproducible results without wasting too much\ntime.\n\nSimply measuring the runtime of a workload before and after applying a diff is\nunsound because there are so many other variables that impact performance.\n\nYour machine does some things that might not be obvious, which will change\nyour measurements:\n\n  * CPU frequency scaling\n\n    * CPUs will burst to high frequencies for short periods of time to make short tasks run quicker\n    * CPUs will lower their frequencies to use less power over time\n    * CPUs will lower their frequencies to generate less heat over time\n    * did better compilation cause your CPU to heat up? your better code may run slower afterwards\n    * is your laptop running on battery? better code may run slower\n  * is the data you\u2019re reading from disk already in the OS pagecache?\n\n    * your kernel keeps a lot of recently accessed file data in memory to speed up future accesses\n    * the second run of a program that reads data from disk doesn\u2019t pay the disk costs. better code may run slower than slower code with a warmed cache\n  * is your memory becoming more fragmented?\n  * The linking order used to combine otherwise identical compiled code objects when creating a binary\n\n    * can result in 10% more cycles with zero code changes. better code may run slower\n  * yelling near your computer\n\n    * having too much fun? better code may run slower\n  * If the kernel pulls your thread to a different socket than the one that initially allocated a bunch of memory, the latency to keep accessing that memory will increase by 60ish ns as requests need to be proxied through the original socket.\n\n    * threads can be pinned to sets of cores on specific sockets to avoid being migrated to another socket.\n\nIf an experiment were a pure math function, changing our input variables would\nbe the only thing that would influence the change of our observed outputs.\nUnfortunately, our systems are quite complex, and there are many factors which\nmay influence the quality of our measurements.\n\nExperimental design is at the heart of our quest to determine if our code\nchanges made our system better according to our chosen metrics.\n\nHow do we measure our metrics? We seek to make our programs more efficient by\nchanging code. Running a program twice will result in two different\nmeasurements. But the difference in performance is NOT necessarily because the\ncode is faster for realistic workloads.\n\nMany code changes that run faster in microbenchmarks will run more slowly when\ncombined with real program logic because the microbenchmark is able to use a\nlot more CPU cache than it would be able to when running with a real workload.\n\nYou can think of this as being similar to the \u201cmulti-tenancy\u201d performance\nissues that we struggle to deal with in distributed systems engineering. The\nsame applies even more to single-threaded code which radically changes due to\nthe amount of cache it has access to.\n\nVery often, faster code is only faster for a few moments before it causes\nfrequency scaling to kick in aggressively due to heat generation. There have\nbeen some infamous cases where using SIMD causes CPUs to heat up more, causing\nfrequency scaling to kick in more, and result in a slower system when running\nfor longer periods of time. Even without frequency scaling kicking in, faster\ncode often consumes more heat, as well. Maybe a 3% throughput improvement is\nnot worth a 100% power consumption increase.\n\nFailing to exercise experimental discipline will result in a lot of\n\u201coptimizations\u201d that are assumed to improve the situation but in fact only add\ncomplexity to the codebase, reducing maintainability, and making it harder to\nproperly measure future optimizations.\n\nOne of the most basic techniques: don\u2019t just run a workload once:\n\nBad:\n\n    \n    \n    * time compile and run workload 1 * time compile and run workload 2 * compare total times\n\nThe above method is extremely common, especially by people changing code and\nrunning measurements on their laptops without disabling any frequency scaling\nfeatures.\n\nWe need to be careful about being biased by the compilation itself. Frequency\nscaling can kick in when a compiler works harder to produce more optimized\nbinaries. If the compiler worked much harder to produce a better binary, the\nCPU frequency may be lower and any subsequent measurements might be much\nlower. Faster code will appear slower.\n\nBetter:\n\n    \n    \n    * restart your system with `intel_pstate=disable` to linux kernel line. * close slack. close your web browser. * kill as many non-essential processes as you can get away with. * if running a laptop, use the performance governor instead of powersave `for policy in /sys/devices/system/cpu/cpufreq/policy*; do echo $policy; echo \"performance\" | sudo tee $policy/scaling_governor; done` * compile workload 1 * compile workload 2 * disable frequency scaling and turbo boost (after compiling, it will slow things down) * `echo 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo` * time workload 1 * time workload 2 * time workload 1 * time workload 2 ... * time workload 1 * time workload 2 * view distribution of results * re-enable turbo boost for a less deterministic yet more enjoyable computing experience * `echo 0 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo`\n\nBy running workloads interleaved with each other, we reduce the risk of having\nparticular transient system-wide effects impact only a single measurement.\n\nBy taking multiple measurements, we improve our understanding of the\ndistribution of possible results given our current configuration.\n\nHowever, our goal is to increase the chances that we have established a causal\nlink between our code changing and our desired metrics improving. There are\nmany variables that we are always ignorant of. If we want to be more confident\nthat our system is actually better, we can gather corroborating evidence that\ncan help explain the measured effect.\n\nThere are a lot of things that happen in between your code changing and a\ntiming metric changing. A compiler transforms the code into machine code.\nMachine code is linked together in a certain order (causing up to a 10%\nperformance impact due to link order alone, see papers in the further reading\nsection below). Then when we run the binary, we load the various code objects\ninto memory at addresses which may be effected by ASLR, further introducing a\nconsiderable amount of variance. Anything that impacts memory layout could\nhave a strong impact on the effectiveness of our memory caches, which use\nseveral heuristics to predict which memory you may access next, and these may\nbe impacted by changes in physical memory allocation during the run of a\nworkload. Your CPU will run when it has instructions and data to zip together,\nbut it really spends a huge amount of time just waiting around for its\ndependencies to arrive. Whenever your instructions and data finally show up,\nyour CPU will execute them in creative ways that are later verified to conform\nto the dependency structure communicated through the compiled machine code,\nalthough they are often thrown away due to finding out that some data\ndependency has changed because of what some other CPU core published to the\nshared cache coherency subsystem. Importantly, the CPU will update a lot of\nits own performance counters as these various operations occur.\n\nFor our intermediate memory statistics, allocators like jemalloc expose many\nmetrics which can help us to explain changes in our measured resident set\nsize.\n\nVerifying that these intermediate metrics also change in significant ways can\nhelp us to increase our confidence in the causal link between changed code and\nchanges in high-level metrics like overall workload throughput, high-level\noperation latency, overall process memory resident set size, etc... In\ngeneral, the higher-level the metric, the more intermediate metrics may be\nhelpful in attempting to account for the change. The more complex the system\nyou\u2019re changing is, the higher the chance of falling prey to omitted variable\nbias.\n\nWhile running an experiment multiple times can help us account for variance,\nit cannot help us account for bias.\n\nTo reduce the likelihood that we are being impacted by bias, we must be\nconscious that bias is always present, and we must actively search for the\ntruth. Funny enough, Knowing About Biases Can Hurt People, so if we are going\nto accept the presence of bias, we must actively take responsibility for\ncountering it, or we may be even worse off than before we became aware of\nbias.\n\nIf you want to read more about personal debiasing, this book rules:\nRationality: From AI to Zombies.\n\nBut for debiasing our experiments, we can seek corroborating evidence through\nintermediate and related metrics, as well as measuring possible alternative\nexplanations for the high-level change and determining whether they may be the\ntrue explanation for the observed changes.\n\nIn addition to collecting runtime metrics while running our code in a more-or-\nless production configuration, we can also use tools like llvm-mca and\ncachegrind to estimate expected instruction and cache performance of our\nprograms, and DHAT and massif to analyze our heaps. These tools do not run\nyour program in a production configuration, but they still yield interesting\nmetrics that may be used to increase confidence in an effort to influence a\nhigh-level metric.\n\nOur CPUs expose a wide variety of performance counters that may also be\nsampled to compare against our other measurements.\n\n### experiment checklist\n\n  * p-state scaling may be disabled if using the intel_pstate driver by setting the linux kernel boot command argument intel_pstate=no_hwp\n  * turbo boost can be disabled by setting echo 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo if running with intel_pstate or echo 0 | sudo tee /sys/devices/system/cpu/cpufreq/boost if using acpi\n  * pagecache can be dropped via sync && echo 3 | sudo tee /proc/sys/vm/drop_caches (thanks @vertexclique)\n  * system-wide memory can be compacted via echo 1 | sudo tee /proc/sys/vm/compact_memory (thanks @knweiss)\n\nHere are two nice checklists from Raj Jain\u2019s The Art of Computer Systems\nPerformance Analysis (Amazon Associate link which sends the sled project a\nkickback):\n\nBox 2.2 Steps for a Performance Evaluation Study\n\n  1. State the goals of the study and define system boundaries.\n  2. List system services and possible outcomes.\n  3. Select performance metrics.\n  4. List system and workload parameters.\n  5. Select factors and their values.\n  6. Select evaluation techniques.\n  7. Select the workload.\n  8. Design the experiments.\n  9. Analyze and interpret the data.\n  10. Present the results. Start over, if necessary.\n\nBox 2.1 Checklist for Avoiding Common Mistakes in Performance Evaluation\n\n  1. Is the system correctly defined and the goals clearly stated?\n  2. Are the goals stated in an unbiased manner?\n  3. Have all the steps of the analysis followed systematically?\n  4. Is the problem clearly understood before analyzing it?\n  5. Are the performance metrics relevant for this problem?\n  6. Is the workload correct for this problem?\n  7. Is the evaluation technique appropriate?\n  8. Is the list of parameters that affect performance complete?\n  9. Have all parameters that affect performance been chosen as factors to be varied?\n  10. Is the experimental design efficient in terms of time and results?\n  11. Is the level of detail proper?\n  12. Is the measured data presented with analysis and interpretation?\n  13. Is the analysis statistically correct?\n  14. Has the sensitivity analysis been done?\n  15. Would errors in the input cause an insignificant change in the results?\n  16. Have the outliers in the input or output been treated properly?\n  17. Have the future changes in the system and workload been modeled?\n  18. Has the variance of input been taken into account?\n  19. Has the variance of the results been analyzed?\n  20. Is the analysis easy to explain?\n  21. Is the presentation style suitable for its audience?\n  22. Have the results been presented graphically as much as possible?\n  23. Are the assumptions and limitations of the analysis clearly documented?\n\nFurther reading:\n\n  * Five ways not to fool yourself\n  * Performance Analysis Methodology\n  * How Not to Measure Computer System Performance\n  * Producing Wrong Data Without Doing Anything Obviously Wrong! - ASPLOS 2009\n  * ASPLOS 13 - STABILIZER: Statistically Sound Performance Evaluation - ASPLOS 2013\n\n## concurrency and parallelism\n\n  * parallelism is when a task may execute independently of other things\n  * concurrency is when a task may be paused until some dependency is satisfied\n\nThis concurrency definition may feel a bit strange to most people. After all,\nisn\u2019t concurrency when we are spinning up lots of things that will run at\nonce, like a new thread or async task?\n\nWhen we describe our concurrent programs, we are usually writing a lot of code\nthat relates to finding out what happened somewhere else, getting some\ninformation from another place or another task, maybe blocking on a channel or\na socket, or writing some information to a different place. But usually, this\nmeans that we need to block until some activity outside of our local control\nfinishes. Blocking until the kernel can write our data into a socket, blocking\nuntil some other thread pushes an item into a queue that we\u2019re receiving from,\nblocking until all 5000 servers have finished counting the occurrences of the\ntrending term \u201chand sanitizer\u201d from user activity logs in the past 24 hours,\netc...\n\nWhen we talk about concurrency, we\u2019re talking about suspending our execution\nuntil various dependencies are satisfied. It\u2019s kind of declarative, in that we\ndescribe our dependencies, and then we expect an external scheduler to ensure\nthat our code will run when our dependencies are met.\n\nParallelism is sort of the opposite, in that it is when things can run at the\nsame time due to not having dependencies which are unmet. It implies more\nflexibility for your schedulers, because they can just run your code without\nworrying about satisfying dependencies in some order. But it\u2019s more\nrestrictive for the programmer, as they must write their task in such a way\nthat it is able to make progress independently of other tasks. This is simply\nnot possible for many types of problems, like cryptographically hashing a\nnumber 4000 times - as each subsequent round requires the previous one to\ncomplete first.\n\nHowever, if we are able to write programs such that multiple tasks that can\nmake progress independently of each other, it is possible to schedule them (if\nsystem resources are available) to run at the same time, thus reducing overall\nexecution time.\n\nParallelism vs concurrency is a trade-off between:\n\n  * scheduler flexibility vs human flexibility\n  * independence vs potential blocking on dependencies\n  * imperative vs declarative\n\nOne insight is that a sequential, single-threaded execution can be placed\nsquarely in the middle of parallelism and concurrency in each of the above\ntrade-offs.\n\nWhile some schedulers may be able to run concurrent tasks in parallel when\nthey are known not to have any sequential dependencies, the high-level\napproach is already starting at a conservative point that prevents common\ntechniques like optimistic non-blocking algorithms from better taking\nadvantage of the parallelism that our hardware could provide us with.\n\nOver time, our hardware is becoming much more parallel - more CPU cores, more\nmemory channels, more parallel GPU circuitry, etc... And if we want to take\nadvantage of it, we simply must write programs that can make progress on\nmultiple system resources in parallel.\n\nFor a video that goes into the parallelism vs concurrency design space, check\nout a few minutes after 8:18 in this presentation:\n\nCppCon 2019: Eric Niebler, David Hollman \u201cA Unifying Abstraction for Async in\nC++\u201d\n\nIf you have high costs associated with putting work back together, or\nassociated with contending on a resource (too many threads for your cpus,\nmutexes, etc...) then your parallelism gains will be negatively impacted. So,\nto properly take advantage of parallelism, we must minimize the amount of work\nwe spend waiting for a shared resource, and blocking on other tasks to\ncomplete, or paying costs of merging parallel work together. Less sharing at\nall means more throughput because things can run independently of each other.\nsled tries to share very little across requests for this reason.\n\n## amdahl\u2019s law\n\nAmdahl\u2019s law is a tool for reasoning about the potential speedups that a\nworkload can achieve through parallelization. Some tasks must be executed\nserially, and will see no benefit from parallelization. Calculating the result\nof cryptographically hashing a number many times must be performed serially\n(ignoring rainbow tables for this example), because each step requires the\noutput of the previous step, and it cannot begin until that result is known.\n\nCalculating the result of multiplying and dividing many numbers with each\nother is completely parallelizable because we can start operating on sub-\narrays of arguments, and then combine the results at the end.\n\nFun fact: our CPUs aggressively predict inputs to operations and start\nexecuting their results before knowing if the guesses are correct. The better\nwe can predict inputs, the more we can speculatively execute results in\nparallel. Margo Seltzer has given some great talks on a project to take this\nconcept much farther than would seem plausible.\n\nThere are sometimes parts of our programs that can be parallelized, while\nothers must be executed in a serial fashion. Amdahl\u2019s law shows that throwing\nmore resources at a workload will not usually cause that workload to complete\nin a proportional fraction of time.\n\nThe main takeaway:\n\n  * only part of a program can be sped up through parallelization\n\n## universal scalability law\n\nThe USL is an excellent way to reason about the potential gains (or negative\nimpacts) of concurrency. It goes farther than Amdahl\u2019s law because it starts\nto be able to describe how concurrency can start to introduce significant\nnegative costs due to the work that must be done by virtue of making a\nworkload concurrent at all.\n\nThe USL describes two costs associated with adding more concurrency to a\nworkload:\n\n  * contention - the amount that more concurrent work leads to more waiting for shared resources like a mutex, disk head, CPU core, etc...\n  * coherency - the amount that costs must be paid to reassemble or block on other work as the amount of concurrency increases in the system\n\nThe USL shows that concurrency can reduce possible gains of parallelism by\nintroducing contention and coherency costs. Contention is effectively the part\nof the program that Amdahl\u2019s law describes as being non-parallelizable.\nCoherency is the cost of putting it all back together.\n\nFrank McSherry showed that a single laptop running a properly tuned single-\nthreaded implementation can outperform the throughput of an expensive cluster\nfor several graph processing algorithms by completely avoiding the massive\ncontention and coherency costs that such distributed systems must pay.\n\nFurther reading:\n\n  * Original USL article\n\n## trade-offs\n\n### the RUM conjecture\n\nWhen designing access methods for data structures, databases, systems, etc...\nthere exists a 3-way trade-off:\n\n  * read overhead\n  * update overhead\n  * memory (space) overhead\n\nBy choosing to optimize any 2 of the above properties, the third one tends to\nsuffer. The way that they suffer can be thought of through either contention\nor cohesion costs.\n\nLet\u2019s apply this to an in-memory ordered map:\n\n  * R + U: here, we can trade higher space usage for cheap reads and writes. An immutable, purely-functional tree allows readers and writers to work without contending on a mutex, and space may never be reclaimed.\n  * U + M: we trade slow reads for fast writes and low space. A tree where tree nodes are represented as a linked-list of updates applied to a base node. Writers may simply attach an update to the head of the linked list representing the node. Readers must scan through the list and replace it with a compacted version, freeing the space used by the previous updates and base node, and removing any outdated versions / deleting items that had a tombstone placed in the update list. Readers pay compaction costs, writers proceed quickly.\n  * R + M: we trade slow writes for fast reads and low space. This is effectively how classic B-tree databases often work. Writers must take out a lock to update the tree node, and pay the costs associated with cleaning up outdated state. Writers can perform their operations in ways that are more expensive for the writer, but allow readers not to block at all, such as multi-version concurrency control. However, effort needs to be spent (by the writers in this case) to keep overall space usage low and perform more eager garbage collection work to remove unnecessary old versions.\n\nThere are many ways to push this work around to achieve desired properties,\nand we can generally be pretty creative in how we do so, but we can\u2019t be\nabsolutely optimal in all 3 categories.\n\nThis trade-off was introduced in the paper Designing Access Methods: The RUM\nConjecture and Mark Callaghan frequently writes about applying these trade-\noffs in what he calls \u201cdatabase economics\u201d where decisions can be made to\noptimize for various properties at the expense of others.\n\nIn Mark\u2019s writings, he uses slightly different terminology, and also\nintroduces a few other trade-offs that impact database workloads:\n\n  * read amplification - how much work a read request performs\n  * write amplification - how often data is rewritten to keep overall space low\n  * space amplification - how much extra space is used\n\nIn many database designs, you can trade write amplification for space\namplification fairly easily by throttling the amount of effort spent\nperforming compaction/defragmentation work. By compacting a file, you are\nspending effort rewriting the data in order to make the total space smaller.\n\nThis also has storage endurance implications where we reduce the total\nlifespan of the device by performing more writes over time.\n\nMark also introduces the concept of cache amplification, which he describes as\nthe amount of in-memory overhead there needs to be in order to service one\nread request with at most one disk read.\n\nAnother look at these trade-offs has been presented in the paper The Periodic\nTable of Data Structures.\n\n### memory pressure vs contention\n\nContention can be caused when multiple threads are trying to access a shared\nresource, such as some state that is protected by a mutex or a reader-writer\nlock. In many cases, it\u2019s possible to reduce contention by spending more\nmemory.\n\nFor example, we can remove the requirement to have readers take out locks at\nall by having any thread that acquires the mutex first make a local copy,\nperform its desired changes on the local copy, and then when finished swap an\natomic pointer to install their update. Readers may follow this pointer\nwithout ever acquiring a lock first. In this case, we will do a new allocation\neach time we want to modify the data. But for read-heavy structures, this\ntechnique can keep a system running without needless blocking. Care needs to\nbe used to avoid destroying and freeing the previous version while readers may\nstill be accessing it, either by using a garbage collected language, atomic\nreference counters, or something like hazard pointers or epoch-based\nreclamation.\n\nIn Rust, we have the Arc::make_mut method which will look at the reference\ncount, and either provide a mutable reference (if the count is 1) or make a\nclone and return a mutable reference to that. We can use a RwLock<Arc<T>> to\nget a mutable reference to the Arc, and either apply mutations without\nblocking existing readers. This \u201csnapshot readers\u201d pattern is another point in\nthe contention-memory pressure spectrum. Readers take a reader lock and clone\nthe Arc before dropping it, giving them access to an immutable \u201csnapshot\u201d.\nWriters take a writer lock and call Arc::make_mut on it to then mutate the Arc\nthat exists inside the RwLock, blocking the next group of readers and writers\nuntil its mutation is done.\n\nThe compiler will make all kinds of copies of our constant data at compile-\ntime and inline the constants into code to reduce cache misses during runtime.\nData that never changes can be aggressively copied and cached without fear of\ninvalidation.\n\n### speculation\n\nWhile things like speculative execution have gotten kind of a bad rap due to\nSpectre and other CPU information leak vulnerabilities that relate to it,\noptimistic and speculative execution is fundamental for many areas of systems\nprogramming. It shows up in many places in our operating systems, browsers,\nstandard libraries, etc... Ubiquitous lock-free algorithms that power our\nqueues rely on this heavily.\n\nThere are two forms of speculation:\n\n  * predictive execution: instead of spending the (often high) cost for acquiring exclusive access to a resource or knowing with certainty about some outcome, we just make a guess that our desired outcome will happen, and we find out later on if it was a correct bet or not. This bet makes sense when the cost of being wrong about the bet times the chance of being wrong about the bet is lower than the cost of acquiring exclusive access or determining some outcome with certainty first on every request.\n  * eager execution: perform an operation based on every possible input to the computation. Later on, when the actual input is known, just choose the result that you already computed. This sounds kind of absurd, but Margo\u2019s linked talk above shows that this can really be pushed far in some situations.\n\nIn general, it\u2019s more common in software to write code that is predictively\nspeculative. Lock-free algorithms often make a bet about the current value of\na pointer, perform some work, and then use an atomic compare-and-swap (CAS)\noperation to try to mutate the shared pointer to a new modified value if the\npredicted previous value was properly guessed. If some other thread changed\nthe pointer already, the guess about the previous value will be incorrect, and\nthe operation will either retry using the new value or it will fail.\n\nSpeculation is a key tenant in taking advantage of parallel hardware, because\nit lets us make bets about contention and possibly throw work away if our bet\ndidn\u2019t pan out.\n\nMost modern databases are usually designed to perform transactions using\nOptimistic Concurrency Control. Rather than taking out locks for items\ninvolved in a transaction, OCC avoids locks but performs a validation step\nbefore committing the transaction to check whether other transactions have\ninterfered. The assumption is that for many workloads, transactions tend to\noperate on non-overlapping data. Validating the correctness of a transaction\ndoes not require using locks, either.\n\nUsing the framework of the USL, OCC trades blocking contention of acquiring\nlocks pessimistically for the coherency costs of performing validation without\nblocking. A single-threaded database can avoid both of those costs, but if you\nwant to scale your system beyond a single thread, you will need to perform\nsome form of concurrency control anyway.\n\n###### auto-tuning speculation\n\nHowever, you can build databases to be auto-tuning and avoid any concurrency\ncontrol as long as only a single thread is running, and \u201cupgrade\u201d the system\ndynamically when the database is being accessed by multiple threads by re-\nconfiguring the concurrency control configuration, and then waiting for any\nthreads operating under the old configuration to finish their tasks to avoid\nconflicts. The sled database allows lock-free single-key operations to avoid\nperforming any concurrency control usually, but as soon as a single multi-key\ntransaction happens, it re-configures the system to perform concurrency\ncontrol for all operations. This embodies the philosophy of \u201cpay for what you\nuse\u201d and avoids paying concurrency control costs for operations that only\nrequire single-key linearizability, rather than serializability.\n\nSpeculation can cause wasted work to happen if the bet being made tends not to\nresult in less overall work. For example, if 10,000 threads are all trying to\ndo a compare and swap on a single item in a tight loop, it will result in only\na single one of those threads ever succeeding after reading the last\nsuccessful value, and all of the other threads failing to make any progress\ndespite doing work. In this case, it may make more sense to just take out a\nmutex and prevent all of the other threads from needing to throw a lot of work\naway. We can use cheap thread-local variables for tracking contention, and\nfall-back to more pessimistic execution techniques when contention is measured\nto result in a lot of wasted work.\n\n##### fairness\n\nIn the above example with 10,000 threads in a CAS loop on a piece of memory,\nthere is a significant chance to introduce unfairness where only a single\nthread tends to be winning the CAS loops. This is because it takes time for\nupdates to memory to propagate to other cores and sockets, and the core where\nthe last change happened is the one with the most up-to-date cache. It has a\nsignificant head-start compared to other threads that need to wait for this\npropagation latency before being able to make a bet in their own CAS attempt.\n\nThis is said to be unfair because most threads will see very little progress,\nbut the previous thread to \u201cwin\u201d gets a higher chance to keep winning. This\ncan lead to starvation where some threads are unable to make as much progress.\n\nThis has implications for mutexes, among many other things. Modern mutex\nimplementations tend to implement some sort of fairness-mechanism to combat\nthis effect. It can actually be quite expensive to have a mutex be completely\nfair, as it requires maintaining essentially a FIFO queue that mediates\naccess, and bending on the FIFO property can lower the effort of using a\nmutex, which reduces contention and overhead. So, fairness can be expensive.\nBut you can use tricks to get some fairness for a very low price. For\ninstance, the parking_lot crates implements an idea called \u201ceventual fairness\u201d\nwhere fairness measures are taken occasionally, which adds a very low amount\nof overhead while achieving a useful amount of fairness in many situations.\n\nCombating starvation is also a vital aspect of a high quality scheduler.\n\n### scheduling\n\nThis section is essentially about multiplexing. Sometimes we want to share\nsome physical or logical resource among multiple computational tasks. Maybe we\nwant to run multiple operating systems, processes, threads, or async tasks on\na single CPU. Or maybe we want to multiplex multiple data streams on a single\nTCP socket. Maybe we want to map some set of files or objects onto some set of\nstorage devices.\n\nOur ability to map the appropriate tasks to appropriate computational\nresources will determine how effectively that task can do its job of\ntransforming its own computational dependencies into its computational\nresults.\n\nkey terms:\n\n  * over-subscription\n  * interactive workloads\n  * batch workloads\n\nUltimately, if we want to do more work, we can either make our work more\nefficient for getting more done on the computational resources we already\nhave, or we can figure out a way to run our work on more hardware. In cases\nwhere additional computational resources (CPUs, GPUs, disks, memory, etc...)\nare available, it often makes sense to figure out how to use them rather than\ntrying to figure out how to better use existing ones.\n\nIn Parallel Disk IO, Dmitry Vyukov goes into some nice things to keep in mind\nwhile building a scheduler. In general, you should be mindful of the way that\nwork is queued up in your system, because using things like FIFO queues will\nguarantee that cache locality gets worse as the queues fill up. Using a stack\ninstead of a FIFO will generally help to improve cache locality for both work\nitems and threads that are woken to perform work.\n\nCrucially, Dmitry points out that work should be prioritized in a way that is\nmindful of the state of completion of a particular task. If you are building a\nservice that responds to requests, you probably want to prioritize work in\nthis way:\n\n  * first run things that are ready to write, as they signify work that is finished\n  * things that are ready to read, as they are work that has been accepted and the timer is ticking for\n  * only accept based on a desired queue depth based on your latency/throughput position. If you care about latency above everything, never accept unless all writes and reads are serviced and blocked. If you care about throughput above all else, you want to oversubscribe and accept a lot more work to reduce the frequency that your system bottoms out and has no work to do. You don\u2019t want to accept work that you\u2019re not servicing though if latency is a priority, and you want a smaller TCP backlog that will fill up and provide back-pressure for your load balancer so it can do its job.\n\nThe general idea is to keep the pipeline busy, but favoring work toward the\nend of the pipeline as they will free resources when they complete, reducing\noverall system load and increasing responsiveness by reducing queue depths.\n\nIn Task Scheduling Strategies, Dmitry mentions some trade-offs between work-\nstealing and other approaches.\n\nWork stealing is when threads have their own work queues, but they will try to\nsteal work from other threads when they run out of work to do. This imposes\ncoordination costs because threads need to synchronize access to their queues\nwith threads that may steal from them. It also imposes cache locality hits\nbecause it might take work from a thread with the data in a closer cache and\nforces a stealing thread to pay more cacheline retrieval costs to work with\nthe stolen data.\n\nDmitry recommends that most general-purpose schedulers use work-stealing due\nto the downsides being not too bad in most cases, but he points out that in\nsome situations where you are scheduling small computational work (cough cough\nRust async tasks) you may want to use an Erlang-style work-balancing strategy\nthat has perfect knowledge of the progress that various threads are making,\nand from a centralized position performing re-balancing work.\n\nThere are also two other forms of work balancing:\n\n  * work requesting is when a thread that runs out of work makes a request to a thread with work to give it over. This allows threads to avoid synchronizing access to their own queues, which can significantly speed up their own consumption. But it increases the latency that threads sit idle before having work given to them by the threads with excess work. And it imposes some extra work on the giving side at the time of hand-off, which may or may not be worse than the consumption benefits, depending on the workload.\n  * work balancing is when you distribute work onto a set of threads that may not have enough work to do.\n\nIn scheduling research, there is a quest to try to build systems that are able\nto make the best decisions for various workloads. It is not possible to build\na scheduler that is optimal for all use cases. There are many latency-\nthroughput, contention vs space, fairness, centralized optimality vs\ndistributed throughput trade-offs to be made.\n\nIf you want to dig in deeper, this distributed scheduling paper made a lot of\nwaves in 2016 and comes to some nice conclusions: Firmament: Fast, Centralized\nCluster Scheduling at Scale.\n\n## scouting ahead\n\nOne of the most important lessons that pilots learn is to make decisions based\non the readings from their flight instruments. Humans are surprisingly poor\njudges of issues like \u201cwhich direction is the ground\u201d especially when flying\nat night and in storms. Learning to trust the readings of the instruments is\nvital for staying alive over many flights in varied weather situations.\n\nProgrammers often don\u2019t do this. We tend to make random guesses based on some\nprimitive feeling, resulting in months or years lost pursuing the impossible\nover the course of a career. But our machines are quite amenable to\nmeasurement, and we can avoid wasting so much of our lives by paying better\nattention to metrics that are easy to acquire before spending time optimizing\nsomething that isn\u2019t a problem to begin with.\n\nMany potential optimizations can take hours or weeks of coding and refactoring\nbefore you can get the system to a place where you can perform any\nmeasurements at all. Sometimes you have a strong hunch, or sometimes you have\n5000 engineers whose time you don\u2019t respect, but in any case, sometimes\noptimizations take a lot of effort. Fortunately, there are a lot of techniques\navailable to us for avoiding performing work that will be thrown away, and in\nthe process, increasing the amount of respect we can show ourselves and our\nteammates by not asking them to do impossible things.\n\nHere are three techniques that can provide evidence that may help to inform\nhigh quality decisions about where engineering effort may be spent with a\nhigher chance of success before just jumping into a suspected optimization\nsomewhere:\n\n  1. flamegraphs\n  2. deletion profiling\n  3. causal profiling\n\n#### flamegraphs\n\nFlamegraphs are useful for visualizing the costs associated with specific code\nlocations by showing the expense in terms of visual area. They are a good\n\u201cfirst glance\u201d at where a program may be spending a lot of time, instructions,\nor some other measurable resource but there are a number of caveats that come\nwith their interpretation, and you should only use them as a kind of\nflashlight that may illuminate where specific costs are arising in code. In\nmany cases, they will not show costs associated with time spent off-cpu\nwaiting for blocking IO etc... so you need to be mindful of this, or use a\ntool that specifically supports off-cpu profiling.\n\nI\u2019ve already written a bit about them on the flamegraph-rs/flamegraph README\n(scroll down for a guide on how to use them).\n\nIt\u2019s important to note that you can associate a variety of different kinds of\ncosts with code using flamegraphs, including allocations, off-cpu time, branch\nmispredictions, cache misses, etc...\n\nI also strongly recommend checking out KDAB/Hotspot for both generating and\nvisualizing perf data. It has a nice UI and may be preferable for some users\nto the above flamegraph utility.\n\nPersonally, I frequently use a mix of the above flamegraph tool, hotspot, and\nperf report directly in the terminal, depending on the kind of visualization I\nam trying to generate.\n\nAlso see Brendan Gregg\u2019s page on flamegraphs. He has probably done more than\nanyone in recent years to help people to better understand their system\u2019s\nperformance. His book Systems Performance: Enterprise and the Cloud is also\nextremely high quality, especially chapter 2: Methodology.\n\n#### deletion profiling\n\nThis is one of my favorites because it\u2019s so easy when it\u2019s applicable. Just\ncomment out some function call that you are curious about the cost of and see\nhow much faster the program runs. If it\u2019s not any faster, it doesn\u2019t matter\nhow much you improve it, you will never be able to match the benefit of\ncompletely deleting it (assuming it doesn\u2019t get compiled away anyway). Don\u2019t\nspent time speeding up code that does not contribute to the overall cost.\n\n#### causal profiling\n\nCausal profiling is kind of the opposite of deletion profiling. Instead of\ndeleting code, it adds delays in different places to see how much worse the\noverall runtime is. Unlike deletion profiling, it doesn\u2019t potentially break\nthe program by using it, and it can be widely employed to find opportunities\nfor tuning things.\n\nCoz is one of the best-known implementations of it. There is also support for\nrust, which was upstreamed after beginning in alexcrichton/coz-rs.\n\nUsing these profilers and others like DHAT, massif, cachegrind, etc... will\nshow you where you may stand to benefit from additional optimization effort.\n\nRemember, profiling tends to rely on sampling techniques that are quite lossy.\nUse profiling to find optimization targets, but then rely on a variety of\ncorrelated metrics to gain confidence that your changes are actually improving\nthe performance of the system instead of just shifting effort into a blind\nspot of the profiler.\n\n# that\u2019s it for now\n\nThis guide has touched on a variety of techniques, theories, models, and\nmindsets that have proven to be invaluable while writing sled and working with\na variety of other stateful systems.\n\nThe most important thing is to ask yourself what really matters. Your time is\nprecious. Try to give yourself high quality information, and base decisions on\nthe estimated expectation that they may help you with what matters to you. We\nare wrong about everything to some extent, but at least when we work with\ncomputers we usually have a variety of measurement techniques available. We\ncan perform lightweight experiments to feel out the potential impact of\nlonger-term engineering efforts. One of the most important aspects of some of\nthese ideas is that they illuminate the \u201cnegative space\u201d that is unlikely to\nbe possible without extreme expense. Gaining confidence in potential positive\nimpacts before investing significant effort is ultimately about respecting\nyourself and your teammates. Be considerate.\n\nIf you found this article to be useful, please consider supporting my efforts\nto share knowledge and productionize cutting edge database research with\nimplementations in Rust :)\n\n", "frontpage": false}
