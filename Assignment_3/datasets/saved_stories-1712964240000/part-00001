{"aid": "40015346", "title": "Compute Governance", "url": "https://www.aisafetybook.com/textbook/8-4", "domain": "aisafetybook.com", "votes": 1, "user": "pcfwik", "posted_at": "2024-04-12 17:22:59", "comments": 0, "source_title": "8.4: Compute Governance | AI Safety, Ethics, and Society Textbook", "source_text": "8.4: Compute Governance | AI Safety, Ethics, and Society Textbook\n\nTake Action\n\nCourse\n\nCurriculum\n\nDownload Textbook\n\n1\\. Overview of Catastrophic AI Risks\n\n0.1\n\nPreface\n\n1.1\n\nOverview of Catastrophic AI Risks\n\n1.2\n\nMalicious Use\n\n1.3\n\nAI Race\n\n1.4\n\nOrganizational Risks\n\n1.5\n\nRogue AIs\n\n1.6\n\nDiscussion of Connections Between Risks\n\n2\\. AI Fundamentals\n\n2.1\n\nAI Fundamentals\n\n2.2\n\nArtificial Intelligence & Machine Learning\n\n2.3\n\nDeep Learning\n\n2.4\n\nScaling Laws\n\n2.5\n\nConclusion\n\n3\\. Single Agent Safety\n\n3.1\n\nSingle Agent Safety\n\n3.2\n\nMonitoring\n\n3.3\n\nRobustness\n\n3.4\n\nAlignment\n\n3.5\n\nSystemic Safety\n\n3.6\n\nSafety and General Capabilities\n\n3.7\n\nConclusion\n\n4\\. Safety Engineering\n\n4.1\n\nSafety Engineering\n\n4.2\n\nRisk Decomposition\n\n4.3\n\nNines of Reliability\n\n4.4\n\nSafe Design Principles\n\n4.5\n\nComponent Failure Accident Models and Methods\n\n4.6\n\nSystemic Factors\n\n4.7\n\nTail Events and Black Swans\n\n4.8\n\nConclusion\n\n5\\. Complex Systems\n\n5.1\n\nComplex Systems\n\n5.2\n\nIntroduction to Complex Systems\n\n5.3\n\nComplex Systems for AI Safety\n\n5.4\n\nConclusion\n\n6\\. Beneficial AI and Machine Ethics\n\n6.1\n\nBeneficial AI and Machine Ethics\n\n6.2\n\nLaw\n\n6.3\n\nFairness\n\n6.4\n\nThe Economic Engine\n\n6.5\n\nWellbeing\n\n6.6\n\nPreferences\n\n6.7\n\nHappiness\n\n6.8\n\nSocial Welfare Functions\n\n6.9\n\nMoral Uncertainty\n\n7\\. Collective Action Problems\n\n7.1\n\nCollective Action Problems\n\n7.2\n\nGame Theory\n\n7.3\n\nCooperation\n\n7.4\n\nConflict\n\n7.5\n\nEvolutionary Pressures\n\n7.6\n\nConclusion\n\n8\\. Governance\n\n8.1\n\nGovernance\n\n8.2\n\nTimelines\n\n8.3\n\nDistribution\n\n8.4\n\nCompute Governance\n\n8.4.1 Compute Is Indispensable for AI Development and Deployment8.4.2 Compute\nIs Physical, Excludable, and Quantifiable\n\n8.5\n\nCorporate Governance\n\n8.6\n\nNational Governance\n\n8.7\n\nInternational Governance\n\n8.8\n\nConclusion\n\n9\\. Appendices\n\n9.1\n\nAcknowledgements\n\n10.1\n\nApp. A: Normative Ethics\n\n11.1\n\nApp. B: Utility Functions\n\n12.1\n\nApp. C: Reinforcement Learning\n\n13.1\n\nApp. D: Long-Tailed and Thin-Tailed Distributions\n\n14.1\n\nApp. E: Evolutionary Game Theory\n\n15.1\n\nApp. F: Other Cooperation Mechanisms\n\n16.1\n\nApp. G: Intrasystem Conflict Causes\n\nDownload TextbookCourse MaterialsCourseCurriculumTake Action\n\nAll\n\nSection\n\nAppendix\n\n8.4\n\n# Compute Governance\n\n## A common shorthand for computational resources or computing power used for\nAI is compute. In this section, we will discuss how compute governance enables\nAI governance.\n\nNo items found.\n\n# 8.4 Compute Governance\n\nA common shorthand for computational resources or computing power used for AI\nis compute. In this section, we will discuss how compute governance enables AI\ngovernance. First, we will examine how since compute is indispensable for AI\ndevelopment, governing it would help us govern AIs. Then, we will examine the\nkey properties of compute that make it governable\u2014\u2013physicality, excludability,\nand quantifiability. These features make it more feasible to track, monitor\nand, if appropriate, restrict the development of potentially dangerous AI\nsystems, and more generally facilitate the enforcement of AI governance\nmeasures. We also consider why governing compute is more promising than\ngoverning other factors used in AI production.\n\n## 8.4.1 Compute Is Indispensable for AI Development and Deployment\n\nCompute enables the development of more capable AIs. In addition, compute is\nnecessary to deploy AIs. If we restrict someone\u2019s access to compute, they\ncannot develop or deploy any AIs. As a result, we can use compute to govern\nAIs, determining how and when they are deployed.\n\nHardware is necessary for AI systems. Like the uranium in a nuclear weapon,\ncompute is fundamental to running AIs. In its simplest form, we can think of\ncompute as a select group of high-performance chips like GPUs and TPUs that\nare designed to run modern AIs. These are often the latest chips, tailor-made\nfor AI tasks and found in large data centers. As AI technology changes, so too\nwill the hardware, adapting to new tech developments, regulations, and the\nevolving requirements of AI applications.\n\nThe metric FLOP/s is common across forms of compute. To measure compute, we\noften use the metric FLOP/s, which is the number of floating-point operations\n(such as addition or multiplication) a computer can do in a second. When we\ntalk about \u201cincreasing\u201d compute, we\u2019re referring to using more processors,\nusing better processors, or allowing these processors to run for extended\nperiods, effectively increasing the number of floating-point operations done\nin total. An analogous escalation of this is improving a nuclear arsenal by\nadding more weapons to it, developing more dangerous weapons like H-bombs, or\ncreating bigger bombs by using more uranium.\n\nMore compute allows for the development of more AI capabilities. Compute plays\na pivotal role in the evolution of AI capabilities. More compute means that AI\nsystems can be built with more parameters and effectively utilize larger\ndatasets. In the Artificial Intelligence Fundamentals chapter, we looked at\nscaling laws, which show us that many AIs have their performance increase\nreliably with an increase in model size and dataset size. Richard Sutton\u2019s\n\u201cThe Bitter Lesson\u201d states that general methods in AI that harness computation\ntend to be the most effective by a significant margin [1]. Having more compute\nmeans training AI systems that are more capable and advanced, which means that\nknowing how much compute an AI uses lets us approximate its capabilities.\n\nOften, pushing the boundaries in AI development requires having vast compute.\nAIs can require training on large supercomputers that cost hundreds of\nmillions or even billions of dollars. Moreover, computational demands for\nthese AI models are constantly intensifying, with their compute requirements\ndoubling roughly every six months. This growth rate surpasses the 2.5-year\ndoubling time we see for the price-performance of AI chips [2]. Given this\ntrend, it\u2019s likely that future AI models will demand even greater investment\nin computational resources and, in turn, possess greater capabilities.\n\nMore compute enables better results. Compute is not only essential in training\nAI-\u2014it is also necessary to run powerful AI models effectively. Just as we\nrely on our brains to think and make decisions even after we\u2019ve learned, AI\nmodels need compute to process information and execute tasks even after\ntraining. If developers have access to more compute, they can run bigger\nmodels. Since bigger models usually yield better results, having more compute\ncan enable better results.\n\nLarge-scale compute isn\u2019t a strict requirement for all future AI applications.\nAI efficiency research aims to reduce compute requirements while preserving\nperformance by improving other factors like algorithmic efficiency. If\nalgorithms become so refined that high-capability systems can train on less\npowerful devices, compute\u2019s significance for governance might diminish. Some\nexisting systems like AI-powered drug discovery tools do not require much\ncompute, but it has been demonstrated that they be can repurposed to create\nchemical weapons [3].\n\nAdditionally, there\u2019s continued interest towards creating efficient AI models\ncapable of running on everyday devices such as smartphones and laptops. Though\nprojections from current trends suggest it will be decades before data center-\nbound systems like GPT-4 could train on a basic GPU [4], the shift towards\ngreater efficiency might speed up dramatically with just a few breakthroughs.\nIf AI models require less compute, especially to the point that they become\ncommonplace on consumer devices, regulating AI systems based on compute access\nmight not be the most effective approach.\n\n## 8.4.2 Compute Is Physical, Excludable, and Quantifiable\n\nTo produce AI, developers need three primary factors: data, algorithms, and\ncompute. In this section, we will explore why governing compute appears to be\na more promising avenue than governing the other factors. A resource is\ngovernable when the entity with legitimate claims to control it\u2014\u2013such as a\ngovernment\u2013\u2014has the ability to control and direct it. Compute is governable\nbecause\n\n  1. It can be determined who has access to compute and how they utilize it.\n\n  2. It is possible to establish and enforce specific rules about compute.\n\nThese are true because compute is physical, excludable, and quantifiable.\nThese characteristics allow us to govern compute, making it a potential point\nof control in the broader domain of AI governance. We will now consider each\nof these in turn.\n\n### Compute Is Physical\n\nThe first key characteristic that makes compute governable is its physicality.\nCompute is physical, unlike datasets, which are virtual, or algorithms, which\nare intangible ideas. This makes compute rivalrous and enables tracking and\nmonitoring, both of which are crucial to governance.\n\nSince compute is physical, it is rivalrous. Compute is rivalrous: it cannot be\nused by multiple entities simultaneously. This is unlike other factors in AI\nproduction, such as algorithms which can be used by anyone who knows them or\ndata which can be acquired from the same source or even stolen or copied and\nused simultaneously (although this may be difficult due to information\nsecurity and the size of training datasets). Because compute cannot be\nsimultaneously accessed by multiple users or easily duplicated, regulators can\nbe confident that when it is being used by an approved entity, it is not also\nbeing used by someone else. This makes it easier to regulate and control the\nuse of compute. GPUs can\u2019t be downloaded but instead must be fabricated,\npurchased, and shipped.\n\nFigure 8.2: Advanced semiconductor manufacturing tools (such as the ASML\nTwinscan NXE) are large, highly specialized machines. [5]\n\nSince compute is physical, it is trackable. Compute is trackable, from chip\nfabrication to its use in data centers. This is because compute is tangible\nand often sizable: Figure 8.2 shows a cutting-edge semiconductor tool used as\ncompute that costs $200 million and requires 40 freight containers, 20 trucks,\nand 3 cargo planes to ship anywhere.\n\nUnlike uranium, which is difficult to procure but not impossible to steal and\ntransport small amounts of, acquiring large-scale compute requires the\ninvestment of resources in a relatively complicated and visible process.\nStakeholders, whether semiconductor firms, regulatory bodies, or other\ninvolved entities, can accurately evaluate and trace the overall quantity of\nthese assets. For instance, if we monitor the sales and distribution of chips,\nwe know who possesses which computational capacities and their intended\napplications. The complete supply chain, from the semiconductor origins to the\nextensive data centers harboring vast AI computational power, can be\nmonitored, which means it can be governed. By contrast, data acquisition and\nalgorithmic improvements can be done discreetly: possession of these within a\ncomputing infrastructure can be concealed more easily than the possession of\nthe infrastructure itself.\n\n### Compute Is Excludable\n\nThe second key characteristic that makes compute governable is its\nexcludability. Something is excludable if it is feasible to stop others from\nusing it. Most privately produced goods like automobiles are excludable\nwhereas others, such as clean air or street lighting, are difficult to exclude\npeople from consuming even if a government or company doesn\u2019t want to let them\nuse it. Compute is excludable because a few entities, such as the US and the\nEU, can control crucial parts of its supply chain. This means that these\nactors can monitor and prevent others from using compute.\n\nThe compute supply chain makes monitoring easier. In 2023, the vast majority\nof advanced AI chips globally are crafted by a single firm, Taiwan\nSemiconductor Manufacturing Company (TSMC). These chips are based on designs\nfrom a few major companies, such as Nvidia and Google, and TSMC\u2019s production\nprocesses rely on photolithography machines from a similarly monopolistic\nindustry led by ASML [6]. Entities such as the US and EU can, therefore,\nregulate these companies to control the supply of compute\u2014if the supply chain\ndynamics do not change dramatically over time [7]. This simplifies the\ntracking of frontier AI chips and enforcing of regulatory guidelines; it\u2019s\nwhat made the US export ban of cutting-edge AI chips to China in 2022\nfeasible. This example illustrates that these chips can be governed. By\ncontrast, data can be purchased from anywhere or found online, and algorithmic\nadvances are not excludable either, especially given the open science and\ncollaborative norms in the AI community.\n\nFrequent chip replacements means governance is effective quickly. The price\nperformance of AI chips is increasing exponentially. With new chips frequently\nmaking recent products obsolete, compute becomes more excludable. Historical\ntrends show that GPUs double their price performance approximately every 2.5\nyears [8]. In conjunction with the rapidly increasing demand for more compute,\ndata centers frequently refresh their chips and purchase vast quantities of\nnew compute regularly to retain competitiveness. This frequent chip turnover\noffers a significant window for governance since regulations on new chips will\nbe relevant quickly.\n\n### Compute Is Quantifiable\n\nThe third key characteristic that makes compute governable is its\nquantifiability. Quantifiability refers to the ability to measure and compare\nboth the quantity and quality of resources. Metrics such as FLOP/s serve as a\nyardstick for comparing computational capabilities across different entities.\nIf a developer has more chips of the same type, we can accurately deduce that\nthey have access to more compute, which means we can use compute to set\nthresholds and monitor compliance.\n\nQuantifiability facilitates clear threshold setting. While chips and other\nforms of compute differ in many ways, they can all be quantified in FLOP/s.\nThis allows regulators to determine how important it is to regulate a model\nthat is being developed: models that use large amounts of compute are likely\nmore important to regulate. Suppose a regulator aims to regulate new models\nthat are large and highly capable. A simple way to do this is to set a FLOP/s\nthreshold, above which more regulations, permissions, and scrutiny take\neffect. By contrast, setting a threshold on dataset size is less meaningful:\nquality of data varies enough that 25 GB of data could contain all the text in\nWikipedia or one high-definition photo album. Even worse, algorithms are\ndifficult to quantify at all.\n\nQuantifiability is key to monitoring compliance. Beyond the creation of\nthresholds, quantifiability also helps us monitor compliance. Given the\nphysical nature and finite capacity of compute, we can tell whether an actor\nhas sufficient computational power from the type and quantity of chips they\npossess. A regulator might require organizations with at least 1000 chips at\nleast as good as A100s to submit themselves for additional auditing processes.\nA higher number of chips directly correlates to more substantial computational\ncapabilities, unlike with algorithms where there is no comparable metric and\ndata for which metrics are much less precise. In addition to compute being\nphysical and so traceable, this enables the enforcement of rules and\nthresholds.\n\n### Conclusions About Compute Governance\n\nCompute governance is a promising route to AI governance. Compute is necessary\nfor the development and deployment of AIs, as well as being well-suited to\ngovernance. Relative to governing algorithms and datasets, the other factors\nused to produce AIs, governing compute is promising because it is physical,\nexcludable, and quantifiable.\n\nThese features are useful for national governments, as they make it more\nfeasible to monitor the development and deployment of potentially dangerous AI\nsystems. This is a precondition for effective enforcement of AI regulations.\nSimilarly, compute\u2019s features can support verification of compliance with\nnational or international standards and agreements. This makes it easier to\nimplement regulatory regimes, particularly at an international level.\n\nCompute governance is currently made simpler by certain factors such as the\ncontrollability of the supply chain or requirement of large-scale compute for\nhighly capable models. If the supply chain of hardware for training and\nrunning AI systems became much less concentrated, this would make it harder to\nenforce potential restrictions on access to relevant hardware as a governance\nmechanism. Similarly, if the required compute resources to train dangerous AI\nsystems were within the means of a small business or even an individual, this\nwould dramatically increase the challenge of monitoring relevant compute\nresources as part of AI governance.\n\nWhile describing compute governance, we have touched upon various ideas about\nhow to use compute to ensure companies produce safe AIs, governments create\nand enforce regulations, and international cooperation can control the compute\nsupply chain. In the rest of this chapter, we further discuss governance tools\naimed at corporate, national, and international governance.\n\n### References\n\n[1] R. Sutton, \u201cThe bitter lesson.\u201d [Online]. Available:\nhttp://www.incompleteideas.net/IncIdeas/BitterLesson.html\n\n[2] D. Amodei, D. Hernandez, G. Sastry, J. Clark, G. Brockman, and I.\nSutskever, \u201cAI and compute. OpenAI.\u201d 2018.\n\n[3] F. L. Urbina, F. Lentzos, C. Invernizzi, and S. Ekins, \u201cDual use of\nartificial-intelligence-powered drug discovery,\u201d Nature Machine Intelligence,\n2022.\n\n[4] Epoch, \u201cKey trends and figures in machine learning.\u201d 2023. [Online].\nAvailable: https://epochai.org/trends\n\n[5] \u201cThe $150m Machine Keeping Moore's Law Alive.\u201d 2021. Available: Wired\n\n[6] Z. Arnold, J. VerWey, J. Melot, and N. Signh, \u201cETO supply chain explorer.\u201d\n[Online]. Available: https://cset.georgetown.edu/publication/eto-supply-chain-\nexplorer/\n\n[7] S. M. Khan, A. Mann, and D. Peterson, \u201cThe semiconductor supply chain:\nAssessing national competitiveness,\u201d Center for Security and Emerging\nTechnology, vol. 8, no. 8, 2021.\n\n## Review Questions\n\nWhat makes compute an important resource to govern for AI development?\n\n##### Answer:\n\nCompute is indispensable for developing and deploying AIs. Restricting access\nto compute allows control over what AIs are created and used.\n\nView Answer\n\nWhy is compute more governable than other resources needed for AI like data\nand algorithms?\n\n##### Answer:\n\nCompute is physical, excludable, and quantifiable which allows it to be\ntracked, restricted, and measured. Data and algorithms lack these\ncharacteristics to the same degree.\n\nView Answer\n\nHow does the quantifiability of compute help with governance?\n\n##### Answer:\n\nMetrics like FLOPs allow clear thresholds to be set for regulation.\nQuantifiability also enables monitoring compliance by correlating compute\namounts with capabilities.\n\nView Answer\n\n0.1\n\n1.1\n\n1.2\n\n1.3\n\n1.4\n\n1.5\n\n1.6\n\n2.1\n\n2.2\n\n2.3\n\n2.4\n\n2.5\n\n3.1\n\n3.2\n\n3.3\n\n3.4\n\n3.5\n\n3.6\n\n3.7\n\n4.1\n\n4.2\n\n4.3\n\n4.4\n\n4.5\n\n4.6\n\n4.7\n\n4.8\n\n5.1\n\n5.2\n\n5.3\n\n5.4\n\n6.1\n\n6.2\n\n6.3\n\n6.4\n\n6.5\n\n6.6\n\n6.7\n\n6.8\n\n6.9\n\n7.1\n\n7.2\n\n7.3\n\n7.4\n\n7.5\n\n7.6\n\n8.1\n\n8.2\n\n8.4\n\n8.6\n\n8.7\n\n8.8\n\n9.1\n\n10.1\n\n11.1\n\n12.1\n\n13.1\n\n14.1\n\n15.1\n\n16.1\n\n8.3\n\n8.5\n\nGet Involved\n\nCourseFacilitateTake ActionSubmit Feedback\n\nTextbook Resources\n\nTable of ContentsDownload TextbookContent for Curriculum\n\nTerms of ServicePrivacy Policy\n\n\u00a9 2024 Center for AI Safety\n\nBuilt by Osborn Design Works\n\n", "frontpage": false}
