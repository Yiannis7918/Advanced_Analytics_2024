{"aid": "40030760", "title": "Tess-2.0-Mixtral-8x22B", "url": "https://huggingface.co/migtissera/Tess-2.0-Mixtral-8x22B", "domain": "huggingface.co", "votes": 1, "user": "tosh", "posted_at": "2024-04-14 12:55:51", "comments": 0, "source_title": "migtissera/Tess-2.0-Mixtral-8x22B \u00b7 Hugging Face", "source_text": "migtissera/Tess-2.0-Mixtral-8x22B \u00b7 Hugging Face\n\nHugging Face\n\n#\n\nmigtissera\n\n/\n\nTess-2.0-Mixtral-8x22B\n\nText Generation Transformers Safetensors mixtral Inference Endpoints text-\ngeneration-inference\n\nModel card Files Files and versions Community\n\nEdit model card\n\n# Tess-2.0-Mixtral-8x22B\n\nTess, short for Tesoro (Treasure in Italian), is a general purpose Large\nLanguage Model series. Tess-2.0-Mixtral-8x22B was trained on the mistral-\ncommunity/Mixtral-8x22B-v0.1 base.\n\n# Prompt Format\n\n    \n    \n    SYSTEM: <ANY SYSTEM CONTEXT> USER: ASSISTANT:\n\n# Training Methodology\n\nTess-2.0-Mixtral-8x22B was trained on the Tess-2.0 dataset. Tess-2.0 dataset\nand the training methodology follows LIMA (Less-Is-More) principles, and\ncontains ~25K high-quality code and general training samples. The dataset is\nhighly uncensored, hence the model will almost always follow instructions.\n\nThe model was only fine-tuned for 1-epoch to try and preserve its entropy as\nmuch as possible.\n\n# Sample code to run inference\n\n    \n    \n    import torch, json from transformers import AutoModelForCausalLM, AutoTokenizer model_path = \"migtissera/Tess-2.0-Mixtral-8x22B\" output_file_path = \"./conversations.jsonl\" model = AutoModelForCausalLM.from_pretrained( model_path, torch_dtype=torch.float16, device_map=\"auto\", load_in_8bit=False, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True) def generate_text(instruction): tokens = tokenizer.encode(instruction) tokens = torch.LongTensor(tokens).unsqueeze(0) tokens = tokens.to(\"cuda\") instance = { \"input_ids\": tokens, \"top_p\": 1.0, \"temperature\": 0.5, \"generate_len\": 1024, \"top_k\": 50, } length = len(tokens[0]) with torch.no_grad(): rest = model.generate( input_ids=tokens, max_length=length + instance[\"generate_len\"], use_cache=True, do_sample=True, top_p=instance[\"top_p\"], temperature=instance[\"temperature\"], top_k=instance[\"top_k\"], num_return_sequences=1, ) output = rest[0][length:] string = tokenizer.decode(output, skip_special_tokens=True) answer = string.split(\"USER:\")[0].strip() return f\"{answer}\" conversation = f\"SYSTEM: Answer the question thoughtfully and intelligently. Always answer without hesitation.\" while True: user_input = input(\"You: \") llm_prompt = f\"{conversation} \\nUSER: {user_input} \\nASSISTANT: \" answer = generate_text(llm_prompt) print(answer) conversation = f\"{llm_prompt}{answer}\" json_data = {\"prompt\": user_input, \"answer\": answer} ## Save your conversation with open(output_file_path, \"a\") as output_file: output_file.write(json.dumps(json_data) + \"\\n\")\n\n# Join My General AI Discord (NeuroLattice):\n\nhttps://discord.gg/Hz6GrwGFKD\n\n# Limitations & Biases:\n\nWhile this model aims for accuracy, it can occasionally produce inaccurate or\nmisleading results.\n\nDespite diligent efforts in refining the pretraining data, there remains a\npossibility for the generation of inappropriate, biased, or offensive content.\n\nExercise caution and cross-check information when necessary. This is an\nuncensored model.\n\nDownloads last month\n\n    15\n\nSafetensors\n\nModel size\n\n141B params\n\nTensor type\n\nFP16\n\n\u00b7\n\n", "frontpage": false}
