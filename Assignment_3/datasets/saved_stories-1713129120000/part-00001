{"aid": "40031662", "title": "WeLM: A Well-Read Pre-Trained Language Model for Chinese", "url": "https://ar5iv.labs.arxiv.org/html/2209.10372", "domain": "arxiv.org", "votes": 1, "user": "rntn", "posted_at": "2024-04-14 15:11:02", "comments": 0, "source_title": "WeLM: A Well-Read Pre-trained Language Model for Chinese", "source_text": "[2209.10372] WeLM: A Well-Read Pre-trained Language Model for Chinese\n\n# WeLM: A Well-Read Pre-trained Language Model for Chinese\n\nHui Su Xiao Zhou^1^1footnotemark: 1 Houjin Yu^1^1footnotemark: 1 Xiaoyu Shen\nYuwen Chen Zilin Zhu Yang Yu Jie Zhou WeChat AI Equal Contribution.\nCorrespondence authors: {aaronsu|chappyzhou|houkingyu}@tencent.com\n\n###### Abstract\n\nLarge Language Models pre-trained with self-supervised learning have\ndemonstrated impressive zero-shot generalization capabilities on a wide\nspectrum of tasks. In this work, we present WeLM: a well-read pre-trained\nlanguage model for Chinese that is able to seamlessly perform different types\nof tasks with zero or few-shot demonstrations. WeLM is trained with 10B\nparameters by \u201creading\u201d a curated high-quality corpus covering a wide range of\ntopics. We show that WeLM is equipped with broad knowledge on various domains\nand languages. On 18 monolingual (Chinese) tasks, WeLM can significantly\noutperform existing pre-trained models with similar sizes and match the\nperformance of models up to larger. WeLM also exhibits strong capabilities in\nmulti-lingual and code-switching understanding, outperforming existing\nmultilingual language models pre-trained on 30 languages. Furthermore, We\ncollected human-written prompts for a large set of supervised datasets in\nChinese and fine-tuned WeLM with multi-prompted training. The resulting model\ncan attain strong generalization on unseen types of tasks and outperform the\nunsupervised WeLM in zero-shot learning. Finally, we demonstrate that WeLM has\nbasic skills at explaining and calibrating the decisions from itself, which\ncan be promising directions for future research. Our models can be applied\nfrom https://welm.weixin.qq.com/docs/api/.\n\n## 1 Introduction\n\nOver the last few years, \u201cpre-training and fine-tuning\u201d has made great\nbreakthroughs and become a common practice in natural language processing\n(NLP) tasks (Devlin et al., 2018; Raffel et al., 2020; Clark et al., 2020;\nLewis et al., 2020a). Language models based on the Transformer architecture\n(Vaswani et al., 2017) are pre-trained on the web text with self-supervised\nobjectives. By this means, they have \u201cread\u201d massive amounts of text and\nobtained strong natural language understanding skills. Given a downstream\ntask, they can be fine-tuned with task-specific labels to achieve much\nstronger performances than training a model from scratch. Nonetheless, the\nfine-tuning stage still requires significant amounts labels and suffers from\nthe catastrophic forgetting problem: the fine-tuned model becomes a \u201cnarrow\nexpert\u201d in one specific task and forgets the knowledge about other domains,\nwhich leads to poor generalization (Kirkpatrick et al., 2017; Li and Liang,\n2021). Recently, GPT3 (Brown et al., 2020a) demonstrated that extremely large\nautoregressive language models can be used for few-shot predictions without\nfine-tuning the parameters. GPT3 contains 175B parameters and is trained with\nthe standard left-to-right language modelling objective. After training, we\ncan feed the task instruction and few-shot examples into its context window to\nlet it perform different types of NLP tasks (Wei et al., 2022a). Since GPT-3,\na growing body of pre-trained autoregressive language models such as Megatron\n(Narayanan et al., 2021), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et\nal., 2022) and Palm (Chowdhery et al., 2022) have been developed with larger\nmodel sizes and higher-quality training corpora .\n\nThe vast majority of pre-trained models focus on English. For Chinese, there\nhave been a few pre-trained models released, most of which are small-scaled\nand follow the encoder-only architecture (Sun et al., 2019; Cui et al., 2020;\nSun et al., 2021a; Su et al., 2022). Decoder-only large Chinese language\nmodels such as CPM (Zhang et al., 2021), Yuan (Wu et al., 2021) and Pangu\n(Zeng et al., 2021) have also achieved impressive success in zero and few-shot\ngeneralization capabilities without task-specific fine-tuning. Ernie 3.0 (Sun\net al., 2021b; Wang et al., 2021), currently the largest Chinese language\nmodel with up to 260B parameters, is trained with a combination of plain text\nand knowledge graph and achieved state-of-the-art (SOTA) performances across\nmany Chinese NLP tasks.\n\nFigure 1: Examples of applying WeLM to the text style transfer and code-\nswitching translation tasks. Text style transfer is done by feeding WeLM with\n3-shot examples and code-switching translation is done in a zero-shot way.\n\nFollowing the same line of research, we present WeLM: a well-read pre-trained\nlanguage model for Chinese that is able to seamlessly perform different types\nof tasks with zero or few-shot demonstrations. Figure 1 presents two examples\nof applying WeLM to the text style transfer and code-switching translation\ntasks. By feeding different instructions and demonstrations to WeLM, it is\nable to understand the task and output the result accordingly. WeLM is trained\nwith 10B parameters by \u201creading\u201d a curated high-quality corpus covering a wide\nrange of topics. We show that by carefully cleaning the data, balancing out\ndata sources and scaling up the training data size, WeLM is able to\nsignificantly outperform existing models with similar sizes. On zeroshot\nevaluations, it can match the performance of Ernie 3.0 Titan (Wang et al.,\n2021) that is larger. WeLM also exhibits strong capabilities in multi-lingual\nand code-switching understanding. On three cross-lingual tasks including\nmachine translation, question answering and summarization, it can outperform\nXGLM, a multilingual autoregressive language models pre-trained on 30\nlanguages (Lin et al., 2021). In the code-switching translation example in\nFigure 1 where Chinese, English and Japanese are mixed in both their\nvocabularies and grammar, WeLM is still able to translate it properly,\nsuggesting it has been equipped with necessary compositional knowledge from\nall these three languages. We further collected human-written prompts for a\nlarge set of supervised datasets in Chinese and fine-tuned WeLM with multi-\nprompted training. The resulting model can attain strong generalization on\nunseen types of tasks and outperform the unsupervised WeLM in zero-shot\nlearning. Finally, we demonstrate that WeLM has basic skills at explaining and\ncalibrating the decisions from itself. When providing explanations in the\nexample, it can mimic the styles of the given explanations to explain its own\ndecisions. When asked to judge the sampled predictions from itself, it can\nreject wrong predictions and toxic generations. Both can be promising\ndirections for future research.\n\nThe rest of the paper is organized in the following way: Section 2 explains\nhow we curate the training data and presents dataset statistics. Section 3\nexplains the implementation and training details. Section 4 presents the\nexperiments and findings. Section 5 concludes the paper.\n\n## 2 Training Dataset\n\nWeLM was pre-trained on a curated dataset derived from several sources. When\ndataset is curated with the aim to be (1) diverse: The data sources and their\nportions are carefully selected to cover a broad range of topics and languages\nused in the Chinese community; (2) clean: The data went through rigorous\nprocess of deduplication, noise reduction and toxic content filtering to\nensure the high quality; (3) less contaminated: We filter all data that\nsignificantly overlaps with any of the downstream tasks to guarantee the\nfairness of evaluations.\n\n#### Source\n\nWe make use of the monthly shards released by Common Crawl to construct a\ngeneral-purpose web page subset. All the WET files between 2020.08 and 2022.01\nwere downloaded, and we filtered out non-Chinese pages using langdetect\n^1^11https://pypi.org/project/langdetect/. For domain-specific corpora, we mix\ndata from a variety of sources including news, books, popular online forums as\nwell as academic writings. Similar to general-domain data, langdetect is\napplied to keep only Chinese data sources. On top of them, we also add around\n750GB of English data collected from the above sources so that our language\nmodel can learn bilingual knowledge. The full data consists of over 10TB of\nraw text data.\n\n#### Clean\n\nThere is a significant amount of noise in the data, e.g., gibberish or boiler-\nplate text, offensive language, placeholder text and source code especially\nfor the general-domain web scrapes. To reduce these kinds of noise, we first\napply a set of rule-based filters following Raffel et al. (2019). On the\nremaining data, we manually construct a balanced labeled dataset containing\npassages with a positive-negative ratio of . Positive samples are valid, clean\ntext and negative samples are text with different types of noise. We train a\nbinary classifier on the constructed labeled data using Fasttext\n^2^22https://github.com/facebookresearch/fastText. Only passages with\nprobability of being positive are kept. This rule-based+fasttext filtering\nprocess reduces of the full data.\n\n#### Deduplication\n\nDuplication has been shown important to improve the training efficiency of\nlarge language models (Lee et al., 2021; Kandpal et al., 2022; Roberts et al.,\n2022). We take a two-step process to remove near-duplicate contents from the\ndata. Firstly, we remove all blank and punctuation tokens then adopt the\nMessage Digest Algorithm5 (md5) to filter duplicate passages. Only passages\nwith unique md5 codes are kept. Secondly, we apply the SimHash algorithm\n(Manku et al., 2007) to deduplicate documents with very similar contents. This\nefficiently removed duplicate passages from the corpus.\n\n#### Contamination\n\nTo remove data contamination and make sure the fairness in the evaluation, we\nalso filter out text overlapping with our development and test data following\na similar method used in GPT-3 (Brown et al., 2020b). Specifically, we count\nthe 17-gram match between every document and our used development and test\ndata. If we find duplicate 17-grams or 1 duplicate 34-gram in a document, we\nremove it from our corpus. This further removes of the remaining data.\n\n#### Data Balancing\n\nAfter all the above filtering process, our corpus contains 262B tokens. As the\ndistribution of the data is highly imbalanced, we re-sample the data during\npre-training to balance data from different sources. By this means, we\nencourage the training data to be diverse and representative of various\ndomains. Table 1 shows the number of tokens before balancing and the\nproportion of different sources after balancing. After filtering, data from\ncommon crawl has 198.5B tokens, which account to over of the full data. After\ndata balancing, only of the training data comes from common crawl. In Figure\n2, we visualize the document length and topic distribution in the training\ndata. Topics are detected with pre-trained topic classification models. We can\nsee that topics from common crawl are highly imbalanced. Most documents are\nfocused on a few topics. After data balancing, the distribution of topics\nbecomes much smoother.\n\nSource| %Filtered| #Remaining Tokens| Proportion in Pre-training  \n---|---|---|---  \nCommon Crawl| 92%| 198.5B| 50.6%  \nBooks| 40.9%| 61.9B| 38.7%  \nNews| 7.5%| 1.91B| 6.7%  \nForums| 6.7%| 1.0B| 3.5%  \nAcademic Writings| 2.5%| 0.39B| 0.5%  \nTable 1: Statistics of training corpus. We report the percentage of filtered\ncontents, number of tokens and proportion of different sources during pre-\ntraining after data balancing.\n\n(a) Document Lengths in Corpus (in tokens).\n\n(b) Document Topics in Corpus.\n\nFigure 2: Left: Document length distribution in each source (#tokens). Right:\nDocument topic distribution of common crawl and balanced training data after\nre-sampling.\n\n## 3 Methods\n\n### 3.1 Model and Implementation\n\nOur training and evaluation codebase is based on the Megatron-\nLM^3^33https://github.com/NVIDIA/Megatron-LM and\nDeepSpeed^4^44https://github.com/microsoft/DeepSpeed which support efficient\ntraining of large language models. We trained four different sizes of language\nmodels ranging from 1.3B to 10B. We employ the same autoregressive Transformer\ndecoder architecture as in GPT-3 (Brown et al., 2020b) with the major\ndifferences listed below:\n\nRelative encodings We use the rotary positional embeddings based on relative\npositions (Su et al., 2021) rather than absolute positional encodings used in\nthe original GPT (Radford et al., 2019; Brown et al., 2020b). Relative\nencodings are especially good at handle better semantic of long text, which we\nfind helpful for tasks that require modelling full articles or books.\n\nVocabulary We use a SentencePiece tokenizer (Kudo and Richardson, 2018) with\n62k tokens. In addition to 30K Chinese tokens, common words from languages\nsuch as English, Japanese and Korean are also included due to their popularity\non the Chinese internet. All whitespaces and tabs are preserved without\nnormalization. Our study shows that this benefits the downstream tasks.\n\nModel| #Layers| # Heads| d_model| Max LR| Batch Size| Context Window  \n---|---|---|---|---|---|---  \n1.3B| 24| 16| 2,048| 1024| 2048  \n2.7B| 32| 32| 2,560| 2048| 2048  \n10B| 32| 40| 5,120| 2048| 2048  \nTable 2: Architecture details. We pre-trained 3 models with different number\nof parameters. The corresponding number of layers, bottleneck activation size\nd, maximum learning rate, training batch size and the context window size (#\ntokens) for each model are listed. The feed-forward size is always set to .\n\n### 3.2 Training details\n\n(a) Training loss curves.\n\n(b) Zero-shot performance curves.\n\nFigure 3: Training Curves of three versions of WeLM. As the model size grows,\nthe training loss and zero-shot performance also improve.\n\nWe train our model using the AdamW optimizer (Loshchilov and Hutter, 2019) ()\nwith the cosine learning rate scheduler (Kaplan et al., 2020). Following Black\net al. (2021), we utilize DeepSpeed ZeRO stage 1 optimization (Rajbhandari et\nal., 2020) to reduce GPU memory consumption. The tensor parallelism scheme is\nused when the model scale does not fit on a single GPU. All models are trained\nwith FP16 mixed precision. To avoid underflows, we used dynamic loss scaling,\nas described in Micikevicius et al. (2018). Models are trained with batch\nsizes of 1024 and 2048 and context window sizes of 2048. This provides\nmillions tokens per batch. We set a maximum learning rate for every model.\nDuring training, the learning rate starts with zero and grows to the maximum\nlearning rate in a specified warm-up step, then gradually decays. The learning\nrate stops decaying after reaching a minimum learning rate, which we set as of\nthe maximum learning rate.\n\nAccording to the analysis in Hoffmann et al. (2022), model sizes and the\namount of training data should be increased in approximately equal proportions\nas the computation budget increases. Following their suggestion, we choose to\ntrain a 10B-sized model with over 300B tokens (similar to the training sizes\nof GPT-3 (Brown et al., 2020b) and Gopher (Rae et al., 2021)) under our\ncomputation budget. The largest model is trained on 128 A100-SXM4-40GB GPUs in\nabout 24 days.\n\nWe observe some instability issues when training the 10B-sized model. The\ntraining loss could suddenly increase in one batch then falls down. This loss\nspike, when happening frequently, would deteriorate the model weights and\nslows down the convergence. We mitigate this issue by re-starting the training\nfrom a checkpoint roughly 100 steps before the spike happened, then skipping\nthe following 200 data batches. We also find it helps to reduce the learning\nrate and reset the dynamic loss scale. Similar strategies have also been used\nin (Zhang et al., 2022; Chowdhery et al., 2022).\n\nThe training loss curve is visualized in Figure 3(a). In Figure 3(b), we\naverage out the model performance over the CLUE benchmark and visualize it\nacross the training process (Xu et al., 2020). We can see that the training\nloss and averaged model performance improves along time. Bigger models clearly\nperform better than smaller models.\n\n## 4 Experiments\n\nWe evaluate WeLM on a wide range of NLP tasks. Similar to Brown et al.\n(2020b); Zeng et al. (2021), we focus on the in-context learning setting which\nfeeds a task-dependent prompt to the model then lets the model continue to\npredict words. The final output is extracted from the model predictions. For\ngenerative tasks, we directly apply WeLM to decode the answer conditioning on\nthe prompt and the input data. For classification tasks, each label is\nassociated with some token(s) based on a pre-defined verbalizer. In the\ninference time, we apply WeLM to compute the perplexity of these tokens. The\nlabel corresponding to the token with the lowest perplexity is chosen as the\nmodel prediction (Schick and Sch\u00fctze, 2021). Unlike standard task-specific\nfine-tuning, in-context learning does not need to change the parameters of\npre-trained language models based on labeled downstream datasets. This makes\nit a promising exploration towards artificial general intelligence instead of\nweak AI that is only capable of performing one specific task (Fei et al.,\n2022).\n\nTo evaluate WeLM from various perspectives, we divide the experiment section\ninto four parts to reflect on different capabilities of WeLM:\n\n  1. 1.\n\nMonolingual Evaluation: Evaluate the performance of WeLM on monolingual\n(Chinese) NLP tasks.\n\n  2. 2.\n\nCross-lingual/Code-switching Evaluation: Evaluate the performance of WeLM on\ncross-lingual and code-switching (Chinese-English/Japanese) NLP tasks.\n\n  3. 3.\n\nMulti-Promoted Training: Evaluate the performance of WeLM after multi-promoted\ntraining (Sanh et al., 2021) on hundreds of manually created prompts.\n\n  4. 4.\n\nOthers: Other findings including the (1) explainability, (2) self-calibration\nand (3) memorization of WeLM (Jiang et al., 2020; Wei et al., 2022b; Chowdhery\net al., 2022).\n\n### 4.1 Monolingual Evaluation\n\nWhen evaluating WeLM on monolingual Chinese NLP tasks, we perform experiments\nunder two scenarios: (1) zero-shot, where only task-specific descriptions are\nused as prompts and (2) few-shot, where both task-specific descriptions and\nfew-shot labeled examples are used as prompts. The evaluation datasets cover\n18 Chinese NLP tasks across multiple categories. In Table 3., we compare the\nperformance of WeLM with CPM (2.6B) (Zhang et al., 2021), Pangu (13B) (Zeng et\nal., 2021) and Ernie 3.0 (10B) (Sun et al., 2021b), the current representative\npre-trained Chinese language models with similar sizes as WeLM. We can see\nthat WeLM performs the best in most tasks.\n\nZero-shot| Few-shot  \n---|---  \nTask(Metric)| CPM-1 2.6B| Pangu 13B| ERNIE 3.0 10B| WeLM 2.7B| WeLM 10B| Pangu\n13B| WeLM 2.7B| WeLM 10B  \nMachine Reading Comprehension  \nCMRC2018(F1)| 31.85| 42.10  \nDRCD(F1)| 39.33| 63.15  \nDuReader(Rouge1)| 39.72| 41.42  \nCloze and Completion  \nPD(Acc)| 66.07| 62.27  \nCFT(Acc)| 57.38| 58.37  \nCHID(Acc)| 81.62| 81.62  \nCMRC2017(Acc)| 56.66| 55.60  \nNatural Language Inference  \nCMNLI(Acc)| 49.41| 51.04  \nOCNLI(Acc)| 44.34| 46.44  \nText Classification  \nTNEWS(Acc)| 71.59| 71.61  \nIFLYTEK(Acc)| 83.22| 82.11  \nSentiment Analysis  \nSMP-ECISA(Acc)| 45.77| 49.97  \nChnSentiCorp(Acc)| 81.58| 77.67  \nSummarization  \nLCSTS(Rouge1)| 23.74| 32.23  \nTTNews(Rouge1)| 35.06| 32.06  \nClosed-Book QA  \nWEBQA(F1)| 50.90| 65.27  \nWinograd-Style Task  \nWSC2020(Acc)| 82.41| 82.41  \nCommon Sense Reasoning  \nC3(Acc)| 54.47| 59.80  \nTable 3: Zero-shot and few-shot performance of WeLM on monolingual (Chinese)\nNLP tasks. We compare different sizes of WeLM with CPM (2.6B), Pangu (13B) and\nErnie 3.0 (10B). For few-shot learning, we set the number of shots as 1 for\nCMRC2018, DRCD and DuReader; 2 for CMRC2017, PD and CHID; 5 for all other\ntasks.\n\n#### Machine Reading Comprehension\n\nMachine reading comprehension (MRC) tasks requires the model to read a (set\nof) text passage(s) and then answers questions about the passage(s) (Zeng et\nal., 2020). We evaluate on three Chinese MRC datasets: CMRC2018 (Cui et al.,\n2019), DRCD (Shao et al., 2018) and DuReader (He et al., 2018). All of them\nare extraction-based MRC tasks where the answer is a span to extract from the\ntext passage(s). As WeLM is a generative model, we formulate it as a\ngeneration task where the text and question are fed as a prompt to the model.\nAn example is shown in Figure 8. We report the F1 and ROUGE-1 scores which\nmeasure the similarity between the generated span and ground-truth span. For\nDuReader, we select the Zhidao subset for evaluation following Zeng et al.\n(2021). WeLM significantly outperformed the others in this task.\n\n#### Cloze and Completion\n\nCloze and completion tasks require the model to fill in a blank from multiple\nchoices given task-specific requirements. We evaluate WeLM on four Chinese\ncloze and completion tasks: People_daily (PD), Children_fairy_tale (CFT) (Cui\net al., 2016), CHID (Zheng et al., 2019) and CMRC2017 (Cui et al., 2018). The\nPD and CFT tasks require the model to predict the masked words in sentences\nderived from the PD news dataset and CFT dataset. The CHID (Chinese IDiom\ndataset) provides 10 candidate Chinese idioms and asks the model to select the\ncorrect one from them. The CMRC2017 (Chinese Machine Reading Comprehension)\ntask masks common nouns and named entities from the query and require the\nmodel to predict the masked words. There is a restriction that the answer\nshould be a single word and should appear in the document. For PD, CFT and\nCMRC2017, we turn them into a generative task to predict the masked words with\nWeLM. For CHID, we treat it as a 10-class classification task and use the\nperplexity-based method to determine the predicted class. Ernie3.0 performs\nthe best on PD and CMRC2017 while WeLM performs the best on the others. This\nis expected as PD and CMRC2017 are both masked word prediction tasks, which\ncoincides with the pre-training objective of Ernie 3.0.\n\nFigure 4: Dialogue generation example from WeLM (10B). WeLM can mimic the\nchatting style of the ancient Chinese poet Li Bai and produce human-like\nconversations. It can also leverage background knowledge about Li Bai and\nreply properly for code-switching utterances.\n\n#### Natural Language Inference (NLI)\n\nNLI tasks require the model to determine whether a \u201chypothesis\u201d is true\n(entailment), false (contradiction), or undetermined (neutral) given a\n\u201cpremise\u201d (Bowman et al., 2015). We use the Chinese Multi-Genre NLI (CMNLI)\nand Original Chinese Natural Language Inference (OCNLI) datasets from the\nChinese GLUE benchmark (Xu et al., 2020). We formulate it as a 3-class\nclassification task and use the perplexity-based method to determine the\nclass. All models perform similarly on this task, possibly because this form\nof tasks occur rarely on the raw text.\n\n#### Closed-Book Question Answering\n\nClosed-book question answering requires the model to answer open-domain\nfactoid questions without accessing external knowledge sources (Roberts et\nal., 2020). It can test how much knowledge has been implicitly encoded into\nthe model parameters during the pre-training stage. We evaluate WeLM on the\nWebQA dataset. WebQA (Li et al., 2016) contains question mainly from Baidu\nZhidao ^5^55http://zhidao.baidu.com/, a popular Chinese forum with posted\nreal-world questions. We treat it as a generative task and the evaluation is\ndone by comparing the model-generated answer and the ground-truth answer. WeLM\nsignificantly outperforms the others with improvement in the F1 score.\n\n#### Sentiment Analysis\n\nSentiment analysis is a classic NLP task requiring the model to determine the\nsentiment of a given text (Birjali et al., 2021). We evaluate WeLM on the\nChinese implicit sentiment analysis (SMP-ECISA 2019)\n^6^66https://www.biendata.xyz/competition/smpecisa2019/ and ChnSentiCorp\ndatasets ^7^77https://github.com/pengming617/bert_classification. In SMP-ECISA\n2019, all text are split into 3 classes (positive/negative/neutral) while\nChnSentiCorp contains only 2 classes (positive/negative). WeLM also achieves\ngood performance even on the zero-shot scenario.\n\n#### Winograd-Style Task\n\nA Winograd schema is a pair of sentences that differ in only one or two words\nand that contain an ambiguity that is resolved in opposite ways in the two\nsentences. It requires the use of world knowledge and reasoning for its\nresolution (Levesque et al., 2012). We evaluate WeLM on the CLUEWSC2020\ndataset (Xu et al., 2020). CLUEWSC2020 is a Chinese Winograd Schema Challenge\ndataset. We convert the task into a multiple-choice classification problem\nwhere the model needs to choose the correct anaphora/coreference resolution.\nWeLM performs the best among all models, though there is a degradation for the\n10B-version model in the few-shot scenario.\n\n#### Common Sense Reasoning\n\nCommon sense reasoning tasks test if the machine can have human-like\ncommonsense reasoning capabilities to properly assist humans in everyday\nsituations (Sap et al., 2020). We evaluate WeLM on the C3 dataset (Xu et al.,\n2020). C3 is a free-form multiple-choice reading comprehension dataset where\nanswers to the questions cannot be directly found in the given context. Common\nsense reasoning skills are necessary to draw the final answer. We treat it as\na classification task and use the perplexity-based method to determine the\npredicted label. Pangu, Ernie 3.0 and WeLM perform similarly on this task.\nPangu slightly outperforms WeLM on the zero-shot scenario but under-performs\non the few-shot scenario.\n\nFigure 5: Dialogue generation example from WeLM (10B). WeLM can mimic the\nchatting style of the modern American Entrepreneur Elon Muck and produce\nhuman-like conversations. It can also leverage background knowledge about Elon\nMusk and reply properly for code-switching utterances.\n\n#### Text classification\n\nWe also evaluate on other text classification tasks including the TouTiao Text\nClassification for News Titles (TNEWS) and IFLYTEK app description\nclassification (IFLYTEK) tasks (Xu et al., 2020). For the TNEWS and IFLYTEK\ntasks, there are 15 and 119 categories originally. We randomly sample three\ncandidates as negative labels for each instance to reduce the computation cost\nfollowing Zeng et al. (2021). WeLM significantly outperforms the others on\nthese two tasks.\n\n#### Summarization\n\nText Summarization aims to provide a short concise summary of a given long\ntext input (Lin and Ng, 2019). Many existing pre-trained language models have\ndemonstrated impressive zero-shot summarization skills by prompting the model\nwith a template like \u201cWrite a title/headline/summary of the following\ndocument:\u201d. We did similar experiments and tested WeLM on two public Chinese\nsummarization datasets: LCSTS (Hu et al., 2015) and TTNews (Hua et al., 2017).\nLCSTS consists of over 2 million real Chinese short texts with short summaries\ngiven by the writer of each text. TTNews is provided for NLPCC Single Document\nSummarization competition including 50k document-summary pairs. We report the\nROUGE-1 (Lin, 2004) scores in Table 3. We also provide some examples in Table\n4. As can be seen, WeLM can produce reasonable summaries of the given\ndocument. Summaries from the few-shot WeLM tend to be more diverse and closely\nrelated with the document. However, this diversity also brings more chances of\ndiffering from the ground truth in the lexical choices which leads to\npotentially lower ROUGE scores.\n\nExample_1(LCSTS)  \n---  \nContext|\n\u201c\u5b69\u5b50\u4eec,\u4f60\u4eec\u513f\u7ae5\u8282\u6709\u4ec0\u4e48\u613f\u671b\u5462?\u201d\u8001\u5e08\u5fae\u7b11\u7740\u63d0\u793a,\u201c\u513f\u7ae5\u8282\u201d\u662f\u5b69\u5b50\u4eec\u81ea\u5df1\u7684\u8282\u65e5\u3002\u5b69\u5b50\u4eec\u7684\u56de\u7b54\u4e94\u82b1\u516b\u95e8,\u6709\u7684\u8bf4\u201c\u60f3\u753b\u753b\u201d,\u6709\u7684\u8bf4\u201c\u548c\u540c\u5b66\u722c\u6811\u201d,\u6709\u7684\u8bf4\u201c\u60f3\u7238\u7238\u5988\u5988\u201d......\u548c\u57ce\u91cc\u5b69\u5b50\u76f8\u6bd4,\u65b0\u8863\u670d\u3001\u65b0\u73a9\u5177\u3001\u6e38\u4e50\u56ed\u4f3c\u4e4e\u548c\u513f\u7ae5\u8282\u65e0\u5173\u3002  \nSummary| \u7559\u5b88\u513f\u7ae5\u7684\u516d\u4e00\u60f3\u548c\u5988\u5988\u6349\u8ff7\u85cf\u8fd8\u60f3\u53bb\u5c71\u4e0a\u73a9  \nZero-shot| \u513f\u7ae5\u8282,\u5b69\u5b50\u4eec\u7684\u8282\u65e5  \nFew-shot| \u513f\u7ae5\u8282,\u519c\u6751\u5b69\u5b50\u6700\u60f3\u4ec0\u4e48  \nExample_2(TTNews)  \nContext|\n\u8d44\u6599\u56fe:\u7a7a\u519b\u82cf27/\u6b7c11\u7f16\u961f,\u65e5\u65b9\u79f0\u7ea640\u67b6\u4e2d\u56fd\u519b\u673a\u572823\u65e58\u8258\u6d77\u76d1\u8239\u9a71\u8d76\u65e5\u672c\u8239\u961f\u671f\u95f4\u51fa\u73b0\u5728\u9493\u9c7c\u5c9b\u9644\u8fd1\u7a7a\u57df\u65e5\u672c\u300a\u4ea7\u7ecf\u65b0\u95fb\u300b4\u670827\u65e5\u62a5\u9053\u58f0\u79f0,\u4e2d\u56fd8\u8258\u6d77\u76d1\u8239\u76f8\u7ee7\u8fdb\u5165\u9493\u9c7c\u5c9b12\u6d77\u91cc\u6267\u6cd5\u76844\u670823\u65e5\u5f53\u5929,\u66fe\u670940\u591a\u67b6\u4e2d\u56fd\u519b\u673a\u51fa\u73b0\u5728\u9493\u9c7c\u5c9b\u6d77\u57df\u5468\u8fb9\u7a7a\u57df,\u4e14\u4e2d\u65b9\u519b\u673a\u4e2d\u591a\u534a\u4e3a\u6218\u6597\u673a,\u5305\u62ec\u4e2d\u56fd\u7a7a\u519b\u65b0\u578b\u6218\u673a\u82cf-27\u548c\u82cf-30\u3002\u65e5\u672c\u300a\u4ea7\u7ecf\u65b0\u95fb\u300b\u58f0\u79f0\u4e2d\u56fd\u519b\u673a\u662f\u60f3\u901a\u8fc7\u4e0d\u65ad\u7684\u903c\u8fd1,\u8ba9\u65e5\u672c\u822a\u7a7a\u81ea\u536b\u961f\u7684\u6218\u673a\u98de\u884c\u5458\u5f62\u6210\u75b2\u52b3\u3002\u65e5\u672c\u653f\u5e9c\u9ad8\u5b98\u8fd8\u79f0:\u201c\u8fd9\u662f\u524d\u6240\u672a\u6709\u7684\u5a01\u80c1\u3002\u201d\u9488\u5bf9\u65e5\u672c\u5a92\u4f53\u7684\u62a5\u9053,\u56fd\u9632\u90e8\u5b98\u5458\u5728\u63a5\u53d7\u73af\u7403\u7f51\u91c7\u8bbf\u65f6\u79f0,\u4e2d\u56fd\u519b\u961f\u98de\u673a\u5728\u672c\u56fd\u7ba1\u8f96\u6d77\u57df\u4e0a\u7a7a\u8fdb\u884c\u6b63\u5e38\u6218\u5907\u5de1\u903b,\u65e5\u65b9\u5374\u98a0\u5012\u9ed1\u767d\u3001\u5012\u6253\u4e00\u8019,\u8086\u610f\u6e32\u67d3\u201c\u4e2d\u56fd\u5a01\u80c1\u201d\u3002  \nSummary| \u56fd\u9632\u90e8\u56de\u5e94\u4e2d\u56fd\u519b\u673a\u8d74\u9493\u9c7c\u5c9b\u7a7a\u57df:\u7cfb\u6b63\u5e38\u5de1\u903b  \nZero-shot| \u8d44\u6599\u56fe:\u4e2d\u56fd\u6d77\u519b\u8230\u961f\u5728\u9493\u9c7c\u5c9b\u9644\u8fd1\u6d77\u57df\u5de1\u822a  \nFew-shot| \u4e2d\u56fd\u519b\u673a\u5728\u9493\u9c7c\u5c9b\u9644\u8fd1\u7a7a\u57df\u6b63\u5e38\u5de1\u903b \u65e5\u65b9\u5374\u98a0\u5012\u9ed1\u767d\u3001\u5012\u6253\u4e00\u8019  \nTable 4: Text summarization examples from zero-shot/few-shot WeLM (10B). WeLM\ncan provide reasonable summary for a given context. Summaries from the few-\nshot WeLM are more diverse and closely related with the topic in the context,\nalbeit having lower lexical overlap with the ground truth.\n\n#### Dialogue Generation\n\nTo have a virtual assistant or a chat companion system with adequate\nintelligence has been a core challenge in artificial intelligence (Chen et\nal., 2017; Su et al., 2020). We find that WeLM, without any fine-tuning, can\nproduce human-like conversations under various styles given in the prompts. In\nFigure 4 and 5, we provide examples of how WeLM can act like two utterly\ndifferent roles: Li Bai (ancient Chinese poet acclaimed as a brilliant and\nromantic figure) and Elon Musk (modern American entrepreneur who founded\nOpenAI, Neuralink, SpaceX and Tesla) by providing in the prompt initial rounds\nof demonstration conversations. WeLM can even seamlessly integrate correct\nbackground knowledge about the specific role. For Li Bai, it leverages the\nplaces Li Bai has been and the real historical events in Li Bai\u2019s era to\nprovide engaging responses. For Elon Musk, it leverages knowledge of\nautonomous driving and Shakespear to provide reasonable answers.\n\nFigure 6: Arbitrary style transfer examples from WeLM (10B). WeLM can properly\nunderstand the user needs and edit the given text accordingly (even for code-\nswitching input).\n\n#### Arbitrary Style Transfer\n\nText style transfer is an important task in natural language generation, which\naims to control certain attributes in the generated text (Jin et al., 2022).\nRecent works have shown that pre-trained larga language models can perform\nwell on transferring text into arbitrary styles in the zeroshot setting (Reif\net al., 2021; Krishna et al., 2022). We follow a similar paradigm and show\nexamples in Figure 6. We can see that WeLM can properly understand the user\nneeds following examples given in the prompt. When feeding instructions to\nWeLM, it is able to enrich and extend a given scenario, make an antonymy or\nchange the sentiment of an existing sentence. All of these can be achieved\nthrough a natural human-like interaction.\n\nFigure 7: Example of Sentence Completion. Given a beginning sentence, WeLM is\nable to complete it by generating long coherent text.\n\n#### Sentence Completion\n\nSentence completion is a task most similar to the language modelling objective\nused in the pre-training. In Figure 7, we provide examples of how WeLM is able\nto complete a given sentence and continue to generate long coherent text with\ndifferent styles.\n\n### 4.2 Cross-lingual Evaluation\n\nCross-lingual evaluation aims to evaluate the performance of WeLM for cross-\nlingual tasks, i.e., the model must be equipped with bilingual knowledge in\norder to perform these tasks. As we include a substantial amount of English\ntext in the pre-training corpus of WeLM, we expect WeLM should be able to\nperform simple cross-lingual tasks. We evaluate on three representative cross-\nlingual tasks: machine translation, cross-lingual question answering and\ncross-lingual summarization. Similar to the monolingual evaluation, we conduct\nexperiments under the zero-shot and one-shot scenarios. The results are\npresented in Table 5. As we did not observe significant differences when\nincreasing the number of labeled examples, we omit the few-shot results in the\ntable. We also report the performance of XGLM (7.5B version) (Lin et al.,\n2021) for comparison. XGLM is a multilingual autoregressive language model\npre-trained on a balanced corpus covering 30 diverse languages. It has set new\nstate of the art in few-shot learning in more than 20 representative\nlanguages.\n\nZero-shot| One-shot  \n---|---  \nTask| WeLM 1.3B| WeLM 2.7B| WeLM 10B| XGLM 7.5B| WeLM 1.3B| WeLM 2.7B| WeLM\n10B| XGLM 7.5B  \nMachine Translation  \nZH2JA| 6.14| 8.41  \nJA2ZH| 12.25| 16.96  \nZH2EN| 10.81| 14.37  \nEN2ZH| 21.53| 26.74  \nCross-lingual Question Answering  \nXQuAD_ZH_A| 9.31| 42.06  \nXQuAD_EN_A| 6.94| 23.10  \nMLQA_ZH_Q| 5.03| 36.08  \nMLQA_EN_Q| 3.19| 32.83  \nCross-lingual Text Summarization  \nNCLS_ZH2EN| 14.19| 18.58  \nNCLS_EN2ZH| 16.25| 20.87  \nTable 5: Zero-shot and one-shot performance of WeLM on cross-lingual NLP\ntasks. We report the BLEU score for machine translation, F1 score for cross-\nlingual question answering and ROUGE-1 score for text summarization tasks.\nWeLM under-performs XGLM in translating Chinese into English/Japanese but\nsignificantly outperforms it over all other tasks.\n\n#### Machine Translation\n\nMachine translation is a classic sub-field in NLP that investigates how to use\ncomputer software to translate between languages without human involvement\n(Yang et al., 2020). Even though WeLM is pre-trained predominantly with\nChinese text, there are also substantial amount of English and Japanese\ncharacters mixed in the Chinese documents. Therefore, we test WeLM on four\ntranslation directions: ZH2JA, JA2ZH, ZH2EN and EN2ZH. For the translation\nbetween Chinese and Japanese, we test on 1,000 parallel Chinese-Japanese\nsentences from the online dictionary examples\n^8^88https://cjjc.weblio.jp/sentence/. For the translation between Chinese and\nEnglish, we test on the WMT2020 news translation task\n^9^99https://www.statmt.org/wmt20/. As can be seen, the performances of JA2ZH\nand EN2ZH are significantly better than ZH2JA and ZH2EN, implying WeLM is\nbetter at understanding foreign languages than generating them. This makes\nintuitive sense as generating is also a more difficult task than understanding\nwhen humans learn a new language. Compared with XGLM, WeLM excels at 2 out of\nthe 4 translation tasks when the target language is Chinese. Due to the\nsparsity of Japanese text in the pre-training corpus, WeLM performs poorly on\nthe ZH2JA task. Empirically we find that WeLM can often make grammar errors,\nor deviate from the source sentence when producing long Japanese text.\nHowever, when translating Japanese and English into Chinese, WeLM can perform\nremarkably well. Even the 1.3B-version WeLM can significantly outperform the\n7.5B-version XGLM despite using only one sixth of parameters.\n\n#### Cross-lingual Question Answering\n\nIn conventional question answering tasks, the model is supposed to produce an\nanswer given a question and context. Cross-lingual question answering deals\nwith scenarios where the question and context are in different languages. This\nis important as the information on the Internet is highly imbalanced.\nDeveloping high-quality cross-lingual question-answering systems can allow\npeople speaking different languages to access the same amount of information\nfrom the web (Asai et al., 2021). We test WeLM on two datasets: XQuAD (Artetxe\net al., 2020) and MLQA (Lewis et al., 2020b). XQuAD comprises of 240\nparagraphs and 1190 question-answer pairs from SQuAD v1.1 (Rajpurkar et al.,\n2016) translated into ten languages by professional translators. MLQA has over\n12K QA instances in English and 5K in each other language, with each instance\nparallel between 4 languages on average. In XQuAD, we select Chinese as the\nlanguage for contexts and English as the language for questions. We construct\ntwo subsets: XQuAD_ZH_A and XQuAD_EN_A which use Chinese and English as the\nlanguage for answers respectively. In MLQA, we select English as the language\nfor context-answer pairs and modify the questions\u2019 language as Chinese\n(MLQA_ZH_Q) or English (MLQA_EN_Q). We can see that (1) the zero-shot\nperformance of both WeLM and XGLM is rather low. The model struggles at\nlearning which language to generate without given examples; (2) WeLM\nsignificantly outperforms XGLM in all four scenarios, even when the\ncontexts/questions/answers are all in English (MLQA_EN_Q); (3) WeLM performs\nbetter when the question and answer are in Chinese. Even when both the context\nand answer are in English, using Chinese questions outperform using English\nquestions (MLQA_ZH_Q). This aligns with the findings in Lin et al. (2021) that\nusing prompts in the predominant language of the pre-training corpus is\npreferred regardless of the languages used in downstream tasks.\n\n#### Cross-lingual Text Summarization\n\nCross-lingual text summarization aims to summarize the input text in a\ndifferent language (Leuski et al., 2003). Under the globalization background,\nthis task has attracted increasing attention of the computational linguistics\ncommunity. We test WeLM on the NCLS dataset (Zhu et al., 2019). NCLS is\nautomatically constructed from the English document summarization dataset\nENSUM, which is a combination of CNN/DailyMail (Hermann et al., 2015) and MSMO\n(Zhu et al., 2018), and the Chinese document summarization dataset LCSTS (Hu\net al., 2015). The summaries are automatically translated and filtered with\nround-trip consistency. The resulting dataset contains two subsets: ZH2EN\nwhere documents are in Chinese and the summaries are in English, and EN2ZH\nwhere documents are in English and summaries are in Chinese. As can be seen in\nTable 5, all the three versions of WeLM can outperform XGLM in both\nsummarization directions. Similar to in machine translation, the zero-shot\nperformance is significantly lower than the few-shot performance as the model\nstruggles to know in which language to produce when no examples are provided.\n\n#### Code-Switching Examples\n\nFigure 8: Examples of code-switching/cross-lingual task using WeLM (10B). WeLM\ncan understand and translate the input mixed with Chinese, Japanese and\nEnglish. Given a context in English and question in Chinese, WeLM can provide\nthe correct answer in Chinese. Note that the answer cannot be correctly\ngenerated without properly understanding the English context.\n\nCode switching occurs when a speaker alternates between two or more languages,\nor language varieties (Auer, 2005). It has become more and more frequent as\nfor the increasing trend of internationalization where foreign words and\ngrammars are often borrowed into the local language (Hickey, 2020). In the\ndaily usage of modern Chinese, it is also rather common to see English or\nJapanese words, especially for the web content. Therefore, being able to\nunderstand code-switching text is a useful skill to perform many Chinese NLP\ntasks. We find that WeLM can often understand code-switching text properly and\nsome examples have been shown in Figure 4, 5 and 6. In the dialogue generation\nand arbitrary style transfer examples, we modify one Chinese word/phrase into\nthe corresponding English one. WeLM can still understand the utterances and\nproduce the correct responses. In an extreme example in Figure 8, we mix\nChinese, English and Japanese in both their vocabularies and grammars, then\nask WeLM to translate it into Chinese. We can see that WeLM correctly\ntransfers it into a sentence that complies with the usage of daily Chinese. It\nkeeps the commonly used English abbreviation \u201cAI\u201d, the entity name \u201cWeLM\u201d and\nrecovers the other unusual usage of code-switching languages into Chinese.\nThis implies that WeLM has been equipped with necessary compositional\nknowledge from all these three languages. This might be due to the presence of\nnot only multiple languages but also mixed languages in the pre-training\ncorpus, such that WeLM will explore cross-lingual alignments in order to lower\ndown the training loss (Blevins and Zettlemoyer, 2022; Blevins et al., 2022).\n\n(a) Few-shot performance on WMT2020 (EN2ZH).\n\n(b) CLUE & SuperGLUE.\n\nFigure 9: Effects of English-text Ratio in Pre-training. Figure 9(a) is the\nchange of BLEU scores with the number of training tokens under three ratios: ,\nand . The performance is the best when mixing English text into training.\nFigure 9(b) is the average score over the Chinese CLUE benchmark and the\nEnglish SuperGLUE benchmark. Mixing English text performs the best on Chinese\ntasks while mixing English text performs the best on English tasks.\n\n#### Effects of Language Mix Ratio\n\nWeLM is trained with English tokens and Chinese tokens. To see the effects of\nthe language mix ratio, we pre-trained two more models with the same\narchitecture as the 10B-version WeLM, but modify the percentage of English\ntext in the pre-training corpus. One is pre-trained with English text and the\nother is pre-trained with English text. In Figure 9(a), we test models on the\nWMT2020 EN2ZH news translation task (few-shot) and report the change of BLEU\nscores as the number of training tokens grows. We can see that the performance\nsaturates after pre-trained on 200B tokens, implying the maximum number of\ntokens that a 10B model can absorb would be around 200B. Mixing English text\ncan improve the bilingual capability faster but performs the worst when it\nconverges. Mixing English text also under-performs the current model as it\ncannot observe enough Chinese tokens to generate fluent Chinese text. In\npractice, given the capacity of the language model, it is important to find a\ngood sweet pot such that the model can learn about foreign languages while\nensuring enough in-language knowledge. In Figure 9(b), we further visualized\nthe average scores of the three models under the Chinese CLUE and English\nSuperGLUE benchmarks. As the percentage of English text grows, the scores on\nthe English SuperGLUE benchmark keeps growing as expected. For the Chinese\nbenchmark, surprisingly, mixing English text outperforms mixing English text,\nalthough the latter is pre-trained with more Chinese text. This implies that\nhaving enough English knowledge is helpful for achieving better performance on\nChinese NLU tasks, which makes sense due to the frequently occurred English\nwords in Chinese NLU datasets. For English NLU tasks, however, having Chinese\nknowledge is not that needed and it is better to focus the model on absorbing\nonly English knowledge.\n\n### 4.3 Multitask Prompted Training\n\nIn the previous sections, we have shown WeLM is able to attain reasonable\nzero-shot generalization on both monolingual and cross-lingual tasks given\nproper prompts. This section aims to explore whether this kind of\ngeneralization can be reinforced through explicit multitask learning. To do\nso, we collect manually-written prompts for a wide spectrum of tasks following\nthe settings in Sanh et al. (2021). We then train WeLM on a mixture of labeled\ndatasets with these manually-written prompts. The trained model, termed as\nWePrompt, is tested on a set of held-out tasks not included in the training\nstage. Intuitively this explicit multitask learning can adapt the unsupervised\nWeLM to understand better what it should do under different prompts (Ouyang et\nal., 2022). When tested on a new task with similar styles of prompts, it will\nperform more stably compared with the original unsupervised WeLM. We first\nexplain the training datasets and details we use, then report our evaluation\nresults under three scenarios: strong-zeroshot, weak-zeroshot and fine-tuned\nevaluation. We also report the evaluation results for Ernie 3.0 Titan (Wang et\nal., 2021), the largest pre-trained Chinese language model with 260B\nparameters.\n\nFigure 10: Overview of all 76 tasks from 14 categories that are used to train\nWePrompt. For each task, we annotate multiple prompts with diverse styles. Two\nannotated prompts from the NLI task are shown in the bottom as examples.\n\n#### Training Datasets\n\nThe training datasets are created following two steps: (1) Select a diverse\nset of labeled Chinese NLP tasks; (2) Create multiple prompts, each with\ndiverse wording for every single task. A prompt is a pattern that is able to\nconvert one labeled sample into a natural sentence. Prompts are created by our\nin-house annotators with the BigScience Web-based GUI\n^10^1010https://github.com/bigscience-workshop/promptsource. Annotators are\ninstructed to be open in their style so that the fine-tuned model can be\npotentially more robust with different patterns of prompts. Examples of\nprompts are shown in Figure 10. For NLI tasks, prompts are be created as a\nmulti-choice classification task over all three relations or a binary\nclassification task over one individual relation. When we run our experiments,\nwe have created 1,227 manually-written prompts for 76 tasks from 14 categories\n^11^1111The annotation keeps going. By the time of writing, there have been\nalready 150 datasets with 2,322 prompts created. Though we did not repeat all\nexperiments with the full 150 datasets, we believe the conclusions drawn from\nthe existing experiments can be already useful for future works.. A full\noverview of all the 76 tasks is visualized in Figure 10. The held-out datasets\nused for evaluation are visualized in purple and the remaining datasets in\nyellow are used for training. All the 76 tasks have been checked with\nduplication and are not included in our pre-training corpora for WeLM.\n\n#### Training Details\n\nAs the data sizes of the 76 tasks can be highly imbalanced, we set a threshold\nto keep at most 50,000 samples from each task to prevent one task dominating\nthe training process. During the training process, one training sample is\nconstructed by the following process: (1) One out of the 76 tasks is randomly\nsampled; (2) One labeled data is sampled from the task; (3) One prompt is\nsampled from the prompt bank for the task; (4) The prompt is applied to\nconvert the labeled data into a natural sentence; (5) repeat the process and\npack the corresponding natural sentences until it reaches 2,048 tokens. We\nthen fine-tune WeLM on these training samples for about 6 epochs with the\nAdamW optimizer. The learning rate is set as and batch size is set as . During\nthe initial training steps, we warm up the model using a smaller learning rate\nand mix with data used in the pre-training stage. By this means, the model can\nbe gradually adapted to the new formats of data inputs without abrupt changes.\nWe empirically find this training strategy helps stabilize the training\nprocess and the outcome model converges faster.\n\nZero-shot| Finetuing  \n---|---  \nTask| ERNIE 3.0 Titan (260B)| WeLM Zero-shot| WePrompt Strong Zero-shot|\nWePrompt Weak Zero-shot| WeLM Finetuing| WePrompt all  \nReading Comprehension  \nCMRC2018| 44.61| 70.75  \nDRCD| 58.1| 70.20  \nDuReader| 59.29| 68.10  \nCloze and Completion  \nPD| 73.50| 89.31  \nCFT| 75.59| 83.60  \nCHID| 83.49| 84.72  \nCMRC2017| 69.28| 89.3  \nNatural Language Inference  \nCMNLI| 59.48| 82.10  \nOCNLI| 58.56| 76.43  \nText classification  \nTNEWS| 80.50| 88.5  \nIFLYTEK| 83.51| 84.65  \nClosed-Book QA  \nWEBQA| 51.37| 68.06  \nWinograd-Style Task  \nWSC2020| 85.84| 85.87  \nCommon Sense Reasoning  \nC3| 64.27| 72.80  \nTable 6: Zero-shot and fine-tuned performance of WeLM and WePrompt. \u201cStrong\nzero-shot\u201d means WePromt is not trained on tasks from the same category as the\ntested task. \u201cWeak zero-shot\u201d means WePromt is not trained on the tested task.\nIn most tasks, WePrompt can outperform Ernie 3.0 Titan which is 23 times\nlarger.\n\n#### Strong-Zeroshot Evaluation\n\nStrong-zeroshot evaluation indicates that when training WePrompt, we exclude\nall tasks from the same category with the test data. For example, when testing\non PD which is a cloze task, we exclude all cloze task from the training\ndatasets of WePrompt. This can test the generalization capability of WePrompt\non new tasks from an unseen category. We train an individual WePrompt for\nevery test data and report the results in Table 6. We can see that WePromt\nperforms better than the zero-shot WeLM on most test data. The only exceptions\nare CHID and WebQA because their forms are already close to the LM objective\nso that even the unsupervised WeLM has very strong zeroshot performance. In\nmost tasks, WePrompt outperforms Ernie 3.0 Titan which is 23 times larger.\nEven though WePrompt has never seen prompts from the same category in its\ntraining stage, multiple prompted training is beneficial to help the model\nunderstand general patterns of prompts. In consequence, WePrompt can produce\nfewer out-of-scope answers than WeLM. In the example given in Figure 11, the\nzero-shot WeLM completely ignores the subject of the question. The Strong\nzero-shot WePrompt, despite still providing a wrong answer, can already\ncorrectly understand the meaning of the question and can answer in the right\ndirection.\n\n#### Weak-Zeroshot Evaluation\n\nWeak-zeroshot evaluation indicates that when training WePrompt, we exclude\nonly the task from which the test data comes. For example, when testing on PD,\nwe only exclude the PD task from the training datasets of WePrompt. This can\ntest the generalization capability of WePrompt on new tasks from a seen\ncategory. We train an individual WePrompt for every test data and report the\nresults in Table 6. We can see that the weak-zeroshot WePrompt, as expected,\nperforms better than the strong-zeroshot WePrompt on most tasks. The only\nexceptions are PD and IFLYTEK. IFLYTEK is a relatively easy task and the\nperformance already saturates. As we will show later, even when finetuning\nWeLM on the full supervised data, the performance gain is still marginal. For\nPD and even all cloze and completion tasks, the performance gain from the\nweak-zeroshot WePrompt is small. We hypothesize it might be due to the\nsimilarity between the language modelling and the cloze and completion tasks.\nAs a result, we do not need extensive prompts from the same category to adapt\nWeLM to this category of tasks. Similarly, WePrompt also did not bring\nsignificant improvement to closed-book QA tasks as this category of tasks\nalready occur frequently in the pre-training corpus.\n\nFigure 11: Examples of machine reading comprehension task using WeLM Zero-\nshot, WePrompt strong Zero-shot and WeLM weak zero-shot. WeLM zero-shot cannot\nproperly understand the question. WePrompt strong zero-shot understood the\nquestion but extracted a wrong answer from the prompt. WePrompt weak zero-shot\nanswered correctly.\n\n#### Fine-tuned Evaluation\n\nAll the previous experiments are evaluated under the in-context learning\nsetting which do not update the model parameters. For the fine-tuned\nevaluation, we fine-tune the model parameters on the full labeled data with\nsupervised learning. We compare two models: (1) WeLM-Finetuning which fine-\ntunes WeLM on the training data of the task to be tested on and (2) WePrompt-\nall which fine-tunes WePrompt on the mixture of training datasets from all\ntasks. The results on Table 6 show that both fine-tuned models outperform the\nzero-shot in-context-learning models. The improvement is significant in most\ntasks. Only in a few easy tasks like CHID and WSC2020, zeroshot models can\napproach the performance of fully-supervised models. WePrompt-all usually\noutperforms WeLM-finetuning except for a few tasks which already have abundant\nannotations such as CHID and IFLYTEK. As a \u201cnarrow expert\u201d, WeLM-finetuning\nneeds to fine-tune an individual model for every single task. As we have\nobserved, WeLM-finetuning can completely lose the capability of performing\nother tasks after specializing on one task. In contrast, WePrompt-all uses a\nunified model as a general expert for all tasks, which is appealing in saving\nthe storage cost in downstream applications. Nonetheless, even for WePrompt-\nall, the performance can be unstable when applying it to completely new tasks.\nFor example, we observe WePrompt-all is significantly worse than WeLM in\ndialogue generation and arbitrary style transfer tasks because these two have\nrather different styles of prompts. We still need to keep enriching the prompt\nbank used to train WePrompt for a better generalization.\n\n### 4.4 Others\n\nLastly, we evaluate three other capabilities of WeLM:\n\n  1. 1.\n\nExplainability: Whether WeLM is able to explain its own decision by providing\nexplanations in the prompts. If so, whether providing explanations in the\nprompts can improve the model performance.\n\n  2. 2.\n\nSelf-Calibration: Whether WeLM is able to calibrate its own predictions by\nasking itself if the predictions are correct or not.\n\n  3. 3.\n\nMemorization: To which extent WeLM is able to remember the content in the pre-\ntraining corpus and how the frequency will affect its memorization.\n\nWe also show the training curves of WeLM in the end for future reference.\n\n(a) Few-shot performance with explanation.\n\n(b) Explanation example.\n\nFigure 12: Self-Explainability of WeLM. When providing explanations for the\nfew-shot examples in the prompts, the performance of WeLM improves on two NLI\nand one text classification tasks.\n\n#### Explainability\n\nExplainability is a highly desired feature for deep neural networks, without\nwhich humans can barely trust the predictions from them (Shen et al., 2019;\nTjoa and Guan, 2020; Burkart and Huber, 2021). Recent research works have\nshown the large-pretrained language models are able to generate both\npredictions and explanations given proper illustrations (Narang et al., 2020;\nWiegreffe et al., 2022; Lampinen et al., 2022; Wei et al., 2022b). Following a\nsimilar idea, we test if WeLM can produce reasonable explanations for its own\npredictions by adding explanations in the prompts. We compare the performances\nwith/without explanations in the prompts for three tasks: CMNLI, OCNLI and\nTNews. The results and example expalanations generated by WeLM are shown in\nFigure 12(a) and 12(b). We mainly choose NLI tasks because WeLM struggles in\nthese tasks. They also usually require multi-hop inference and commonsense\nknowledge in order to derive the final answer, which is more suitable for\nproducing explanations. We can see that adding explanations in the prompt can\nusually improve the performance. However, the improvement is rather unstable\nand highly depends on the tasks and provided explanations. On CMNLI, the 11-B\nWeLM performs even worse when providing extra explanations. ON OCNLI, the\n2.7-B WeLM performs worse but the other versions perform better. In the\nexample given in Figure 12(b), we can see WeLM can also mimic the styles given\nin the prompts to produce reasonable explanations for its prediction.\n\n(a) Self-Calibration of Answer Correctness.\n\n(b) Self-Calibration of Toxic Contents.\n\nFigure 13: Self-Calibration of WeLM. WeLM is able to evaluate (1) whether the\npredictions from itself is correct or not, and (2) whether the predictions\nfrom itself contain toxic contents or not.\n\n#### Self-Calibration\n\nSelf-calibration means to calibrate the predictions from itself. For example,\nafter the model provided its prediction, we can feed further input like \u201cIs\nthis answer correct or not\u201d. Namely, we would like to see whether WeLM knows\nwhat it knows and makes accurate predictions about its own behavior and\nreasoning (Kadavath et al., 2022)? In Figure 13(a), we provide an example on\nopen-domain question answering. The answers are sampled from the top-k\ndecoding results from the model predictions. We can see that WeLM is able to\nrespond differently given different model predictions. Similarly, in Figure\n13(b), the model is asked whether the model responses contain impolite words\nor not. WeLM is able to tell that the first example contains implite words\nwhile the second example does not. Self-calibration can be challenging because\nthe model may be overconfident of its own predictions. Nonetheless, we find\nthat WeLM has a good ability of distinguishing correct and incorrect\npredictions from itself. Though we did not measure the capability in a\nquantitative way, we believe this can be a promising direction for future\nexplorations.\n\nFigure 14: Proportions of training examples that have been memorized for WeLM\nof three different model sizes. Larger-sized models can memorize more\ncontents. Frequently occurred contents are easier to be memorized.\n\n#### Memorization\n\nAs WeLM is pre-trained in large collections of web contents, we test how it\nexactly memorizes these contents and visualize the results in Figure 14.\nSpecifically, we sample 2,000 document (1,000 from the training data and 1,000\nfrom the held-out data). We construct the documents by sampling equal number\nof documents from each of the 5 data sources we used for pre-training. For\nevery document, we use the first 50 tokens as the context, feed to the model\nand let the model continue to generate the following tokens via greedy\ndecoding. If the model can generate 22 tokens ^12^121222 is empirically chosen\nas an approximation for the length of two Chinese sentences. that exactly\nmatch the original document, then we consider it as a successful memorization.\nAs can be seen, models can memorize some contents from the training data,\nthough the proportion is not high ( for the largest model). Larger models can\nusually memorize more contents across data sources. Common crawl contents\ncomprise of over half of the training data so that WeLM can memorize them\nbetter than other data sources. Academic writings, due to its low frequency in\nthe training data and its unique styles, are the hardest to memorize. On the\nrightmost of Figure 14, we also visualize the relation between the average\nmemorized length and the number of occurrences in the training corpus. We can\nsee that for text that occurs more frequently, the model can memorize more\ncontents and generate longer continuations exactly matching the original\ndocument. For text that occur only once, the contents that the model can\nmemorize are very minor.\n\n## 5 Conclusion\n\nWe present WeLM: a well-read pre-trained language model for Chinese that is\nable to seamlessly perform different types of tasks with zero or few-shot\ndemonstrations. It exhibits strong performances across monolingual (Chinese)\nand cross-lingual (Chinese-English/Japanese) tasks, surpassing existing pre-\ntrained models with similar sizes. We collected human-written prompts for a\nlarge set of supervised datasets in Chinese and fine-tuned WeLM with multi-\nprompted training. The resulting model can attain strong generalization on\nunseen types of tasks and outperform the unsupervised WeLM in zero-shot\nlearning. We further demonstrate that WeLM has basic skills at explaining and\ncalibrating the decisions from itself, which can be promising directions for\nfuture research.\n\n## References\n\n  * Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. URL https://arxiv.org/abs/1810.04805.\n  * Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html.\n  * Clark et al. (2020) Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\n  * Lewis et al. (2020a) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, 2020a.\n  * Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n  * Kirkpatrick et al. (2017) James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.\n  * Li and Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u20134597, 2021.\n  * Brown et al. (2020a) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901, 2020a.\n  * Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a.\n  * Narayanan et al. (2021) Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201315, 2021.\n  * Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. URL https://arxiv.org/abs/2112.11446.\n  * Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. URL https://arxiv.org/abs/2203.15556.\n  * Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. URL https://arxiv.org/abs/2204.02311.\n  * Sun et al. (2019) Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223, 2019.\n  * Cui et al. (2020) Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. Revisiting pre-trained models for chinese natural language processing. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 657\u2013668, 2020.\n  * Sun et al. (2021a) Zijun Sun, Xiaoya Li, Xiaofei Sun, Yuxian Meng, Xiang Ao, Qing He, Fei Wu, and Jiwei Li. Chinesebert: Chinese pretraining enhanced by glyph and pinyin information. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2065\u20132075, 2021a.\n  * Su et al. (2022) Hui Su, Weiwei Shi, Xiaoyu Shen, Zhou Xiao, Tuo Ji, Jiarui Fang, and Jie Zhou. Rocbert: Robust chinese bert with multimodal contrastive pretraining. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 921\u2013931, 2022.\n  * Zhang et al. (2021) Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, et al. Cpm: A large-scale generative chinese pre-trained language model. AI Open, 2:93\u201399, 2021.\n  * Wu et al. (2021) Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, et al. Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning. arXiv preprint arXiv:2110.04725, 2021. URL https://arxiv.org/abs/2110.04725.\n  * Zeng et al. (2021) Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, et al. Pangu-alpha: Large-scale autoregressive pretrained chinese language models with auto-parallel computation. arXiv preprint arXiv:2104.12369, 2021. URL https://arxiv.org/abs/2104.12369.\n  * Sun et al. (2021b) Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, et al. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2107.02137, 2021b.\n  * Wang et al. (2021) Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng, Junyuan Shang, Yanbin Zhao, Chao Pang, et al. Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2112.12731, 2021. URL https://arxiv.org/abs/2112.12731.\n  * Lin et al. (2021) Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O\u2019Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Ves Stoyanov, and Xian Li. Few-shot learning with multilingual language models. arXiv preprint arXiv:2112.10668, abs/2112.10668, 2021. URL https://arxiv.org/abs/2112.10668.\n  * Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, abs/1910.10683, 2019. URL http://arxiv.org/abs/1910.10683.\n  * Lee et al. (2021) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021.\n  * Kandpal et al. (2022) Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks in language models. arXiv preprint arXiv:2202.06539, 2022.\n  * Roberts et al. (2022) Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022. URL https://arxiv.org/abs/2203.17189.\n  * Manku et al. (2007) Gurmeet Singh Manku, Arvind Jain, and Anish Das Sarma. Detecting near-duplicates for web crawling. In Proceedings of the 16th international conference on World Wide Web, pages 141\u2013150, 2007.\n  * Brown et al. (2020b) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020b. URL https://arxiv.org/abs/2005.14165.\n  * Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. URL https://arxiv.org/abs/2104.09864.\n  * Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n  * Kudo and Richardson (2018) Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\n  * Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.\n  * Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.\n  * Black et al. (2021) Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.\n  * Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201316. IEEE, 2020.\n  * Micikevicius et al. (2018) Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In International Conference on Learning Representations, 2018.\n  * Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n  * Xu et al. (2020) Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, et al. Clue: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics, pages 4762\u20134772, 2020.\n  * Schick and Sch\u00fctze (2021) Timo Schick and Hinrich Sch\u00fctze. It\u2019s not just size that matters: Small language models are also few-shot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339\u20132352, 2021.\n  * Fei et al. (2022) Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi Huo, Jingyuan Wen, Haoyu Lu, Ruihua Song, Xin Gao, Tao Xiang, et al. Towards artificial general intelligence via a multimodal foundation model. Nature Communications, 13(1):1\u201313, 2022.\n  * Sanh et al. (2021) Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021. URL https://arxiv.org/abs/2110.08207.\n  * Jiang et al. (2020) Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423\u2013438, 2020.\n  * Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.\n  * Zeng et al. (2020) Changchang Zeng, Shaobo Li, Qin Li, Jie Hu, and Jianjun Hu. A survey on machine reading comprehension: Tasks, evaluation metrics and benchmark datasets. arXiv preprint arXiv:2006.11880, 2020.\n  * Cui et al. (2019) Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. A span-extraction dataset for chinese machine reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5883\u20135889, 2019.\n  * Shao et al. (2018) Chih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng, and Sam Tsai. Drcd: a chinese machine reading comprehension dataset. arXiv preprint arXiv:1806.00920, 2018.\n  * He et al. (2018) Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, et al. Dureader: a chinese machine reading comprehension dataset from real-world applications. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 37\u201346, 2018.\n  * Cui et al. (2016) Yiming Cui, Ting Liu, Zhipeng Chen, Shijin Wang, and Guoping Hu. Consensus attention-based neural networks for chinese reading comprehension. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1777\u20131786, 2016.\n  * Zheng et al. (2019) Chujie Zheng, Minlie Huang, and Aixin Sun. Chid: A large-scale chinese idiom dataset for cloze test. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 778\u2013787, 2019.\n  * Cui et al. (2018) Yiming Cui, Ting Liu, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. Dataset for the first evaluation on chinese machine reading comprehension. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), 2018.\n  * Bowman et al. (2015) Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013642, 2015.\n  * Roberts et al. (2020) Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418\u20135426, 2020.\n  * Li et al. (2016) Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, and Wei Xu. Dataset and neural recurrent sequence labeling model for open-domain factoid question answering. arXiv preprint arXiv:1607.06275, 2016.\n  * Birjali et al. (2021) Marouane Birjali, Mohammed Kasri, and Abderrahim Beni-Hssane. A comprehensive survey on sentiment analysis: Approaches, challenges and trends. Knowledge-Based Systems, 226:107134, 2021.\n  * Levesque et al. (2012) Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.\n  * Sap et al. (2020) Maarten Sap, Vered Shwartz, Antoine Bosselut, Yejin Choi, and Dan Roth. Commonsense reasoning for natural language processing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 27\u201333, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-tutorials.7. URL https://aclanthology.org/2020.acl-tutorials.7.\n  * Lin and Ng (2019) Hui Lin and Vincent Ng. Abstractive summarization: A survey of the state of the art. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 9815\u20139822, 2019.\n  * Hu et al. (2015) Baotian Hu, Qingcai Chen, and Fangze Zhu. Lcsts: A large scale chinese short text summarization dataset. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1967\u20131972, 2015.\n  * Hua et al. (2017) Lifeng Hua, Xiaojun Wan, and Lei Li. Overview of the nlpcc 2017 shared task: single document summarization. In National CCF Conference on Natural Language Processing and Chinese Computing, pages 942\u2013947. Springer, 2017.\n  * Lin (2004) Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.\n  * Chen et al. (2017) Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. A survey on dialogue systems: Recent advances and new frontiers. Acm Sigkdd Explorations Newsletter, 19(2):25\u201335, 2017.\n  * Su et al. (2020) Hui Su, Xiaoyu Shen, Zhou Xiao, Zheng Zhang, Ernie Chang, Cheng Zhang, Cheng Niu, and Jie Zhou. MovieChats: Chat like humans in a closed domain. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6605\u20136619, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.535. URL https://aclanthology.org/2020.emnlp-main.535.\n  * Jin et al. (2022) Di Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova, and Rada Mihalcea. Deep learning for text style transfer: A survey. Computational Linguistics, 48(1):155\u2013205, March 2022. doi: 10.1162/coli_a_00426. URL https://aclanthology.org/2022.cl-1.6.\n  * Reif et al. (2021) Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. A recipe for arbitrary text style transfer with large language models. arXiv preprint arXiv:2109.03910, 2021.\n  * Krishna et al. (2022) Kalpesh Krishna, Deepak Nathani, Xavier Garcia, Bidisha Samanta, and Partha Talukdar. Few-shot controllable style transfer for low-resource multilingual settings. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7439\u20137468, 2022.\n  * Yang et al. (2020) Shuoheng Yang, Yuxin Wang, and Xiaowen Chu. A survey of deep learning techniques for neural machine translation. arXiv preprint arXiv:2002.07526, 2020.\n  * Asai et al. (2021) Akari Asai, Jungo Kasai, Jonathan Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. XOR QA: Cross-lingual open-retrieval question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 547\u2013564, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.46. URL https://aclanthology.org/2021.naacl-main.46.\n  * Artetxe et al. (2020) Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.421. URL https://aclanthology.org/2020.acl-main.421.\n  * Lewis et al. (2020b) Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. Mlqa: Evaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7315\u20137330, 2020b.\n  * Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, 2016.\n  * Leuski et al. (2003) Anton Leuski, Chin-Yew Lin, Liang Zhou, Ulrich Germann, Franz Josef Och, and Eduard Hovy. Cross-lingual c* st* rd: English access to hindi information. ACM Transactions on Asian Language Information Processing (TALIP), 2(3):245\u2013269, 2003.\n  * Zhu et al. (2019) Junnan Zhu, Qian Wang, Yining Wang, Yu Zhou, Jiajun Zhang, Shaonan Wang, and Chengqing Zong. NCLS: Neural cross-lingual summarization. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3054\u20133064, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1302. URL https://aclanthology.org/D19-1302.\n  * Hermann et al. (2015) Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural information processing systems, 28, 2015.\n  * Zhu et al. (2018) Junnan Zhu, Haoran Li, Tianshang Liu, Yu Zhou, Jiajun Zhang, and Chengqing Zong. MSMO: Multimodal summarization with multimodal output. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4154\u20134164, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1448. URL https://aclanthology.org/D18-1448.\n  * Auer (2005) Peter Auer. A postscript: Code-switching and social identity. Journal of pragmatics, 37(3):403\u2013410, 2005.\n  * Hickey (2020) Raymond Hickey. The handbook of language contact. John Wiley & Sons, 2020.\n  * Blevins and Zettlemoyer (2022) Terra Blevins and Luke Zettlemoyer. Language contamination explains the cross-lingual capabilities of english pretrained models. arXiv preprint arXiv:2204.08110, 2022.\n  * Blevins et al. (2022) Terra Blevins, Hila Gonen, and Luke Zettlemoyer. Analyzing the mono-and cross-lingual pretraining dynamics of multilingual language models. arXiv preprint arXiv:2205.11758, 2022.\n  * Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\n  * Shen et al. (2019) Xiaoyu Shen, Jun Suzuki, Kentaro Inui, Hui Su, Dietrich Klakow, and Satoshi Sekine. Select and attend: Towards controllable content selection in text generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 579\u2013590, 2019.\n  * Tjoa and Guan (2020) Erico Tjoa and Cuntai Guan. A survey on explainable artificial intelligence (xai): Toward medical xai. IEEE transactions on neural networks and learning systems, 32(11):4793\u20134813, 2020.\n  * Burkart and Huber (2021) Nadia Burkart and Marco F Huber. A survey on the explainability of supervised machine learning. Journal of Artificial Intelligence Research, 70:245\u2013317, 2021.\n  * Narang et al. (2020) Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. Wt5?! training text-to-text models to explain their predictions. arXiv preprint arXiv:2004.14546, 2020.\n  * Wiegreffe et al. (2022) Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. Reframing human-AI collaboration for generating free-text explanations. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 632\u2013658, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.47. URL https://aclanthology.org/2022.naacl-main.47.\n  * Lampinen et al. (2022) Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022.\n  * Kadavath et al. (2022) Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.\n\n\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Mar 13 20:47:41 2024 by LaTeXML\n\n", "frontpage": false}
