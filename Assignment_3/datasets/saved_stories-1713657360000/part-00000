{"aid": "40099252", "title": "Self-Reasoning Tokens, teaching models to think ahead", "url": "https://reasoning-tokens.ghost.io/reasoning-tokens/", "domain": "reasoning-tokens.ghost.io", "votes": 19, "user": "fesens", "posted_at": "2024-04-20 17:54:27", "comments": 0, "source_title": "Self-Reasoning Tokens, teaching models to think ahead.", "source_text": "Self-Reasoning Tokens, teaching models to think ahead.\n\nReasoning Tokens\n\nSign in Subscribe\n\n# Self-Reasoning Tokens, teaching models to think ahead.\n\n#### Felipe Sens Bonetto\n\nApr 20, 2024 \u2014 4 min read\n\nWhat is the mathematical formulation of reasoning? How can we make LLMs like\nchatGPT think before they speak? And how can we make that baked into the model\nso it can learn to think in a self-supervised way without having to \"explain\nit step by step\" (or another famous prompt we use when we want to improve\nchatGPT performance drastically)? How can we teach models to think ahead? I\nwill share with you the results of some experiments that may cast light on the\npath of \"Reasoning Tokens.\"\n\nIntroduction\n\nAs the authors of \"Interpretability in the wild\" have taught us, from looking\ninside transformers, we know that the computation of the next token includes\nsome information computed in previous steps. This may seem obvious at first\nglance, but there is more to this affirmation than what meets the eye. This\nmeans the language model expends some internal \"cognitive power\" processing\nand storing information that will be used, not for predicting the very next\ntoken but 2, 3, or even 10 tokens ahead.\n\nInternal computation of GPT-2, extracted from the \"Interpretability in the\nwild\" paper\n\nAs we can see from the image above, the attention heads produce computations\nthat will be helpful only in the far future, and even some calculations that\n\"headge\" against the wrong answers, exposed in the paper as \"Negative Name\nMover Heads\" or attention heads that suppress specific tokens.\n\nVisual explanation extracted from the \"Do Language Models Plan for Future\nTokens?\" paper\n\nFurther work has shown that LLMs indeed plan for future tokens. In the paper\n\"Do Language Models Plan for Future Tokens?\" the authors carefully crafted a\nmathematical formulation to impede what they call \"Pre-Caching,\" or the\nability of the model to make intermediary computations that would be useful\nbeyond the very next token. Their experiments found a small performance gap\nwhen the model was \"myopic\" or incapable of planning for future tokens. This\nis promising but could be better. This indicates that while GPTs plan ahead,\nmost of their power is used to predict only the next word in the sequence. As\na sanity check, this gap should increase as the length of the predicted text\ngrows because the model would have more tokens to produce said computations,\nand indeed, that was what they found in the paper.\n\nHow do we leverage that?\n\nWhat if we incentivized those intermediary calculations, which are useful only\nin future tokens, teaching the model to think ahead in a self-supervised way?\nIt turns out that the formulation for such a task doesn't need to be that\ncomplicated.\n\nGradient flow of Reasoning tokens!\n\nIn this first experiment, we introduce reasoning tokens! The model will\nproduce two tokens for each token in the original sequence. As usual, the\nfirst token will be used to predict the next token. The second token, however,\nduplicates the input of the first one and does not receive a gradient \"answer\"\nfrom the very next token, only from future tokens; in fact, this token doesn't\neven participate in the calculation of the very next token. This incentivizes\nthe model to \"pre-cache\" or only put information that is useful for the future\nin this spot. But talk is cheap. Show me the results.\n\nMini GPT-2 (10M params) trained on 82M tokens.\n\nAnd the results are very promising, showing a reduction of 35% in the loss!\nFrom 0.621 to 0.401. The experiment also shows that the model benefits from\nhaving multiple tokens to do its \"reasoning,\" forecasting the capability to\nform long-range dependencies. This validates the hypothesis that we can teach\nthe models to plan for the future, an important first step to get to\nreasoning.\n\nA GPT-2 Small (124M params) model was also trained on 300B tokens of the \"Open\nWeb Text Corpus,\" and its results were also very promising, resulting in a\n0.04 validation loss reduction from 2.85 to 2.81. In context, going from GPT-2\nLarge (~700M) to GPT-2 XL (1.5B) drops the validation loss by 0.13 in the same\ndataset. All training code was derived from Andrej Karpathy amazing GPT-2\nimplementation.\n\nGPT-2 Small trained on 300B params - 1 Reasoning token\n\nWhat is next for Reasoning Tokens?\n\nCurrently, I'm experimenting with reasoning tokens in fine-tuned instruction\nfollowing models, where planning can be much more useful. The formulation is\nvery close to the first experiment. Still, this time, the model can choose\nwhen this internal reasoning will start, allowing it to choose when to reason\nbefore producing the next word in the sequence.\n\nReasoning tokens in instruction tasks\n\nThe hypothesis being tested is that the addition of Reasoning Tokens can\nsubstitute and outperform models where a \"step by step\" explanation is\nincluded in the training phase. This would be useful because those\nexplanations are expensive to produce/obtain. Although such explanations can\nbe useful to the model, gradient descent could find other ways to do that\nreasoning using all the internal mathematical dimensions of the model in a way\nthat does not necessarily make sense to us. It would be a great fit for\n\"Mixture of Experts\" (MoE) models, where we can have an expert just for the\nreasoning phase.\n\nThe future is bright. Stay tuned for the next advancements.\n\n## Read more\n\n### Coming soon\n\nThis is Reasoning Tokens, a brand new site by Felipe Sens Bonetto that's just\ngetting started. Things will be up and running here shortly, but you can\nsubscribe in the meantime if you'd like to stay up to date and receive emails\nwhen new content is\n\nBy Felipe Sens Bonetto Apr 19, 2024\n\nReasoning Tokens\n\nPowered by Ghost\n\n## Reasoning Tokens\n\nThoughts, stories and ideas.\n\n", "frontpage": true}
