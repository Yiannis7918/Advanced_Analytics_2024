{"aid": "40010816", "title": "Multi-modal emotional information via personalized skin facial interface", "url": "https://www.nature.com/articles/s41467-023-44673-2", "domain": "nature.com", "votes": 1, "user": "taubek", "posted_at": "2024-04-12 09:27:37", "comments": 0, "source_title": "Encoding of multi-modal emotional information via personalized skin-integrated wireless facial interface", "source_text": "Encoding of multi-modal emotional information via personalized skin-integrated wireless facial interface | Nature Communications\n\nSkip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n\n## Search\n\nAdvanced search\n\n### Quick links\n\n    * Explore articles by subject\n    * Find a job\n    * Guide to authors\n    * Editorial policies\n\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n\n  * Sign up for alerts\n  * RSS feed\n\nEncoding of multi-modal emotional information via personalized skin-integrated\nwireless facial interface\n\nDownload PDF\n\nDownload PDF\n\n  * Article\n  * Open access\n  * Published: 15 January 2024\n\n# Encoding of multi-modal emotional information via personalized skin-\nintegrated wireless facial interface\n\n  * Jin Pyo Lee^1,2,\n  * Hanhyeok Jang^1,\n  * Yeonwoo Jang ORCID: orcid.org/0000-0002-8636-4432^1,\n  * Hyeonseo Song^1,\n  * Suwoo Lee^1,\n  * Pooi See Lee ORCID: orcid.org/0000-0003-1383-1623^2 &\n  * ...\n  * Jiyun Kim ORCID: orcid.org/0000-0001-9756-7254^1,3\n\nNature Communications volume 15, Article number: 530 (2024) Cite this article\n\n  * 5682 Accesses\n\n  * 330 Altmetric\n\n  * Metrics details\n\n## Abstract\n\nHuman affects such as emotions, moods, feelings are increasingly being\nconsidered as key parameter to enhance the interaction of human with diverse\nmachines and systems. However, their intrinsically abstract and ambiguous\nnature make it challenging to accurately extract and exploit the emotional\ninformation. Here, we develop a multi-modal human emotion recognition system\nwhich can efficiently utilize comprehensive emotional information by combining\nverbal and non-verbal expression data. This system is composed of personalized\nskin-integrated facial interface (PSiFI) system that is self-powered, facile,\nstretchable, transparent, featuring a first bidirectional triboelectric strain\nand vibration sensor enabling us to sense and combine the verbal and non-\nverbal expression data for the first time. It is fully integrated with a data\nprocessing circuit for wireless data transfer allowing real-time emotion\nrecognition to be performed. With the help of machine learning, various human\nemotion recognition tasks are done accurately in real time even while wearing\nmask and demonstrated digital concierge application in VR environment.\n\n### Similar content being viewed by others\n\n### A systematic review and multivariate meta-analysis of the physical and\nmental health benefits of touch interventions\n\nArticle Open access 08 April 2024\n\nJulian Packheiser, Helena Hartmann, ... Fr\u00e9d\u00e9ric Michon\n\n### High-speed and large-scale intrinsically stretchable integrated circuits\n\nArticle 13 March 2024\n\nDonglai Zhong, Can Wu, ... Zhenan Bao\n\n### A modular organic neuromorphic spiking circuit for retina-inspired sensory\ncoding and neurotransmitter-mediated neural pathways\n\nArticle Open access 03 April 2024\n\nGiovanni Maria Matrone, Eveline R. W. van Doremaele, ... Yoeri van de Burgt\n\n## Introduction\n\nThe utilization of human affects, encompassing emotions, moods, and feelings,\nis increasingly recognized as a crucial factor in improving the interaction\nbetween humans and diverse machines and systems^1,2,3. Consequently, there is\na growing expectation that technologies capable of detecting and recognizing\nemotions will contribute to advancements across multiple domains, including\nHMI device^4,5,6, robotics^7,8,9, marketing^10,11,12, healthcare^13,14,15,\neducation^16,17,18, etc. By discerning personal preferences and delivering\nimmersive interaction experiences, these technologies have the potential to\noffer more user-friendly and customized services. Nonetheless, decoding and\nencoding emotional information poses significant challenges due to the\ninherent abstraction, complexity, and personalized nature of emotions^19,20.\nTo overcome these challenges, the successful utilization of comprehensive\nemotional information necessitates the extraction of meaningful patterns\nthrough the detection and processing of combined data from multiple\nmodalities, such as speech, facial expression, gesture, and various\nphysiological signals (e.g., temperature, electrodermal activity)^21,22,23.\nEncoding these extracted patterns into interaction parameters tailored for\nspecific applications also becomes essential.\n\nConventional approaches for recognizing emotional information from humans\noften rely on analyzing images of facial expressions^24,25,26 or speech of\nverbal expression^27,28,29. However, these methods are frequently impeded by\nenvironmental factors such as lighting conditions, noise interference, and\nphysical obstructions. As an alternative, text analysis techniques^30,31,32\nhave been explored for emotion detection, utilizing vast amounts of\ninformation available on diverse social media platforms. However, this\napproach presents challenges due to the diverse ambiguities and new\nterminologies being introduced, which further complicates the accurate\ndetection of emotions from the text. To overcome these limitations, sensing\ndevices capable of capturing changes in physiological signals, including\nEEG^33,34,35, EMG^36,37,38, ECG^39,40,41 and GSR^42,43,44 have been employed\nto collect more accurate and reliable data. These devices can establish\ncorrelations between these signals and human emotions irrespective of\nenvironmental factors, but the requirement of bulky equipment limits their\napplication to everyday communication scenarios.\n\nIn recent studies, flexible skin-integrated devices have shown the possibility\nof providing real-time detection and recognition of emotional information\nthrough various modalities such as facial expressions, speech, text, hand\ngestures, physiological signals, etc.^45,46,47,48,49,50,51,52,53,54,55,56.\nSpecifically, a resistive strain sensor has been employed to directly detect\nfacial strain deformations that occur during facial expressions^46,47,51,52.\nThis approach offers simplicity by using thin and soft skin-integrated\nelectrode interfaces for current flow, allowing for wearable or portable\napplications. However, an additional power source, low working frequency\nrange, and extra components for the signal conversion cause simple modality\nonly limited to one-to-one correlation that imposes constraints on the range\nof applications such as healthcare, VR where complementary information is\nneeded to approximate natural interaction, and user experience can be enhanced\nby multiple ways of inputs. Furthermore, most existing studies have primarily\nfocused on recognizing and exploiting human emotions, intentions or commands\nusing the single-modal data that can have weaknesses in specific context, thus\nlimiting the use of higher-level and comprehensive emotional\ncontexts^45,48,49,50,53,54,55,56. On the other hand, to overcome the drawbacks\nof each modality for a more resilient system, multi-modal emotion recognition\nwas conducted to draw embedded high-level information by using the combined\nknowledge from all the accessible data sensing^57,58,59. Consequently, to\neffectively and precisely encode emotional information, an advanced format of\nthe skin-integrated device necessitates improved wearability seamlessly\nintegrating with individuals, while possessing multi-modal sensing\ncapabilities to process and extract higher-level of information. Also, this\npersonalized device, capable of real-time collection of reliable and accurate\nmulti-modal data regardless of external environmental factors, should be\naccompanied by the corresponding classification technique to encode the\ngathered data into personalized feedback parameters for target applications.\n\nHere, we proposed a human emotion recognition system in an attempt to utilize\ncomplex emotional states with our personalized skin-integrated facial\ninterface (PSiFI) offering simultaneous detection and integration of facial\nexpression and vocal speech. The PSiFI incorporates a personalized facial mask\nthat is self-powered, easily applicable, stretchable, transparent, capable of\nwireless communication, and highly customized to conformally fit into an\nindividual\u2019s face curvatures based on 3D face reconstruction. These features\nenhance the device\u2019s usability and reliability in capturing and analyzing\nemotional cues, facilitating the real-time detection of multi-modal sensing\nsignals derived from facial strains and vocal vibrations. To encode the\ncombinatorial sensing signals into personalized feedback parameters, we employ\na convolutional neural network (CNN)-based classification technique that\nrapidly adapts to an individual\u2019s context through transfer learning. In the\ncontext of human emotion recognition, we specifically focus on facial\nexpression and vocal speech as the chosen multi-modal data, considering their\nconvenience for data collection and classification based on prior research\nfindings.\n\nThe PSiFI device is basically comprised of strain and vibration sensing units\nbased on triboelectrification to detect facial strain for facial expression\nand vocal vibration for speech recognition, respectively. The incorporation of\na triboelectric nanogenerator (TENG) enables the sensor device to possess\nself-powering capabilities while offering a broad range of design\npossibilities in terms of materials and architectures^60,61, thus fulfilling\nthe requirements of personalized and multi-modal sensing devices. The sensing\nunits are made of PDMS film as a dielectric layer and PEDOT:PSS coated PDMS\nfilm as an electrode layer prepared by the semi-curing method which enables\nthe film to exhibit good transparency with decent electrical conductivity.\nFurthermore, we demonstrated real-time emotion recognition with data\nprocessing circuit for wireless data transfer and real-time classification\nbased on rapidly adapting convolution neural network (CNN) model with the help\nof transfer learning using data augmentation methods. Last, we demonstrated\ndigital concierge application as an exciting possibility in virtual reality\n(VR) environment via human machine interfaces (HMIs) with our PSiFI. The\ndigital concierge recognizes a user\u2019s intention and interactively offers\nhelpful services depending on the user\u2019s affectivity. Our work presents a\npromising way to help to consistently collect data regarding emotional speech\nwith barrier-free communication and can pave the way toward acceleration of\ndigital transformation.\n\n## Results\n\n### Personalized skin-integrated facial interface (PSiFI) system\n\nWe devised personalized skin-integrated facial interface (PSiFI) system\nconsisting of multimodal triboelectric sensors (TES), data processing circuit\nfor wireless data transfer and deep-learned classifier. Figure 1A illustrates\nthe schematics of overall process for human emotion recognition with PSiFI\nfrom fabrication to classification task. As for making personalized device, we\nbrought in 3D face reconstruction process by collecting 3D data of user\u2019s\nappearance from scanned photos and converting the data to digital models. This\nprocess allowed us to fabricate personalized device fitted in well with\nvarious user faces and successfully secure individual user data for accurate\nrecognition task. (Supplementary Fig. 1). Subsequently, we utilized both\nverbal/non-verbal expression information detected from multimodal sensors and\nclassified human emotions in real-time using transfer learning applied\nconvolution neural network (CNN).\n\nFig. 1: The system overview with PSiFI.\n\nA Schematic illustration of personalized skin-integrated facial interfaces\n(PSiFI) including triboelectric sensors (TES), data processing circuit for\nwireless communication and deep-learned classifier for facial expression and\nvoice recognition. B Schemes showing 2d layout for the PSiFI in the form of\nwearable mask and depicting two different types of TES in terms of sensory\nstimulus such as facial strain and vocal vibration. C Schematic diagram of the\nTES which consists of simple two-layer structure such as electrode layer and\ndielectric layer and photograph of the TES components, respectively. Scale\nbar: 1 cm. D Schematics demonstrating fabricated components for our TES. As\nfor the electrode layer, PEDOT:PSS based electrode was made via semi-curing\nprocess. (left). As for the dielectric layer, it was designed differently\nconsidering sensing stimuli such as strain and vibration to achieve optimal\nsensing performance. The inset in center showing SEM image for nanostructured\nsurface of strain type dielectric layer and in right showing photograph for\npunched holes as acoustic holes of vibration type dielectric layer. Scale bar:\n2 \u03bcm and 1 mm.\n\nFull size image\n\nAs shown in Fig.1B, the emotional information based on verbal/non-verbal\nexpression in the form of digital signals was sent to be the PSiFI mask and\nwirelessly transferred with data processing circuit. To effectively detect the\nsignals for the emotional information, the PSiFI was integrated with multi-\nmodal TES to capture facial skin strains and vocal cord vibrations by\ndetecting electrical signals from glabella, eye, nose, lip, chin and vocal\ncord selected as representative regions based on previous studies regarding\nfacial muscle activation patterns during facial expression^62,63,64.\n\nFigure 1C provides the schematic and real image of the TES consisting of\nsimple two-layer structure where PEDOT:PSS-coated polydimethylsiloxane (PDMS)\nand nanostructured PDMS were used as stretchable electrode and dielectric\nlayer respectively so that our TES are based on single electrode mode in\nprinciple. Figure 1D shows schematics of the PEDOT:PSS-coated PDMS and\ndielectric layers for each strain and vibration type. The PEDOT:PSS-coated\nPDMS was fabricated by semi-cured process^65,66 where coating is conducted\nbefore full-curing of the elastomer (Supplementary Movie 1). Our stretchable\nelectrode based on the semi-curing process was characterized and showed better\nperformance when it compared to conventional surface treated electrode in\nterms of optical, mechanical, and electrical aspects. (Supplementary Fig. 2)\nAs shown in scanning electron microscope (SEM) image in Fig.1D, for the\ndielectric layers we fabricated, nano surface engineering was introduced by\ninductively coupled plasma reactive ion etching process (ICP-RIE) to improve\ntriboelectric performance by enhancing specific surface area. (Supplementary\nFig. 3) Additionally, the dielectric layer for the vibration sensing was\nperforated like the acoustic holes which enhance vibrate the volume of air\ninside (Supplementary Movie 2).\n\n### Working mechanism and characterization of the strain sensing unit\n\nConverting facial skin strain during facial expression into distinct\nelectrical signals and sending the data as non-verbal information to the\ncircuit system is the function of our strain sensing unit. As depicted\nschematically in Fig. 2A, the strain sensing unit was fabricated with the\nnanostructured PDMS for its high effective contact area as a dielectric layer\nand PEDOT:PSS embedded PDMS as an electrode layer to make TES with the single\nelectrode structure for simple configuration to be facilitated as wearable\nsensors. These two layers were separated by double sided tapes applied to both\nends of the layers as a spacer to be consistently generate a series of\nelectrical signals during the operation cycle. Besides, all the parts in the\nsensing units are made of stretchable and skin-friendly viable materials and\ncan be prepared through scalable fabrication processes (for the details see\nthe \u201cMethods\u201d section and Supplementary Fig. 4). These characteristics of the\nmaterials used in the strain sensing unit allow our strain sensor to retain\nrelatively good electrical conductivity even under stretching in the range of\nfacial skin strain during facial expression and guarantee robustness of the\nsensing unit. As schematically shown in Fig. 2B, an electrical potential\nbuilds up due to the difference between triboelectric series based on\ndifferent affinity for electrons, which the PDMS played a triboelectrically\nnegative material by receiving electrons and the PEDOT:PSS based stretchable\nelectrode played a triboelectrically positive material by donating electrons\nin TES. On top of that, our strain sensing unit makes the contact area changes\nwhen stretched and achieved even buckled states so that it can detect\nbidirectional strain motion among the triboelectric based strain sensors for\nthe first time, according to our knowledge. Correspondingly, the generated\noutput signals of our strain sensing unit during the buckle-stretch cycle were\nshown in Fig. 2C. The comprehensive working mechanism of the bidirectional\nstrain sensor for each mode was demonstrated in Supplementary Fig. 5.\n\nFig. 2: Working mechanism and characterization of the strain sensing unit.\n\nA Schematic illustration of the strain sensing unit. Inset: enlarged view of\nthe sensing unit detecting facial strain. B Electrical potential distribution\nof the strain sensing unit under buckled and stretched state. C Output\nelectrical signals of the strain sensing unit during the buckle-stretch cycle.\nD Real image of experimental set-up for output measurements. Scale bar: 1 cm.\nE and F Sensitivity measurement during buckling (E) and stretching of the\nsensing unit (F). G Response time measurement with various frequencies.\nInsets: enlarged views of the loading and unloading processes in one cycle. H\nGenerated voltage signals of the sensing unit with various frequencies at a\nconstant strain of 40%. I Mechanical durability test for up to 3000 continuous\nworking cycles and enlarged views of different operation cycles, respectively.\n\nFull size image\n\nTo characterize the strain sensing unit in terms of mechanical and electrical\nproperties, a linear motor was employed to exert a cyclic force on the sensing\nunit as shown in Fig. 2D. Figure 2E and F provides our strain sensing unit\nsensitivity measurement in a strain range from 0% to 100% by buckling and\nstretching, respectively. The sensitivity was derived from S = \u0394V/\u0394\u03b5 where \u0394V\nis the relative potential change and \u03b5 is the strain. As for the buckling\nstrain, linearity of the electrical responses and a sensitivity of 5 mV was\nobtained in a strain range up to 50% despite non-linear region occurred beyond\nthe strain due to anomalous shape change. The signals in the non-linear region\nwere differentiated with the difference in the width of time as shown in\nSupplementary Fig. 6. As for the stretching strain, an acceptable linearity\nand sensitivity of 3 mV was obtained in wide strain range up to 90%. We\nmeasured the response time of the strain sensing unit to evaluate the\nperformance of the unit as it can be executed real-time classification tasks.\nAs shown in Fig. 2G, there is no apparent latency time between the stretching\nforce and corresponding the output voltage so that we can make sure the\nsensing unit can detect the sensing in real time. The stretch\u2013release of one\ncycle (Fig. 2G, inset) exhibits a response time of below 20 ms. Therefore,\ncompared with other strain sensors, our strain sensing unit has an advantage\nbecause of its high sensitivity in bi-direction, fast-response time and high\nstretchability, which can ensure an accurate sensing of the facial expression\nvia converted electrical signals in real time.\n\nWe also measured the output voltage at constant strain of 40% depending on the\nworking frequencies ranging from 0.5 to 3 Hz and confirmed that our strain\nsensing unit can show reliable performance regardless of the frequencies as\nshown in Fig. 2H. When it comes to long-term use in practical application, the\nmechanical stability of our sensing unit also can be considered as important\nproperty. As demonstrated in Fig. 2I, apparent output voltages changes were\nnot observed for the strain sensing unit after 3000 continuous working cycles\nunder 40% strain. It is noteworthy that the 40% strain change is way beyond\nthe requirement for most facial skin strain during facial expression\ndemonstrations^45,67.\n\n### Working mechanism and characterization of the vocal sensing unit\n\nOur vocal sensing unit has a function of capturing vocal vibrations on the\nvocal cord during verbal expression and sending the data as verbal information\nto the circuit system. As shown in Fig. 3A, the vocal sensing unit was\nfabricated with the holes patterned PDMS as dielectric layer and PEDOT:PSS\nembedded PDMS as an electrode layer to make TES. The holes were introduced\ninto the vocal sensing unit as acoustic holes which not only act as\ncommunicating vessels to ventilate an air between two contact surfaces to the\nambient air, which results in enhanced flat frequency response but also reduce\nthe stiffness by improving the movement of the rim of diaphragms^68,69,70\n(Supplementary Fig. 7 and Table S1). To be configured into TES, like the\nstrain sensing unit, the dielectric and electrode layer were separated by\ndouble-sided tapes applied to both ends of the layers as a spacer for\nconsistent operations during working cycles. The inset to Fig. 3A provides an\nenlarged view of the vocal sensing unit capturing vocal vibrations on vocal\ncord. As schematically depicted in Fig. 3B, an electrical potential builds up\ndue to triboelectric series difference based on an electron affinity. Figure\n3C provides the schematic drawing showing hole pattern configuration applied\nin vocal vibration sensor to see how the pattern influence the output and SEM\nimages of the holes.\n\nFig. 3: Working mechanism and characterization of the vibration sensing unit.\n\nA Schematic illustration of the vibration sensing unit. Inset: enlarged view\nof the sensing unit detecting vocal-cord vibration. B Electrical potential\ndistribution of the sensing unit during working cycle. C Schematic of hole\npattern configuration applied in vocal vibration sensor and SEM images of the\nholes in 32-hole configuration. Scale bar: 2 mm (inset: magnified view showing\nan acoustic hole. Scale bar: 400 \u03bcm). D Frequency response data (V_oc as a\nfunction of acoustic frequency) for the vibration sensing unit with different\nopen ratios (ORs) of 5, 10 and 20. The vocal cord frequency ranges for male\nand female are colored blue and red, respectively. E Measured data plots of\noutput voltage signals per each different OR at the testing frequency of 100\nHz. F, G Effects of support thickness and number of holes on vibration\nsensitivity at working frequency of 100 Hz. For each graph, PDMS used as\ndiaphragm material, acoustic holes were patterned on the diaphragm, and the\nstructural parameters were fixed as follows unless otherwise specified:\ndiaphragm thickness of 50 \u03bcm, support thickness of 50 \u03bcm and an array of 32\nholes. The error bars indicate the s.d. of the normalized V_oc at the measured\nfrequency of 100 Hz. H Comparison of measured output voltage between the\nvibration sensing unit with and without holes.\n\nFull size image\n\nWe measured output voltage signals of the vibration sensing units with\ndifferent open ratios (ORs) considered the proportion of area perforated with\nacoustic holes in the whole area on the frequency response of the devices as\nshown in Fig. 3D. The frequency ranges we tested encompass the fundamental\nfrequency of typical adult men and women ranging from 100 to 150 Hz (Fig. 3D,\nblue) and from 200 to 250 Hz (Fig. 3D, red), respectively^71. The results\nindicate that the vibration sensing unit with OR value of 10 exhibited best\noutput voltage performance and the wideset bandwidth of flat frequency\nresponse. This experimental observation is originated from a trade-off between\nthe deflection of dielectric layer and the effective contact area. Larger OR\nleads to a larger deflection of the dielectric diaphragm and thus a higher\nelectric output. However, increased OR will reduce the effective contact area\nfor triboelectrification, and thus a lower electrical output. Accordingly, an\noptimized value of OR is needed for maximization of the electrical output.\nFigure 3E provides measured data plots of output voltage signals per each\ndifferent OR at the testing frequency of 100 Hz.\n\nAs shown in Fig. 3F and G, the output voltage of the vibration sensing unit\nwas affected by structural parameters such as the support thickness and number\nof holes. As the support thickness is increased, the gap between the\ntriboelectric layers is larger so that the effective contact area can be\nreduced thus the generated triboelectric output signals is decreased. On the\nother hand, the larger number of holes with the same OR condition makes the\ndiaphragms deflect more vigorously, thus enhancing the triboelectric output\nperformance. These experiments were carried out at the testing frequency of\n100 Hz. Lastly, as shown in Fig. 3H, we measured the output voltage between\nthe vibration sensing unit with and without holes as a function of input\nvibration acceleration in the ranging from 0.1 to 1.0 g at the same testing\nfrequency of 100 Hz. Both sensing units have a uniform sensitivity obtained\nfrom dividing the measured output voltage by the vibration acceleration. As\nfor the sensitivity, the hole-patterned vibration sensing units exhibits 5.78\nV/g around 2.8 times larger than that of the pristine vibration sensing unit.\n\n### Wireless data processing process and machine learning based real time\nclassification\n\nFigure 4A and B provides real images of the whole PSiFI mask and the\nparticipant wearing the PSiFI mask properly laminated onto the participant\u2019s\nface, which made it look transparent and comfortable enough to be worn for\nlong time and communicate well without interrupting expressions that can be\ncaused by a colored device. As schematically depicted in Fig. 4C, our wireless\ndata acquisition and transfer process was carried out from the data collection\nof the skin-integrated facial mask by the several centimeter size of circuit\nboard as a signal transmitter powered by a tiny portable battery to wirelessly\ntransmitted data received by the main board as the receiver connected to the\nlaptop for storing data used to be datasets for the machined learning.\n\nFig. 4: Real-time emotional speech acquisition.\n\nA Photograph showing the multimodality of the PSiFI attached to active units\nsuch as glabellar, eye, nose, lip, chin, and vocal cord for simultaneous\nverbal/non-verbal data collection. Scale bar: 2 cm. B Real images of front\n(top) and side view (bottom) of the participant wearing the PSiFI. C Schematic\ndiagrams of the wireless emotional speech classifying system including PSiFI,\nsignal processing board for wireless data transfer. D Facial strain and vocal\nvibration signals were collected from the skin-integrated interface. E The\nprocesses of learning algorithm architecture implemented in our classification\nsystem where machine learning methods such as data augmentation and transfer\nlearning were applied to efficiently reduce training time for the real-time\nclassification. F Comparison of confusion matrix (left) and captured images\n(right) in real-time classification between without and with an obstacle such\nas a mask.\n\nFull size image\n\nFigure 4D provides collected triboelectric signal patterns from each modal\nsensor such as lip, eye, glabella, nose, chin (for strain sensing unit) and\nvocal cord (for vibration sensing unit). As for the acquired signals from the\nstrain sensing units, distinct patterns were exhibited in accordance with the\ndifferent facial expressions such as happiness, surprise, disgust, anger and\nsadness that the participant expressed. As for the signals from the vocal\nsensing unit, each signals for different speech from the syllable such as \u201cA\u201d,\n\u201cB\u201d, \u201cC\u201d to the simple sentence such as \u201cI love you\u201d clearly exhibited its own\ndistinct patterns and were further transformed by fast Fourier transformation\n(FFT) which converts data from time domain to frequency domain to find\nremarkable patterns in frequency domain so that the pattern recognition\nperformed well. We conducted separate training for the vocal and strain\nsignals as the interdependence between verbal and non-verbal expressions\nappears to be relatively insignificant when compared to the distinct and\nconcurrent measurements of the multi-modal inputs (Supplementary Fig. 8).\n\nWhen it comes to machine learning, we applied the CNN algorithm as an example\nof algorithm for classification. Specifically, we utilized one-dimensional CNN\nto classify the facial expressions and two-dimensional CNN for speech\nclassification, respectively (Supplementary Fig. 9 and Table S2). Generally,\nthe more datasets our classifier trains, the better performance it shows.\nHowever, it is not viable and time consuming to test the sensor integrated\nwearable mask to many people in practical terms. The facial muscle movements,\nvocal cord vibration and sensor values corresponding to the verbal/non-verbal\nexpressions of the new users would be different from those of the previous\nusers since every human has its own characteristics. We therefore need to\nadapt to a network which can be trained with even small amounts of datasets\nand tuned with the new datasets from the new users.\n\nFigure 4E provides schematic diagrams showing the overall process from data\nachieving pre-trained model trained with enhanced accuracy by introducing data\naugmentation technique (Supplementary Fig. 10 and Table S3) to fine-tunned\nnetwork for personalization by exploiting pre-trained parameters called as\ntransfer learning, which enables the network to be trained in reduced time and\neffectively adapt to new user\u2019s datasets so that it made the real time\nclassification possible. In detail, a participant repeated, respectively,\nverbal and non-verbal expression 20 times to demonstrate reliability for a\ntotal acquisition of 100 recognition signal patterns per each expression. 70\npatterns of total were randomly selected from the acquired signals to serve as\nthe training set which are subsequently augmented 8-fold based on different\nmethods (Jittering, Scaling, Time-warping, Magnitude-warping) for effective\nlearning, and the remaining 30 signals were assigned as the test set.\nFurthermore, according to the previous report, it was found that the movement\nand activation patterns of facial muscles during facial expressions was not\ndissimilar depending on the individuals^62,63,64. Based on this fact, we\nanticipate that the network can get used to adapt to new expressions from new\nusers by rapidly training the corresponding learning data. As for the transfer\nlearning, after the initial participant had firstly trained with the\nclassifier by the above-mentioned training method, the following participants\nwere wearing with the PSiFI device and able to fast train with the classifier\nby only repeating 10 times each on both expressions, which successfully allow\nthe real-time classification to be demonstrated. When it comes to practical\napplication, compared with other classification methods based on various kinds\nof video camera and microphone, our PSiFI mask is free from environmental\nrestrictions such as the location, obstruction, and time. As shown in Fig. 4F,\nthe real-time classification result for combined verbal/nonverbal expressions\nwithout any restriction exhibited very high accuracy of 93.3% and even the\ndecent accuracy of 80.0% was achieved despite carrying out the classification\nwith obstruction such as wearing a facial mask (Supplementary Movie 3).\n\n### Digital concierge application in VR environment\n\nAs for the application with the PSiFI, we brought in VR environment which\nallows individuals to experiment with how their emotions could influence and\ncan be expressed and implemented into specific situations in the virtual\nworld^72,73,74. This in turn can deepen communications in VR environment by\nengaging with human emotions. In this sense, we selected digital concierge\napplication that can be enriched with emotional information in terms of\npractical use and usability. The digital concierge is likely to be anticipated\nthat it can provide user-oriented services which improve quality of user\u2019s\nlife by promoting user\u2019s experience. Herein, for the first time, we\ndemonstrated the application which offers a digital concierge service operated\nwith our PSiFI based on HMI in VR environment of Unity software as shown in\nFig. 5.\n\nFig. 5: The demonstration for digital concierge based on the emotional speech\nclassifying system in VR environment.\n\nA Conceptual illustration of human machine interaction with personalized\nemotional context achieved by wearing user\u2019s PSiFI. B Schematic diagram of the\nway the user interacts with the digital concierge providing various helpful\nservices. C The corresponding captured images of three different scenarios as\ntasks (such as mood interactive feedback, automatic keyword search and user-\nfriendly advertising) of digital concierge which likely take place in various\nplaces such as home, office and theater in VR environment of Unity software.\n\nFull size image\n\nFigure 5A provides conceptual schematic showing how human and machine can\ninteract smartly with personalized emotional context by wearing the PSiFI. To\nrealize this, we demonstrate VR-based digital concierge application via HMI\nwith our PSiFI as the overall process was shown in Fig. 5B. Specifically, the\ndigital concierge system was operated based on conversation between the user\u2019s\navatar and randomly generated avatar who serves as the virtual concierge.\nAdditionally, we built the digital concierge to provide various application\nservices from smart home to entertainment by taking into account the\nsituations which take place very probably in real life.\n\nFigure 5C provides three different scenarios demonstrating smart home, office,\nand entertainment application in Unity space (Supplementary Movie 4; for\ndetails, see the \u201cMethods\u201d secton). As for the first scenario for smart home\napplication, the digital concierge accessed the user\u2019s mood of sadness and\nrecommend some playlist from website to relieve the mood despite of user\u2019s\nsimple word. As for the second scenario for office application, the digital\nconcierge was able to check if the user understands contents of presentation\nand pop out new window showing content interpretation that helps to promote\nuser\u2019s understanding. As for the last scenario for entertainment application,\nthe digital concierge identifies user\u2019s reaction to the movie trailer and\ncurates user-friendly contents in accordance with user\u2019s reaction. The\napplications with our PSiFI-based HMI and built-in VR space can be greatly\ndiversified with learning and adapting new data regarding verbal and non-\nverbal expressions from new users so that we strongly anticipate our highly\npersonalized PSiFI platform contributes to various practical applications such\nas education, marketing, and advertisements that can be enriched with\nemotional information.\n\n## Discussion\n\nIn this work, we proposed a machine-learning assisted PSiFI for wearable human\nemotion recognition system. The PSiFI was made of PDMS-based dielectric and\nstretchable conductor layers that are highly transparent and comfortable as\npossible to wear in real life. By endowing our PSiFI with multi-modality to\ndetect simultaneously both facial and vocal expressions using self-powered\ntriboelectric-based sensing units, we can acquire better emotional information\nregardless of external factors such as time, place, and obstacles.\nFurthermore, we realized wireless data communication for real-time human\nemotion recognition with the help of designed data-processing circuit unit and\nthe rapid adapting learning model and achieved acceptable standard in terms of\ntest accuracy even with the barrier such as mask. Finally, we first\ndemonstrated digital concierge application in VR environment capable of\nresponding to user\u2019s intention based on the user\u2019s emotional speech\ninformation. We believe that the PSiFI could assist and accelerate the active\nusage of emotions for digital transformation in the near future.\n\n## Methods\n\n### Materials\n\nPDMS was purchased from Dow corning which consists of elastomer base and\ncuring set (10:1 wt/wt). Aqueous dispersions of PEDOT:PSS solution (>3%),\nethylene glycol (99.8%), and Au nanoparticles (Au NPs) (~100 nm) dispersion in\ndeionized water (DI) was purchased from Sigma-Aldrich. Acetone (99.5%) and\nisopropyl alcohol (IPA) (99.5%) were purchased from Samchun Chemical.\n\n### Preparation of conductive dispersion and stretchable conductor\n\nAn aqueous solution of PEDOT:PSS was firstly filtered through a 0.45 mm nylon\nsyringe filter. Next, 5 wt% DMSO was added to the solution, and it was then\nmixed with 50 wt% IPA solvent by vigorously stirring at room temperature for\nhalf an hour. Subsequently, the base monomer and curing agent were mixed with\na weight ratio of 10:1 at room temperature and then, placed into the vacuum\ndesiccator to degas the PDMS mixture. After 40 min, 1 mL of mixture was spread\nin the form of a continuous layer onto the cleaned Kapton film as a substrate\nusing a micrometer adjustable film applicator, and allowed to solidify into an\namorphous free-standing film by heating on an oven at 90 \u00b0C for 5 min. The\nprepared conductive dispersion was subsequently coated on the PDMS to anchor\nthe conductive polymers within the PDMS matrix before the film is fully\nsolidified.\n\n### Fabrication of nanowire-based surface modification of dielectric film\n\nNanowires on the surface of the PDMS film were formed by using inductively\ncoupled plasma (ICP) reactive ion etching. The dielectric films with a\nthickness of 50 \u03bcm were first cleaned subsequently by Acetone, IPA and DI,\nthen blown dry with nitrogen gas. In the etching process, Au NPs were prepared\nby vortex mixer for homogeneous distribution and deposited by drop-casting.\nAfter 30 min of drying in oven at 80 \u00b0C, the Au NPs were coated on the\ndielectric surface as a nano-patterned mask. Subsequently, a mixed gas\nincluding Ar, O_2, and CF_4 was introduced in the ICP chamber, with a\ncorresponding flow rate of 15.0, 10.0, and 30.0 sccm, respectively. The\ndielectric films were etched for 300 s to obtain a nanowire structure on the\nsurface. One power source of 400 W was used to yield a large density of\nplasma, while another 100 W was used to accelerate the plasma ions.\n\n### Fabrication of hole-patterned dielectric films\n\nArrays of circular acoustic holes with various shapes and distributions were\nfabricated and punched through the PDMS film (thickness 100 \u03bcm) using laser-\ncutting technology (Universal Laser Systems Inc.). The diameter of the\nsmallest hole is 500 \u03bcm, which is close to the line-width limitation of the\nlaser cutting on a plate surface.\n\n### Fabrication of self-powered sensing units\n\nAs for the strain sensing unit, the prepared stretchable conductor was cut in\nthe size of 1 cm \u00d7 1 cm. Next, a flat flexible cable (FFC) was attached with\nthe double-sided medical silicone tape (3M 2476P, 3M Co., Ltd) for electrical\nconnection (Supplementary Fig. 11). Then, the surface modified dielectric film\n(thickness 50 \u03bcm) was subsequently placed on the layer and used as space-\ncharge carrying layer.\n\nAs for the vibration sensing unit, the prepared stretchable conductor was cut\nin the size of 1 cm \u00d7 1 cm. Next, the FFC was attached with the double-sided\nmedical tape for electrical connection like in the strain sensing unit. Then,\nthe 50 \u03bcm-thick surface modified and hole patterned PDMS film as dielectric\nlayer was sequentially applied on the layer and used as diaphragm deflecting\nwith the vocal vibration.\n\n### Characterization and measurement\n\nThe morphologies and thickness of the PEDOT:PSS embedded stretchable conductor\nand the nano-patterned dielectrics were investigated by using a Nano 230\nfield-emission scanning electron microscope (FEI, USA) at an accelerating\nvoltage of 10 kV. Optical transmission measurements of the stretchable\nconductors were performed on ultraviolet\u2013visible spectrophotometer (Cary 5000,\nAgilent) from 400 to 800 nm. The sheet resistances (Rs) of the stretchable\nconductors were measured using the four-point van der Pauw method with\ncollinear probes (0.5 cm spacing) connected to a four-point probing system\n(CMT2000N, AIT). For the electrical measurement of the strain sensor unit, an\nexternal shear force was applied by a commercial linear mechanical motor\n(X-LSM 100b, Zaber Technologies) and a programmable electrometer (Keithley\nmodel 6514) was used to measure the open-circuit voltage and short-circuit\ncurrent. For the vibration sensor unit, a Digital Phosphor Oscilloscope (DPO\n3052, Tektronix) was used to measure the electrical output signals at the\nsampling rate of 2.5 GS/s. For the multi-channel sensing system, a DAQ system\n(PCIe-6351, NI) was used to simultaneously measure electrical output signals\nof multi-channel sensor units.\n\n### Attachment of the device on the skin\n\nTo mount the sensor device completely onto the facial and neck skin, we\napplied a bio-compatible, ultrathin, and transparent medical tape (Tegaderm TM\nFilm 1622W, 3M) over the edge of the sensor and the metal lines connected to\nthe interface circuit. The medical tape is developed and widely utilized for\nskin-friendly adhesive solution. Therefore, there was no skin irritation or\nitch during several hours of wearing. The test was exempted from IRB in\naccordance with the approval by UNIST IRB Committee. The authors affirm that\nhuman research participants provided informed consent prior to inclusion in\nthis study and for publication of the images in Figs. 4 and 5.\n\n### Machine learning for emotion recognition\n\nFor the pre-training, a total acquisition of 100 recognition signal patterns\nper each expression were collected from a participant repeating 20 times each\non both verbal and non-verbal expressions, respectively. 70 patterns of total\nwere randomly selected as training set, further augmented 8-fold based on\ndifferent augmentation methods (Jittering, Scaling, Time-warping, Magnitude-\nwarping), and the remaining 30 signals were assigned as the test set. After\npre-processing step for the datasets such as trimming in accordance with input\nsize of the neural network and converting to image by FFT, the 1D-CNN and\n2D-CNN were applied for non-verbal expression and verbal-expression training.\nWith this pre-trained classifier, a new user can rapidly customize the\nclassifier with its own data by repeating 10 times each on both expressions,\nknown as transfer learning, the real-time classification was successfully\ndemonstrated.\n\n### Demonstration of the application\n\nThe three-dimensional (3D) VR environment that the user saw was provided by\nUnity3D on a computer, the facial strain and vocal vibration sensing data were\nsent to Unity3D through wireless serial communication from Buleinno2, and the\ninteraction between PSiFI and the computer was done by PySerial package in\npython. We built VR-based digital concierge scenario comprising of\nenvironmental assets and generated avatars as follows. The virtual\nenvironments assets such as home, office, and theater were downloaded at Unity\nAsset Store. The avatars used in the VR environments were simply created from\nindividual photo using readyplayer.me website. In demonstration, the generated\navatar proceeded the scenario based on the real-time information transmitted\nfrom PSiFI and got adaptive responses from the avatar called MIBOT virtually\ncreated for digital concierge.\n\n### Reporting summary\n\nFurther information on research design is available in the Nature Portfolio\nReporting Summary linked to this article.\n\n## Data availability\n\nThe data that support the plots within this paper and other finding of the\nstudy are present in the paper and/or the Supplementary Information. The\noriginal datasets for human emotion recognition are available from\nhttps://github.com/MATTER-INTEL-LAB/PSIFI.git.\n\n## Code availability\n\nAll codes used for implementation of the data augmentation and classification\nare available from https://github.com/MATTER-INTEL-LAB/PSIFI.git.\n\n## References\n\n  1. Rahman, M. M., Poddar, A., Alam, M. G. R. & Dey, S. K. Affective state recognition through EEG signals feature level fusion and ensemble classifier. Preprint at https://doi.org/10.48550/arXiv.2102.07127 (2021).\n\n  2. Niklander, S. & Niklander, G. Combining sentimental and content analysis for recognizing and interpreting human affects. in HCI International 2017\u2014Posters\u2019 Extended Abstracts (ed. Stephanidis, C.) 465\u2013468 (Springer International Publishing, 2017).\n\n  3. Torres, E. P., Torres, E. A., Hern\u00e1ndez-\u00c1lvarez, M., Yoo, S. G. & EEG-Based, B. C. I. Emotion recognition: a survey. Sensors 20, 5083 (2020).\n\nArticle ADS PubMed PubMed Central Google Scholar\n\n  4. Palaniswamy, S. & Suchitra, A. Robust pose & illumination invariant emotion recognition from facial images using deep learning for human\u2013machine interface. In 2019 4th International Conference on Computational Systems and Information Technology for Sustainable Solution (CSITSS) 1\u20136 (2019).\n\n  5. Thirunavukkarasu, G. S., Abdi, H. & Mohajer, N. A smart HMI for driving safety using emotion prediction of EEG signals. In 2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC) 004148\u2013004153 (2016).\n\n  6. Huo, F., Zhao, Y., Chai, C. & Fang, F. A user experience map design method based on emotional quantification of in-vehicle HMI. Humanit. Sci. Soc. Commun. 10, 1\u201310 (2023).\n\nArticle Google Scholar\n\n  7. Breazeal, C. Emotion and sociable humanoid robots. Int. J. Hum.\u2013Comput. Stud. 59, 119\u2013155 (2003).\n\nArticle Google Scholar\n\n  8. Stock-Homburg, R. Survey of emotions in human\u2013robot interactions: perspectives from robotic psychology on 20 years of research. Int. J. Soc. Robot. 14, 389\u2013411 (2022).\n\nArticle Google Scholar\n\n  9. Chuah, S. H.-W. & Yu, J. The future of service: The power of emotion in human-robot interaction. J. Retail. Consum. Serv. 61, 102551 (2021).\n\nArticle Google Scholar\n\n  10. Consoli, D. A new concept of marketing: the emotional marketing. BRAND Broad Res. Account. Negot. Distrib. 1, 52\u201359 (2010).\n\nGoogle Scholar\n\n  11. Bagozzi, R. P., Gopinath, M. & Nyer, P. U. The role. Emot. Mark. J. Acad. Mark. Sci. 27, 184\u2013206 (1999).\n\nArticle Google Scholar\n\n  12. Yung, R., Khoo-Lattimore, C. & Potter, L. E. Virtual reality and tourism marketing: conceptualizing a framework on presence, emotion, and intention. Curr. Issues Tour. 24, 1505\u20131525 (2021).\n\nArticle Google Scholar\n\n  13. Hasnul, M. A., Aziz, N. A. A., Alelyani, S., Mohana, M. & Aziz, A. A. Electrocardiogram-based emotion recognition systems and their applications in healthcare\u2014a review. Sensors 21, 5015 (2021).\n\nArticle ADS PubMed PubMed Central Google Scholar\n\n  14. Dhuheir, M. et al. Emotion recognition for healthcare surveillance systems using neural networks: a survey. Preprint at https://doi.org/10.48550/arXiv.2107.05989 (2021).\n\n  15. Jim\u00e9nez-Herrera, M. F. et al. Emotions and feelings in critical and emergency caring situations: a qualitative study. BMC Nurs. 19, 60 (2020).\n\nArticle PubMed PubMed Central Google Scholar\n\n  16. Schutz, P. A., Hong, J. Y., Cross, D. I. & Osbon, J. N. Reflections on investigating emotion in educational activity settings. Educ. Psychol. Rev. 18, 343\u2013360 (2006).\n\nArticle Google Scholar\n\n  17. Tyng, C. M., Amin, H. U., Saad, M. N. M. & Malik, A. S. The influences of emotion on learning and memory. Front. Psychol. 8, 1454 (2017).\n\nArticle PubMed PubMed Central Google Scholar\n\n  18. Li, L., Gow, A. D. I. & Zhou, J. The role of positive emotions in education: a neuroscience perspective. Mind Brain Educ. 14, 220\u2013234 (2020).\n\nArticle Google Scholar\n\n  19. Ben-Ze\u2019Ev, A. The Subtlety of Emotions (MIT Press, 2001).\n\n  20. Lane, R. D. & Pollermann, B. Z. Complexity of emotion representations. in The Wisdom in Feeling: Psychological Processes in Emotional Intelligence 271\u2013293 (The Guilford Press, 2002).\n\n  21. Boehner, K., DePaula, R., Dourish, P. & Sengers, P. How emotion is made and measured. Int. J. Hum.\u2013Comput. Stud. 65, 275\u2013291 (2007).\n\nArticle Google Scholar\n\n  22. Mauss, I. B. & Robinson, M. D. Measures of emotion: a review. Cogn. Emot. 23, 209\u2013237 (2009).\n\nArticle PubMed PubMed Central Google Scholar\n\n  23. Meiselman, H. L. Emotion Measurement (Woodhead Publishing, 2016).\n\n  24. Ioannou, S. V. et al. Emotion recognition through facial expression analysis based on a neurofuzzy network. Neural Netw. 18, 423\u2013435 (2005).\n\nArticle PubMed Google Scholar\n\n  25. Tarnowski, P., Ko\u0142odziej, M., Majkowski, A. & Rak, R. J. Emotion recognition using facial expressions. Procedia Comput. Sci. 108, 1175\u20131184 (2017).\n\n  26. Abdat, F., Maaoui, C. & Pruski, A. Human\u2013computer interaction using emotion recognition from facial expression. In 2011 UKSim 5th European Symposium on Computer Modeling and Simulation (ed Sterritt, R.) 196\u2013201 (IEEE computer society, 2011).\n\n  27. Ak\u00e7ay, M. B. & O\u011fuz, K. Speech emotion recognition: emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers. Speech Commun. 116, 56\u201376 (2020).\n\nArticle Google Scholar\n\n  28. Issa, D., Fatih Demirci, M. & Yazici, A. Speech emotion recognition with deep convolutional neural networks. Biomed. Signal Process. Control 59, 101894 (2020).\n\nArticle Google Scholar\n\n  29. Lech, M., Stolar, M., Best, C. & Bolia, R. Real-time speech emotion recognition using a pre-trained image classification network: effects of bandwidth reduction and companding. Front. Comput. Sci. 2, 14 (2020).\n\n  30. Nandwani, P. & Verma, R. A review on sentiment analysis and emotion detection from text. Soc. Netw. Anal. Min. 11, 81 (2021).\n\nArticle PubMed PubMed Central Google Scholar\n\n  31. Acheampong, F. A., Wenyu, C. & Nunoo-Mensah, H. Text-based emotion detection: advances, challenges, and opportunities. Eng. Rep. 2, e12189 (2020).\n\nArticle Google Scholar\n\n  32. Alm, C. O., Roth, D. & Sproat, R. Emotions from text: machine learning for text-based emotion prediction. In Proc. Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing 579\u2013586 (Association for Computational Linguistics, 2005).\n\n  33. Murugappan, M., Ramachandran, N. & Sazali, Y. Classification of human emotion from EEG using discrete wavelet transform. J. Biomedical Science and Engineering 3, 390\u2013396 (2010).\n\n  34. Gannouni, S., Aledaily, A., Belwafi, K. & Aboalsamh, H. Emotion detection using electroencephalography signals and a zero-time windowing-based epoch estimation and relevant electrode identification. Sci. Rep. 11, 7071 (2021).\n\n  35. Jenke, R., Peer, A. & Buss, M. Feature Extraction and Selection for Emotion Recognition from EEG. IEEE Trans. Affect. Comput. 5, 327\u2013339 (2014).\n\n  36. Balconi, M., Bortolotti, A. & Gonzaga, L. Emotional face recognition, EMG response, and medial prefrontal activity in empathic behaviour. Neurosci. Res. 71, 251\u2013259 (2011).\n\nArticle PubMed Google Scholar\n\n  37. K\u00fcnecke, J., Hildebrandt, A., Recio, G., Sommer, W. & Wilhelm, O. Facial EMG responses to emotional expressions are related to emotion perception ability. PLoS ONE 9, e84053 (2014).\n\nArticle ADS PubMed PubMed Central Google Scholar\n\n  38. Kulke, L., Feyerabend, D. & Schacht, A. A comparison of the affectiva imotions facial expression analysis software with EMG for identifying facial expressions of emotion. Front. Psychol. 11, 329 (2020).\n\nArticle PubMed PubMed Central Google Scholar\n\n  39. Br\u00e1s, S., Ferreira, J. H. T., Soares, S. C. & Pinho, A. J. Biometric and emotion identification: an ECG compression based method. Front. Psychol. 9, 467 (2018).\n\nArticle PubMed PubMed Central Google Scholar\n\n  40. Selvaraj, J., Murugappan, M., Wan, K. & Yaacob, S. Classification of emotional states from electrocardiogram signals: a non-linear approach based on hurst. Biomed. Eng. OnLine 12, 44 (2013).\n\nArticle PubMed PubMed Central Google Scholar\n\n  41. Agrafioti, F., Hatzinakos, D. & Anderson, A. K. ECG pattern analysis for emotion detection. IEEE Trans. Affect. Comput. 3, 102\u2013115 (2012).\n\nArticle Google Scholar\n\n  42. Goshvarpour, A., Abbasi, A. & Goshvarpour, A. An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomed. J. 40, 355\u2013368 (2017).\n\nArticle PubMed Google Scholar\n\n  43. Dutta, S., Mishra, B. K., Mitra, A. & Chakraborty, A. An analysis of emotion recognition based on GSR signal. ECS Trans. 107, 12535 (2022).\n\nArticle ADS Google Scholar\n\n  44. Wu, G., Liu, G. & Hao, M. The analysis of emotion recognition from GSR based on PSO. In 2010 International Symposium on Intelligence Information Processing and Trusted Computing. (ed Sterritt, R.) 360\u2013363 (IEEE computer society, 2010).\n\n  45. Wang, Y. et al. A durable nanomesh on-skin strain gauge for natural skin motion monitoring with minimum mechanical constraints. Sci. Adv. 6, eabb7043 (2020).\n\nArticle ADS CAS PubMed PubMed Central Google Scholar\n\n  46. Roh, E., Hwang, B.-U., Kim, D., Kim, B.-Y. & Lee, N.-E. Stretchable, transparent, ultrasensitive, and patchable strain sensor for human\u2013machine interfaces comprising a nanohybrid of carbon nanotubes and conductive elastomers. ACS Nano 9, 6252\u20136261 (2015).\n\nArticle CAS PubMed Google Scholar\n\n  47. Su, M. et al. Nanoparticle based curve arrays for multirecognition flexible electronics. Adv. Mater. 28, 1369\u20131374 (2016).\n\nArticle ADS CAS PubMed Google Scholar\n\n  48. Yoon, S., Sim, J. K. & Cho, Y.-H. A flexible and wearable human stress monitoring patch. Sci. Rep. 6, 23468 (2016).\n\nArticle ADS CAS PubMed PubMed Central Google Scholar\n\n  49. Jeong, Y. R. et al. A skin-attachable, stretchable integrated system based on liquid GaInSn for wireless human motion monitoring with multi-site sensing capabilities. NPG Asia Mater. 9, e443\u2013e443 (2017).\n\nArticle CAS Google Scholar\n\n  50. Hua, Q. et al. Skin-inspired highly stretchable and conformable matrix networks for multifunctional sensing. Nat. Commun. 9, 244 (2018).\n\nArticle ADS PubMed PubMed Central Google Scholar\n\n  51. Ramli, N. A., Nordin, A. N. & Zainul Azlan, N. Development of low cost screen-printed piezoresistive strain sensor for facial expressions recognition systems. Microelectron. Eng. 234, 111440 (2020).\n\nArticle CAS Google Scholar\n\n  52. Sun, T. et al. Decoding of facial strains via conformable piezoelectric interfaces. Nat. Biomed. Eng. 4, 954\u2013972 (2020).\n\nArticle PubMed Google Scholar\n\n  53. Wang, M. et al. Gesture recognition using a bioinspired learning architecture that integrates visual data with somatosensory data from stretchable sensors. Nat. Electron. 3, 563\u2013570 (2020).\n\nArticle Google Scholar\n\n  54. Zhou, Z. et al. Sign-to-speech translation using machine-learning-assisted stretchable sensor arrays. Nat. Electron. 3, 571\u2013578 (2020).\n\nArticle Google Scholar\n\n  55. Wang, Y. et al. All-weather, natural silent speech recognition via machine-learning-assisted tattoo-like electronics. Npj Flex. Electron. 5, 20 (2021).\n\nArticle Google Scholar\n\n  56. Zhuang, M. et al. Highly robust and wearable facial expression recognition via deep-learning-assisted, soft epidermal electronics. Research 2021, 2021/9759601 (2021).\n\nArticle PubMed Google Scholar\n\n  57. Zheng, W.-L., Dong, B.-N. & Lu, B.-L. Multimodal emotion recognition using EEG and eye tracking data. In 2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, (ed Melley, D.) 5040\u20135043 (IEEE express conference publishing, Chicago, IL, USA, 2014).\n\n  58. Schirmer, A. & Adolphs, R. Emotion perception from face, voice, and touch: comparisons and convergence. Trends Cogn. Sci. 21, 216\u2013228 (2017).\n\nArticle PubMed PubMed Central Google Scholar\n\n  59. Ahmed, N., Aghbari, Z. A. & Girija, S. A systematic survey on multimodal emotion recognition using learning algorithms. Intell. Syst. Appl. 17, 200171 (2023).\n\nGoogle Scholar\n\n  60. Zhang, R. & Olin, H. Material choices for triboelectric nanogenerators: a critical review. EcoMat 2, e12062 (2020).\n\nArticle CAS Google Scholar\n\n  61. Kim, W.-G. et al. Triboelectric nanogenerator: structure, mechanism, and applications. ACS Nano 15, 258\u2013287 (2021).\n\nArticle CAS PubMed Google Scholar\n\n  62. Schumann, N. P., Bongers, K., Guntinas-Lichius, O. & Scholle, H. C. Facial muscle activation patterns in healthy male humans: a multi-channel surface EMG study. J. Neurosci. Methods 187, 120\u2013128 (2010).\n\nArticle PubMed Google Scholar\n\n  63. Lee, J.-G. et al. Quantitative anatomical analysis of facial expression using a 3D motion capture system: application to cosmetic surgery and facial recognition technology: quantitative anatomical analysis of facial expression. Clin. Anat. 28, 735\u2013744 (2015).\n\nArticle PubMed Google Scholar\n\n  64. Zarins, U. Anatomy of Facial Expression (Exonicus Incorporated, 2018).\n\n  65. Kim, K. N. et al. Surface dipole enhanced instantaneous charge pair generation in triboelectric nanogenerator. Nano Energy 26, 360\u2013370 (2016).\n\nArticle CAS Google Scholar\n\n  66. Lee, J. P. et al. Boosting the energy conversion efficiency of a combined triboelectric nanogenerator-capacitor. Nano Energy 56, 571\u2013580 (2019).\n\nArticle CAS Google Scholar\n\n  67. Lu, Y. et al. Decoding lip language using triboelectric sensors with deep learning. Nat. Commun. 13, 1401 (2022).\n\nArticle ADS CAS PubMed PubMed Central Google Scholar\n\n  68. Yang, J. et al. Triboelectrification-based organic film nanogenerator for acoustic energy harvesting and self-powered active acoustic sensing. ACS Nano 8, 2649\u20132657 (2014).\n\nArticle CAS PubMed Google Scholar\n\n  69. Yang, J. et al. Eardrum-inspired active sensors for self-powered cardiovascular system characterization and throat-attached anti-interference voice recognition. Adv. Mater. 27, 1316\u20131326 (2015).\n\nArticle CAS PubMed Google Scholar\n\n  70. Lee, S. et al. An ultrathin conformable vibration-responsive electronic skin for quantitative vocal recognition. Nat. Commun. 10, 2468 (2019).\n\nArticle ADS PubMed PubMed Central Google Scholar\n\n  71. Calvert, D. R. Clinical measurement of speech and voice, by Ronald J. Baken, PhD, 528 pp, paper, College-Hill Press, Boston, MA, 1987, $35.00. Laryngoscope 98, 905\u2013906 (1988).\n\nArticle Google Scholar\n\n  72. Diemer, J., Alpers, G. W., Peperkorn, H. M., Shiban, Y. & M\u00fchlberger, A. The impact of perception and presence on emotional reactions: a review of research in virtual reality. Front. Psychol. 6, 26 (2015).\n\n  73. Allcoat, D. & M\u00fchlenen, A. von. Learning in virtual reality: Effects on performance, emotion and engagement. Res. Learn. Technol. 26, 2140 (2018).\n\n  74. Colombo, D., D\u00edaz-Garc\u00eda, A., Fernandez-\u00c1lvarez, J. & Botella, C. Virtual reality for the enhancement of emotion regulation. Clin. Psychol. Psychother. 28, 519\u2013537 (2021).\n\nArticle PubMed Google Scholar\n\nDownload references\n\n## Acknowledgements\n\nThis work was supported by National Research Foundation of Korea (NRF) grants\nfunded by the Korean government, NRF-2020R1A2C2102842, NRF-2021R1A4A3033149,\nNRF-RS-2023-00302525, the Fundamental Research Program of the Korea Institute\nof Material Science, PNK7630 and Korea Institute for Advancement of Technology\n(KIAT) grant funded by the Korea Government (MOTIE) (P0023703, HRD Program for\nIndustrial Innovation).\n\n## Author information\n\n### Authors and Affiliations\n\n  1. School of Material Science and Engineering, Ulsan National Institute of Science and Technology, Ulsan, 44919, South Korea\n\nJin Pyo Lee, Hanhyeok Jang, Yeonwoo Jang, Hyeonseo Song, Suwoo Lee & Jiyun Kim\n\n  2. School of Materials Science and Engineering, Nanyang Technological University, 50 Nanyang Avenue, Singapore, 639798, Singapore\n\nJin Pyo Lee & Pooi See Lee\n\n  3. Center for Multidimensional Programmable Matter, Ulsan National Institute of Science and Technology, Ulsan, 44919, South Korea\n\nJiyun Kim\n\nAuthors\n\n  1. Jin Pyo Lee\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  2. Hanhyeok Jang\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  3. Yeonwoo Jang\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  4. Hyeonseo Song\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  5. Suwoo Lee\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  6. Pooi See Lee\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  7. Jiyun Kim\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n### Contributions\n\nJ.P.L. carried out and designed most of the experimental work and data\nanalysis. H.J., H.S., and S.L. assisted in the materials processing and device\nfabrication. Y.J. assisted in the machine learning algorithms and analysis of\nthe results. P.S.L. revised and improved the manuscript with technical\ncomments. J.K. supervised the whole project and was the lead contact. All\nauthors discussed and wrote and commented on the manuscript.\n\n### Corresponding authors\n\nCorrespondence to Pooi See Lee or Jiyun Kim.\n\n## Ethics declarations\n\n### Competing interests\n\nThe authors declare no competing interests.\n\n## Peer review\n\n### Peer review information\n\nNature Communications thanks Canan Dagdeviren, YongAn Huang and the other,\nanonymous, reviewer(s) for their contribution to the peer review of this work.\nA peer review file is available.\n\n## Additional information\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional affiliations.\n\n## Supplementary information\n\n### Supplementary Information\n\n### Peer Review File\n\n### Description of Additional Supplementary Files\n\n### Supplementary Movie 1\n\n### Supplementary Movie 2\n\n### Supplementary Movie 3\n\n### Supplementary Movie 4\n\n### Reporting Summary\n\n## Rights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0\nInternational License, which permits use, sharing, adaptation, distribution\nand reproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons license, and indicate if changes were made. The images or\nother third party material in this article are included in the article\u2019s\nCreative Commons license, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article\u2019s Creative Commons\nlicense and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\n## About this article\n\n### Cite this article\n\nLee, J.P., Jang, H., Jang, Y. et al. Encoding of multi-modal emotional\ninformation via personalized skin-integrated wireless facial interface. Nat\nCommun 15, 530 (2024). https://doi.org/10.1038/s41467-023-44673-2\n\nDownload citation\n\n  * Received: 30 August 2023\n\n  * Accepted: 28 December 2023\n\n  * Published: 15 January 2024\n\n  * DOI: https://doi.org/10.1038/s41467-023-44673-2\n\n### Share this article\n\nAnyone you share the following link with will be able to read this content:\n\nSorry, a shareable link is not currently available for this article.\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\n### Subjects\n\n  * Information technology\n  * Polymers\n  * Sensors and biosensors\n\n## Comments\n\nBy submitting a comment you agree to abide by our Terms and Community\nGuidelines. If you find something abusive or that does not comply with our\nterms or guidelines please flag it as inappropriate.\n\nDownload PDF\n\nAdvertisement\n\nNature Communications (Nat Commun) ISSN 2041-1723 (online)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n  * Cancel contracts here\n\n\u00a9 2024 Springer Nature Limited\n\nSign up for the Nature Briefing newsletter \u2014 what matters in science, free to\nyour inbox daily.\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing\n\n", "frontpage": false}
