{"aid": "40011184", "title": "From the 80s to 2024 \u2013 how CI tests were invented and optimized", "url": "https://graphite.dev/blog/invention-of-modern-ci", "domain": "graphite.dev", "votes": 1, "user": "thunderbong", "posted_at": "2024-04-12 10:50:19", "comments": 0, "source_title": "From the 80's to 2024 - how CI tests were invented and optimized", "source_text": "From the 80's to 2024 - how CI tests were invented and optimized\n\nHomeFeaturesPricingDocsBlogLog inSign up\n\n\u00a9 2024 Graphite, All rights reserved.\n\n#Engineering\n\n# From the 80's to 2024 - how CI tests were invented and optimized\n\nApril 11, 2024\n\nWritten by\n\nGreg Foster\n\nIn this post\n\nThe slowest way to test code - by reading\n\nTesting it yourself, sometimes\n\nRun an automated server to test the changes\n\nPay someone else to test your changes automatically\n\nMaximizing how fast a single change can be tested\n\nSpeeding up how fast all changes can be tested\n\nTesting faster than running code\n\nStay unblocked. Ship faster.\n\nExperience the new developer workflow - create, review, and merge code\ncontinuously. Get started with one command.\n\nGet started\n\nModern dev teams test every code change before merging. This isn\u2019t just a\nconvention; right along with code review, it\u2019s a default standard enforced\nacross almost all company codebases, We call this \u201ccontinuous integration\ntests,\u201d and the net result is that the average organization runs hundreds of\ntest suites a day.\n\nLooking back in time, continuous integration testing hasn\u2019t been around\nforever - but testing software roughly has. From what I can tell, CI is a\nresult of testing getting faster and faster over time. How did things get this\nway, and where can future test speedups be found?\n\n## The slowest way to test code - by reading\n\nSoftware testing was slow in the 1980s. Much of the testing was focused on\nfinding possible errors in the code above all else. Michael Fagan popularized\n\u201cFagan Inspections,\u201d which involved groups of engineers pouring over code\nprintouts looking for mistakes. It was time-intensive, manual, and looked more\nlike intensive code review than what we think of as software testing today.\n\nIn the 1990s, software unit tests became more prevalent. But for a time, unit\ntests were predominantly written by specialized software testers in the\ncompany using custom tooling and practices. Some thought that the original\ncode authors might have blind spots in testing their own code\u2014and to be fair,\nwe still don\u2019t trust engineers to review their own code changes for similar\nreasons.\n\nTests were run infrequently for two reasons at this point: they were not\nalways written by the software authors themselves, and they could be slow to\nexecute. Depending on the complexity of the tests, combined with the slower\ncomputers of the time, test suites could take hours or even a full day to\ncomplete. If a separate engineer was needed to write tests for your code, and\nthe test suite didn\u2019t run until the following evening, it might be days before\nan engineer got feedback on why their change broke builds.\n\n## Testing it yourself, sometimes\n\nKent Beck\u2019s 1999 book Extreme Programming (XP) helped usher in a cultural\nchange. Engineers were encouraged to write small isolated tests for each new\npiece of code they contributed. \u201cThe XPer view was that programmers could\nlearn to be effective testers, at least at the unit level, and that if you\ninvolved a separate group, the feedback loop that tests gave you would be\nhopelessly slow. Xunit played an essential role here; it was designed\nspecifically to minimize the friction for programmers writing tests.\u201d\n\nBy having code authors write their own tests, there was a chance that new code\ncould be tested more frequently than at the point of integration. The faster\ntesting led to shorter feedback cycle times for developers. But self-testing\nwas an opt-in process, relying on authors to diligently run local tests before\nmerging. What's more, the test\u2019s success was dependent on the author\u2019s local\ncomputer running it rather than a source-of-truth server. Codebases were still\nat risk of breaking the next time a build was cut and test suites executed.\n\n## Run an automated server to test the changes\n\nWhile Google started automating its build tests in 2003, the engineering\nindustry took slightly longer to do the same. But automation was sorely\nneeded:\n\n> Software systems are growing larger and ever more complex... To make matters\n> worse, new versions are pushed to users frequently, sometimes multiple times\n> each day. This is a far cry from the world of shrink-wrapped software that\n> saw updates only once or twice a year.\n>\n> The ability for humans to manually validate every behavior in a system has\n> been unable to keep pace with the explosion of features and platforms in\n> most software. - Software Engineering at Google\n\nSun Microsystem\u2019s engineer, Kohsuke Kawaguchi, was key to ushering in the next\nera of testing. In 2004, he created \u201cHudson\u201d (later renamed to Jenkins in fun\nOracle drama). At his day job, Kohsuke \u201cgot tired of incurring the wrath of\nhis team every time his code broke the build.\u201d He could have manually\ntriggered tests before each code contribution, but instead, Kohsuke chose the\nclassic engineering solution and created an automated program. The Hudson tool\nacted as a long-lived test server that could automatically verify each code\nchange as it integrated into the codebase.\n\nKohsuke open-sourced Hudson, and it exploded in popularity. The first\ngeneration of automated continuous integration tests had begun, and for the\nfirst time, it became common to test every code change as it was written.\nSimilar tools like Bamboo and Team City quickly spun up, but Hudson\u2019s open-\nsource popularity reigned dominant.\n\n## Pay someone else to test your changes automatically\n\nTowards the late 2000s, code hosting shifted to the cloud. Rather than teams\nrunning their own Subversion servers to host and integrate code changes, more\nand more folks moved to host their code on GitHub. Continuous integration\ntests followed the trend and shifted to the cloud as well, with the likes of\nCircleCI and Travis CI launching in 2011. Not only were engineering teams\ncommitting to a culture of testing every change, but now, smaller companies\ncould outsource the maintenance of the test runners themselves. Larger older\ncompanies mostly remained on Jenkins because they could afford to continue\nmaintaining CI servers themselves and because Jenkins offered more advanced\ncontrol.\n\nDuring the mid-2010s, we witnessed two evolutions of cloud-based CI systems.\n\n  1. Zero-maintenance CI systems merged with code hosting services. GitLab was the first to offer this all-in-one solution, offering users a way to trigger their CI tests in the same platform that they were reviewing and merging the changes. Microsoft acquired GitHub in 2018 and pushed for the release of GitHub Actions backed by Microsoft\u2019s Azure DevOps product. In both cases, the two most popular code hosting platforms began natively offering integrated CI test execution.\n\n  2. Larger organizations shifted off Jenkins to more modern self-hosted options. BuildKite was the first popular modern solution, launching in 2013. It offered a way for companies to get the benefits of web dashboards and coordination while still hosting their code and test executions on their own compute. GitHub and GitLab later offered their own self-hosted runners, and some very manual companies opted to execute their own tests in AWS\u2019s CodeDeploy pipelines or Azure\u2019s DevOps platform.\n\nThe arc of software testing can be viewed along a spectrum of velocity and\ncheapness:\n\n  * [Days & weeks] In the 1980s, software code changes were slowly reviewed by hand to find bugs. Test suites might be run overnight or only before releases.\n\n  * [Days and nights] In the 90s, automated tests became more commonly written, whether by specialized software testers or eventually by the code authors themselves. Code changes are started to get tested before rather than after merging with the rest of the codebase.\n\n  * [Hours and minutes] In the early 2000s, the first automatic integration testing servers launched and became popular, leading to the testing of every change as it merged into the codebase.\n\n  * [Minutes] Around 2011, zero-maintenance CI test services became available. Now, small teams could also benefit from testing every change.\n\n## Maximizing how fast a single change can be tested\n\nBest practices aim to keep CI times around 10-15 minutes so that developers\ncan uphold short iteration speeds - but this becomes ever more challenging as\ncodebases and test suites grow in size every year.\n\n> Engineers don\u2019t wait for slow tests. The slower a test is, the less\n> frequently an engineer will run it, and the longer the wait after a failure\n> until it is passing again. Software Engineering at Google\n\nThere are only three ways to speed up something in software: vertical scaling,\nparallelization, or caching. In the case of CI, all three are used - with\nincreased focus on caching and parallelization in recent years.\n\nFor decades, Moore\u2019s law ensured that increasingly powerful CPUs could execute\ntest suites faster\u2014though at a cost. Using on-demand cloud services, engineers\ncan toggle a setting in AWS or GitHub actions to pay for a larger box in hopes\nthat their test suite will execute faster.\n\nSecondly, CI providers became progressively sophisticated in parallelization.\nBuildKite, GitHub Actions, and other providers let users define graphs of\ntesting steps, allowing for different computers to hand off context and\nexecute tests in parallel for the same code change. Cloud computing allows\norganizations to provision seemingly infinite parallel hosts to execute tests\nwithout fear of resource contention. Lastly, sophisticated build tooling like\nBazel and Buck allow for large codebases to compute build DAGs and parallelize\nbuild and test execution based on the dependency graph within the code itself.\n\nThirdly, CI caching systems have evolved to minimize repeated work. CI runners\ncommonly support remote caching of install and build steps, allowing tests to\nskip upfront setup work if parts of the codebase haven\u2019t changed.\n\n## Speeding up how fast all changes can be tested\n\nEngineering teams are reaching the theoretical limit of how fast a single code\nchange can be validated, assuming the validation requirements are to \u201crun all\ntests and build every code change.\u201d\n\nAnd yet, dev patterns continue to optimize for velocity.\n\nQ: What's faster than running CI on a code change using fast computers,\nparallel tests, and heavy caching?\n\nA: Skipping running some tests on the change altogether.\n\nIn a near regression back to pre-CI days, some high-velocity organizations\nleverage batching and dependencies between PRs in order to save computing\nresources and give engineers feedback faster. At Graphite, we see this\nhappening in two areas. The first is in company merge queues. Internal merge\nqueues at large companies like Uber offer batching and skipping of test\nexecution. The premise is that if you enforce an order to code changes, you no\nlonger need to test every change in the queue as rigorously as before - though\nthere are some downsides.\n\n> Chromium uses a variant of this approach called Commit Queue [4] to ensure\n> an always green mainline... Changes passing the first step are picked every\n> few hours by the second step to undergo a large suite of tests which\n> typically take around four hours. If the build breaks at this stage, the\n> entire batch gets rejected. Build sheriffs and their deputies spring into\n> action at this point to attribute failures to particular faulty changes...\n> Finally, observe that this approach leads to shippable batches, and not\n> shippable commits.\n\nThe second place where CI can be skipped is in stacked code changes -\npopularized by Facebook. If the developer stacks a series of small pull\nrequests, they\u2019re implicitly describing a required order of merging those\nchanges. Like in a merge queue, CI can be batched up the stack of changes and\nbisected if any failures are found. Failures at the bottom of the stack can\nnotify developers before even starting the execution of changes upstack.\n\nWhile test dependency graphs had previously offered early failures and saved\ncompute while testing a single change, dependency graphs across many different\nPRs can create the same gains. Saved compute time is meaningful because while\ncloud resources offer infinitely horizontal scalability, spending on testing\ncan be as high as 10-20% of companies\u2019 total cloud cost spend.\n\n## Testing faster than running code\n\nThe fastest form of CI testing I'm currently aware of involves batching the\ntesting of many changes at once while also parallelizing and caching as much\nas possible.\n\nBefore batched execution of integration tests however, we had humans reviewing\ncode changes by hand - sometimes pouring over printouts on a table, looking\nfor mistakes. We moved away from this pattern of verification because machines\ncould execute code faster than humans could read and reason about it.\n\nThat equation might be close to changing with the advent of large language\nmodels. I suspect we may be on the cusp of fast, cheap, AI code review.\nPreviously, I said there were only three ways to speed up computing - faster\nchips, parallelization, or caching. Technically, there\u2019s a fourth option if\nyou\u2019re willing to accept some fuzzy results - probabilistically predicting the\noutput. (Fun fact: CPUs already do this today).\n\nWhile it might not replace unit tests and human review, AI code review might\nbe able to scan diffs for common mistakes in ten seconds or less. It could\nflag linter concerns, misaligned patterns in the codebase, spelling mistakes,\nand other forms of errors. Existing CI coordinators might trigger the AI\nreview and return faster than other test results. Alternatively, AI review\nmight become so fast and cheap that it begins running passively in engineers\u2019\neditors, not unlike CoPilot. Unlike traditional CI tests, AI Review doesn\u2019t\nrequire completeness before it can scan for issues.\n\nWill we ever see AI-style tests catch on? Unclear, but companies are already\ntrying in the space. If AI reviews ever get good enough to catch on, it might\njust become another tech example of \u201cwhat\u2019s old is new.\u201d\n\nRelated posts\n\n#Engineering\n\nWhy we use AWS instead of Vercel to host our Next.js app\n\nNovember 28, 2023\n\nGreg Foster\n\n#Engineering\n\nThe core principles of building a good AI feature\n\nMarch 21, 2024\n\nGreg Foster\n\n#Engineering\n\nOnboarding roulette: deleting our employee accounts daily\n\nMarch 14, 2024\n\nGreg Foster\n\n\u00a9 2024\n\nProduct\n\nFeaturesPricingDocsCustomers\n\nCompany\n\nBlogCareersContact us\n\nResources\n\nCommunityPrivacy & securityTerms of serviceStacking workflow\n\nDevelopers\n\nStatusGitHub\n\n", "frontpage": false}
