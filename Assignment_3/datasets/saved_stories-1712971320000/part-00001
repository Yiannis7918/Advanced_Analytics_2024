{"aid": "40016673", "title": "Symbolic Regression", "url": "https://en.wikipedia.org/wiki/Symbolic_regression", "domain": "wikipedia.org", "votes": 2, "user": "downboots", "posted_at": "2024-04-12 19:21:17", "comments": 0, "source_title": "Symbolic regression", "source_text": "Symbolic regression - Wikipedia\n\nJump to content\n\nSearch\n\n# Symbolic regression\n\n  * Deutsch\n  * \u65e5\u672c\u8a9e\n  * \u0420\u0443\u0441\u0441\u043a\u0438\u0439\n\nEdit links\n\nFrom Wikipedia, the free encyclopedia\n\nType of regression analysis\n\nExpression tree as it can be used in symbolic regression to represent a\nfunction.\n\nSymbolic regression (SR) is a type of regression analysis that searches the\nspace of mathematical expressions to find the model that best fits a given\ndataset, both in terms of accuracy and simplicity.\n\nNo particular model is provided as a starting point for symbolic regression.\nInstead, initial expressions are formed by randomly combining mathematical\nbuilding blocks such as mathematical operators, analytic functions, constants,\nand state variables. Usually, a subset of these primitives will be specified\nby the person operating it, but that's not a requirement of the technique. The\nsymbolic regression problem for mathematical functions has been tackled with a\nvariety of methods, including recombining equations most commonly using\ngenetic programming,^[1] as well as more recent methods utilizing Bayesian\nmethods^[2] and neural networks.^[3] Another non-classical alternative method\nto SR is called Universal Functions Originator (UFO), which has a different\nmechanism, search-space, and building strategy.^[4] Further methods such as\nExact Learning attempt to transform the fitting problem into a moments problem\nin a natural function space, usually built around generalizations of the\nMeijer-G function.^[5]\n\nBy not requiring a priori specification of a model, symbolic regression isn't\naffected by human bias, or unknown gaps in domain knowledge. It attempts to\nuncover the intrinsic relationships of the dataset, by letting the patterns in\nthe data itself reveal the appropriate models, rather than imposing a model\nstructure that is deemed mathematically tractable from a human perspective.\nThe fitness function that drives the evolution of the models takes into\naccount not only error metrics (to ensure the models accurately predict the\ndata), but also special complexity measures,^[6] thus ensuring that the\nresulting models reveal the data's underlying structure in a way that's\nunderstandable from a human perspective. This facilitates reasoning and favors\nthe odds of getting insights about the data-generating system, as well as\nimproving generalisability and extrapolation behaviour by preventing\noverfitting. Accuracy and simplicity may be left as two separate objectives of\nthe regression\u2014in which case the optimum solutions form a Pareto front\u2014or they\nmay be combined into a single objective by means of a model selection\nprinciple such as minimum description length.\n\nIt has been proven that symbolic regression is an NP-hard problem, in the\nsense that one cannot always find the best possible mathematical expression to\nfit to a given dataset in polynomial time.^[7] Nevertheless, if the sought-for\nequation is not too complex it is possible to solve the symbolic regression\nproblem exactly by generating every possible function (built from some\npredefined set of operators) and evaluating them on the dataset in\nquestion.^[8]\n\n## Difference from classical regression[edit]\n\nWhile conventional regression techniques seek to optimize the parameters for a\npre-specified model structure, symbolic regression avoids imposing prior\nassumptions, and instead infers the model from the data. In other words, it\nattempts to discover both model structures and model parameters.\n\nThis approach has the disadvantage of having a much larger space to search,\nbecause not only the search space in symbolic regression is infinite, but\nthere are an infinite number of models which will perfectly fit a finite data\nset (provided that the model complexity isn't artificially limited). This\nmeans that it will possibly take a symbolic regression algorithm longer to\nfind an appropriate model and parametrization, than traditional regression\ntechniques. This can be attenuated by limiting the set of building blocks\nprovided to the algorithm, based on existing knowledge of the system that\nproduced the data; but in the end, using symbolic regression is a decision\nthat has to be balanced with how much is known about the underlying system.\n\nNevertheless, this characteristic of symbolic regression also has advantages:\nbecause the evolutionary algorithm requires diversity in order to effectively\nexplore the search space, the result is likely to be a selection of high-\nscoring models (and their corresponding set of parameters). Examining this\ncollection could provide better insight into the underlying process, and\nallows the user to identify an approximation that better fits their needs in\nterms of accuracy and simplicity.\n\n## Benchmarking[edit]\n\n### SRBench[edit]\n\nIn 2021, SRBench^[9] was proposed as a large benchmark for symbolic\nregression. In its inception, SRBench featured 14 symbolic regression methods,\n7 other ML methods, and 252 datasets from PMLB. The benchmark intends to be a\nliving project: it encourages the submission of improvements, new datasets,\nand new methods, to keep track of the state of the art in SR.\n\n### SRBench Competition 2022[edit]\n\nIn 2022, SRBench announced the competition Interpretable Symbolic Regression\nfor Data Science, which was held at the GECCO conference in Boston, MA. The\ncompetition pitted nine leading symbolic regression algorithms against each\nother on a novel set of data problems and considered different evaluation\ncriteria. The competition was organized in two tracks, a synthetic track and a\nreal-world data track.^[10]\n\n#### Synthetic Track[edit]\n\nIn the synthetic track, methods were compared according to five properties:\nre-discovery of exact expressions; feature selection; resistance to local\noptima; extrapolation; and sensitivity to noise. Rankings of the methods were:\n\n  1. QLattice\n  2. PySR (Python Symbolic Regression)\n  3. uDSR (Deep Symbolic Optimization)\n\n#### Real-world Track[edit]\n\nIn the real-world track, methods were trained to build interpretable\npredictive models for 14-day forecast counts of COVID-19 cases,\nhospitalizations, and deaths in New York State. These models were reviewed by\na subject expert and assigned trust ratings and evaluated for accuracy and\nsimplicity. The ranking of the methods was:\n\n  1. uDSR (Deep Symbolic Optimization)\n  2. QLattice\n  3. geneticengine (Genetic Engine)\n\n## Non-standard methods[edit]\n\nMost symbolic regression algorithms prevent combinatorial explosion by\nimplementing evolutionary algorithms that iteratively improve the best-fit\nexpression over many generations. Recently, researchers have proposed\nalgorithms utilizing other tactics in AI.\n\nSilviu-Marian Udrescu and Max Tegmark developed the \"AI Feynman\"\nalgorithm,^[11]^[12] which attempts symbolic regression by training a neural\nnetwork to represent the mystery function, then runs tests against the neural\nnetwork to attempt to break up the problem into smaller parts. For example, if\n, tests against the neural network can recognize the separation and proceed to\nsolve for and separately and with different variables as inputs. This is an\nexample of divide and conquer, which reduces the size of the problem to be\nmore manageable. AI Feynman also transforms the inputs and outputs of the\nmystery function in order to produce a new function which can be solved with\nother techniques, and performs dimensional analysis to reduce the number of\nindependent variables involved. The algorithm was able to \"discover\" 100\nequations from The Feynman Lectures on Physics, while a leading software using\nevolutionary algorithms, Eureqa, solved only 71. AI Feynman, in contrast to\nclassic symbolic regression methods, requires a very large dataset in order to\nfirst train the neural network and is naturally biased towards equations that\nare common in elementary physics.\n\n## Software[edit]\n\n### End-user software[edit]\n\n  * QLattice is a quantum-inspired simulation and machine learning technology that helps search through an infinite list of potential mathematical models to solve a problem.^[13]^[14]\n  * Evolutionary Forest is a Genetic Programming-based automated feature construction algorithm for symbolic regression.^[15]^[16]\n  * uDSR is a deep learning framework for symbolic optimization tasks^[17]\n  * dCGP, differentiable Cartesian Genetic Programming in python (free, open source) ^[18]^[19]\n  * HeuristicLab, a software environment for heuristic and evolutionary algorithms, including symbolic regression (free, open source)\n  * GeneXProTools, - an implementation of Gene expression programming technique for various problems including symbolic regression (commercial)\n  * Multi Expression Programming X, an implementation of Multi expression programming for symbolic regression and classification (free, open source)\n  * Eureqa, evolutionary symbolic regression software (commercial), and software library\n  * TuringBot, symbolic regression software based on simulated annealing (commercial)\n  * PySR,^[20] symbolic regression environment written in Python and Julia, using regularized evolution, simulated annealing, and gradient-free optimization (free, open source)^[21]\n  * GP-GOMEA, fast (C++ back-end) evolutionary symbolic regression with Python scikit-learn-compatible interface, achieved one of the best trade-offs between accuracy and simplicity of discovered models on SRBench in 2021 (free, open source)\n\n## See also[edit]\n\n  * Closed-form expression \u00a7 Conversion from numerical forms\n  * Genetic programming\n  * Gene expression programming\n  * Kolmogorov complexity\n  * Linear genetic programming\n  * Mathematical optimization\n  * Multi expression programming\n  * Regression analysis\n  * Reverse mathematics\n  * Discovery system (AI research)^[3]\n\n## References[edit]\n\n  1. ^ Michael Schmidt; Hod Lipson (2009). \"Distilling free-form natural laws from experimental data\". Science. 324 (5923). American Association for the Advancement of Science: 81\u201385. Bibcode:2009Sci...324...81S. CiteSeerX 10.1.1.308.2245. doi:10.1126/science.1165893. PMID 19342586. S2CID 7366016.\n  2. ^ Ying Jin; Weilin Fu; Jian Kang; Jiadong Guo; Jian Guo (2019). \"Bayesian Symbolic Regression\". arXiv:1910.08892 [stat.ME].\n  3. ^ Jump up to: ^a ^b Silviu-Marian Udrescu; Max Tegmark (2020). \"AI Feynman: A physics-inspired method for symbolic regression\". Science_Advances. 6 (16). American Association for the Advancement of Science: eaay2631. Bibcode:2020SciA....6.2631U. doi:10.1126/sciadv.aay2631. PMC 7159912. PMID 32426452.\n  4. ^ Ali R. Al-Roomi; Mohamed E. El-Hawary (2020). \"Universal Functions Originator\". Applied Soft Computing. 94. Elsevier B.V.: 106417. doi:10.1016/j.asoc.2020.106417. ISSN 1568-4946. S2CID 219743405.\n  5. ^ Benedict W. J. Irwin (2021). \"Exact Learning\" (PDF). doi:10.21203/rs.3.rs-149856/v1. S2CID 234014141. {{cite journal}}: Cite journal requires |journal= (help)\n  6. ^ Ekaterina J. Vladislavleva; Guido F. Smits; Dick Den Hertog (2009). \"Order of nonlinearity as a complexity measure for models generated by symbolic regression via pareto genetic programming\" (PDF). IEEE Transactions on Evolutionary Computation. 13 (2): 333\u2013349. doi:10.1109/tevc.2008.926486. S2CID 12072764.\n  7. ^ Virgolin, Marco; Pissis, Solon P. (2022-07-05). \"Symbolic Regression is NP-hard\". arXiv:2207.01018 [cs.NE].\n  8. ^ Bartlett, Deaglan; Desmond, Harry; Ferreira, Pedro (2023). \"Exhaustive Symbolic Regression\". IEEE Transactions on Evolutionary Computation: 1. arXiv:2211.11461. doi:10.1109/TEVC.2023.3280250. S2CID 253735380.\n  9. ^ La Cava, William; Orzechowski, Patryk; Burlacu, Bogdan; de Franca, Fabricio; Virgolin, Marco; Jin, Ying; Kommenda, Michael; Moore, Jason (2021). \"Contemporary Symbolic Regression Methods and their Relative Performance\". Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. 1. arXiv:2107.14351.\n  10. ^ Michael Kommenda; William La Cava; Maimuna Majumder; Fabricio Olivetti de Fran\u00e7a; Marco Virgolin. \"SRBench Competition 2022: Interpretable Symbolic Regression for Data Science\".\n  11. ^ Udrescu, Silviu-Marian; Tegmark, Max (2020-04-17). \"AI Feynman: A physics-inspired method for symbolic regression\". Science Advances. 6 (16): eaay2631. arXiv:1905.11481. Bibcode:2020SciA....6.2631U. doi:10.1126/sciadv.aay2631. ISSN 2375-2548. PMC 7159912. PMID 32426452.\n  12. ^ Udrescu, Silviu-Marian; Tan, Andrew; Feng, Jiahai; Neto, Orisvaldo; Wu, Tailin; Tegmark, Max (2020-12-16). \"AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity\". arXiv:2006.10782 [cs.LG].\n  13. ^ \"Feyn is a Python module for running the QLattice\". June 22, 2022.\n  14. ^ Kevin Ren\u00e9 Brol\u00f8s; Meera Vieira Machado; Chris Cave; Jaan Kasak; Valdemar Stentoft-Hansen; Victor Galindo Batanero; Tom Jelen; Casper Wilstrup (2021-04-12). \"An Approach to Symbolic Regression Using Feyn\". arXiv:2104.05417 [cs.LG].\n  15. ^ Zhang, Hengzhe; Zhou, Aimin; Zhang, Hu (August 2022). \"An Evolutionary Forest for Regression\". IEEE Transactions on Evolutionary Computation. 26 (4): 735\u2013749. doi:10.1109/TEVC.2021.3136667. ISSN 1089-778X.\n  16. ^ Zhang, Hengzhe; Zhou, Aimin; Chen, Qi; Xue, Bing; Zhang, Mengjie (2023). \"SR-Forest: A Genetic Programming based Heterogeneous Ensemble Learning Method\". IEEE Transactions on Evolutionary Computation: 1\u20131. doi:10.1109/TEVC.2023.3243172. ISSN 1089-778X.\n  17. ^ \"Deep symbolic optimization\". GitHub. June 22, 2022.\n  18. ^ \"Differentiable Cartesian Genetic Programming, v1.6 Documentation\". June 10, 2022.\n  19. ^ Izzo, Dario; Biscani, Francesco; Mereta, Alessio (2016). \"Differentiable genetic programming\". Proceedings of the European Conference on Genetic Programming. arXiv:1611.04766.\n  20. ^ \"High-Performance Symbolic Regression in Python\". GitHub. 18 August 2022.\n  21. ^ \"'Machine Scientists' Distill the Laws of Physics From Raw Data\". Quanta Magazine. May 10, 2022.\n\n## Further reading[edit]\n\n  * Mark J. Willis; Hugo G. Hiden; Ben McKay; Gary A. Montague; Peter Marenbach (1997). \"Genetic programming: An introduction and survey of applications\" (PDF). IEE Conference Publications. IEE. pp. 314\u2013319.\n  * Wouter Minnebo; Sean Stijven (2011). \"Chapter 4: Symbolic Regression\" (PDF). Empowering Knowledge Computing with Variable Selection (M.Sc. thesis). University of Antwerp.\n  * John R. Koza; Martin A. Keane; James P. Rice (1993). \"Performance improvement of machine learning via automatic discovery of facilitating functions as applied to a problem of symbolic system identification\" (PDF). IEEE International Conference on Neural Networks. San Francisco: IEEE. pp. 191\u2013198.\n\n## External links[edit]\n\n  * Ivan Zelinka (2004). \"Symbolic regression \u2014 an overview\".\n  * Hansueli Gerber (1998). \"Simple Symbolic Regression Using Genetic Programming\". (Java applet) \u2014 approximates a function by evolving combinations of simple arithmetic operators, using algorithms developed by John Koza.\n  * Katya Vladislavleva. \"Symbolic Regression: Function Discovery & More\". Archived from the original on 2014-12-18.\n\nRetrieved from\n\"https://en.wikipedia.org/w/index.php?title=Symbolic_regression&oldid=1212352618\"\n\nCategories:\n\n  * Regression analysis\n  * Genetic programming\n  * Computer algebra\n\nHidden categories:\n\n  * CS1 errors: missing periodical\n  * Articles with short description\n  * Short description matches Wikidata\n  * Use American English from January 2019\n  * All Wikipedia articles written in American English\n\n  * This page was last edited on 7 March 2024, at 12:10 (UTC).\n  * Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n\n  * Privacy policy\n  * About Wikipedia\n  * Disclaimers\n  * Contact Wikipedia\n  * Code of Conduct\n  * Developers\n  * Statistics\n  * Cookie statement\n  * Mobile view\n  * Edit preview settings\n\n", "frontpage": false}
