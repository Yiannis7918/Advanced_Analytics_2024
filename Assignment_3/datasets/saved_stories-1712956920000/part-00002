{"aid": "40013933", "title": "Cloudzip: Mount a zip file from S3 without downloading it", "url": "https://github.com/ozkatz/cloudzip", "domain": "github.com/ozkatz", "votes": 2, "user": "ozkatz", "posted_at": "2024-04-12 15:20:32", "comments": 0, "source_title": "GitHub - ozkatz/cloudzip: list and get specific files from remote zip archives without downloading the whole thing", "source_text": "GitHub - ozkatz/cloudzip: list and get specific files from remote zip archives\nwithout downloading the whole thing\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nozkatz / cloudzip Public\n\n  * Notifications\n  * Fork 1\n  * Star 54\n\nlist and get specific files from remote zip archives without downloading the\nwhole thing\n\n### License\n\nApache-2.0 license\n\n54 stars 1 fork Branches Tags Activity\n\nStar\n\nNotifications\n\n# ozkatz/cloudzip\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n6 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nozkatzimprove README on Kaggle, mounts0b0ec1f \u00b7\n\n## History\n\n32 Commits  \n  \n### docs\n\n|\n\n### docs\n\n| improve README on Kaggle, mounts  \n  \n### pkg\n\n|\n\n### pkg\n\n| communicate errors back to caller, increased handle cache  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| basic nfs transparent mounting!  \n  \n### LICENSE.txt\n\n|\n\n### LICENSE.txt\n\n| initial commit  \n  \n### README.md\n\n|\n\n### README.md\n\n| improve README on Kaggle, mounts  \n  \n### go.mod\n\n|\n\n### go.mod\n\n| removed syscall dependency  \n  \n### go.sum\n\n|\n\n### go.sum\n\n| removed syscall dependency  \n  \n### main.go\n\n|\n\n### main.go\n\n| improve README on Kaggle, mounts  \n  \n## Repository files navigation\n\n# cz - Cloud Zip\n\nlist and get specific files from remote zip archives without downloading the\nwhole thing\n\nTip\n\nNew: Experimental support for mounting a remote zip file as a local directory.\nSee mounting below.\n\n## Installation\n\nClone and build the project (no binaries available atm, sorry!)\n\n    \n    \n    git clone https://github.com/ozkatz/cloudzip.git cd cloudzip go build -o cz main.go\n\nThen copy the cz binary into a location in your $PATH\n\n    \n    \n    cp cz /usr/local/bin/\n\n## Usage\n\nListing the contents of a zip file without downloading it:\n\n    \n    \n    cz ls s3://example-bucket/path/to/archive.zip\n\nPrinting a summary of the contents (number of files, total size\ncompressed/uncompressed):\n\n    \n    \n    cz info s3://example-bucket/path/to/archive.zip\n\nDownloading and extracting a specific object from within a zip file:\n\n    \n    \n    cz cat s3://example-bucket/path/to/archive.zip images/cat.png > cat.png\n\nMounting (See below):\n\n    \n    \n    cz mount s3://example-bucket/path/to/archive.zip some_dir/\n\nUnmounting:\n\n    \n    \n    cz umount some_dir\n\n## Why does cz exist?\n\nMy use case was a pretty specific access pattern:\n\n> Upload lots of small (~1-100Kb) files as quickly as possible, while still\n> allowing random access to them\n\nHow does cz solve this?\n\nWell, uploading many small files to object stores is hard to do efficiently.\n\nBundling them as a large object and using multipart uploads to parallelize the\nupload while retaining bigger chunks is the most efficient way.\n\nWhile this is commonly done with tar, the tar format doesn't keep an index of\nthe files included in it. Scanning the archive until we find the file we're\nlooking for means we might end up downloading the whole thing.\n\nZip, on the other hand, has a central directory, which is an index! It stores\npaths in the archive and their offset in the file.\n\nThis index, together with byte range requests (supported by all major object\nstores), allow reading a small file(s) from large archives without having to\nfetch the entire thing!\n\nWe can even write a zip file directly to remote storage without saving it\nlocally:\n\n    \n    \n    zip -r - -0 * | aws s3 cp - \"s3://example-bucket/path/to/archive.zip\"\n\n#### but what about CPU usage? Won't compression slow down the upload?\n\nZip files don't have to be compressed! zip -0 will result in an uncompressed\narchive, so there's no additional overhead.\n\n## How Does it Work?\n\n#### cz ls\n\nListing is done by issuing 2 HTTP range requests:\n\n  1. Fetch the last 64kB of the zip file, looking for the End Of Central Directory (EOCD), and possibly EOCD64.\n  2. The EOCD contains the exact start offset and size of the Central Directory, which is then read by issuing another HTTP range request\n\nOnce the central directory is read, it is parsed and written to stdout,\nsimilar to the output of unzip -l.\n\n#### cz cat\n\nReading a file from the remote zip involves another HTTP range request: once\nwe have the central directory, we find the relevant entry for the file we wish\nto get, and figure out its offset and size. This is then used to issue a 3rd\nHTTP range request.\n\nBecause zip files store each file (whether compressed or not) independently,\nthis is enough to uncompress and write the file to stdout.\n\n#### \u26a0\ufe0f Experimental: cz mount\n\nInstead of listing and downloading individual files from the remote zip, you\ncan now mount it to a local directory.\n\n    \n    \n    cz mount s3://example-bucket/path/to/archive.zip my_dir/\n\nThis would show up on your local filesystem as a directory with the contents\nof the zip archive inside it - as if you've downloaded and extracted it.\n\nHowever... behind the scenes, it would fetch only the file listing from the\nremote zip (just like cz ls) and spin up a small NFS server, listening on\nlocalhost, and mount it to my_dir/.\n\nWhen reading files from my_dir/, they will first be downloaded and\ndecompressed on-the-fly, just like cz cat does.\n\nThese files are downloaded into a cache dir, which if not explicitly set, will\nbe purged when unmounted. To set it to a specific location (and retain it\nacross mount/umount cycles), set the CLOUDZIP_CACHE_DIR environment variable:\n\n    \n    \n    export CLOUDZIP_CACHE_DIR=\"/nvme/fast/cache\" cz mount s3://example-bucket/path/to/archive.zip my_dir/\n\nTo unmount:\n\n    \n    \n    cz umount my_dir\n\nwhich will unmount the NFS share from the directory, and terminate the local\nNFS server for you.\n\n#### Mounting, illustrated:\n\n#### Demo\n\nMounting a 32GB dataset, directly from Kaggle's storage (See Kaggle usage\nbelow) as a local directory, with DuckDB reading a single file with ~1 second\nload time:\n\nCaution\n\nThis is still experimental (and only supported on Linux and MacOS for now)\n\n## Logging\n\nSet the $CLOUDZIP_LOGGING environment variable to DEBUG to log storage calls\nto stderr:\n\n    \n    \n    export CLOUDZIP_LOGGING=\"DEBUG\" cz ls s3://example-bucket/path/to/archive.zip # will log S3 calls to stderr\n\n## Supported backends\n\n### AWS S3\n\nWill use the default AWS credentials resolution order\n\nExample:\n\n    \n    \n    cz ls s3://example-bucket/path/to/archive.zip\n\n### HTTP / HTTPS\n\nExample:\n\n    \n    \n    cz ls https://example.com/path/to/archive.zip\n\n### Kaggle\n\nKaggle's Dataset Download API returns an URL for a zip file, so we can use it\neasily with cz! Before getting started, generate an API key and store the json\nfile in ~/.kaggle/kaggle.json (see \"Authentication\" on the Kaggle API docs).\n\nAlternatively, you can store the kaggle.json in a different location and set\nthe KAGGLE_KEY_FILE environment variable with its path.\n\nExample:\n\n    \n    \n    cz ls kaggle://{userSlug}/{datasetSlug}\n\nFor example, for the dataset at\nhttps://www.kaggle.com/datasets/datasnaek/youtube-new, the cz url should be\nkaggle://datasnaek/youtube-new.\n\n### lakeFS\n\nSince lakeFS can return pre-signed URLs which are HTTP(s), we can simply do:\n\n    \n    \n    lakectl fs presign lakefs://repository/ref/archive.zip | cz ls -\n\nWarning\n\nA note about pre-signed URLs and mounts: Since pre-signed URLs have a\nrelatively short expiration, mounting them could lead to some undefined\nbehavior. Typically, this would require a mechanism to \"refresh\" them,\nrequesting a new url from the server. This is currently not yet implemented\nfor lakeFS\n\n### Local files\n\nPrefix the path with file:// to read from the local filesystem. Can accept\neither relative path or absolute path.\n\nExample:\n\n    \n    \n    cz ls file://archive.zip # relative to current directory (./archive.zip) cz ls file:///home/user/archive.zip # absolute path (/home/user/archive.zip)\n\n## About\n\nlist and get specific files from remote zip archives without downloading the\nwhole thing\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\nActivity\n\n### Stars\n\n54 stars\n\n### Watchers\n\n1 watching\n\n### Forks\n\n1 fork\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Languages\n\n  * Go 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
