{"aid": "40077000", "title": "AI isn't useless. But is it worth it?", "url": "https://www.citationneeded.news/ai-isnt-useless/", "domain": "citationneeded.news", "votes": 3, "user": "ywnzzn", "posted_at": "2024-04-18 15:03:00", "comments": 0, "source_title": "AI isn't useless. But is it worth it?", "source_text": "AI isn't useless. But is it worth it?\n\n[citation needed]\n\na newsletter by Molly White\n\nSign in Subscribe\n\nNewsletter\n\n# AI isn't useless. But is it worth it?\n\nAI can be kind of useful, but I'm not sure that a \"kind of useful\" tool\njustifies the harm.\n\n#### Molly White\n\nApr 17, 2024 \u2014 15 min read\n\nAI isn't useless. But is it worth it?\n\n0:00\n\n/21:51\n\nListen to a voiceover of this post, download the recording for later, or\nsubscribe to the feed in your podcast app.\n\nAs someone known for my criticism of the previous deeply flawed technology to\nbecome the subject of the tech world's overinflated aspirations, I have had\npeople express surprise when I've remarked that generative artificial\nintelligence tools^a can be useful. In fact, I was a little surprised myself.\n\nBut there is a yawning gap between \"AI tools can be handy for some things\" and\nthe kinds of stories AI companies are telling (and the media is uncritically\nreprinting). And when it comes to the massively harmful ways in which large\nlanguage models (LLMs) are being developed and trained, the feeble argument\nthat \"well, they can sometimes be handy...\" doesn't offer much of a\njustification.\n\nSome are surprised when they discover I don't think blockchains are useless,\neither. Like so many technologies, blockchains are designed to prioritize a\nfew specific characteristics (coordination among parties who don't trust one\nanother, censorship-resistance, etc.) at the expense of many others (speed,\ncost, etc.). And as they became trendy, people often used them for purposes\nwhere their characteristics weren't necessary \u2014 or were sometimes even\nunwanted \u2014 and so they got all of the flaws with none of the benefits. The\nthing with blockchains is that the things they are suited for are not things I\npersonally find to be terribly desirable, such as the massive casinos that\nhave emerged around gambling on token prices, or financial transactions that\ncannot be reversed.\n\nWhen I boil it down, I find my feelings about AI are actually pretty similar\nto my feelings about blockchains: they do a poor job of much of what people\ntry to do with them, they can't do the things their creators claim they one\nday might, and many of the things they are well suited to do may not be\naltogether that beneficial. And while I do think that AI tools are more\nbroadly useful than blockchains, they also come with similarly monstrous\ncosts.\n\nSubscribe\n\nI've been slow to get around to writing about artificial intelligence in any\ndepth, mostly because I've been trying to take the time to interrogate my own\nknee-jerk response to a clearly overhyped technology. After spending so much\ntime writing about a niche that's practically all hype with little practical\nfunctionality, it's all too easy to look at such a frothy mania around a\ndifferent type of technology and assume it's all the same.\n\nIn the earliest months of the LLM mania, my ethical concerns about the tools\nmade me hesitant to try them at all. When my early tests were met with\nmediocre to outright unhelpful results, I'll admit I was quick to internally\ndismiss the technology as more or less useless. It takes time to experiment\nwith these models and learn how to prompt them to produce useful outputs,^b\nand I just didn't have that time then.^c But as the hype around AI has grown,\nand with it my desire to understand the space in more depth, I wanted to\nreally understand what these tools can do, to develop as strong an\nunderstanding as possible of their potential capabilities as well as their\nlimitations and tradeoffs, to ensure my opinions are well-formed.\n\nI, like many others who have experimented with or adopted these products, have\nfound that these tools actually can be pretty useful for some tasks. Though AI\ncompanies are prone to making overblown promises that the tools will shortly\nbe able to replace your content writing team or generate feature-length films\nor develop a video game from scratch, the reality is far more mundane: they\nare handy in the same way that it might occasionally be useful to delegate\nsome tasks to an inexperienced and sometimes sloppy intern.\n\nStill, I do think acknowledging the usefulness is important, while also\nholding companies to account for their false or impossible promises, abusive\nlabor practices, and myriad other issues. When critics dismiss AI outright, I\nthink in many cases this weakens the criticism, as readers who have used and\nbenefited from AI tools think \"wait, that's not been my experience at all\".\n\n## Use cases\n\nI've found AI tools to be useful to my writing, though not for the actual\nwriting bit. When I'm writing, I often find myself with a word on the \"tip of\nmy tongue\" (so to speak), and I've had more success with ChatGPT than with\nGoogle for these circumstances \u2014 although I can usually find the word with\nGoogle if I try hard enough.\n\nUser: What's a word for something that will not affect the final outcome\nAssistant: Inconsequential\n\nLike many people, I also find it challenging to proofread my own writing, and\nI sometimes miss typos or weird grammar accidentally left in from changing a\nsentence halfway through.\n\nProofreading a newsletter post with Anthropic's Claude model\n\nLLMs are pretty decent at proofreading, and although they sometimes spit out a\nfew false positives, this example from proofreading my most recent recap issue\nshows where it caught several mistakes (points 1, 2, 4, and 8; point 5 was\nalso a genuine error, but it was within a quote).\n\nHowever, I don't think I need generative AI to do this, either. There are a\nlot of proofreading tools^d that work quite well, and, helpfully, don't invent\nerrors that weren't in the original text (as I've found the ChatGPT models are\nparticularly wont to do).\n\nCoding has been the far more compelling use case for me. Copilot, Github's AI\ncoding assistant, integrates directly into VSCode and other IDEs. I've also\nplayed with using the more general models, like ChatGPT, for coding tasks.\nThey are certainly flawed \u2014 Copilot has an annoying habit of \"hallucinating\"\n(fabricating) imports instead of deferring to VSCode's perfectly good non-AI\nauto-import, for example \u2014 but in other cases they are genuinely helpful.\n\nI've found these tools to be particularly good at simple tasks that would\nnormally pull me out of my workflow to consult documentation or StackOverflow,\nlike generating finicky CSS selectors or helping me craft database aggregation\noperations. On at least one occasion, they've pointed me towards useful\nfunctionality I never knew about and wouldn't even think to look up. They're\nalso great at saving you some typing by spitting out the kind of boilerplate-y\ncode you have to write for things like new unit tests.\n\nThe tools can also do the kind of simple, repetitive tasks I'd previously\nwrite a quick script to do for me \u2014 or they can generate that quick script.\nFor example, here's me asking ChatGPT to write a quick Python script to turn\nmy blogroll OPML file into the JSON file I wanted while I was adding a\nblogroll page to my website:\n\nAfter changing the feeds.opml file path to the location of the file on my\ncomputer, the code it suggested worked without any modification:\n\nBesides my own experimentation, others are using these tools in ways that are\nreally hard to argue aren't useful. Someone I know in real life has told me\nabout creating a custom model based on their own emails, which they then query\nas needed, or use to create some fairly boilerplate documents they previously\nhad to spend hours on. Open source developer Simon Willison has been\ndocumenting his own AI coding experiments on his blog, and has described how\nLLMs have made him more ambitious with his projects and more likely to embark\non what he calls \"sidequests\".^e Sumana Harihareswara uses OpenAI's speech\nrecognition tools to create subtitles for her videos and recorded talks, or to\n\"mine\" them for material she can later reuse. Elsewhere on the internet, those\nwho speak English as a second language have spoken of LLMs' usefulness in\nrevising their professional communications. Others use it to summarize meeting\nnotes. Some use it as a starting point for documentation.\n\n## Reality check\n\nDespite some unarguably useful features, the limitations of these tools make\nthemselves readily apparent.\n\nWhen it comes to coding, while it can make for a handy assistant to an\nexperienced developer, it can't replace an experienced developer. Microsoft's\nSuper Bowl commercial, which shows a person prompting Copilot to \"Write code\nfor my 3D open world game\", is pure fantasy.\n\nAnd in my experience, it sometimes gets in the way more than it helps, as when\nI experimented with it while working on a Chrome extension I was writing\nrecently and ultimately had to turn it off. It constantly suggested plausible\nbut completely non-functional code, scaffolded the project in an outdated\nformat, and autogenerated CSS classes that looked like they could be Bootstrap\nclasses, but weren't. It's good at short functions and common boilerplate, but\nit's not going to architect a project for you, and, as with writing, it's not\ngoing to \"think\" of novel ideas. I like it for getting annoying, repetitive\ntasks out of my way; I don't worry it's going to take my job.\n\nArguably the most widely-described use case for generative AI is writing.\nIndeed, as media companies lay off journalists in droves, some outlets are\nreplacing their work with shoddy, AI-generated approximations. Freelance\nwriters are reporting challenges in finding work as their former clients\ndecide that ChatGPT can do a good enough job. But what these companies and\nclients fail to recognize is that ChatGPT does not write, it generates text,\nand anyone who's spotted obviously LLM-generated content in the wild\nimmediately knows the difference.\n\nYou've gotten this far into my article, so you're recently familiar with a\ncouple dozen paragraphs of purely human writing. Contrast that with LLMs'\nattempts, from prompts with varying degrees of detail, with my very best\nefforts put into trying to get it to sound halfway normal:\n\n(spreadsheet)\n\nYikes. I particularly like how, when I ask them to try to sound like me, or to\nat least sound less like a chatbot, they adopt a sort of \"cool teacher\"\npersona, as if they're sitting backwards on a chair to have a heart-to-heart.\nBack when I used to wait tables, the other waitresses and I would joke to each\nother about our \"waitress voice\", which were the personas we all\nsubconsciously seemed to slip into when talking to customers. They varied\nsomewhat, but they were all uniformly saccharine, with slightly higher-pitched\nvoices, and with the general demeanor as though you were talking to someone\nyou didn't think was very bright. Every LLM's writing \"voice\" reminds me of\nthat.\n\nEven if the telltale tone is surmountable, LLMs are good at generating text\nbut not at generating novel ideas. This is, of course, an inherent feature of\ntechnology that's designed to generate plausible mathematical approximations\nof what you've asked it for based on its large corpus of training data; it\ndoesn't think, and so the best you're ever going to get from it is some mashup\nof other peoples' thinking.^f\n\nLLM-generated text is good enough for some use cases, which I'll return to in\na moment. But I think most people, myself certainly included, would be\nmortified to replace any of our writing with this kind of stuff.^g\n\nFurthermore, LLMs' \"hallucination\" problem means that everything it does must\nbe carefully combed over for errors, which can sometimes be hard to spot.\nBecause of this, while it's handy for proofreading newsletters or helping me\nquickly add a fun feature to my website, I wouldn't trust LLMs to do anything\nof real import. And the tendency for people to put too much trust into these\ntools^h is among their most serious problems: no amount of warning labels and\ndisclaimers seem to be sufficient to stop people from trying to use them to\nprovide legal advice or sell AI \"therapy\" services.\n\nFinally, advertisements that LLMs might someday generate feature-length films\nor replace artists seem neither feasible nor desirable. AI-generated images\ntend to suffer from a similar bland \"tone\" as its writing, and their\nproliferation only makes me desire real human artwork more. With generated\nvideo, they inevitably trend towards the uncanny, and the technology's\ninherent limitations \u2014 as a tool that is probabilistically generating \"likely\"\nimages rather than ones based on some kind of understanding \u2014 seem unlikely to\never overcome that. And the idea that we all should be striving to \"replace\nartists\" \u2014 or any kind of labor \u2014 is deeply concerning, and I think incredibly\nillustrative of the true desires of these companies: to increase corporate\nprofits at any cost.\n\n## When LLMs are good enough\n\nAs I mentioned before, there are some circumstances in which LLMs are good\nenough. There are some types of writing where LLMs are already being widely\nused: for example, by businesspeople who use them to generate meeting notes,\nfluff up their outgoing emails or summarize their incoming ones, or spit out\nlengthy, largely identical reports that they're required to write regularly.\n\nYou can also spot LLMs in all sorts of places on the internet, where they're\nbeing used to try to boost websites' search engine rankings. That weird,\nbubbly GPT voice is well suited to marketing copy and social media posts, too.\nAny place on the web that incentivizes high-volume, low effort text is being\ninundated by generated text, like e-book stores, online marketplaces, and\npractically any review or comment section.\n\nBut I find one common thread among the things AI tools are particularly suited\nto doing: do we even want to be doing these things? If all you want out of a\nmeeting is the AI-generated summary, maybe that meeting could've been an\nemail. If you're using AI to write your emails, and your recipient is using AI\nto read them, could you maybe cut out the whole thing entirely? If mediocre,\nauto-generated reports are passing muster, is anyone actually reading them? Or\nis it just middle-management busywork?\n\nAs for the AI enshittification of the internet, we all seem to agree already\nthat we don't want this, and yet here it is. No one wants to open up Etsy to\nlook for a thoughtful birthday gift, only to give up after scrolling through\npages of low-quality print-on-demand items or resold Aliexpress items that\nhave flooded the site.\n\nYour AI model is showing\n\nNo one wants to Google search a question only to end up on several pages of\nkeyword-spam vomit before finding an authoritative answer.\n\nBut the incentives at play on these platforms, mean that AI junk is\ninevitable. In fact, the LLMs may be new, but the behavior is not; just like\nkeyword stuffing and content farms and the myriad ways people used software to\ngenerate reams upon reams of low-quality text before ChatGPT ever came on the\nscene, if the incentive is there, the behavior will follow. If the internet's\nenshittification feels worse post-ChatGPT, it's because of the quantity and\nspeed at which this junk is being produced, not because the junk is new.\n\n## Costs and benefits\n\nThroughout all this exploration and experimentation I've felt a lingering\nguilt, and a question: is this even worth it? And is it ethical for me to be\nusing these tools, even just to learn more about them in hopes of later\ncriticizing them more effectively?\n\nThe costs of these AI models are huge, and not just in terms of the billions\nof dollars of VC funds they're burning through at incredible speed. These\nmodels are well known to require far more computing power (and thus\nelectricity and water) than a traditional web search or spellcheck. Although\nAI company datacenters are not intentionally wasting electricity in the same\nway that bitcoin miners perform millions of useless computations, I'm also not\nsure that generating a picture of a person with twelve fingers on each hand or\ntext that reads as though written by an endlessly smiling children's\ntelevision star who's being held hostage is altogether that much more useful\nthan a bitcoin.\n\nThere's a huge human cost as well. Artificial intelligence relies heavily upon\n\"ghost labor\": work that appears to be performed by a computer, but is\nactually delegated to often terribly underpaid contractors, working in\nhorrible conditions, with few labor protections and no benefits. There is a\nhuge amount of work that goes into compiling and labeling data to feed into\nthese models, and each new model depends on ever-greater amounts of said data\n\u2014 training data which is well known to be scraped from just about any possible\nsource, regardless of copyright or consent. And some of these workers suffer\nserious psychological harm as a result of exposure to deeply traumatizing\nmaterial in the course of sanitizing datasets or training models to perform\ncontent moderation tasks.\n\nThen there's the question of opportunity cost to those who are increasingly\nbeing edged out of jobs by LLMs,^i despite the fact that AI often can't\ncapably perform the work they were doing. Should I really be using AI tools to\nproofread my newsletters when I could otherwise pay a real person to do that\nproofreading? Even if I never intended to hire such a person?\n\nFinally, there's the issue of how these tools are being used, and the lack of\neffort from their creators to limit their abuse. We're seeing them used to\ngenerate disinformation via increasingly convincing deepfaked images, audio,\nor video, and the reckless use of them by previously reputable news outlets\nand others who publish unedited AI content is also contributing to\nmisinformation. Even where AI isn't being directly used, it's degrading trust\nso badly that people have to question whether the content they're seeing is\ngenerated, or whether the \"person\" they're interacting with online might just\nbe ChatGPT. Generative AI is being used to harass and sexually abuse. Other AI\nmodels are enabling increased surveillance in the workplace and for \"security\"\npurposes \u2014 where their well-known biases are worsening discrimination by\npolice who are wooed by promises of \"predictive policing\". The list goes on.\n\nI'm glad that I took the time to experiment with AI tools, both because I\nunderstand them better and because I have found them to be useful in my day-\nto-day life. But even as someone who has used them and found them helpful,\nit's remarkable to see the gap between what they can do and what their\npromoters promise they will someday be able to do. The benefits, though\nextant, seem to pale in comparison to the costs.\n\nBut the reality is that you can't build a hundred-billion-dollar industry\naround a technology that's kind of useful, mostly in mundane ways, and that\nboasts perhaps small increases in productivity if and only if the people who\nuse it fully understand its limitations. And you certainly can't justify the\nkind of exploitation, extraction, and environmental cost that the industry has\nbeen mostly getting away with, in part because people have believed their\nlofty promises of someday changing the world.\n\nI would love to live in a world where the technology industry widely valued\nmaking incrementally useful tools to improve peoples' lives, and were honest\nabout what those tools could do, while also carefully weighing the\ntechnology's costs. But that's not the world we live in. Instead, we need to\npush back against endless tech manias and overhyped narratives, and oppose the\n\"innovation at any cost\" mindset that has infected the tech sector.\n\n#### Footnotes\n\n  1. When I refer to \"AI\" in this piece, I'm mostly referring to the much narrower field of generative artificial intelligence and large language models (LLMs), which is what people generally mean these days when they say \"AI\". \u21a9\n\n  2. While much fun has been made of those describing themselves as \"prompt engineers\", I have to say I kind of get it. It takes some experience to be able to open up a ChatGPT window or other LLM interface and actually provide instructions that will produce useful output. I've heard this compared to \"google-fu\" in the early days of Google, when the search engine was much worse at interpreting natural language queries, and I think that's rather apt. \u21a9\n\n  3. ChatGPT was publicly released in November 2022, right as the cryptocurrency industry was in peak meltdown. \u21a9\n\n  4. Many of which are built with various other kinds of machine learning or artificial intelligence, if not necessarily generative AI. \u21a9\n\n  5. As it happens, he has also written about the \"AI isn't useful\" criticism. \u21a9\n\n  6. Some AI boosters will argue that most or all original thought is also merely a mashup of other peoples' thoughts, which I think is a rather insulting minimization of human ingenuity. \u21a9\n\n  7. Nor do I want to, by the way. I performed these tests for the purposes of illustration, but I neither intend nor want to start using these tools to replace my writing. I'm here to write, and you're here to read my writing, and that's how it will remain. See my about page. \u21a9\n\n  8. Something that is absolutely encouraged by the tools' creators, who give them chat-like interfaces, animations suggesting that the tool is \"typing\" messages back to you, and a confident writing style that encourages people to envision the software as another thinking human being. \u21a9\n\n  9. Or, more accurately, by managers and executives who believe the marketing hype out of AI companies that proclaim that their tools can replace workers, without seeming to understand at all what those workers do. \u21a9\n\nLoved this post? Consider signing up for a pay-what-you-want subscription or\nleaving a tip to support Molly White's work, which is entirely funded by\nreaders like you.\n\n## Read more\n\n### Issue 55 \u2013 Halving a bad time\n\nThe bitcoin \"halving\" looms, and that may not be as good news as coiners hope.\nAlso, Terra committed fraud and Uniswap got a Wells notice.\n\nApr 13, 2024\n\n### \"The Monkey Fraud\": An interview with Ryder Ripps\n\nAn interview with Ryder Ripps, a defendant in the Yuga Labs v. Ripps case\nabout Bored Ape Yacht Club trademark infringement and racism.\n\nApr 4, 2024\n\n### Issue 54 \u2013 Cases continue\n\nCrypto-related litigation is in full swing, as the Terra civil fraud trial has\nkicked off and two other cases against crypto companies have survived motions\nto dismiss.\n\nApr 2, 2024\n\n### 25 years for Sam Bankman-Fried\n\n\"The judgment has to adequately reflect the seriousness of the crime, and this\nwas a very serious crime.\"\n\nMar 28, 2024\n\nCitation Needed features critical coverage of the cryptocurrency industry and\nof issues in the broader technology world.\n\nIt is independently published by Molly White, and entirely supported by\nreaders like you.\n\nSubscribe\n\n\u00a9 2024 Molly White.\n\n", "frontpage": false}
