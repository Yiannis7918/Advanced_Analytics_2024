{"aid": "39966756", "title": "Data Anywhere with Pipelines, Event Notifications, and Workflows", "url": "https://blog.cloudflare.com/data-anywhere-events-pipelines-durable-execution-workflows", "domain": "cloudflare.com", "votes": 2, "user": "taipeidev", "posted_at": "2024-04-08 06:21:31", "comments": 0, "source_title": "Data Anywhere with Pipelines, Event Notifications, and Workflows", "source_text": "Data Anywhere with Pipelines, Event Notifications, and Workflows\n\nGet Started Free|Contact Sales\n\n## The Cloudflare Blog\n\nSubscribe to receive notifications of new posts:\n\n# Data Anywhere with Pipelines, Event Notifications, and Workflows\n\n04/03/2024\n\n  * Matt Silverlock\n\n9 min read\n\nData is fundamental to any real-world application: the database storing your\nuser data and inventory, the analytics tracking sales events and/or error\nrates, the object storage with your web assets and/or the Parquet files\ndriving your data science team, and the vector database enabling semantic\nsearch or AI-powered recommendations for your users.\n\nWhen we first announced Workers back in 2017, and then Workers KV, Cloudflare\nR2, and D1, it was obvious that the next big challenge to solve for developers\nwould be in making it easier to ingest, store, and query the data needed to\nbuild scalable, full-stack applications.\n\nTo that end, as part of our quest to make building stateful, distributed-by-\ndefault applications even easier, we\u2019re launching our new Event Notifications\nservice; a preview of our upcoming streaming ingestion product, Pipelines; and\na sneak peek into our take on durable execution, Workflows.\n\n### Event-based architectures\n\nWhen you\u2019re writing data \u2014 whether that\u2019s new data, changing existing data, or\ndeleting old data \u2014 you often want to trigger other, asynchronous work to run\nin response. That could be processing user-driven uploads, updating search\nindexes as the underlying data changes, or removing associated rows in your\nSQL database when content is removed.\n\nIn order to make these event-driven workflows far easier to build across\nCloudflare, we\u2019re launching the first step towards a wider Event Notifications\nplatform across Cloudflare, starting with notifications support in R2.\n\nYou can read more in the deep-dive on Event Notifications for R2, but in a\nnutshell: you can configure changes to content in any R2 bucket to write\ndirectly to a Queue, allowing you to reliably consume those events in a Worker\nor to pull from compute in a legacy cloud.\n\nEvent Notifications for R2 are just the beginning, though. There are many\nkinds of events you might want to trigger as a developer \u2014 these are just some\nof the event types we\u2019re planning to support:\n\n  * Changes (writes) to key-value pairs in your Workers KV namespaces.\n  * Updates to your D1 databases, including changed rows or triggers.\n  * Deployments to your Cloudflare Workers\n\nConsuming event notifications from a single Worker is just one approach,\nthough. As you start to consume events, you may want to trigger multi-step\nworkflows that execute reliably, resume from errors or exceptions, and ensure\nthat previous steps aren\u2019t duplicated or repeated unnecessarily. An event\nnotification framework turns out to be just the thing needed to drive a\nworkflow engine that executes durably...\n\n### Making it even easier to ingest data\n\nWhen we launched Cloudflare R2, our object storage service, we knew that\nsupporting the de facto-standard S3 API was critical in order to allow\ndevelopers to bring the tooling and services they already had over to R2. But\nthe S3 API is designed to be simple: at its core, it provides APIs for upload,\ndownload, multipart and metadata operations, and many tools don\u2019t support the\nS3 API.\n\nWhat if you want to batch clickstream data from your web services so that it\u2019s\nefficient (and cost-effective) to query by your analytics team? Or partition\ndata by customer ID, merchant ID, or locale within a structured data format\nlike JSON?\n\nWell, we want to help solve this problem too, and so we\u2019re announcing\nPipelines, an upcoming streaming ingestion service designed to ingest data at\nscale, aggregate it, and write it directly to R2, without you having to manage\ninfrastructure, partitions, runners, or worry about durability.\n\nWith Pipelines, creating a globally scalable ingestion endpoint that can\ningest tens-of-thousands of events per second doesn\u2019t require any code:\n\n    \n    \n    $ wrangler pipelines create clickstream-ingest-prod --batch-size=\"1MB\" --batch-timeout-secs=120 --batch-on-json-key=\".merchantId\" --destination-bucket=\"prod-cs-data\" \u2705 Successfully created new pipeline \"clickstream-ingest-prod\" \ud83d\udce5 Created endpoints: \u27a1 HTTPS: https://d458dbe698b8eef41837f941d73bc5b3.pipelines.cloudflarestorage.com/clickstream-ingest-prod \u27a1 WebSocket: wss://d458dbe698b8eef41837f941d73bc5b3.pipelines.cloudflarestorage.com:8443/clickstream-ingest-prod \u27a1 Kafka: d458dbe698b8eef41837f941d73bc5b3.pipelines.cloudflarestorage.com:9092 (topic: clickstream-ingest-prod)\n\nAs you can see here, we\u2019re already thinking about how to make Pipelines\nprotocol-agnostic: write from a HTTP client, stream events over a WebSocket,\nand/or redirect your existing Kafka producer (and stop having to manage and\nscale Kafka) directly to Pipelines.\n\nBut that\u2019s just the beginning of our vision here. Scalable ingestion and\nsimple batching is one thing, but what about if you have more complex needs?\nWell, we have a massively scalable compute platform (Cloudflare Workers) that\ncan help address this too.\n\nThe code below is just an initial exploration for how we\u2019re thinking about an\nAPI for running transforms over streaming data. If you\u2019re aware of projects\nlike Apache Beam or Flink, this programming model might even look familiar:\n\n    \n    \n    export default { // Pipeline handler is invoked when batch criteria are met async pipeline(stream: StreamPipeline, env: Env, ctx: ExecutionContext): Promise<StreamingPipeline> { // ... return stream // Type: transform(label: string, transformFunc: TransformFunction): Promise<StreamPipeline> // Each transform has a label that is used in metrics to provide // per-transform observability and debugging .transform(\"human readable label\", (events: Array<StreamEvent>) => { return events.map((e) => ...) }) .transform(\"another transform\", (events: Array<StreamEvent>) => { return events.map((e) => ...) }) .writeToR2({ format: \"json\", bucket: \"MY_BUCKET_NAME\", prefix: somePrefix, batchSize: \"10MB\" }) } }\n\nSpecifically:\n\n  * The Worker describes a pipeline of transformations (mapping, reducing, filtering) that operates over each subset of events (records)\n  * You can call out to other services \u2014 including D1 or KV \u2014 in order to synchronously or asynchronously hydrate data or lookup values during your stream processing\n  * We take care of scaling horizontally based on records-per-second and/or any concurrency settings you configure based on processing latency requirements.\n\nWe\u2019ll be bringing Pipelines into open beta later in 2024, and it will\ninitially launch with support for HTTP ingestion and R2 as a destination\n(sink), but we\u2019re already thinking bigger.\n\nWe\u2019ll be sharing more as Pipelines gets closer to release. In the meantime,\nyou can register your interest and share your use-case, and we\u2019ll reach out\nwhen Pipelines reaches open beta.\n\n### Durable Execution\n\nIf the term \u201cDurable Execution\u201d is new to you, don\u2019t worry: the term comes\nfrom the desire to run applications that can resume execution from where they\nleft off, even if the underlying host or compute fails (where the \u201cdurable\u201d\npart comes from).\n\nAs we\u2019ve continued to build out our data and AI platforms, we\u2019ve been acutely\naware that developers need ways to create reliable, repeatable workflows that\noperate over that data, turn unstructured data into structured data, trigger\non fresh data (or periodically), and automatically retry, restart, and export\nmetrics for each step along the way. The industry calls this Durable\nExecution: we\u2019re just calling it Workflows.\n\nWhat makes Workflows different from other takes on Durable Execution is that\nwe provide the underlying compute as part of the platform. You don\u2019t have to\nbring-your-own compute, or worry about scaling it or provisioning it in the\nright locations. Workflows runs on top of Cloudflare Workers \u2013 you write the\nworkflow, and we take care of the rest.\n\nHere\u2019s an early example of writing a Workflow that generates text embeddings\nusing Workers AI and stores them (ready to query) in Vectorize as new content\nis written to (or updated within) R2.\n\n  * Each Workflow run is triggered by an Event Notification consumed from a Queue, but could also be triggered by a HTTP request, another Worker, or even scheduled on a timer.\n  * Individual steps within the Workflow allow us to define individually retriable units of work: in this case, we\u2019re reading the new objects from R2, creating text embeddings using Workers AI, and then inserting.\n  * State is durably persisted between steps: each step can emit state, and Workflows will automatically persist that so that any underlying failures, uncaught exceptions or network retries can resume execution from the last successful step.\n  * Every call to step() automatically emits metrics associated with the unique Workflow run, making it easier to debug within each step and/or break down your application into its smallest units of execution, without having to worry about observability.\n\nStep-by-step, it looks like this:\n\nTransforming this series of steps into real code, here\u2019s what this would look\nlike with Workflows:\n\n    \n    \n    import { Ai } from \"@cloudflare/ai\"; import { Workflow } from \"cloudflare:workers\"; export interface Env { R2: R2Bucket; AI: any; VECTOR_INDEX: VectorizeIndex; } export default class extends Workflow { async run(event: Event) { const ai = new Ai(this.env.AI); // List of keys to fetch from our incoming event notification const keysToFetch = event.messages.map((val) => { return val.object.key; }); // The return value of each step is stored (the \"durable\" part // of \"durable execution\") // This ensures that state can be persisted between steps, reducing // the need to recompute results ($$, time) should subsequent // steps fail. const inputs = await this.ctx.run( // Each step has a user-defined label // Metrics are emitted as each step runs (to success or failure) // with this label attached and available within per-Workflow // analytics in near-real-time. \"read objects from R2\", async () => { const objects = []; for (const key of keysToFetch) { const object = await this.env.R2.get(key); objects.push(await object.text()); } return objects; }); // Persist the output of this step. const embeddings = await this.ctx.run( \"generate embeddings\", async () => { const { data } = await ai.run(\"@cf/baai/bge-small-en-v1.5\", { text: inputs, }); if (data.length) { return data; } else { // Uncaught exceptions trigger an automatic retry of the step // Retries and timeouts have sane defaults and can be overridden // per step throw new Error(\"Failed to generate embeddings\"); } }, { retries: { limit: 5, delayMs: 1000, backoff: \"exponential\", }, } ); await this.ctx.run(\"insert vectors\", async () => { const vectors = []; keysToFetch.forEach((key, index) => { vectors.push({ id: crypto.randomUUID(), // Our embeddings from the previous step values: embeddings[index].values, // The path to each R2 object to map back to during // vector search metadata: { r2Path: key }, }); }); return this.env.VECTOR_INDEX.upsert(vectors); }); } }\n\nThis is just one example of what a Workflow can do. The ability to durably\nexecute an application, modeled as a series of steps, applies to a wide number\nof domains. You can apply this model of execution to a number of use-cases,\nincluding:\n\n  * Deploying software: each step can define a build step and subsequent health check, gating further progress until your deployment meets your criteria for \u201chealthy\u201d.\n  * Post-processing user data: triggering a workflow based on user uploads (e.g. to Cloudflare R2) that then subsequently parses that data asynchronously, redacts PII or sensitive data, writes the sanitized output, and triggers a notification via email, webhook, or mobile push.\n  * Payment and batch workflows: aggregating raw customer usage data on a periodic schedule by querying your data warehouse (or Workers Analytics Engine), triggering usage or spend alerts, and/or generating PDF invoices.\n\nEach of these use cases model tasks that you want to run to completion,\nminimize redundant retries by persisting intermediate state, and (importantly)\neasily observe success and failure.\n\nWe\u2019ll be sharing more about Workflows during the second quarter of 2024 as we\nwork towards an open (public!) beta. This includes how we\u2019re thinking about\nidempotency and interactions with our storage, per-instance observability and\nmetrics, local development, and templates to bootstrap common workflows.\n\n### Putting it together\n\nWe\u2019ve often thought of Cloudflare\u2019s own network as one massively scalable\nparallel data processing cluster: data centers in 310+ cities, with the\nability to run compute close to users and/or close to data, keep it within the\nbounds of regulatory or compliance requirements, and most importantly, use our\nmassive scale to enable our customers to scale as well.\n\nRecapping, a fully-fledged data platform needs to enable three things:\n\n  1. Ingesting data: getting data into the platform (in the right format, from the right sources)\n  2. Storing data: securely, reliably, and durably.\n  3. Querying data: understanding and extracting insights from the data, and/or transforming it for use by other tools.\n\nWhen we launched R2 we tackled the second part, but knew that we\u2019d need to\nfollow up with the first and third parts in order to make it easier for\ndevelopers to get data in and make use of it.\n\nIf we look at how we can build a system that helps us solve each of these\nthree parts together with Pipelines, Event Notifications, R2, and Workflows,\nwe end up with an architecture that resembles this:\n\nSpecifically, we have Pipelines (1) scaling out to ingest data, batch it,\nfilter it, and then durably store it in R2 (2) in a format that\u2019s ready and\noptimized for querying. Workflows, ClickHouse, Databricks, or the query engine\nof your choice can then query (3) that data as soon as it\u2019s ready \u2014 with\n\u201cready\u201d being automatically triggered by an Event Notification as soon as the\ndata is ingested and written to R2.\n\nThere\u2019s no need to poll, no need to batch after the fact, no need to have your\nquery engine slow down on data that wasn\u2019t pre-aggregated or filtered, and no\nneed to manage and scale infrastructure in order to keep up with load or data\njurisdiction requirements. Create a Pipeline, write your data directly to R2,\nand query directly from it.\n\nIf you\u2019re also looking at this and wondering about the costs of moving this\ndata around, then we\u2019re holding to one important principle: zero egress fees\nacross all of our data products. Just as we set the stage for this with our R2\nobject storage, we intend to apply this to every data product we\u2019re building,\nPipelines included.\n\n### Start Building\n\nWe\u2019ve shared a lot of what we\u2019re building so that developers have an\nopportunity to provide feedback (including via our Developer Discord), share\nuse-cases, and think about how to build their next application on Cloudflare.\n\nWe protect entire corporate networks, help customers build Internet-scale\napplications efficiently, accelerate any website or Internet application, ward\noff DDoS attacks, keep hackers at bay, and can help you on your journey to\nZero Trust.\n\nVisit 1.1.1.1 from any device to get started with our free app that makes your\nInternet faster and safer.\n\nTo learn more about our mission to help build a better Internet, start here.\nIf you're looking for a new career direction, check out our open positions.\n\nDiscuss on Hacker News\n\nDeveloper WeekDevelopersDeveloper PlatformCloudflare WorkersR2 Storage\n\nFollow on X\n\nMatt Silverlock|@elithrar\n\nCloudflare|@cloudflare\n\nRelated posts\n\nApril 05, 2024 3:45 PM\n\n## Cloudflare acquires Baselime to expand serverless application observability\ncapabilities\n\nToday, we\u2019re thrilled to announce that Cloudflare has acquired Baselime, a\nserverless observability company...\n\nBy\n\n  * Boris Tane,\n\n  * Rita Kozlov\n\nDeveloper Week, Developers, Developer Platform, Product News, Cloudflare\nWorkers, Observability, Acquisitions\n\nApril 05, 2024 1:01 PM\n\n## Browser Rendering API GA, rolling out Cloudflare Snippets, SWR, and\nbringing Workers for Platforms to all users\n\nBrowser Rendering API is now available to all paid Workers customers with\nimproved session management...\n\nBy\n\n  * Tanushree Sharma,\n\n  * Celso Martinho,\n\n  * Nikita Cano,\n\n  * Matt Bullock,\n\n  * Tim Kornhammar\n\nDeveloper Week, Developers, Developer Platform, Turnstile, Application\nServices, Product News, General Availability, Cloudflare Workers\n\nApril 05, 2024 1:00 PM\n\n## Cloudflare acquires PartyKit to allow developers to build real-time multi-\nuser applications\n\nWe're thrilled to announce that PartyKit, a trailblazer in enabling developers\nto craft ambitious real-time, collaborative, multiplayer applications, is now\na part of Cloudflare...\n\nBy\n\n  * Sunil Pai,\n\n  * Rita Kozlov\n\nDeveloper Week, Acquisitions, Cloudflare Workers, AI, Durable Objects\n\nApril 05, 2024 1:00 PM\n\n## Blazing fast development with full-stack frameworks and Cloudflare\n\nYou can now use your framework\u2019s development server while accessing D1\ndatabases, R2 object stores, AI models, and more. Iterate locally in\nmilliseconds to build sophisticated web apps that run on Cloudflare...\n\nBy\n\n  * Igor Minar,\n\n  * Dario Piotrowicz,\n\n  * James Culveyhouse,\n\n  * Peter Bacon Darwin\n\nDeveloper Week, Developers, Developer Platform, Full Stack, Wrangler,\nMiniflare\n\n  * Sales\n  * Enterprise Sales\n  * Become a Partner\n\nContact Sales:\n\n+1 (888) 993-5273\n\n  * Getting Started\n  * Pricing\n  * Case Studies\n  * White Papers\n  * Webinars\n  * Learning Center\n\n  * Community\n  * Community Hub\n  * Project Galileo\n  * Athenian Project\n  * Cloudflare TV\n\n  * Developers\n  * Developer Hub\n  * Developers Discord\n  * Cloudflare Workers\n  * Integrations\n\n  * Tools\n  * Cloudflare Radar\n  * Speed Test\n  * Is BGP Safe Yet?\n  * RPKI Toolkit\n  * Certificate Transparency\n\n  * Support\n  * Support\n  * Cloudflare Status\n  * Compliance\n  * GDPR\n\n  * Company\n  * About Cloudflare\n  * Our Team\n  * Press\n  * Analysts\n  * Careers\n  * Logo\n  * Network Map\n\n\u00a9 2024 Cloudflare, Inc. | Privacy Policy | Terms of Use |Cookie Preferences | Trust & Safety | Trademark\n\n", "frontpage": false}
