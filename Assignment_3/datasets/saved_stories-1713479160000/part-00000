{"aid": "40077835", "title": "Evaluating DeBERTaV3 with the Nonparametric Analysis", "url": "https://nbviewer.org/github/nicholaslourie/opda/blob/main/nbs/experiments/evaluating-debertav3-with-the-nonparametric-analysis.ipynb", "domain": "nbviewer.org", "votes": 1, "user": "nicholaslourie", "posted_at": "2024-04-18 16:22:58", "comments": 0, "source_title": "Jupyter Notebook Viewer", "source_text": "Jupyter Notebook Viewer\n\n  1. opda\n  2. nbs\n  3. experiments\n\nNotebook\n\n# Evaluating DeBERTaV3 with the Nonparametric Analysis\u00b6\n\nA nonparametric analysis of hyperparameter search for fine-tuning DeBERTa and\nDeBERTaV3\n\nFor more background, see the notebook Nonparametric Analysis or our paper:\nShow Your Work with Confidence: Confidence Bands for Tuning Curves.\n\nIn [1]:\n\n    \n    \n    # imports import json import numbers import warnings from matplotlib import pyplot as plt import numpy as np from opda import nonparametric import opda.random # constants evaluations = { \"matched\": { \"name\": \"MultiNLI-m\", }, \"mismatched\": { \"name\": \"MultiNLI-mm\", }, } models = { \"deberta-base\": { \"name\": \"DeBERTa\", \"style\": {\"linestyle\": \"-\", \"color\": \"#FFB000\"}, }, \"deberta-v3-base\": { \"name\": \"DeBERTaV3\", \"style\": {\"linestyle\": \"--\", \"color\": \"#648FFF\"}, }, } epochs = { 1 : {\"style\": {\"linestyle\": \"-\", \"color\": \"#fe6100\"}}, 2 : {\"style\": {\"linestyle\": \"--\", \"color\": \"#dc267f\"}}, 3 : {\"style\": {\"linestyle\": \"-.\", \"color\": \"#785ef0\"}}, 4 : {\"style\": {\"linestyle\": \":\", \"color\": \"#648fff\"}}, \"all\": {\"style\": {\"linestyle\": \"-\", \"color\": \"#ffb000\"}}, } # helper functions def is_number(x): return isinstance(x, numbers.Number) # experimental settings evaluation = \"matched\" model = \"deberta-v3-base\" band = \"ld_highest_density\" sample_sizes = [6, 20, 34, 48] n = sample_sizes[-1] # Later analyses assume n == sample_sizes[-1] ns_grid = np.linspace(1, 4 * n, num=50_000) y_min = 0. y_max = 1. confidence = 0.80 default_num_train_epochs = 3 # Configure environment. generator = np.random.RandomState(0) opda.random.set_seed(generator) warnings.filterwarnings(\"ignore\") # Configure Matplotlib. plt.style.use(\"experiments.default\")\n\n## The DeBERTa Model Family\u00b6\n\nOne approach to natural language processing takes a neural network pretrained\nwith raw text and then fine-tunes it on labeled data. DeBERTa offers one such\npretrained model, with two major versions: DeBERTa (He et al., 2021) and\nDeBERTaV3 (He et al., 2023).\n\nDespite sharing their name, DeBERTa and DeBERTaV3 have significant\ndifferences. For example, DeBERTa is trained with a generative masked-language\nmodeling objective (Devlin et al., 2018), while DeBERTaV3 uses the\ndiscriminative ELECTRA pretraining (Clark et al., 2020).\n\nBoth models have a number of hyperparameters to set when fine-tuning them. We\ncan investigate these models individually and compare them to each other using\noptimal design analysis (OPDA).\n\n## Experimental Design\u00b6\n\nIn our case study, we compare fine-tuning two pretrained models, DeBERTa and\nDeBERTaV3, for natural language inference (NLI) on the MultiNLI dataset (Adina\net al., 2018).\n\nWhen assessing models with OPDA, one must first define the hyperparameter\nsearch distribution and then sample and evaluate hyperparameters from it.\nSince DeBERTa and DeBERTaV3 share the same hyperparameters for fine-tuning, we\nused the same search distribution for both:\n\nbatch_sizenum_epochswarmup_proportionlearning_ratedropout\u223cDiscreteUniform(16,64)\u223cDiscreteUniform(1,4)\u223cUniform(0,0.6)\u223cLogUniform(10\u22126,10\u22123)\u223cUniform(0,0.3)\n\nwhere warmup_proportion is the number of warmup steps as a proportion of the\nfirst epoch.\n\nWe fine-tuned both DeBERTa and DeBERTaV3 on MultiNLI using 1,024\nhyperparameter settings sampled from this search distribution. MultiNLI has\ntwo validation sets: matched (same domains as training) and mismatched\n(different domains from training). Each fine-tuned model was evaluated on\nboth. Thus, for each model and validation set we have 1,024 accuracy\nnumbers\u2014one for each sampled set of hyperparameters.\n\nIn [2]:\n\n    \n    \n    # load the data model_to_evaluation_to_hparams = {} model_to_evaluation_to_ys = {} for model_ in models: for evaluation_ in evaluations: hparams = [] ys = [] with open(f\"../../data/deberta/{model_}.results.jsonl\") as f_in: for ln in f_in: ex = json.loads(ln) hparams.append({ key: ex[key] for key in [ \"num_train_epochs\", \"train_batch_size\", \"learning_rate\", \"warmup_steps\", \"cls_drop_out\", ] }) ys.append(max( score for _, score in ex[\"learning_curves\"][evaluation_] )) if model_ not in model_to_evaluation_to_hparams: model_to_evaluation_to_hparams[model_] = {} model_to_evaluation_to_hparams[model_][evaluation_] = hparams if model_ not in model_to_evaluation_to_ys: model_to_evaluation_to_ys[model_] = {} model_to_evaluation_to_ys[model_][evaluation_] = np.array(ys) # Validate the data. total_samples = 1_024 for model_ in models: for evaluation_ in evaluations: # Check each experiment has the expected number of samples. if ( len(model_to_evaluation_to_hparams[model_][evaluation_]) != total_samples or len(model_to_evaluation_to_ys[model_][evaluation_]) != total_samples ): raise RuntimeError( \"The experiment data does not contain the correct\" \" number of examples.\", ) # Check ``epochs`` matches the epochs used in experiments. for hparams in model_to_evaluation_to_hparams[model_][evaluation_]: if hparams[\"num_train_epochs\"] not in set(filter(is_number, epochs)): raise RuntimeError( \"Found an unexpected value for num_train_epochs.\", )\n\n## Comparing Against a Baseline\u00b6\n\nSuppose we wanted to compare fine-tuning DeBERTa and DeBERTaV3 for NLI using\nMultiNLI (Adina et al., 2018). OPDA recommends that we identify the\nhyperparameters, define a search distribution, and then analyze the tuning\ncurves associated with this search distribution. Assume we used the search\ndistribution defined above:\n\nbatch_sizenum_epochswarmup_proportionlearning_ratedropout\u223cDiscreteUniform(16,64)\u223cDiscreteUniform(1,4)\u223cUniform(0,0.6)\u223cLogUniform(10\u22126,10\u22123)\u223cUniform(0,0.3)\n\nwhere warmup_proportion is the number of warmup steps as a proportion of the\nfirst epoch.\n\nIf our aim is to evaluate DeBERTaV3, then one rule of thumb for picking the\nnumber of search iterations is to determine how many iterations we expect\nmodel users to perform and then multiply that by at least 6.25 (if we plan to\nuse 80% confidence bands). So, if a typical model user will perform 6-8 search\niterations, we'll use at least 37.5 to 50 to construct the tuning curve. For\nthis analysis, we'll use 48.\n\nSince the nonparametric bands from OPDA hold simultaneously, they provide\nquite a strong guarantee, so we'll use an 80% confidence level for\nconstructing our bands.\n\n### Evaluating a Single Model\u00b6\n\nTo evaluate a single model, we construct the tuning curve with confidence\nbands. Potential users can then read the cost-benefit trade-off from these\nplots and determine whether the model achieves sufficient performance for\ntheir application and within their computational budget. For DeBERtaV3 on\nMultiNLI, the tuning curves look as follows:\n\nIn [3]:\n\n    \n    \n    # Subsample the data to obtain a sample of size n. ys = generator.choice( model_to_evaluation_to_ys[model][evaluation], size=n, replace=False, ) # Construct the confidence bands. lower_cdf, point_cdf, upper_cdf =\\ nonparametric.EmpiricalDistribution.confidence_bands( ys, confidence=confidence, a=y_min, b=y_max, method=band, ) # Plot the confidence bands. fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4.25)) ax.plot( ns_grid, point_cdf.quantile_tuning_curve(ns_grid), label=\"point estimate\", **models[model][\"style\"], ) ax.fill_between( ns_grid, upper_cdf.quantile_tuning_curve(ns_grid), lower_cdf.quantile_tuning_curve(ns_grid), alpha=0.275, label=f\"{confidence * 100:.0f}% confidence\", color=models[model][\"style\"][\"color\"], ) # Format the plot. ax.set_xlim(1, n // 4) ax.set_ylim(0.87, 0.91) ax.set_yticks(np.linspace(0.87, 0.91, num=5)) ax.spines[\"right\"].set_color(\"lightgrey\") ax.spines[\"top\"].set_color(\"lightgrey\") ax.legend(loc=\"lower right\") ax.set_xlabel(\"search iterations\") ax.set_ylabel(\"accuracy\") ax.set_title( f'{models[model][\"name\"]} Tuning Curve' f' on {evaluations[evaluation][\"name\"]}', ) fig.tight_layout() plt.show()\n\nThus, we see that with 48 search iterations, we can bound the tuning curve\nfairly tightly up to about 8 search iterations. Below, we visualize what the\ntuning curve bounds look like for various sample sizes and both the matched\nand mismatched MultiNLI test sets:\n\nIn [4]:\n\n    \n    \n    # Choose indices to subsample the data and obtain a sample of size # sample_size_. idxs = generator.choice(total_samples, size=n, replace=False) fig, axes = plt.subplots( nrows=2, ncols=len(sample_sizes), sharex=True, sharey=\"row\", figsize=(12, 6), ) for j, sample_size_ in enumerate(sample_sizes): for i, evaluation_ in enumerate(evaluations): ys = model_to_evaluation_to_ys[model][evaluation_][ idxs[:sample_size_] ] # Construct the confidence bands. lower_cdf, point_cdf, upper_cdf =\\ nonparametric.EmpiricalDistribution.confidence_bands( ys, confidence=confidence, a=y_min, b=y_max, method=band, ) # Plot the confidence bands. axes[i, j].plot( ns_grid, point_cdf.quantile_tuning_curve(ns_grid), **models[model][\"style\"], ) axes[i, j].fill_between( ns_grid, upper_cdf.quantile_tuning_curve(ns_grid), lower_cdf.quantile_tuning_curve(ns_grid), alpha=0.275, color=models[model][\"style\"][\"color\"], ) # Format the plot. for j, sample_size_ in enumerate(sample_sizes): axes[1, j].set_xlabel(\"search iterations\") axes[0, j].set_title(f\"n = {sample_size_}\") for i, evaluation_ in enumerate(evaluations): axes[i, 0].set_ylabel(\"accuracy\") axes[i, -1].yaxis.set_label_position(\"right\") axes[i, -1].set_ylabel(evaluation_, labelpad=18, rotation=270) for i in range(len(evaluations)): for j in range(len(sample_sizes)): axes[i, j].set_xlim(1, n // 4) axes[i, j].set_ylim(0.865, 0.915) axes[i, j].spines[\"right\"].set_color(\"lightgrey\") axes[i, j].spines[\"top\"].set_color(\"lightgrey\") fig.suptitle( f'{models[model][\"name\"]} Tuning Curves on MultiNLI', ) fig.tight_layout() plt.show()\n\nWe see the main effect of increasing sample size is to shift over the point at\nwhich the upper confidence band jumps to the upper bound on performance.\nIncreasing the sample size does tend to narrow the band as well; however, the\neffect is not very noticeable on DeBERTaV3.\n\nJust for fun, let's see what the tuning curve looks like in the large sample\nlimit, with say 1,024 iterations of random search:\n\nIn [5]:\n\n    \n    \n    # Use all the data to obtain a sample of size 1,024. ys = model_to_evaluation_to_ys[model][evaluation] # Construct the confidence bands. lower_cdf, point_cdf, upper_cdf =\\ nonparametric.EmpiricalDistribution.confidence_bands( ys, confidence=confidence, a=y_min, b=y_max, method=band, ) # Plot the confidence bands. fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4.25)) ax.plot( ns_grid, point_cdf.quantile_tuning_curve(ns_grid), label=\"point estimate\", **models[model][\"style\"], ) ax.fill_between( ns_grid, upper_cdf.quantile_tuning_curve(ns_grid), lower_cdf.quantile_tuning_curve(ns_grid), alpha=0.275, label=f\"{confidence * 100:.0f}% confidence\", color=models[model][\"style\"][\"color\"], ) # Format the plot. ax.set_xlim(1, 185) ax.set_ylim(0.87, 0.91) ax.set_yticks(np.linspace(0.87, 0.91, num=5)) ax.spines[\"right\"].set_color(\"lightgrey\") ax.spines[\"top\"].set_color(\"lightgrey\") ax.legend(loc=\"lower right\") ax.set_xlabel(\"search iterations\") ax.set_ylabel(\"accuracy\") ax.set_title( f'{models[model][\"name\"]} Tuning Curve' f' on {evaluations[evaluation][\"name\"]}', ) fig.tight_layout() plt.show()\n\n### Comparing Two Models\u00b6\n\nBeyond evaluating a single model, we may also want to compare two models. To\naccomplish this task, we can plot their tuning curves with confidence bands\nside-by-side:\n\nIn [6]:\n\n    \n    \n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4.25)) for model_ in models: # Subsample the data to obtain a sample of size n. ys = generator.choice( model_to_evaluation_to_ys[model_][evaluation], size=n, replace=False, ) # Construct the confidence bands. lower_cdf, point_cdf, upper_cdf =\\ nonparametric.EmpiricalDistribution.confidence_bands( ys, confidence=confidence, a=y_min, b=y_max, method=band, ) # Plot the confidence bands. ax.plot( ns_grid, point_cdf.quantile_tuning_curve(ns_grid), label=models[model_][\"name\"], **models[model_][\"style\"], ) ax.fill_between( ns_grid, upper_cdf.quantile_tuning_curve(ns_grid), lower_cdf.quantile_tuning_curve(ns_grid), alpha=0.275, color=models[model_][\"style\"][\"color\"], ) # Format the plot. ax.set_xlim(1, n // 4) ax.set_ylim(0.87, 0.91) ax.set_yticks(np.linspace(0.87, 0.91, num=5)) ax.spines[\"right\"].set_color(\"lightgrey\") ax.spines[\"top\"].set_color(\"lightgrey\") ax.legend(loc=\"lower right\") ax.set_xlabel(\"search iterations\") ax.set_ylabel(\"accuracy\") ax.set_title( f'DeBERTa vs. DeBERTaV3 on {evaluations[evaluation][\"name\"]}', ) fig.tight_layout() plt.show()\n\nIn general, it's tempting to require that there's no overlap between the\nconfidence bands before concluding that one model is better than the other;\nhowever, this criterion is too conservative. Inspired by Minka (2002), one\ncould use the following guidelines: the evidence for an improvement is weak if\none point estimate is excluded by the other's confidence bands, fair if each\npoint estimate is excluded by the other's confidence bands, and strong if the\nconfidence bands have no overlap, over a nontrivial portion of the tuning\ncurve.\n\nWe see clear separation between DeBERTa and DeBERTaV3's tuning curves. Thus,\nwe can confidently conclude that DeBERTaV3 outperforms DeBERTa for all\nhyperparameter tuning budgets (or at least up to a budget of 8 search\niterations).\n\nAgain, we can also explore what our plots would've looked like for various\nother sample sizes:\n\nIn [7]:\n\n    \n    \n    # Choose indices to subsample the data and obtain a sample of size # sample_size_. idxs = generator.choice(total_samples, size=n, replace=False) fig, axes = plt.subplots( nrows=2, ncols=len(sample_sizes), sharex=True, sharey=\"row\", figsize=(12, 6), ) for j, sample_size_ in enumerate(sample_sizes): for i, evaluation_ in enumerate(evaluations): for model_ in models: ys = model_to_evaluation_to_ys[model_][evaluation_][ idxs[:sample_size_] ] # Construct the confidence bands. lower_cdf, point_cdf, upper_cdf =\\ nonparametric.EmpiricalDistribution.confidence_bands( ys, confidence=confidence, a=y_min, b=y_max, method=band, ) # Plot the confidence bands. axes[i, j].plot( ns_grid, point_cdf.quantile_tuning_curve(ns_grid), label=models[model_][\"name\"], **models[model_][\"style\"], ) axes[i, j].fill_between( ns_grid, upper_cdf.quantile_tuning_curve(ns_grid), lower_cdf.quantile_tuning_curve(ns_grid), alpha=0.275, color=models[model_][\"style\"][\"color\"], ) # Format the plot. for j, sample_size_ in enumerate(sample_sizes): axes[1, j].set_xlabel(\"search iterations\") axes[0, j].set_title(f\"n = {sample_size_}\") for i, evaluation_ in enumerate(evaluations): axes[i, 0].set_ylabel(\"accuracy\") axes[i, -1].yaxis.set_label_position(\"right\") axes[i, -1].set_ylabel(evaluation_, labelpad=18, rotation=270) for i in range(len(evaluations)): for j in range(len(sample_sizes)): axes[i, j].set_xlim(1, n // 4) axes[i, j].set_ylim(0.865, 0.915) axes[i, j].spines[\"right\"].set_color(\"lightgrey\") axes[i, j].spines[\"top\"].set_color(\"lightgrey\") fig.suptitle(\"DeBERTa vs. DeBERTaV3 on MultiNLI\") fig.tight_layout() plt.show()\n\nSimilarly to before, our conclusion would've remained largely the same\nregardless of sample size. The main difference is at what number of search\niterations our confidence bands become trivial, and thus for how far we can\nconfidently claim DeBERTaV3 outperforms DeBERTa.\n\nAgain, just for fun, let's see how the tuning curves would compare with a much\nlarger sample size: 1,024 iterations of random search.\n\nIn [8]:\n\n    \n    \n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4.25)) for model_ in models: # Use all the data to obtain a sample of size 1,024. ys = model_to_evaluation_to_ys[model_][evaluation] # Construct the confidence bands. lower_cdf, point_cdf, upper_cdf =\\ nonparametric.EmpiricalDistribution.confidence_bands( ys, confidence=confidence, a=y_min, b=y_max, method=band, ) # Plot the confidence bands. ax.plot( ns_grid, point_cdf.quantile_tuning_curve(ns_grid), label=models[model_][\"name\"], **models[model_][\"style\"], ) ax.fill_between( ns_grid, upper_cdf.quantile_tuning_curve(ns_grid), lower_cdf.quantile_tuning_curve(ns_grid), alpha=0.275, color=models[model_][\"style\"][\"color\"], ) # Format the plot. ax.set_xlim(1, 185) ax.set_ylim(0.87, 0.91) ax.set_yticks(np.linspace(0.87, 0.91, num=5)) ax.spines[\"right\"].set_color(\"lightgrey\") ax.spines[\"top\"].set_color(\"lightgrey\") ax.legend(loc=\"lower right\") ax.set_xlabel(\"search iterations\") ax.set_ylabel(\"accuracy\") ax.set_title( f'DeBERTa vs. DeBERTaV3 on {evaluations[evaluation][\"name\"]}', ) fig.tight_layout() plt.show()\n\n## Analyzing a Hyperparameter\u00b6\n\nAfter establishing DeBERTaV3 improves on DeBERTa, we should investigate which\nhyperparameters were most important. For example, we could ablate different\nparts of the model to find out what improved performance the most; or, we\ncould probe how sensitive different hyperparameters are, and provide advice\nfor tuning them.\n\nThere's a whole literature on how to define hyperparameter importance (van\nRijn et al., 2018; Probst et al., 2019; Weerts et al., 2020). Perhaps the most\nintuitive approach is formalized by Weerts et al. (2020). They suggest\nmeasuring hyperparameter importance by the tuning risk, or \"the performance\nloss that is incurred when a hyperparameter is not tuned, but set to a default\nvalue.\" In other words, tuning risk is the difference in the test score when\nyou tune all the hyperparameters vs. all except a specfic one. It's what\nresearchers measure when they run ablation studies.\n\nWe can operationalize this idea with tuning curves. Just compare the full\ntuning curve to one where the hyperparameter is fixed to a default value.\nLet's illustrate this with epochs, assuming the default number of epochs is 3:\n\nIn [9]:\n\n    \n    \n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4.25)) for num_train_epochs in [default_num_train_epochs, \"all\"]: # Subsample the data to obtain a sample of size n. average_epochs = ( num_train_epochs if num_train_epochs != \"all\" else np.mean(list(filter(is_number, epochs))) ) ys = generator.choice( [ y for hparams, y in zip( model_to_evaluation_to_hparams[model][evaluation], model_to_evaluation_to_ys[model][evaluation], ) if ( num_train_epochs == \"all\" or hparams[\"num_train_epochs\"] == num_train_epochs ) ], size=n, replace=False, ) # Construct the confidence bands. lower_cdf, point_cdf, upper_cdf =\\ nonparametric.EmpiricalDistribution.confidence_bands( ys, confidence=confidence, a=y_min, b=y_max, method=band, ) # Plot the confidence bands. ax.plot( average_epochs * ns_grid, point_cdf.quantile_tuning_curve(ns_grid), label=( f\"default epochs ({default_num_train_epochs})\" if num_train_epochs == default_num_train_epochs else f\"tuned epochs ({min(filter(is_number, epochs))}-{max(filter(is_number, epochs))})\" ), **epochs[num_train_epochs][\"style\"], ) ax.fill_between( average_epochs * ns_grid, upper_cdf.quantile_tuning_curve(ns_grid), lower_cdf.quantile_tuning_curve(ns_grid), alpha=0.275, color=epochs[num_train_epochs][\"style\"][\"color\"], ) # Format the plot. ax.set_xlim(1, default_num_train_epochs * n // 4) ax.set_ylim(0.89, 0.91) ax.set_yticks(np.linspace(0.89, 0.91, num=5)) ax.spines[\"right\"].set_color(\"lightgrey\") ax.spines[\"top\"].set_color(\"lightgrey\") ax.legend(loc=\"lower right\") ax.set_xlabel(\"total training epochs\") ax.set_ylabel(\"accuracy\") ax.set_title( f'Epoch Importance for {models[model][\"name\"]} on {evaluations[evaluation][\"name\"]}', ) fig.tight_layout() plt.show()\n\nThis plot illustrates an important idea. Tuning the number of epochs vs.\nleaving it at a fixed value changes the average cost per training run. If\nepochs is chosen uniformly from 1 to 4, then it is 2.5 on average. In\ncontrast, leaving epochs at the default always gives a value of 3. Thus, each\niteration of hyperparameter search runs 4 / 2.5 = 1.6 times slower on average.\nFor that reason, we compare tuning curves in terms of total training epochs\nrather than the raw number of search iterations. A similar cost normalization\nis necessary when, for example, comparing models of different sizes.\n\nThe point estimates suggest that tuning the number of epochs is about as good\nas using a default of 3. Looking at the confidence bands, it seems likely that\nthey're at least within about .003 of each other. Let's see what would happen\nif we'd had more search iterations for our analysis, for example 245:\n\nIn [10]:\n\n    \n    \n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4.25)) for num_train_epochs in [default_num_train_epochs, \"all\"]: # Subsample the data to obtain a sample of size 245. average_epochs = ( num_train_epochs if num_train_epochs != \"all\" else np.mean(list(filter(is_number, epochs))) ) ys = generator.choice( [ y for hparams, y in zip( model_to_evaluation_to_hparams[model][evaluation], model_to_evaluation_to_ys[model][evaluation], ) if ( num_train_epochs == \"all\" or hparams[\"num_train_epochs\"] == num_train_epochs ) ], size=245, replace=False, ) # Construct the confidence bands. lower_cdf, point_cdf, upper_cdf =\\ nonparametric.EmpiricalDistribution.confidence_bands( ys, confidence=confidence, a=y_min, b=y_max, method=band, ) # Plot the confidence bands. ax.plot( average_epochs * ns_grid, point_cdf.quantile_tuning_curve(ns_grid), label=( f\"default epochs ({default_num_train_epochs})\" if num_train_epochs == default_num_train_epochs else f\"tuned epochs ({min(filter(is_number, epochs))}-{max(filter(is_number, epochs))})\" ), **epochs[num_train_epochs][\"style\"], ) ax.fill_between( average_epochs * ns_grid, upper_cdf.quantile_tuning_curve(ns_grid), lower_cdf.quantile_tuning_curve(ns_grid), alpha=0.275, color=epochs[num_train_epochs][\"style\"][\"color\"], ) # Format the plot. ax.set_xlim(1, 150) ax.set_ylim(0.90, 0.91) ax.set_yticks(np.linspace(0.90, 0.91, num=5)) ax.spines[\"right\"].set_color(\"lightgrey\") ax.spines[\"top\"].set_color(\"lightgrey\") ax.legend(loc=\"lower right\") ax.set_xlabel(\"total training epochs\") ax.set_ylabel(\"accuracy\") ax.set_title( f'Epoch Importance for {models[model][\"name\"]} on {evaluations[evaluation][\"name\"]}', ) fig.tight_layout() plt.show()\n\nAs expected, the findings hold up. Adjusted for total epochs, leaving epochs\nat the default seems to have little effect on the tuning curve: it changes by\nat most a fraction of a percent.\n\nWe might hypothesize a few explanations: perhaps the number of epochs doesn't\nmatter? Or, maybe 3 is a bad default? With this larger sample, we can drill\ndown into the epochs' effect by plotting tuning curves for various values of\nit:\n\nIn [11]:\n\n    \n    \n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4.25)) for num_train_epochs in filter(is_number, epochs): # Subsample the data to obtain a sample of size 245. average_epochs = num_train_epochs ys = generator.choice( [ y for hparams, y in zip( model_to_evaluation_to_hparams[model][evaluation], model_to_evaluation_to_ys[model][evaluation], ) if hparams[\"num_train_epochs\"] == num_train_epochs ], size=245, replace=False, ) # Construct the confidence bands. lower_cdf, point_cdf, upper_cdf =\\ nonparametric.EmpiricalDistribution.confidence_bands( ys, confidence=confidence, a=y_min, b=y_max, method=band, ) # Plot the confidence bands. ax.plot( average_epochs * ns_grid, point_cdf.quantile_tuning_curve(ns_grid), label=f'{num_train_epochs} epoch{\"s\" if num_train_epochs > 1 else \"\"}', **epochs[num_train_epochs][\"style\"], ) ax.fill_between( average_epochs * ns_grid, upper_cdf.quantile_tuning_curve(ns_grid), lower_cdf.quantile_tuning_curve(ns_grid), alpha=0.275, color=epochs[num_train_epochs][\"style\"][\"color\"], ) # Format the plot. ax.set_xlim(1, 150) ax.set_ylim(0.90, 0.91) ax.set_yticks(np.linspace(0.90, 0.91, num=5)) ax.spines[\"right\"].set_color(\"lightgrey\") ax.spines[\"top\"].set_color(\"lightgrey\") ax.legend(loc=\"lower right\") ax.set_xlabel(\"total training epochs\") ax.set_ylabel(\"accuracy\") ax.set_title( f'Effect of Epochs for {models[model][\"name\"]} on {evaluations[evaluation][\"name\"]}', ) fig.tight_layout() plt.show()\n\nWe see moderate evidence that 3 or 4 epochs is best. Tentatively, 3 epochs may\nhave a slight advantage over 4 early on because it can explore a bit\nfaster\u2014but more data is ultimately necessary to reach this conclusion.\n\nThis website does not host notebooks, it only renders notebooks available on\nother websites.\n\nDelivered by Fastly, Rendered by OVHcloud\n\nnbviewer GitHub repository.\n\nnbviewer version: 8b013f7\n\nnbconvert version: 7.2.3\n\nRendered 4 minutes ago\n\n", "frontpage": false}
