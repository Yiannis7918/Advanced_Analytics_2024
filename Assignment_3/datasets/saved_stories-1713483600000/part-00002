{"aid": "40078692", "title": "Using ClickHouse to count unique users at scale", "url": "https://segment.com/blog/using-clickhouse-to-count-unique-users-at-scale/", "domain": "segment.com", "votes": 1, "user": "n2parko", "posted_at": "2024-04-18 17:37:35", "comments": 0, "source_title": "Twilio Segment Blog", "source_text": "Twilio Segment Blog\n\nEngineers & Developers\n\n# Using ClickHouse to count unique users at scale\n\nBy implementing semantic sharding and optimizing filtering and grouping with\nClickHouse, we transformed query times from minutes to seconds, ensuring\nefficient handling of high-volume journeys in production while paving the way\nfor future enhancements.\n\nApr 16, 2024\n\nBy Rahul Ramakrishna, Lew Gordon, Clayton McClure\n\nShare article\n\nWith Twilio Engage, customers can define a customer\u2019s journey using our\nJourneys product. When we launched this product, we provided a way to show how\nmany users were in each step of the journey, but it was difficult to find\nstatistics on how the campaign was doing overall. It was so hard to understand\nthe correct for the step level stats.\n\nWe wanted to provide Engage customers with a way to see overall stats per\njourney, as well as provide more accurate step level stats.\n\nTwo factors that made this particularly challenging came from trying to\ncalculate exact unique users who interacted with the campaigns over arbitrary\ndate ranges (ex. entered, exited, completed.) The Segment product provides\nsome use cases, and for those we generally use a self managed ClickHouse.\n\nWe have a rough common pattern of writing to the individual shards as separate\ntables within ClickHouse and using a distributed table to serve queries. Most\nof the use cases we\u2019d had so far required basic total counts where we could\nutilize pre-aggregation via materialized views. For this scenario, we could\nnot pre-aggregate due to the requirement of exact unique counts over arbitrary\ndate ranges.\n\nInitially, we launched this feature using some standard setups - we had to\ningest the data and query it.\n\nOur general approach is to randomly distribute incoming events into the\nmultiple shards of the cluster to prevent hotspotting. The goal is to get an\neven amount of data volume per shard since we are not using an elastic object\nstore for persistence. Instead, use an instance store with NVMe disks for\nperformance. Since we do not have an elastic object store to back any of our\nqueryable data, excessive storage use on one node would lead to failure.\n\nAt query time, we query the distributed table which fans out to the various\nshards to collect the data and return a result. This has worked very well for\nother use cases where we cannot pre-aggregate the data.\n\nHowever, upon launching this, we noticed that the performance results for\nhigh-volume journeys were not adequate. Despite the success of this approach\nin other use cases, the performance was not up to par once we actually\nimplemented it.\n\nFor 95% of all journeys, we did not have a problem, but like most products\nthat we have, data volume per customer follows a power law distribution where\na handful of customers are generating significantly more data than others.\n\nWhat this means for us is that only a handful (< 1%) of journeys would fail to\nbe queried in the UI. This was either due to memory consumption on the query\nwhere we hit the max memory query limit or the query took too long and we hit\na service timeout of 60 seconds. We could not just increase the memory limit\nbecause the only way to do so here was to vertical scale the cluster which\nwould be cost-prohibitive. We also didn\u2019t want to increase the service timeout\nwhich could negatively impact the user experience. While ClickHouse served us\nwell in brute forcing most customers, analyzing journeys with billions of\nevents required a different approach.\n\nHow does someone begin to solve this?\n\nLet\u2019s look at logically what our query was trying to do:\n\n    \n    \n    SELECT event, count(distinct user_id) FROM journey_events WHERE journey_id = 'jou_123' AND timestamp between now() - interval 7 day and now()\n\nThe semantic meaning of this query is to \u201cFind me all unique users for a\njourney over the last 7 days by event (e.g. entered/completed/exited the\njourney)\u201d.\n\nNow how could you solve this without SQL? One way is to create a hash table\nand throw all the user ids in there. Once you\u2019ve iterated through all the\nrecords, the key set is your unique set. Assuming this is what ClickHouse does\nunder the hood might explain the large memory consumption on query since many\nof these large journeys could have 100 million or more unique users and\nbillions of events.\n\nDue to the data being spread across multiple ClickHouse nodes, when you query\nfor unique counts, you have to gather all the rows that are relevant back to\nthe initiator node to perform the uniqueness.\n\nWhat are potential ways to fix this?\n\n  * Spill to disk? Even though we are using NVMe instance storage, this was a non-starter because of the perceived drastic performance impact.\n\n  * Use an approximation algorithm (HyperLogLog, uniqCombined) or sample the data? This was not a viable approach based on the product requirements.\n\nWhat we came up with:\n\nWe wanted to keep the requirement of providing exact unique values for our\ncustomers so that they\u2019d have an accurate view of their campaigns. Without the\nskill set to effectively dive into the ClickHouse code and truly understand\nwhat was going on, we had to make a hypothesis of what ClickHouse was doing.\n\nWe hypothesized that because we were randomly distributing events across all\nnodes, the initiating query node would need to pull all Segment IDs to one\nnode. From here, it would build a set to determine the unique keys. One simple\nway to solve this would be to constantly vertically scale the nodes in the\ncluster, though this would be cost-prohibitive and give us an effective upper\nbound of how many events we could process. Could we do something else to\nmaintain a distributed workload and fully use multiple machines?\n\n## Semantic sharding:\n\nInstead of randomly placing events on nodes, we could be a bit more clever and\nensure that for a given user, they always end up on the same node.\n\nWhy would this matter? Imagine if we could guarantee that a user exists on one\nnode at all times. In this case, we could assume that our unique sets wouldn\u2019t\nhave overlap. This gives the nice property of being able to calculate the\nunique number of users on each node and simply transfer the resulting unique\ncounts from each node to the initiator node and sum those numbers up. This\nwould drastically reduce the memory footprint of calculating uniques per node\nand the final aggregation would be trivial.\n\nThe question now is, how can we actually do this in ClickHouse? Luckily there\nis a distributed_group_by_no_merge setting that allowed us to do this with a\nslight tweak of the query. That is, instead of getting one aggregated count we\nalso needed to do a final sum on the query to get the true result.\n\nImplementing this made 100% of our queries usable against any journey at any\ntime range. Previously sluggish 50-second queries now fly at sub-second\nspeeds, unlocking the ability to instantly analyze any journey data near real-\ntime.\n\n## Hashing of Filter and Group By Keys:\n\nAnother major factor of performance turned out to be how we were filtering and\ngrouping by our UUID keys. All of our UUIDs were 32 character UUID strings.\nUnder the hood, string comparison is of course O(n), so when doing a group by\nor a filter with a string with billions or trillions of rows, each of those\ncomparisons was O(n) and while the strings would usually start differing\nrapidly, sometimes the similarities would last several characters into the\nstring. So what was the solution? We started using CityHash64 to turn the\nstrings into integers. Integer comparison is O(1). This reduced the query time\nby 80%.\n\nWhat about collisions? With a 64 bit hash, there were\n9,223,372,036,854,775,807 possible values. So even with a billion unique\nvalues, the odds of collisions were roughly 9 billion to 1. For our use case,\nwe tested with our hashed unique values vs our UUID string values and had the\nexact same number of unique values, verifying we had no collisions.\n\n## Post Optimization Results:\n\nToday in Production, we continued to see large journeys generate a lot of\nevents. However, with the optimizations we took here, we are able to handle\nqueries for even the largest of journeys. As we see further product adoption\nand the age of journeys increase, we expect that we\u2019ll need to continue to\nmake improvements to our performance. However, taking these initial steps\nwe\u2019re hoping gets us a long runway.\n\n## Test drive Segment CDP today\n\nIt\u2019s free to connect your data sources and destinations to the Segment CDP.\nUse one API to collect analytics data across any platform.\n\nGet started\n\nShare article\n\nKeep updated\n\n## Recommended articles\n\nEngineers & Developers\n\n### Using ClickHouse to count unique users at scale\n\nBy implementing semantic sharding and optimizing filtering and grouping with\nClickHouse, we transformed query times from minutes to seconds, ensuring\nefficient handling of high-volume journeys in production while paving the way\nfor future enhancements.\n\nCustomer Data Platform\n\n### 5 Myths of the Composable CDP\n\nComposable CDP - fact or fiction? We break down the five most popular myths.\n\nEngineers & Developers\n\n### Accelerating your time to value with effective Observability\n\nTwilio Segment provides a comprehensive suite of tools for efficiently\nmanaging, activating, and analyzing your organization's data, ensuring\ntransparency and empowering you to unlock its full potential effortlessly.\n\n## Want to keep updated on Segment launches, events, and updates?\n\nSee how it works\n\nProducts\n\n  * Connections\n  * Protocols\n  * Unify\n  * Twilio Engage\n  * Customer Data Platform\n  * Integrations Catalog\n  * Pricing\n  * Security\n  * GDPR\n\nFor Developers\n\n  * Documentation\n  * Segment API\n  * Build on Segment\n  * Open Source\n  * Engineering Team\n\nCompany\n\n  * Careers\n  * Blog\n  * Press\n  * Events\n  * Podcast\n  * Growth Center\n  * Data Hub\n\nSupport\n\n  * Help Center\n  * Contact us\n  * Resources\n  * Recipes\n  * Professional Services\n  * Security Bulletins\n  * Documentation\n  * Release Notes\n  * Become a Partner\n  * Guide to Customer Data Platforms\n\n  * \u00a9 2024 Twilio Inc. All Rights Reserved.\n\n  * Privacy policy\n  * Terms of Service\n  * Website Data Collection\n\n", "frontpage": false}
