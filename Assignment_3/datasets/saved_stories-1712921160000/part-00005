{"aid": "40007932", "title": "The Semantic Web (2001)", "url": "https://web.archive.org/web/20011109213848/http://www.scientificamerican.com/2001/0501issue/0501berners-lee.html", "domain": "archive.org", "votes": 1, "user": "aragonite", "posted_at": "2024-04-11 23:32:42", "comments": 0, "source_title": "Scientific American: Feature Article: The Semantic Web: May 2001", "source_text": "Scientific American: Feature Article: The Semantic Web: May 2001\n\nThe Wayback Machine -\nhttp://www.scientificamerican.com:80/2001/0501issue/0501berners-lee.html\n\n# The Semantic Web\n\nA new form of Web content that is meaningful to computers will unleash a\nrevolution of new possibilitiesby TIM BERNERS-LEE, JAMES HENDLER and ORA\nLASSILA  \n---  \n........... SUBTOPICS: Expressing MeaningKnowledge\nRepresentationOntologiesAgentsEvolution of KnowledgeSIDEBARS: Overview /\nSemantic WebGlossaryWhat is the Killer App?ILLUSTRATIONS: Software AgentsWeb\nSearches TodaySemantic Web SearchesFURTHER INFORMATION| The entertainment\nsystem was belting out the Beatles' \"We Can Work It Out\" when the phone rang.\nWhen Pete answered, his phone turned the sound down by sending a message to\nall the other local devices that had a volume control. His sister, Lucy, was\non the line from the doctor's office: \"Mom needs to see a specialist and then\nhas to have a series of physical therapy sessions. Biweekly or something. I'm\ngoing to have my agent set up the appointments.\" Pete immediately agreed to\nshare the chauffeuring.At the doctor's office, Lucy instructed her Semantic\nWeb agent through her handheld Web browser. The agent promptly retrieved\ninformation about Mom's prescribed treatment from the doctor's agent, looked\nup several lists of providers, and checked for the ones in-plan for Mom's\ninsurance within a 20-mile radius of her home and with a rating of excellent\nor very good on trusted rating services. It then began trying to find a match\nbetween available appointment times (supplied by the agents of individual\nproviders through their Web sites) and Pete's and Lucy's busy schedules. (The\nemphasized keywords indicate terms whose semantics, or meaning, were defined\nfor the agent through the Semantic Web.)In a few minutes the agent presented\nthem with a plan. Pete didn't like it\u2014University Hospital was all the way\nacross town from Mom's place, and he'd be driving back in the middle of rush\nhour. He set his own agent to redo the search with stricter preferences about\nlocation and time. Lucy's agent, having complete trust in Pete's agent in the\ncontext of the present task, automatically assisted by supplying access\ncertificates and shortcuts to the data it had already sorted through.Almost\ninstantly the new plan was presented: a much closer clinic and earlier\ntimes\u2014but there were two warning notes. First, Pete would have to reschedule a\ncouple of his less important appointments. He checked what they were\u2014not a\nproblem. The other was something about the insurance company's list failing to\ninclude this provider under physical therapists: \"Service type and insurance\nplan status securely verified by other means,\" the agent reassured him.\n\"(Details?)\"Lucy registered her assent at about the same moment Pete was\nmuttering, \"Spare me the details,\" and it was all set. (Of course, Pete\ncouldn't resist the details and later that night had his agent explain how it\nhad found that provider even though it wasn't on the proper list.)\n\n## Expressing Meaning\n\nPete and Lucy could use their agents to carry out all these tasks thanks not\nto the World Wide Web of today but rather the Semantic Web that it will evolve\ninto tomorrow. Most of the Web's content today is designed for humans to read,\nnot for computer programs to manipulate meaningfully. Computers can adeptly\nparse Web pages for layout and routine processing\u2014here a header, there a link\nto another page\u2014but in general, computers have no reliable way to process the\nsemantics: this is the home page of the Hartman and Strauss Physio Clinic,\nthis link goes to Dr. Hartman's curriculum vitae.The Semantic Web will bring\nstructure to the meaningful content of Web pages, creating an environment\nwhere software agents roaming from page to page can readily carry out\nsophisticated tasks for users. Such an agent coming to the clinic's Web page\nwill know not just that the page has keywords such as \"treatment, medicine,\nphysical, therapy\" (as might be encoded today) but also that Dr. Hartman works\nat this clinic on Mondays, Wednesdays and Fridays and that the script takes a\ndate range in yyyy-mm-dd format and returns appointment times. And it will\n\"know\" all this without needing artificial intelligence on the scale of 2001's\nHal or Star Wars's C-3PO. Instead these semantics were encoded into the Web\npage when the clinic's office manager (who never took Comp Sci 101) massaged\nit into shape using off-the-shelf software for writing Semantic Web pages\nalong with resources listed on the Physical Therapy Association's site.The\nSemantic Web is not a separate Web but an extension of the current one, in\nwhich information is given well-defined meaning, better enabling computers and\npeople to work in cooperation. The first steps in weaving the Semantic Web\ninto the structure of the existing Web are already under way. In the near\nfuture, these developments will usher in significant new functionality as\nmachines become much better able to process and \"understand\" the data that\nthey merely display at present.The essential property of the World Wide Web is\nits universality. The power of a hypertext link is that \"anything can link to\nanything.\" Web technology, therefore, must not discriminate between the\nscribbled draft and the polished performance, between commercial and academic\ninformation, or among cultures, languages, media and so on. Information varies\nalong many axes. One of these is the difference between information produced\nprimarily for human consumption and that produced mainly for machines. At one\nend of the scale we have everything from the five-second TV commercial to\npoetry. At the other end we have databases, programs and sensor output. To\ndate, the Web has developed most rapidly as a medium of documents for people\nrather than for data and information that can be processed automatically. The\nSemantic Web aims to make up for this.Like the Internet, the Semantic Web will\nbe as decentralized as possible. Such Web-like systems generate a lot of\nexcitement at every level, from major corporation to individual user, and\nprovide benefits that are hard or impossible to predict in advance.\nDecentralization requires compromises: the Web had to throw away the ideal of\ntotal consistency of all of its interconnections, ushering in the infamous\nmessage \"Error 404: Not Found\" but allowing unchecked exponential growth.\n\n## Knowledge Representation\n\nFor the semantic web to function, computers must have access to structured\ncollections of information and sets of inference rules that they can use to\nconduct automated reasoning. Artificial-intelligence researchers have studied\nsuch systems since long before the Web was developed. Knowledge\nrepresentation, as this technology is often called, is currently in a state\ncomparable to that of hypertext before the advent of the Web: it is clearly a\ngood idea, and some very nice demonstrations exist, but it has not yet changed\nthe world. It contains the seeds of important applications, but to realize its\nfull potential it must be linked into a single global system.| WEB SEARCHES\nTODAY  \n---  \n  \nTraditional knowledge-representation systems typically have been centralized,\nrequiring everyone to share exactly the same definition of common concepts\nsuch as \"parent\" or \"vehicle.\" But central control is stifling, and increasing\nthe size and scope of such a system rapidly becomes unmanageable.\n\nMoreover, these systems usually carefully limit the questions that can be\nasked so that the computer can answer reliably\u2014 or answer at all. The problem\nis reminiscent of G\u00f6del's theorem from mathematics: any system that is complex\nenough to be useful also encompasses unanswerable questions, much like\nsophisticated versions of the basic paradox \"This sentence is false.\" To avoid\nsuch problems, traditional knowledge-representation systems generally each had\ntheir own narrow and idiosyncratic set of rules for making inferences about\ntheir data. For example, a genealogy system, acting on a database of family\ntrees, might include the rule \"a wife of an uncle is an aunt.\" Even if the\ndata could be transferred from one system to another, the rules, existing in a\ncompletely different form, usually could not.\n\nSemantic Web researchers, in contrast, accept that paradoxes and unanswerable\nquestions are a price that must be paid to achieve versatility. We make the\nlanguage for the rules as expressive as needed to allow the Web to reason as\nwidely as desired. This philosophy is similar to that of the conventional Web:\nearly in the Web's development, detractors pointed out that it could never be\na well-organized library; without a central database and tree structure, one\nwould never be sure of finding everything. They were right. But the expressive\npower of the system made vast amounts of information available, and search\nengines (which would have seemed quite impractical a decade ago) now produce\nremarkably complete indices of a lot of the material out there. The challenge\nof the Semantic Web, therefore, is to provide a language that expresses both\ndata and rules for reasoning about the data and that allows rules from any\nexisting knowledge-representation system to be exported onto the Web.\n\nAdding logic to the Web\u2014the means to use rules to make inferences, choose\ncourses of action and answer questions\u2014is the task before the Semantic Web\ncommunity at the moment. A mixture of mathematical and engineering decisions\ncomplicate this task. The logic must be powerful enough to describe complex\nproperties of objects but not so powerful that agents can be tricked by being\nasked to consider a paradox. Fortunately, a large majority of the information\nwe want to express is along the lines of \"a hex-head bolt is a type of machine\nbolt,\" which is readily written in existing languages with a little extra\nvocabulary.\n\nTwo important technologies for developing the Semantic Web are already in\nplace: eXtensible Markup Language (XML) and the Resource Description Framework\n(RDF). XML lets everyone create their own tags\u2014hidden labels such as <zip\ncode> or <alma mater> that annotate Web pages or sections of text on a page.\nScripts, or programs, can make use of these tags in sophisticated ways, but\nthe script writer has to know what the page writer uses each tag for. In\nshort, XML allows users to add arbitrary structure to their documents but says\nnothing about what the structures mean [see \"XML and the Second-Generation\nWeb,\" by Jon Bosak and Tim Bray; Scientific American, May 1999].\n\nThe Semantic Web will enable machines to COMPREHEND semantic documents and\ndata, not human speech and writings.\n\nHuman language thrives when using the same term to mean somewhat different\nthings, but automation does not. Imagine that I hire a clown messenger service\nto deliver balloons to my customers on their birthdays. Unfortunately, the\nservice transfers the addresses from my database to its database, not knowing\nthat the \"addresses\" in mine are where bills are sent and that many of them\nare post office boxes. My hired clowns end up entertaining a number of postal\nworkers\u2014not necessarily a bad thing but certainly not the intended effect.\nUsing a different URI for each specific concept solves that problem. An\naddress that is a mailing address can be distinguished from one that is a\nstreet address, and both can be distinguished from an address that is a\nspeech.\n\nThe triples of RDF form webs of information about related things. Because RDF\nuses URIs to encode this information in a document, the URIs ensure that\nconcepts are not just words in a document but are tied to a unique definition\nthat everyone can find on the Web. For example, imagine that we have access to\na variety of databases with information about people, including their\naddresses. If we want to find people living in a specific zip code, we need to\nknow which fields in each database represent names and which represent zip\ncodes. RDF can specify that \"(field 5 in database A) (is a field of type) (zip\ncode),\" using URIs rather than phrases for each term.\n\n## Ontologies\n\nOf course, this is not the end of the story, because two databases may use\ndifferent identifiers for what is in fact the same concept, such as zip code.\nA program that wants to compare or combine information across the two\ndatabases has to know that these two terms are being used to mean the same\nthing. Ideally, the program must have a way to discover such common meanings\nfor whatever databases it encounters.\n\nA solution to this problem is provided by the third basic component of the\nSemantic Web, collections of information called ontologies. In philosophy, an\nontology is a theory about the nature of existence, of what types of things\nexist; ontology as a discipline studies such theories. Artificial-intelligence\nand Web researchers have co-opted the term for their own jargon, and for them\nan ontology is a document or file that formally defines the relations among\nterms. The most typical kind of ontology for the Web has a taxonomy and a set\nof inference rules.\n\nThe taxonomy defines classes of objects and relations among them. For example,\nan address may be defined as a type of location, and city codes may be defined\nto apply only to locations, and so on. Classes, subclasses and relations among\nentities are a very powerful tool for Web use. We can express a large number\nof relations among entities by assigning properties to classes and allowing\nsubclasses to inherit such properties. If city codes must be of type city and\ncities generally have Web sites, we can discuss the Web site associated with a\ncity code even if no database links a city code directly to a Web site.\n\nInference rules in ontologies supply further power. An ontology may express\nthe rule \"If a city code is associated with a state code, and an address uses\nthat city code, then that address has the associated state code.\" A program\ncould then readily deduce, for instance, that a Cornell University address,\nbeing in Ithaca, must be in New York State, which is in the U.S., and\ntherefore should be formatted to U.S. standards. The computer doesn't truly\n\"understand\" any of this information, but it can now manipulate the terms much\nmore effectively in ways that are useful and meaningful to the human user.\n\nWith ontology pages on the Web, solutions to terminology (and other) problems\nbegin to emerge. The meaning of terms or XML codes used on a Web page can be\ndefined by pointers from the page to an ontology. Of course, the same problems\nas before now arise if I point to an ontology that defines addresses as\ncontaining a zip code and you point to one that uses postal code. This kind of\nconfusion can be resolved if ontologies (or other Web services) provide\nequivalence relations: one or both of our ontologies may contain the\ninformation that my zip code is equivalent to your postal code.\n\nOur scheme for sending in the clowns to entertain my customers is partially\nsolved when the two databases point to different definitions of address. The\nprogram, using distinct URIs for different concepts of address, will not\nconfuse them and in fact will need to discover that the concepts are related\nat all. The program could then use a service that takes a list of postal\naddresses (defined in the first ontology) and converts it into a list of\nphysical addresses (the second ontology) by recognizing and removing post\noffice boxes and other unsuitable addresses. The structure and semantics\nprovided by ontologies make it easier for an entrepreneur to provide such a\nservice and can make its use completely transparent.\n\nOntologies can enhance the functioning of the Web in many ways. They can be\nused in a simple fashion to improve the accuracy of Web searches\u2014the search\nprogram can look for only those pages that refer to a precise concept instead\nof all the ones using ambiguous keywords. More advanced applications will use\nontologies to relate the information on a page to the associated knowledge\nstructures and inference rules. An example of a page marked up for such use is\nonline at http://www.cs.umd.edu/~hendler. If you send your Web browser to that\npage, you will see the normal Web page entitled \"Dr. James A. Hendler.\" As a\nhuman, you can readily find the link to a short biographical note and read\nthere that Hendler received his Ph.D. from Brown University. A computer\nprogram trying to find such information, however, would have to be very\ncomplex to guess that this information might be in a biography and to\nunderstand the English language used there.\n\nFor computers, the page is linked to an ontology page that defines information\nabout computer science departments. For instance, professors work at\nuniversities and they generally have doctorates. Further markup on the page\n(not displayed by the typical Web browser) uses the ontology's concepts to\nspecify that Hendler received his Ph.D. from the entity described at the URI\nhttp://www. brown.edu \u2014 the Web page for Brown. Computers can also find that\nHendler is a member of a particular research project, has a particular e-mail\naddress, and so on. All that information is readily processed by a computer\nand could be used to answer queries (such as where Dr. Hendler received his\ndegree) that currently would require a human to sift through the content of\nvarious pages turned up by a search engine.\n\nIn addition, this markup makes it much easier to develop programs that can\ntackle complicated questions whose answers do not reside on a single Web page.\nSuppose you wish to find the Ms. Cook you met at a trade conference last year.\nYou don't remember her first name, but you remember that she worked for one of\nyour clients and that her son was a student at your alma mater. An intelligent\nsearch program can sift through all the pages of people whose name is \"Cook\"\n(sidestepping all the pages relating to cooks, cooking, the Cook Islands and\nso forth), find the ones that mention working for a company that's on your\nlist of clients and follow links to Web pages of their children to track down\nif any are in school at the right place.\n\n## Agents\n\nAGENTS  \n---  \n  \nThe real power of the Semantic Web will be realized when people create many\nprograms that collect Web content from diverse sources, process the\ninformation and exchange the results with other programs. The effectiveness of\nsuch software agents will increase exponentially as more machine-readable Web\ncontent and automated services (including other agents) become available. The\nSemantic Web promotes this synergy: even agents that were not expressly\ndesigned to work together can transfer data among themselves when the data\ncome with semantics.\n\nAn important facet of agents' functioning will be the exchange of \"proofs\"\nwritten in the Semantic Web's unifying language (the language that expresses\nlogical inferences made using rules and information such as those specified by\nontologies). For example, suppose Ms. Cook's contact information has been\nlocated by an online service, and to your great surprise it places her in\nJohannesburg. Naturally, you want to check this, so your computer asks the\nservice for a proof of its answer, which it promptly provides by translating\nits internal reasoning into the Semantic Web's unifying language. An inference\nengine in your computer readily verifies that this Ms. Cook indeed matches the\none you were seeking, and it can show you the relevant Web pages if you still\nhave doubts. Although they are still far from plumbing the depths of the\nSemantic Web's potential, some programs can already exchange proofs in this\nway, using the current preliminary versions of the unifying language.\n\nAnother vital feature will be digital signatures, which are encrypted blocks\nof data that computers and agents can use to verify that the attached\ninformation has been provided by a specific trusted source. You want to be\nquite sure that a statement sent to your accounting program that you owe money\nto an online retailer is not a forgery generated by the computer-savvy\nteenager next door. Agents should be skeptical of assertions that they read on\nthe Semantic Web until they have checked the sources of information. (We wish\nmore people would learn to do this on the Web as it is!)\n\nMany automated Web-based services already exist without semantics, but other\nprograms such as agents have no way to locate one that will perform a specific\nfunction. This process, called service discovery, can happen only when there\nis a common language to describe a service in a way that lets other agents\n\"understand\" both the function offered and how to take advantage of it.\nServices and agents can advertise their function by, for example, depositing\nsuch descriptions in directories analogous to the Yellow Pages.\n\nSome low-level service-discovery schemes are currently available, such as\nMicrosoft's Universal Plug and Play, which focuses on connecting different\ntypes of devices, and Sun Microsystems's Jini, which aims to connect services.\nThese initiatives, however, attack the problem at a structural or syntactic\nlevel and rely heavily on standardization of a predetermined set of\nfunctionality descriptions. Standardization can only go so far, because we\ncan't anticipate all possible future needs.\n\nProperly designed, the Semantic Web can assist the evolution of human\nknowledge as a whole.\n\nA typical process will involve the creation of a \"value chain\" in which\nsubassemblies of information are passed from one agent to another, each one\n\"adding value,\" to construct the final product requested by the end user. Make\nno mistake: to create complicated value chains automatically on demand, some\nagents will exploit artificial-intelligence technologies in addition to the\nSemantic Web. But the Semantic Web will provide the foundations and the\nframework to make such technologies more feasible.\n\nPutting all these features together results in the abilities exhibited by\nPete's and Lucy's agents in the scenario that opened this article. Their\nagents would have delegated the task in piecemeal fashion to other services\nand agents discovered through service advertisements. For example, they could\nhave used a trusted service to take a list of providers and determine which of\nthem are in-plan for a specified insurance plan and course of treatment. The\nlist of providers would have been supplied by another search service, et\ncetera. These activities formed chains in which a large amount of data\ndistributed across the Web (and almost worthless in that form) was\nprogressively reduced to the small amount of data of high value to Pete and\nLucy\u2014a plan of appointments to fit their schedules and other requirements.\n\nIn the next step, the Semantic Web will break out of the virtual realm and\nextend into our physical world. URIs can point to anything, including physical\nentities, which means we can use the RDF language to describe devices such as\ncell phones and TVs. Such devices can advertise their functionality\u2014what they\ncan do and how they are controlled\u2014much like software agents. Being much more\nflexible than low-level schemes such as Universal Plug and Play, such a\nsemantic approach opens up a world of exciting possibilities.\n\nFor instance, what today is called home automation requires careful\nconfiguration for appliances to work together. Semantic descriptions of device\ncapabilities and functionality will let us achieve such automation with\nminimal human intervention. A trivial example occurs when Pete answers his\nphone and the stereo sound is turned down. Instead of having to program each\nspecific appliance, he could program such a function once and for all to cover\nevery local device that advertises having a volume control \u2014 the TV, the DVD\nplayer and even the media players on the laptop that he brought home from work\nthis one evening.\n\nThe first concrete steps have already been taken in this area, with work on\ndeveloping a standard for describing functional capabilities of devices (such\nas screen sizes) and user preferences. Built on RDF, this standard is called\nComposite Capability/Preference Profile (CC/PP). Initially it will let cell\nphones and other nonstandard Web clients describe their characteristics so\nthat Web content can be tailored for them on the fly. Later, when we add the\nfull versatility of languages for handling ontologies and logic, devices could\nautomatically seek out and employ services and other devices for added\ninformation or functionality. It is not hard to imagine your Web-enabled\nmicrowave oven consulting the frozen-food manufacturer's Web site for optimal\ncooking parameters.\n\n## Evolution of Knowledge\n\nELABORATE, PRECISE SEARCHES  \n---  \n  \nThe semantic web is not \"merely\" the tool for conducting individual tasks that\nwe have discussed so far. In addition, if properly designed, the Semantic Web\ncan assist the evolution of human knowledge as a whole.\n\nHuman endeavor is caught in an eternal tension between the effectiveness of\nsmall groups acting independently and the need to mesh with the wider\ncommunity. A small group can innovate rapidly and efficiently, but this\nproduces a subculture whose concepts are not understood by others.\nCoordinating actions across a large group, however, is painfully slow and\ntakes an enormous amount of communication. The world works across the spectrum\nbetween these extremes, with a tendency to start small\u2014from the personal\nidea\u2014and move toward a wider understanding over time.\n\nAn essential process is the joining together of subcultures when a wider\ncommon language is needed. Often two groups independently develop very similar\nconcepts, and describing the relation between them brings great benefits. Like\na Finnish-English dictionary, or a weights-and-measures conversion table, the\nrelations allow communication and collaboration even when the commonality of\nconcept has not (yet) led to a commonality of terms.\n\nThe Semantic Web, in naming every concept simply by a URI, lets anyone express\nnew concepts that they invent with minimal effort. Its unifying logical\nlanguage will enable these concepts to be progressively linked into a\nuniversal Web. This structure will open up the knowledge and workings of\nhumankind to meaningful analysis by software agents, providing a new class of\ntools by which we can live, work and learn together.\n\nPHOTOILLUSTRATIONS BY MIGUEL SALMERON\n\nFurther Information:\n\nWeaving the Web: The Original Design and Ultimate Destiny of the World Wide\nWeb by Its Inventor. Tim Berners-Lee, with Mark Fischetti. Harper San\nFrancisco, 1999. An enhanced version of this article is on the Scientific\nAmerican Web site, with additional material and links.\n\nWorld Wide Web Consortium (W3C): www.w3.org/\n\nW3C Semantic Web Activity: www.w3.org/2001/sw/\n\nAn introduction to ontologies: www.SemanticWeb.org/knowmarkup.html\n\nSimple HTML Ontology Extensions Frequently Asked Questions (SHOE FAQ):\nwww.cs.umd.edu/projects/plus/SHOE/faq.html\n\nDARPA Agent Markup Language (DAML) home page: www.daml.org/\n\nThe Authors\n\nTIM BERNERS-LEE, JAMES HENDLER and ORA LASSILA are individually and\ncollectively obsessed with the potential of Semantic Web technology. Berners-\nLee is director of the World Wide Web Consortium (W3C) and a researcher at the\nLaboratory for Computer Science at the Massachusetts Institute of Technology.\nWhen he invented the Web in 1989, he intended it to carry more semantics than\nbecame common practice. Hendler is professor of computer science at the\nUniversity of Maryland at College Park, where he has been doing research on\nknowledge representation in a Web context for a number of years. He and his\ngraduate research group developed SHOE, the first Web-based knowledge\nrepresentation language to demonstrate many of the agent capabilities\ndescribed in this article. Hendler is also responsible for agent-based\ncomputing research at the Defense Advanced Research Projects Agency (DARPA) in\nArlington, Va. Lassila is a research fellow at the Nokia Research Center in\nBoston, chief scientist of Nokia Venture Partners and a member of the W3C\nAdvisory Board. Frustrated with the difficulty of building agents and\nautomating tasks on the Web, he co-authored W3C\u2019s RDF specification, which\nserves as the foundation for many current Semantic Web efforts.\n\n", "frontpage": false}
