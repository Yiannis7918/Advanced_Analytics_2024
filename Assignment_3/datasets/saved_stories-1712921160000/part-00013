{"aid": "40008005", "title": "Intel's \"Gaudi 3\" AI accelerator chip may give Nvidia's H100 a run for its money", "url": "https://arstechnica.com/information-technology/2024/04/intels-gaudi-3-ai-accelerator-chip-may-give-nvidias-h100-a-run-for-the-money/", "domain": "arstechnica.com", "votes": 3, "user": "justinclift", "posted_at": "2024-04-11 23:46:32", "comments": 2, "source_title": "Intel\u2019s \u201cGaudi 3\u201d AI accelerator chip may give Nvidia\u2019s H100 a run for its money", "source_text": "Intel\u2019s \u201cGaudi 3\u201d AI accelerator chip may give Nvidia\u2019s H100 a run for its money | Ars Technica\n\nSkip to main content\n\nSubscribe\n\nClose\n\n### Navigate\n\n  * Store\n  * Subscribe\n  * Videos\n  * Features\n  * Reviews\n\n  * RSS Feeds\n  * Mobile Site\n\n  * About Ars\n  * Staff Directory\n  * Contact Us\n\n  * Advertise with Ars\n  * Reprints\n\n### Filter by topic\n\n  * Biz & IT\n  * Tech\n  * Science\n  * Policy\n  * Cars\n  * Gaming & Culture\n  * Store\n  * Forums\n\n### Settings\n\nFront page layout\n\nGrid\n\nList\n\nSite theme\n\nlight\n\ndark\n\nSign in\n\n#### Adventures in Matrix Multiplication \u2014\n\n# Intel\u2019s \u201cGaudi 3\u201d AI accelerator chip may give Nvidia\u2019s H100 a run for its\nmoney\n\n## Intel claims 50% more speed when running AI language models vs. the market\nleader.\n\nBenj Edwards - 4/11/2024, 8:56 PM\n\nEnlarge / An Intel handout photo of the Gaudi 3 AI accelerator.\n\nIntel\n\n#### reader comments\n\n28\n\nOn Tuesday, Intel revealed a new AI accelerator chip called Gaudi 3 at its\nVision 2024 event in Phoenix. With strong claimed performance while running\nlarge language models (like those that power ChatGPT), the company has\npositioned Gaudi 3 as an alternative to Nvidia's H100, a popular data center\nGPU that has been subject to shortages, though apparently that is easing\nsomewhat.\n\nCompared to Nvidia's H100 chip, Intel projects a 50 percent faster training\ntime on Gaudi 3 for both OpenAI's GPT-3 175B LLM and the 7-billion parameter\nversion of Meta's Llama 2. In terms of inference (running the trained model to\nget outputs), Intel claims that its new AI chip delivers 50 percent faster\nperformance than H100 for Llama 2 and Falcon 180B, which are both relatively\npopular open-weights models.\n\n### Further Reading\n\nReport: Sam Altman seeking trillions for AI chip fabrication from UAE, others\n\nIntel is targeting the H100 because of its high market share, but the chip\nisn't Nvidia's most powerful AI accelerator chip in the pipeline.\nAnnouncements of the H200 and the Blackwell B200 have since surpassed the H100\non paper, but neither of those chips is out yet (the H200 is expected in the\nsecond quarter of 2024\u2014basically any day now).\n\nMeanwhile, the aforementioned H100 supply issues have been a major headache\nfor tech companies and AI researchers who have to fight for access to any\nchips that can train AI models. This has led several tech companies like\nMicrosoft, Meta, and OpenAI (rumor has it) to seek their own AI-accelerator\nchip designs, although that custom silicon is typically manufactured by either\nIntel or TSMC. Google has its own line of tensor processing units (TPUs) that\nit has been using internally since 2015.\n\nGiven those issues, Intel's Gaudi 3 may be a potentially attractive\nalternative to the H100 if Intel can hit an ideal price (which Intel has not\nprovided, but an H100 reportedly costs around $30,000\u2013$40,000) and maintain\nadequate production. AMD also manufactures a competitive range of AI chips,\nsuch as the AMD Instinct MI300 Series, that sell for around $10,000\u2013$15,000.\n\n## Gaudi 3 performance\n\nEnlarge / An Intel handout featuring specifications of the Gaudi 3 AI\naccelerator.\n\nIntel\n\nIntel says the new chip builds upon the architecture of its predecessor, Gaudi\n2, by featuring two identical silicon dies connected by a high-bandwidth\nconnection. Each die contains a central cache memory of 48 megabytes,\nsurrounded by four matrix multiplication engines and 32 programmable tensor\nprocessor cores, bringing the total cores to 64.\n\nAdvertisement\n\n### Further Reading\n\nNvidia unveils Blackwell B200, the \u201cworld\u2019s most powerful chip\u201d designed for\nAI\n\nThe chipmaking giant claims that Gaudi 3 delivers double the AI compute\nperformance of Gaudi 2 using 8-bit floating-point infrastructure, which has\nbecome crucial for training transformer models. The chip also offers a\nfourfold boost for computations using the BFloat 16-number format. Gaudi 3\nalso features 128GB of the less expensive HBMe2 memory capacity (which may\ncontribute to price competitiveness) and features 3.7TB of memory bandwidth.\n\nSince data centers are well-known to be power hungry, Intel emphasizes the\npower efficiency of Gaudi 3, claiming 40 percent greater inference power-\nefficiency across Llama 7B and 70B parameters, and Falcon 180B parameter\nmodels compared to Nvidia's H100. Eitan Medina, chief operating officer of\nIntel's Habana Labs, attributes this advantage to Gaudi's large-matrix math\nengines, which he claims require significantly less memory bandwidth compared\nto other architectures.\n\n## Gaudi vs. Blackwell\n\nEnlarge / An Intel handout photo of the Gaudi 3 AI accelerator.\n\nIntel\n\nLast month, we covered the splashy launch of Nvidia's Blackwell architecture,\nincluding the B200 GPU, which Nvidia claims will be the world's most powerful\nAI chip. It seems natural, then, to compare what we know about Nvidia's\nhighest-performing AI chip to the best of what Intel can currently produce.\n\nFor starters, Gaudi 3 is being manufactured using TSMC's N5 process\ntechnology, according to IEEE Spectrum, narrowing the gap between Intel and\nNvidia in terms of semiconductor fabrication technology. The upcoming Nvidia\nBlackwell chip will use a custom N4P process, which reportedly offers modest\nperformance and efficiency improvements over N5.\n\nGaudi 3's use of HBM2e memory (as we mentioned above) is notable compared to\nthe more expensive HBM3 or HBM3e used in competing chips, offering a balance\nof performance and cost-efficiency. This choice seems to emphasize Intel's\nstrategy to compete not only on performance but also on price.\n\n### Further Reading\n\nYour current PC probably doesn\u2019t have an AI processor, but your next one might\n\nAs far as raw performance comparisons between Gaudi 3 and the B200, that can't\nbe known until the chips have been released and benchmarked by a third party.\n\nAs the race to power the tech industry's thirst for AI computation heats up,\nIEEE Spectrum notes that the next generation of Intel's Gaudi chip, code-named\nFalcon Shores, remains a point of interest. It also remains to be seen whether\nIntel will continue to rely on TSMC's technology or leverage its own foundry\nbusiness and upcoming nanosheet transistor technology to gain a competitive\nedge in the AI accelerator market.\n\n### Ars Video\n\n### How The Callisto Protocol's Gameplay Was Perfected Months Before Release\n\n#### reader comments\n\n28\n\nBenj Edwards Benj Edwards is an AI and Machine Learning Reporter for Ars\nTechnica. In his free time, he writes and records music, collects vintage\ncomputers, and enjoys nature. He lives in Raleigh, NC.\n\nAdvertisement\n\n### Channel Ars Technica\n\n#### Unsolved Mysteries Of Quantum Leap With Donald P. Bellisario\n\nToday \"Quantum Leap\" series creator Donald P. Bellisario joins Ars Technica to\nanswer once and for all the lingering questions we have about his enduringly\npopular show. Was Dr. Sam Beckett really leaping between all those time\nperiods and people or did he simply imagine it all? What do people in the\nwaiting room do while Sam is in their bodies? What happens to Sam's loyal ally\nAl? 30 years following the series finale, answers to these mysteries and more\nawait.\n\n  * ##### Unsolved Mysteries Of Quantum Leap With Donald P. Bellisario\n\n  * ##### Unsolved Mysteries Of Warhammer 40K With Author Dan Abnett\n\n  * ##### SITREP: F-16 replacement search a signal of F-35 fail?\n\n  * ##### Sitrep: Boeing 707\n\n  * ##### Steve Burke of GamersNexus Reacts To Their Top 1000 Comments On YouTube\n\n  * ##### Modern Vintage Gamer Reacts To His Top 1000 Comments On YouTube\n\n  * ##### How The NES Conquered A Skeptical America In 1985\n\n  * ##### Scott Manley Reacts To His Top 1000 YouTube Comments\n\n  * ##### How Horror Works in Amnesia: Rebirth, Soma and Amnesia: The Dark Descent\n\n  * ##### LGR's Clint Basinger Reacts To His Top 1000 YouTube Comments\n\n  * ##### The F-35's next tech upgrade\n\n  * ##### How One Gameplay Decision Changed Diablo Forever\n\n  * ##### Unsolved Mortal Kombat Mysteries With Dominic Cianciolo From NetherRealm Studios\n\n  * ##### US Navy Gets an Italian Accent\n\n  * ##### How Amazon\u2019s \u201cUndone\u201d Animates Dreams With Rotoscoping And Oil Paints\n\n  * ##### Fighter Pilot Breaks Down Every Button in an F-15 Cockpit\n\n  * ##### How NBA JAM Became A Billion-Dollar Slam Dunk\n\n  * ##### Linus \"Tech Tips\" Sebastian Reacts to His Top 1000 YouTube Comments\n\n  * ##### How Alan Wake Was Rebuilt 3 Years Into Development\n\n  * ##### How Prince of Persia Defeated Apple II's Memory Limitations\n\n  * ##### How Crash Bandicoot Hacked The Original Playstation\n\n  * ##### Myst: The challenges of CD-ROM | War Stories\n\n  * ##### Markiplier Reacts To His Top 1000 YouTube Comments\n\n  * ##### How Mind Control Saved Oddworld: Abe's Oddysee\n\n  * ##### Bioware answers unsolved mysteries of the Mass Effect universe\n\n  * ##### Civilization: It's good to take turns | War Stories\n\n  * ##### SITREP: DOD Resets Ballistic Missile Interceptor program\n\n  * ##### Warframe's Rebecca Ford reviews your characters\n\n  * ##### Subnautica: A world without guns | War Stories\n\n  * ##### How Slay the Spire\u2019s Original Interface Almost Killed the Game | War Stories\n\n  * ##### Amnesia: The Dark Descent - The horror facade | War Stories\n\n  * ##### Command & Conquer: Tiberian Sun | War Stories\n\n  * ##### Blade Runner: Skinjobs, voxels, and future noir | War Stories\n\n  * ##### Dead Space: The Drag Tentacle | War Stories\n\n  * ##### Teach the Controversy: Flat Earthers\n\n  * ##### Delta V: The Burgeoning World of Small Rockets, Paul Allen's Huge Plane, and SpaceX Gets a Crucial Green-light\n\n  * ##### Chris Hadfield explains his 'Space Oddity' video\n\n  * ##### The Greatest Leap, Episode 1: Risk\n\n  * ##### Ultima Online: The Virtual Ecology | War Stories\n\nMore videos\n\n\u2190 Previous story Next story \u2192\n\n### Related Stories\n\n### Today on Ars\n\nCNMN Collection WIRED Media Group \u00a9 2024 Cond\u00e9 Nast. All rights reserved. Use of and/or registration on any portion of this site constitutes acceptance of our User Agreement (updated 1/1/20) and Privacy Policy and Cookie Statement (updated 1/1/20) and Ars Technica Addendum (effective 8/21/2018). Ars may earn compensation on sales from links on this site. Read our affiliate link policy. Your California Privacy Rights | Manage Preferences The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond\u00e9 Nast. Ad Choices\n\n## We Care About Your Privacy\n\nWe and our 167 partners store and/or access information on a device, such as\nunique IDs in cookies to process personal data. You may accept or manage your\nchoices by clicking below or at any time in the privacy policy page. These\nchoices will be signaled to our partners and will not affect browsing\ndata.More information about your privacy\n\n### We and our partners process data to provide:\n\nUse precise geolocation data. Actively scan device characteristics for\nidentification. Store and/or access information on a device. Personalised\nadvertising and content, advertising and content measurement, audience\nresearch and services development.\n\n", "frontpage": true}
