{"aid": "40007946", "title": "How Retrieval-Augmented Generation Works", "url": "https://myscale.com/blog/how-does-retrieval-augmented-generation-system-work/", "domain": "myscale.com", "votes": 1, "user": "mountainview", "posted_at": "2024-04-11 23:35:36", "comments": 0, "source_title": "How Does a Retrieval-Augmented Generation System Work", "source_text": "How Does a Retrieval-Augmented Generation System Work\n\nMYSCALE Product Docs Pricing Resources Contact\n\nSign In\n\nFree Sign Up\n\n  * English\n  * Espa\u00f1ol\n  * \u7b80\u4f53\u4e2d\u6587\n  * Deutsch\n  * \u65e5\u672c\u8a9e\n\nMYSCALE\n\n  * Product\n\n    * MyScale Cloud\n    * MyScaleDB\n    * Benchmark\n    * Integration\n    * Comparison\n\n      * Pinecone\n      * Pgvector\n      * Qdrant\n      * Weaviate\n      * Opensearch\n  * Docs\n  * Pricing\n  * Resources\n\n    * Blog\n    * Applications\n  * Contact\n\nSign In\n\nFree Sign Up\n\n  * English\n  * Espa\u00f1ol\n  * \u7b80\u4f53\u4e2d\u6587\n  * Deutsch\n  * \u65e5\u672c\u8a9e\n\n# How Does a Retrieval-Augmented Generation System Work\n\nWed Mar 13 2024\n\n  * RAG\n  * LLM\n  * Vector Database\n\nLarge Language Models (opens new window) (LLMs) have revolutionized the field\nof Natural Language Processing (NLP), introducing a new way to interact with\ntechnology. Advanced models such as GPT (opens new window) and BERT (opens new\nwindow) have inaugurated a new era in semantic understanding. They enable\ncomputers to process and generate human-like text, bridging the gap between\nhuman communication and machine interpretation. LLMs are now powering multiple\napplications, including sentiment analysis, machine translation, question-\nanswering, text summarization, chatbots, virtual assistants, and more.\n\nDespite their practical applications, Large Language Models (LLMs) come with\ntheir own set of challenges. They are designed to be generalized, which means\nthey might lack specificity. Additionally, because they are trained on past\ndata, they might not always provide the latest information. This can result in\nLLMs generating incorrect or outdated responses, leading to a phenomena called\n\u201cHallucination (opens new window)\u201d. It arises when the models make errors or\ngenerate unpredictable information due to gaps in their training data.\n\nRetrieval-Augmented Generation (opens new window) (RAG) systems are used to\naddress issues like, lack of specificity, and real-time updates, offering\npotential solutions to enhance the responsible deployment of LLMs.\n\n## # What is RAG\n\nIn 2020, Meta researchers proposed retrieval-augmented generation (RAG) by\ncombining the Natural Language Generation (opens new window) (NLG) capability\nof LLMs with the Information Retrieval (IR) component to optimize the output.\nIt refers to a reliable knowledge source outside their training data sources\nbefore responding to the query. It extends LLM capabilities without requiring\nmodel retraining, which provides a cost-effective way to enhance output\nrelevancy, accuracy, and usability across various contexts.\n\nThe RAG architecture includes an up-to-date data source to enhance the\naccuracy during Generative AI tasks. It is divided into two main components:\nretrieval and generative component. The retrieval component is attached to a\ndata source, mostly a vector database, that retrieves the updated information\nabout the query. This information, along with the query, is provided to the\ngenerative component. The generative component is an LLM model that generates\nthe response accordingly. RAG improves the LLM understanding, and the\ngenerated response is more accurate and up-to-date.\n\nRelated Article: What to expect from RAG (opens new window)\n\n## # How to Set up the Retrieval Component of an RAG System\n\nFirst of all, you must collect all the data required for your application.\nOnce you complete the data collection, remove the irrelevant data. Divide the\ncollected data into manageable smaller chunks and convert these chunks into\nvector representation using embedding models (opens new window). Vectors are\nnumeric representations where semantically similar content is closer to each\nother. It allows the system to understand and match the user query with the\nrelevant information in the data source. Store vectors in a vector database\nand link the chunks of the source data with its embeddings. This will help in\nretrieving the data chunk of a vector similar to the user query.\n\nMyScale (opens new window) is a cloud-based vector database based on\nClickHouse (opens new window) that combines the usual SQL queries with the\nstrength of a vector database. This allows you to save and find high-\ndimensional data, like image features or text embeddings, using the regular\nSQL queries of asking for information. MyScale is especially powerful for AI\napplications where comparing vectors is important. It's designed to be\naffordable and practical for developers dealing with a lot of vector data in\nAI and machine learning tasks.\n\nAdditionally, MyScale is designed to be more affordable, faster, and more\naccurate than other options. To encourage users to experience its benefits,\nMyScale offers 5 million free vector storage on the free tier. This makes it a\ncost-efficient and user-friendly solution for developers exploring vector\ndatabases in their AI and machine learning endeavors.\n\nRelated Article: Build a RAG-Enabled Chatbot (opens new window)\n\n## # How Does an RAG System Work\n\nAfter setting up the retrieval component, we can now utilize it in our RAG\nsystem. To respond to a user query, we can use it to retrieve the relevant\ninformation and append it to the user query as a context before passing it to\nthe language model for response generation. Let\u2019s understand how to use the\nretrieval component to get the relevant information.\n\n### # Adding Relevant Information to the Query\n\nWhenever we receive a user query, the first step we need to perform is to\nconvert the user query into an embedding or vector representation. Use the\nsame embedding model we used to convert the data source into embeddings while\nsetting up the retrieval component. After the conversion of the user query\ninto a vector representation, find the similar vectors from the vector\ndatabase using any measure such as Euclidean distance (opens new window) or\ncosine similarity (opens new window). Use these vectors to retrieve the\nrelevant chunks of data and append them to the user query.\n\n### # Generating Response Using LLM\n\nNow, we have the query and the related chunks of information. Feed the user\nquery along with the retrieved data to the LLM (the generative component). The\nLLM is capable of understanding the user query and processing the provided\ndata. It will generate the response to the user\u2019s query according to the\ninformation received from the retrieval component.\n\nPassing the relevant information along with the user query to the LLM is a\nmethod that removes the hallucination issue of LLM. Now, the LLM can generate\nresponses to the user query using the information we pass it with the user\nquery.\n\nNote: Remember to update the database regularly with the latest information to\nensure the accuracy of the model.\n\n## # Some RAG Application Scenarios\n\nRAG systems can be used in different applications that need precise and\ncontextually relevant information retrieval. This helps improve the accuracy,\ntimeliness, and reliability of the generated responses. Let\u2019s discuss some\napplications of a RAG system.\n\n  1. Domain-specific questioning: When a RAG system faces questions in a specific domain, it utilizes the retrieval component to dynamically access external knowledge sources, databases, or domain-specific documents. This allows RAG systems to generate responses that are contextually relevant by reflecting the most recent and accurate information within the specified domain. This could be helpful in various domains, such as healthcare, legal interpretation, historical research, technical troubleshooting, etc.\n  2. Factual accuracy: Factual accuracy is crucial in ensuring that the generated content or responses align with accurate and verified data. In situations where inaccuracies may arise, RAG prioritizes factual accuracy to provide information that is consistent with the reality of the subject matter. This is essential for various applications, including news reporting, educational content, and any scenario where the reliability and trustworthiness of information are paramount.\n  3. Research queries: RAG systems are valuable in addressing research queries by dynamically retrieving relevant and up-to-date information from their knowledge sources. For example, suppose a researcher puts a query related to the latest advancements in a specific scientific field. In that case, a RAG system can leverage its retrieval component to access recent research papers, publications, and relevant data to ensure that the researcher receives contextually accurate and current insights.\n\nRelated Article: How to build a recommendation system (opens new window)\n\n## # Challenges to Build an RAG System\n\nAlthough the RAG systems have various use cases and advantages, they also face\na few unique limitations. Let\u2019s pen down them below:\n\n  1. Integration: Integrating a retrieval component with an LLM-based generative component could be difficult. The complexity is increased when working with multiple data sources in different formats. Make sure the consistency over all data sources using separate modules before integrating the retrieval component with the generative component.\n  2. Data Quality: RAG systems are dependent on the attached data source. The quality of a RAG system could be poor for multiple reasons, such as using low-quality content, using different embedding in case of multiple data sources, or using inconsistent data formats. Make sure to maintain the data quality.\n  3. Scalability: The performance of a RAG system is compromised as the amount of external data increases. The tasks of converting data into embedding, comparing the meaning of similar chunks of data, and retrieving in real time may become computationally intensive. This may slow down the RAG system. To address this issue, you can use MyScale that has resolved the issue by providing a 390 QPS(Query Per Second) on the LAION 5M dataset with a 95% recall rate and 17ms of average query latency witt the x1 pod.\n\n## # Conclusion\n\nRAG is one of the techniques to improve the capabilities of LLM by attaching a\nknowledge base to it. You can understand it as a search engine with language\ngeneration abilities. These systems mitigate the hallucination issue of LLMs\nwithout any re-training or fine-tuning cost. Using an external data source\nwhile responding to the user query provides a more accurate and up-to-date\nresponse, especially when working with factual, latest, or regularly updated\ndata.Despite these advantages of RAG systems, it also comes with their\nlimitations.\n\nMyScale offers a powerful solution for large-scale and complex RAG\napplications by combining the strengths of ClickHouse, advanced vector search\nalgorithms and joint SQL vector optimizations. It is specifically designed for\nAI applications, considering all factors including cost and scalability.\nAdditionally, it provides integrations with well-known AI frameworks like\nLangChain (opens new window) and LlamaIndex (opens new window). These\nqualities and features make MyScale the best fit for your next AI application.\n\nIf you have any suggestions and feedbacks, please reach out to us through\nTwitter (opens new window) or Discord (opens new window).\n\nWhat is RAG\n\nHow to Set up the Retrieval Component of an RAG System\n\nHow Does an RAG System Work\n\nAdding Relevant Information to the Query\n\nGenerating Response Using LLM\n\nSome RAG Application Scenarios\n\nChallenges to Build an RAG System\n\nConclusion\n\nMYSCALE\n\n## PRODUCT\n\n  * MyScale Cloud\n  * MyScaleDB\n  * Integration\n  * Pricing\n  * Trust & Security\n\n## RESOURCES\n\n  * Docs\n  * Blog\n  * Applications\n  * Benchmark\n  * System Status\n\n## COMMUNITY\n\n  * Contact Us\n  * GitHub\n  * Discord\n  * Twitter\n  * LinkedIn\n  * Medium\n\n## PROTOCOLS\n\n  * Privacy Policy\n  * Cookie Policy\n  * Terms of Service\n  * Support Policy\n\n\u00a9 2024 MOQI SINGAPORE PTE. LTD. All rights reserved.\n\n", "frontpage": false}
