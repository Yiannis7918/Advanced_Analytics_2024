{"aid": "40077901", "title": "Verbal lie detection using Large Language Models", "url": "https://www.nature.com/articles/s41598-023-50214-0", "domain": "nature.com", "votes": 1, "user": "rntn", "posted_at": "2024-04-18 16:27:36", "comments": 0, "source_title": "Verbal lie detection using Large Language Models", "source_text": "Verbal lie detection using Large Language Models | Scientific Reports\n\nSkip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n\n## Search\n\nAdvanced search\n\n### Quick links\n\n    * Explore articles by subject\n    * Find a job\n    * Guide to authors\n    * Editorial policies\n\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n\n  * Sign up for alerts\n  * RSS feed\n\nVerbal lie detection using Large Language Models\n\nDownload PDF\n\nDownload PDF\n\n  * Article\n  * Open access\n  * Published: 21 December 2023\n\n# Verbal lie detection using Large Language Models\n\n  * Riccardo Loconte^1,\n  * Roberto Russo^2,\n  * Pasquale Capuozzo^3,\n  * Pietro Pietrini^1 &\n  * ...\n  * Giuseppe Sartori^3\n\nScientific Reports volume 13, Article number: 22849 (2023) Cite this article\n\n  * 2238 Accesses\n\n  * 2 Citations\n\n  * 15 Altmetric\n\n  * Metrics details\n\n## Abstract\n\nHuman accuracy in detecting deception with intuitive judgments has been proven\nto not go above the chance level. Therefore, several automatized verbal lie\ndetection techniques employing Machine Learning and Transformer models have\nbeen developed to reach higher levels of accuracy. This study is the first to\nexplore the performance of a Large Language Model, FLAN-T5 (small and base\nsizes), in a lie-detection classification task in three English-language\ndatasets encompassing personal opinions, autobiographical memories, and future\nintentions. After performing stylometric analysis to describe linguistic\ndifferences in the three datasets, we tested the small- and base-sized FLAN-T5\nin three Scenarios using 10-fold cross-validation: one with train and test set\ncoming from the same single dataset, one with train set coming from two\ndatasets and the test set coming from the third remaining dataset, one with\ntrain and test set coming from all the three datasets. We reached state-of-\nthe-art results in Scenarios 1 and 3, outperforming previous benchmarks. The\nresults revealed also that model performance depended on model size, with\nlarger models exhibiting higher performance. Furthermore, stylometric analysis\nwas performed to carry out explainability analysis, finding that linguistic\nfeatures associated with the Cognitive Load framework may influence the\nmodel\u2019s predictions.\n\n### Similar content being viewed by others\n\n### Anger is eliminated with the disposal of a paper written because of\nprovocation\n\nArticle Open access 09 April 2024\n\nYuta Kanaya & Nobuyuki Kawai\n\n### Generative models improve fairness of medical classifiers under\ndistribution shifts\n\nArticle Open access 10 April 2024\n\nIra Ktena, Olivia Wiles, ... Sven Gowal\n\n### The language network as a natural kind within the broader landscape of the\nhuman brain\n\nArticle 12 April 2024\n\nEvelina Fedorenko, Anna A. Ivanova & Tamar I. Regev\n\n## Introduction\n\nLie detection involves the process of determining the veracity of a given\ncommunication. When producing deceptive narratives, liars employ verbal\nstrategies to create false beliefs in the interacting partners and are thus\ninvolved in a specific and temporary psychological and emotional state^1. For\nthis reason, the Undeutsch hypothesis suggests that deceptive narratives\ndiffer in form and content from truthful narratives^2. This topic has always\nbeen under constant investigation and development in the field of cognitive\npsychology, given its significant and promising applications in the forensic\nand legal setting^3. Its potential pivotal role is in determining the honesty\nof witnesses and potential suspects during investigations and legal\nproceedings, impacting both the investigative information-gathering process\nand the final decision-making level^4.\n\nDecades of research have focused on identifying verbal cues for deception and\ndeveloping effective methods to differentiate between truthful and deceptive\nnarratives, with such verbal cues being, at best, subtle and typically\nresulting in both naive and expert individuals performing just above chance\nlevels^5,6. A potential explanation coming from social psychology for this\nunsatisfactory human performance is the intrinsic human inclination to the\ntruth bias^7, i.e., the cognitive heuristic of presumption of honesty, which\nmakes people assume that an interaction partner is truthful unless they have\nreasons to believe otherwise^8,9. However, it is worth mentioning that a more\nrecent study challenged this solid result, finding that instructing\nparticipants to rely only on the best available cue, such as the detailedness\nof the story, enabled them to consistently discriminate lies from the truth\nwith accuracy ranging from 59 to 79%^10. This finding moves the debate on (1)\nthe proper number of cues that judges should combine before providing their\nveracity judgment -with the suggestion that the use-the-best heuristic\napproach is the most straightforward and accurate- and thus on (2) the\ndiagnosticity level of this cue.\n\nMore recently, the issue of verbal lie detection has also been tackled by\nemploying computational techniques, such as stylometry. Stylometry refers to a\nset of methodologies and tools from computational linguistic and artificial\nintelligence that allow to conduct quantitative analysis of linguistic\nfeatures within written texts to uncover distinctive patterns that can infer\nand characterize authorship or other stylistic attributes^11,12,13. Albeit\nwith some limitations, stylometry has been proven to be effective in the\ncontext of lie detection^14,15. The main advantage is the possibility of\ncoding and extracting verbal cues independently from human judgment, hence\nreducing the problem of inter-coder agreement, as researchers using the same\ntechnique for the same data will extract the same indices^15.\n\nAlongside this trend, several recent studies have explored computational\nanalysis of language in different domains, such as fake news^16,17,\ntranscriptions of court cases^18,19,20, evaluations of deceptive product\nreviews^21,22,23, investigations into cyber-crimes^24, analysis of\nautobiographical information^25, and assessments of deceptive intentions\nregarding future events^26. Taken together, most of those studies focused on\nthe usage of Machine Learning and Deep Learning algorithms combined with\nNatural Language Processing (NLP) techniques to detect deception from verbal\ncues automatically (see Const\u00e2ncio et al.^27 for a systematic review of the\ncomputerized techniques employed in lie-detection studies).\n\nMore recently, a great step in advance has been made in the field of AI and\nNLP with the advent of Large Language Models (LLMs). LLMs are Transformer-\nbased language models with hundreds of millions of parameters trained on a\nlarge collection of corpora (i.e., pre-training phase)^28. Thanks to this pre-\ntraining phase, LLMs have proven to capture the intricate patterns and\nstructures of language and develop a robust understanding of syntax,\nsemantics, and pragmatics, being able to generate coherent text resembling\nhuman natural language. In addition, once pre-trained, these models can be\nfine-tuned on specific tasks using smaller task-specific datasets. Fine-tuning\nrefers to the process of continuing the training of a pre-trained model on a\nnew dataset, allowing it to adapt its previously learned knowledge to the\nnuances and specificities of the new data, thereby achieving state-of-the-art\nresults^28. Common tasks for LLMs fine-tuning include NLP tasks, such as\nlanguage translation, text classification (e.g., sentiment analysis),\nquestion-answering, text summarization, and code generation. Therefore, LLMs\nexcel at a wide range of NLP tasks, as opposed to models uniquely trained for\none specific task^28. However, to the best of our knowledge, despite the\nextreme flexibility of LLMs, the procedure of fine-tuning an LLM on small\ncorpora for a lie-detection task has remained unexplored.\n\n## Related works in the psychology field\n\nAmong previous psychological frameworks aimed at identifying reliable cues of\nverbal deception, the Distancing framework, the Cognitive Load (CL) theory,\nthe Reality Monitoring (RM) framework, and the Verifiability Approach (VA)\nhave been extensively studied, gaining empirical support for their efficacy\nnot only from primary research but also from meta-analytic studies.\n\nThe Distancing framework of deception states that liars tend to distance\nthemselves from their narratives as a mechanism to handle the negative\nemotions experienced while lying by using fewer self-references (e.g., \"I,\"\n\"me\") and employing more other-references (e.g., \"he,\" \"they\")^3,29.\n\nThe CL framework states that liars consume more cognitive resources while\nfabricating their fake responses, checking their congruency with other\nfabricated information, and maintaining credibility and consistency in front\nof the examiner^30, resulting in shorter, less elaborate, and less complex\nstatements. A meta-analysis^31 found that approaches based on CL theories\nproduce higher accuracy rates in detecting deception than standard approaches.\n\nThe RM framework bases its assumptions on the memory characteristics\nliterature hypothesizing that truthful recollections are based on experienced\nevents, while deceptive recollections are based on imagined events^32.\nTherefore, RM derives its predictions about truthful narratives from sensory,\nspatial, and temporal information and from emotions and feelings experienced\nduring the event. On the contrary, predictions about deceptions are drawn from\nthe number of cognitive operations (e.g., thoughts and reasonings)^33,34,35.\nThe total RM scores appear to be diagnostic (d = 0.55) in the detection\naccuracy of truthfulness^36,37 (see also^38 for an extensive review of verbal\nlie-detection methods). More recently, the RM framework was investigated\nthrough concreteness in language^39. In this study, one underlying and\npartially supported assumption was the truthful concreteness hypothesis, which\nsuggests that truthful statements usually consist of concrete, specific, and\ncontextually relevant details. In contrast, deceptive or false statements\noften include more abstract and less specific information, being more\nassociated with the RM criterion of cognitive operations.\n\nThe VA in verbal lie detection suggests that truthful statements are more\nlikely to be verifiable than false or deceptive statements, as liars avoid\nmentioning details that could be verified with independent evidence to conceal\ntheir deception^40,41. Verifiable details may be represented by activities\ninvolving or witnessed by identified individuals, documented through video or\nphotographic evidence, or leaving digital or physical traces (e.g., phone\ncalls or receipts)^40,41.\n\nNotably, these frameworks offer detectable linguistic cues that can be readily\nidentified using NLP techniques and have been extensively studied in this\nsense.\n\nHouch et al.^14 conducted a meta-analysis of studies on computer-based lie\ndetection, with most of the included studies relying on the Linguistic Inquiry\nand Word Count software (LIWC)^42,43. LIWC is the gold standard tool for\nstudying lexical diversity and text semantic content. Given a text, LIWC\ncalculates the percentage of total words corresponding to more than 100\ncategories in the dictionary related to different psychosocial dimensions,\nwhich have been validated by human evaluators using rigorous procedures. Among\nHouch\u2019s meta-analysis findings, LIWC metrics reflecting Distancing, CL, and RM\nframeworks of deception found support from the results and can detect verbal\ndeception through computerized techniques.\n\nUsually, for distancing metrics, researchers compute the number of self and\nother-references by summing the frequency of first-person pronouns in contrast\nwith second and third-person pronouns^3,29. When employing CL theory in texts,\nresearchers usually employ and analyze statistics about the number of words\nand sentences, the readability, and the complexity of texts^12,13,14. RM is\noften investigated with LIWC^26,44,45,46. Schutte et al.^47 provided evidence\nthat human coding of perceptual and contextual details in discriminating lies\nfrom truths is not conclusively superior, thereby highlighting the potential\nadvantages of automated techniques. Additionally, recent studies extracted\nverifiable details by using named-entity recognition (NER), proving to be an\neffective automatized procedure for the detection of deception in hotel\nreviews ^23 as well as in participants\u2019 intentions on their weekend plans^48.\n\nThe promising results in applying NLP techniques for psychological research\nsuggest the possibility of combining metrics from different psychological\nframeworks in a new theory-based stylometric analysis, offering the\npossibility to investigate verbal lie detection from multiple perspectives in\none shot.\n\n## Related works in the AI field\n\nPrevious works from the AI field have applied machine learning and deep\nlearning models in a binary classification task for data-driven verbal\ndeception detection.\n\nKleinberg and Verschuere^49 developed a database of future intentions to\ninvestigate whether combining machine and human judgments may improve accuracy\nin predicting deception. While finding that human judgment impairs automated\ndeception detection accuracy, the authors implemented two machine learning\nmodels (i.e., vanilla random forest) trained respectively on LIWC and Part-of-\nSpeech features (e.g., frequency of names, adjectives, adverbs, verbs)\nreaching an accuracy of 69% (95% CI: 63\u201374%) and 64 (95% CI: 58%, 69%),\nrespectively. On the same dataset, Ilias et al.^50 evaluated six deep-learning\nmodels, including combinations of BERT (and RoBERTa), MultiHead Attention, co-\nattentions, and Transformers models. The best accuracy reached was 70.61% (\u00b1\n2.58%) using a BERT with co-attention model. The authors also provided\nexplainaibility analysis to understand how the models reached their decisions\nusing a combination of LIME (a tool used to explain deep learning predictions\nin more straightforward and understandable terms by showing which specific\nwords of the text influenced the outcome) and LIWC.\n\nCapuozzo et al.^51 developed a new cross-domain and cross-language dataset of\nopinions, asking English-speaking and Italian-speaking participants to provide\ntruthful or deceptive opinions on five different topics. After encoding the\ntexts with FastText word-embedding, they trained Transformers models in\nmultiple scenarios using 10-fold cross-validation, with averaged accuracy\nranging from 63% (\u00b1 8.7%) in the \u201cwithin-topic\u201d scenario to a high of 90.1% (\u00b1\n0.16%) in the \u201cauthor-based\u201d scenario.\n\nIn contrast, Sap et al.^52 developed a new dataset of narratives generated\nfrom memories and imagination and used an LLM (GPT-3) to compute a new metric\ncalled \u201csequentiality\u201d. Sequentiality is a metric of narrative flow that\ncompares the probability of a sentence with and without its preceding story\ncontext. While providing insights into the cognitive processes of storytelling\nwith an innovative computational approach, the authors did not employ a fine-\ntuning procedure for an LLM to classify different narratives.\n\nThe findings in the AI domain indicate that as the model\u2019s complexity\nincreases, there is a heightened accuracy in predicting deception from texts.\nHowever, this increase in accuracy often comes at the expense of\nexplainability for these predictions. LLMs are currently among the most\ncutting-edge models capable of handling vast amounts and complexities of\nlinguistic data, and the lack of literature on fine-tuning LLMs for lie-\ndetection tasks provides worthwhile reasons to investigate this area.\n\n## Aims and hypotheses of the study\n\nThe main objectives and hypothesis of this study are outlined as follows:\n\n  * Hypothesis 1a): Fine-tuning an LLM can effectively classify the veracity of short narratives from raw texts, 1b) outperforming classical machine learning and deep learning approaches in verbal lie detection.\n\n  * Hypothesis 2): Fine-tuning an LLM can effectively classify the veracity of short narratives from raw texts, 1b) outperforming classical machine learning and deep learning approaches in verbal lie detection.\n\n  * Hypothesis 3): Fine-tuning an LLM can effectively classify the veracity of short narratives from raw texts, 1b) outperforming classical machine learning and deep learning approaches in verbal lie detection.\n\n  * Hypothesis 4): Fine-tuning an LLM can effectively classify the veracity of short narratives from raw texts, 1b) outperforming classical machine learning and deep learning approaches in verbal lie detection.\n\n  * Hypothesis 5a): The linguistic style distinguishing truthful from deceptive statements varies across different contexts, 5b) and can be a significant feature for model prediction.\n\nTo test Hypothesis 1a, we fine-tuned an open-source LLM, FLAN-T5, using three\ndatasets: personal opinions (the Deceptive Opinions dataset^51),\nautobiographical experiences (the Hippocorpus dataset^52) and future\nintentions (the Intention dataset^49). Given the extreme flexibility of LLMs,\nthis approach is hypothesized to detect deception from raw texts above the\nchance level. To test the advantage of our approach compared to classical\nmachine and deep learning models (Hypothesis 1b), we decided to compare the\nresults with two benchmarks, further described in the Methods and Materials\nsection.\n\nWith regards to Hypotheses 2 and 3, according to empirical evidence, classical\nmachine learning models tend to experience a decline in performance when\ntrained and tested on the aforementioned scenarios^53,54,55. In contrast, LLMs\nhave acquired a comprehensive understanding of language patterns during the\npre-training phase. We posit that a fine-tuned LLM is capable of generalizing\nits learning across various contexts. Related to Hypothesis 4, we believe this\ngeneralization ability is further enhanced in larger models, as their size is\nassociated with a more sophisticated representation of language.\n\nFinally, to test Hypothesis 5, we introduced a new theory-based stylometric\napproach, named DeCLaRatiVE stylometry, to extract linguistic features related\nto the psychological frameworks of Distancing^29, Cognitive Load^31, Reality\nMonitoring^32, and Verifiability Approach^40,41, providing a pragmatic set of\ncriteria to extract features from utterances. We will apply DeCLaRatiVE\nstylometry to compare truthful and deceptive statements in the three\naforementioned datasets in order to explore potential differences in terms of\nlinguistic style. Our hypothesis suggests that the linguistic style\ndistinguishing truthful from deceptive statements may vary across the three\ndatasets, as these types of statements originate from distinct contexts. We\nalso applied the DeCLaRatiVE stylometry technique to provide explainability\nanalysis of the top-performing model.\n\n## Methods and materials\n\n### Datasets\n\nThree datasets were employed for this study: the Deceptive Opinions\ndataset^51, from now on Opinion Dataset, the Hippocorpus dataset^52, from now\non Memory Dataset, and the Intention dataset^49. For each dataset,\nparticipants were required to provide genuine or fabricated statements in\nthree different domains: personal opinions on five different topics (Opinion\ndataset), autobiographical experiences (Memory dataset), and future intentions\n(Intention Dataset). Notably, the specific topic within each domain was\ncounterbalanced among liars and truth-tellers. A more detailed description of\neach dataset is available in Supplementary Information as well as in the\nmethod section of each original article.\n\nTable 1 displays an example of truthful and deceptive statements about\nopinions, memories, and intentions. Table 2 reports descriptive statistics for\neach dataset, both overall and when grouped by truthful and deceptive sets of\nstatements. These statistics include the minimum, maximum, average, and\nstandard deviation of word counts. Word counts were computed after text\ntokenization using spaCy, a Python library for text processing. Additionally,\nTable 2 provides Jaccard similarity index values between truthful and\ndeceptive vocabulary sets. Jaccard's index was derived by calculating the\nintersection (common words) and union (total words) of these two sets^50,56.\nThe resulting index ranges from 0 to 1, with 0 indicating a completely\ndifferent vocabulary between the two sets, and 1 indicating a completely\nidentical vocabulary between the two sets. We reported the Jaccard similarity\nindex to provide a measure of similarity or overlap between the word choices\nof truthful and deceptive statements within the respective datasets.\nSupplementary Information offers a detailed methodology for calculating the\nJaccard similarity index.\n\nTable 1 Truthful and deceptive example statements about opinions, memories,\nand intentions.\n\nFull size table\n\nTable 2 Summary statistics of the number of words for each dataset and\ntruthful and deceptive set of statements.\n\nFull size table\n\n### FLAN-T5\n\nWe adopted FLAN-T5, an LLM developed by Google researchers and freely\navailable through HuggingFace Python\u2019s library Transformers\n(https://huggingface.co/docs/transformers/model_doc/flan-t5). HugginFace is a\ncompany that provides free access to state-of-the-art LLMs through Python API.\nAmong the available LLMs, we chose FLAN-T5 because of its valuable trade-off\nbetween computational load and goodness of the learned representation. FLAN-T5\nis the improved version of MT-5, a text-to-text general model capable of\nsolving many NLP tasks (e.g., sentiment analysis, question answering, and\nmachine translation), which has been improved by pre-training^57. The\npeculiarity of this model is that every task they were trained on is\ntransformed into a text-to-text task. For example, while performing sentiment\nanalysis, the output prediction is the string used in the training set to\nlabel the positive or negative sentiment of each phrase rather than a binary\ninteger output (e.g., 0 = positive; 1 = negative). Hence, their power stands\nin both the generalized representation of natural language learned during the\npre-training phase and the possibility of easily adapting the model to a\ndownstream task with little fine-tuning without adjusting its architecture.\n\n### DeCLaRatiVE stylometric analysis\n\nThis study employed stylometric analysis to achieve two primary objectives.\nFirst, we aimed to describe the linguistic features that distinguished the\nthree datasets before initializing the fine-tuning process. Second, we\nconducted explainability analysis to gain insights into the role of linguistic\nstyle that differentiated truthful and deceptive statements in the model's\nclassification process. For this purpose, a new framework that we referred to\nas DeCLaRatiVE stylometry was adopted, which involved the extraction of 26\nlinguistic features in conjunction with the psychological frameworks of\nDistancing^29, Cognitive Load^30,31, Reality Monitoring^32,34, and\nVErifiability Approach^40,41. A full list of the 26 linguistic features with a\nshort description is shown in Table 3. This comprehensive approach enabled the\nanalysis of verbal cues of deception from a multidimensional perspective.\n\nTable 3 List and short description of the 26 linguistic features pertaining to\nthe DeCLaRatiVE Stylometry technique.\n\nFull size table\n\nFeatures associated with the CL framework consisted of statistics about the\nlength, readability, and complexity of the text^14,58,59,60 and were extracted\nusing the Python library TEXTSTAT. Features related to the Distancing and RM\nframework were computed using LIWC^42,43, the gold standard software for\nanalyzing word usage. Using the English dictionary, we scored each text along\nwith all the categories present in LIWC-22. LIWC scoring was computed on\ntokenized text using the English dictionary. The selection of the LIWC\ncategories related to the Distancing and RM framework was guided by previous\nresearch on computerized verbal lie-detection^29,49,50,52,56 and a recent\nmetanalysis^14. RM was also investigated through linguistic concreteness of\nwords^39. To determine the average level of concreteness for each statement,\nwe utilized the concreteness annotation dataset developed by Brysbaert et\nal.^61. For the calculation of concreteness scores, a preprocessing pipeline\nwas applied to textual data using the Python library SpaCy: text was converted\nto lowercase and tokenized; then stop words were removed, and the remaining\ncontent words were lemmatized. These content words were then cross-referenced\nwith the annotated concreteness dataset to assign the respective concreteness\nvalue when a match was found. The concreteness score for each statement was\nthen computed as the average of the concreteness scores for all the content\nwords in that statement. For what concerns verifiable details, they were\nestimated by the frequency of unique named entities. Named entities were\nextracted with the NER technique using Python\u2019s library SpaCy through the\nTransformer algorithm for English language (en_core_web_trf,\nhttps://spacy.io/models/en#en_core_web_trf).\n\nFurther details on how the 26 linguistic features were computed are provided\nin the Supplementary Information.\n\n### Experimental set-up\n\nIn this section, we describe the methodology that we applied in this work. As\na first step, we wanted to perform a descriptive linguistic analysis of our\ndatasets, trying to provide a response to Hypothesis 5a), i.e., whether the\nlinguistic style distinguishing truthful from deceptive statements varies\nacross different contexts. To achieve this result, we employed the DeCLaRatiVE\nstylometric analysis. As a second step, we proceeded to test the capacity of\nthe FLAN-T5 model to be fine-tuned on a Lie Detection task. To do so, we\nprovided three scenarios to verify the following hypothesis:\n\n  * Hypothesis 1a): Fine-tuning an LLM can effectively classify the veracity of short narratives from raw texts, 1b) outperforming classical machine learning and deep learning approaches in verbal lie detection.\n\n  * Hypothesis 2): Fine-tuning an LLM on deceptive narratives enables the model to also detect new types of deception;\n\n  * Hypothesis 3): Fine-tuning an LLM on deceptive narratives enables the model to also detect new types of deception;\n\n  * Hypothesis 4): Model performance depends on model size, with larger models showing higher accuracy;\n\nWe expected hypotheses 1a, 1b, 3, and 4 to be verified, while we did not have\nany a priori expectation for the second hypothesis. The scenarios are\ndescribed below:\n\n  1. 1.\n\nScenario 1: The model was fine-tuned and tested on a single dataset. This\nprocedure was repeated for each dataset with a different copy of the same\nmodel each time (i.e., the same parameters before the fine-tuning process)\n(Fig. 1). This Scenario assesses the model\u2019s capacity to learn how to detect\nlies related to the same context and responds to Hypothesis 1a;\n\n  2. 2.\n\nScenario 2: The model was fine-tuned on two out of the three datasets and\ntested on the remaining unseen dataset. As for the previous Scenario, this\nprocedure was iterated three times, employing separate instances of the same\nmodel, each time with a distinct combination of dataset pairings (Fig. 2).\nThis Scenario assesses how the model performs on samples from a new context to\nwhich it has never been exposed during the training phase and provides a\nresponse for Hypothesis 2;\n\n  3. 3.\n\nScenario 3: We first aggregated the three train and test sets from Scenario 1.\nThen, we fine-tuned the model on the aggregated datasets and tested the model\non the aggregated test sets (Fig. 1). This Scenario assesses the capacity of\nthe model to learn and generalize from samples of truthful and deceptive\nnarratives from multiple contexts and provides a response for Hypothesis 3.\n\nFigure 1\n\nVisual illustration of the Scenarios 1 and 3.\n\nFull size image\n\nFigure 2\n\nVisual illustration of the Scenario 2.\n\nFull size image\n\nIn Scenarios 1 and 3, each experiment underwent a 10-fold cross-validation.\nN-fold cross-validation is a statistical method used to estimate the\nperformance of a model by dividing the dataset into n partitions (n = 10 for\nthis study). For each partition i, we created a training set composed of the\nremaining n\u22121 partitions using the i partition as a test set (i.e., 90% of the\ndata belongs to the training set, and 10% of the remaining data belongs to the\ntest set). For each iteration, performance metrics are computed on the test\nset, stored, and then averaged. This procedure ensures an unbiased performance\nestimation and allows a fair comparison between different models. For our\nstudy, we employed identical train-test splits within scenarios 1 and 3 and\nfor both model sizes to guarantee a fair performance comparison. The average\ntest accuracy from each fold and its corresponding standard deviation are\npresented as performance metrics. Conversely, in Scenario 2, each pairing\ncombination underwent fine-tuning using the entire two paired datasets as a\ntraining set, while the model's performance was assessed using the complete\nunseen dataset as a test set.\n\nNotably, the Opinion dataset was developed to have each participant's truthful\nand deceptive statements for a total of five opinions. Therefore, we treated\neach opinion as a separate sample. In order to avoid the model exhibiting\ninflated performance on the test set as a result of learning the participants\u2019\nlinguistic style, we adopted the following precautionary measure.\nSpecifically, we ensured an exclusive division of participants between the\ntraining and test sets, such that any individual who had their opinions\nassigned to the training set did not have their opinions assigned to the test\nset, and vice versa.\n\nTogether, Scenarios 2 and 3 provide evidence about the generalized\ncapabilities of the fine-tuned FLAN-T5 model in a lie-detection task when\ntested on unseen data and on a multi-domain dataset. Furthermore, we tested\nwhether model performance may depend on model sizes. Therefore, we first fine-\ntuned the small-sized version of FLAN-T5 in every scenario, and then we\nrepeated the same experiments in every scenario with the base-sized version,\nproviding a response for Hypothesis 4.\n\nTo test Hypothesis 1b, i.e., to test the advantage of our approach when\ncompared to classical machine learning models, we decided to compare the\nresults with two benchmarks:\n\n  1. 1.\n\nA basic approach consisting of a bag-of-words (BoW) encoder plus a logistic\nregression classifier^62 (following the experimental procedure of Scenario 1);\n\n  2. 2.\n\nA literature baseline based on previous studies providing accuracy metrics on\nthe same datasets using a machine learning or a deep learning\napproach^49,50,51. For the Opinion dataset -characterized by opinions on five\ndifferent topics per subject- we compared our results to the performance\nobtained in^51 with respect to their \u201cwithin-topic\u201d experiments because our\napproach is equivalent to theirs, with the only difference that we addressed\nall the topics in one model.\n\nAs a final step, we conducted an explainability analysis to investigate the\ndifferences in linguistic style between the truthful and deceptive statements\nthat were correctly classified and misclassified by the model. This procedure\naimed to provide a response to Hypothesis 5b, i.e., whether the model takes\ninto account the linguistic style of statements for its final predictions. To\nachieve this result, we employed the DeCLaRatiVE stylometric analysis.\n\nIn Fig. 3, we provided a flow chart of the whole experimental set-up.\n\nFigure 3\n\nVisual illustration of the whole experimental set-up. The Opinion, Memory, and\nIntention dataset underwent Descriptive Linguistic Analysis using DeCLaRatiVE\nstylometry. A baseline model consisting of Bag of Words (BoW) and Logistic\nRegression (Scenario 1) was also established for the three datasets. Then, the\nFLAN-T5 model in small and base versions was fine-tuned across Scenarios 1, 2,\nand 3. Finally, an Explainability Analysis was conducted on the top-performing\nmodel using DeCLaRatiVE stylometry to interpret the results.\n\nFull size image\n\n### Fine-tuning strategy\n\nFine-tuning of LLMs consists of adapting a pre-trained language model to a\nspecific task by further training the model on task-specific data, thereby\nenhancing its ability to generate contextually relevant and coherent text in\nline with the desired task objectives^57. We fine-tuned FLAN-T5 in its small\nand base size using the three datasets and following the experimental set-up\ndescribed above. We approached the lie-detection task as a binary\nclassification problem, given that the three datasets comprised raw texts\nassociated with a binary label, specifically instances classified as truthful\nor deceptive.\n\nTo the best of our knowledge, no fine-tuning strategy is available in the\nliterature for this novel downstream NLP task. Therefore, our strategy\nfollowed an adaptation of the Hugginface\u2019s guidelines on fine-tuning an LLM\nfor translation. Specifically, we chose the same optimization strategy used to\npre-train the original model and the same loss function.\n\nNotably, the classification task between deceptive and truthful statements has\nnever been performed during the FLAN-T5 pre-training phase, nor is it included\nin any of the tasks the model has been pre-trained on. Therefore, we performed\nthe same experiments, described in the Experimental set-up section, multiple\ntimes with different learning rate values (i.e., 1e\u22123, 1e\u22124, 1e\u22125), and we\nfinally chose the configuration shown in Table 4, which yielded the best\nperformance in terms of accuracy. All experiments and runs of the three\nscenarios were conducted on Google Colaboratory Pro + using their NVIDIA A100\nTensor Core GPU.\n\nTable 4 FLAN-T5 hyperparameters configuration for the small- and base-sized\nversion.\n\nFull size table\n\n## Statistical procedure for descriptive linguistic analysis\n\nAfter applying the DeCLaRatiVE stylometry technique, we obtained a stylistic\nvector of 26 linguistic features for each text of the three datasets.\n\nIn order to assess the significance of the observed differences between the\ngroups, a permutation t-test was employed^63. This non-parametric method\ninvolves pooling all observations and then randomly redistributing them into\ntwo groups, preserving the original group sizes. The test statistic of\ninterest (i.e., the difference in means) is then computed for these permuted\ngroups. By repeating this process thousands of times (i.e., n = 10,000), we\ngenerated a test statistic distribution under the null hypothesis of no\ndifference between the groups. The observed test statistic from the actual\ndata was then compared to this distribution to compute a p-value, indicating\nthe likelihood of observing such a difference if the null hypothesis was true.\nThe advantage of using a permutation t-test is that no assumption about the\ndistribution of data is needed. This analysis was conducted in Python using\nSciPy and Pingouin library.\n\nFor the Memory and Intention dataset, we computed a permutation t-test (n =\n10,000) for independent samples for the 26 linguistic features to outline\nsignificant differences among the truthful and deceptive texts.\n\nFor the Opinion dataset, our analysis proceeded as follows. Firstly, we\ncomputed the DeCLaRatiVE stylometry technique for all the subjects\u2019 opinions.\nThis resulted in a 2500 (opinions) \u00d7 26 (linguistic features) matrix. Then,\nsince each subject provided five opinions (half truthful and half deceptive),\nwe averaged the stylistic vector separately for the truthful and deceptive\nsets of opinions. This procedure allowed us to obtain two different averaged\nstylistic vectors for the same subject, one for the truthful opinions and one\nfor the deceptive opinions. Importantly, this averaging process enabled us to\nobtain results that are independent of the topic (e.g., abortion or cannabis\nlegalization) and the stance taken by the subject (e.g., in favor or against\nthat particular topic). Finally, we validated the statistical significance of\nthese differences by conducting a paired sample permutation test (n = 10,000).\nResults for each dataset were corrected for multiple comparisons with Holm-\nBonferroni correction.\n\nThe effect size was expressed by Common Language Effect Size (CLES) with a\nconfidence interval of 95% (95% CI), which is a measure of effect size that is\nmeant to be more intuitive in its understanding by providing the probability\nthat a specific linguistic feature, in a picked-at-random truthful statement,\nwill have a higher score than in a picked-at-random deceptive one^64. The null\nvalue for the CLES is the chance level at 0.5 (in a probability range from 0\nto 1) and indicates that, when sampled, one group will be greater than the\nother, with equal chance. Cohen\u2019s d effect size with 95% CI was also computed\nto add interpretation.\n\n## Statistical procedure for explainability analysis\n\nTo examine whether the linguistic style of the input statements exerted an\ninfluence on the resulting output of the model and to provide explanations for\nthe wrong classification outputs, we applied a DeCLaRatiVE stylometric\nanalysis of statements correctly classified and misclassified by the top-\nperforming model identified in Scenario 3 (FLAN-T5 base).\n\nTo this aim, during each iteration from cross-validation, we paired the\nsentences belonging to the test set and their actual labels with the labels\npredicted by the model. After the cross-validation ended, for each of the ten\nfolds and for each of the 26 linguistic features of the sentences that\ncomposed the test set for that fold, we performed a non-parametric permutation\nt-test for independent samples (n = 10,000) for the following comparison of\ninterest:\n\n  1. a.\n\nTruthful statements misclassified as deceptive (False Negatives), with\ndeceptive statements misclassified as truthful (False Positives);\n\n  2. b.\n\nStatements correctly classified as deceptive (True Negatives) vs. truthful\nstatements misclassified as deceptive (False Negatives);\n\n  3. c.\n\nStatements correctly classified as truthful (True Positives) vs. deceptive\nstatements misclassified as truthful (False Positives).\n\n  4. d.\n\nTruthful versus deceptive statements correctly classified by the model (True\nPositives vs. True Negatives).\n\nTo compute the effect size, we computed the average of the CLES and Cohen\u2019s d\neffect size scores with their respective 95% CI obtained from each fold.\n\n## Results\n\n### Descriptive linguistic analysis\n\nThis section outlines the results of the descriptive linguistic analysis in\nterms of DeCLaRatiVE stylometric analysis to compare the three datasets on\nlinguistic features.\n\nFor the three datasets, Figs. 4, 5, and 6 show the differences in the number,\nthe type, the magnitude of the CLES effect size, and the direction of the\neffect for the linguistic features that survived post-hoc corrections. To make\nan example of these differences, the concreteness score of words\n(\u2018concr_score\u2019) presented the largest CLES within the Intention dataset\ntowards the truthful statements (Fig. 6), while in the Opinion dataset, it\nshowed the largest CLES towards the deceptive statements (Fig. 4). Overall,\nthe Intentions dataset displayed fewer significant differences in linguistic\nfeatures among truthful and deceptive statements than the Opinion and Memory\ndatasets. In Table S5 (Supplementary Information), we reported, for all the\nlinguistic features and the three datasets, all the statistics, the corrected\np-values, the effect-size scores expressed by CLES and Cohen\u2019s D with 95% CI,\nand the direction of the effect.\n\nFigure 4\n\nHorizontal stacked bar chart presenting the Common Language Effect Size (CLES)\nestimates for the significant linguistic features that survived post-hoc\ncorrections in the Opinion dataset. The CLES estimates represent the\nprobability (ranging from 0 to 1) of finding a specific linguistic feature in\ntruthful opinions (sky blue) than in deceptive ones (salmon). The CLES for\ntruthful opinions are sorted in descending order, while the CLES for deceptive\nopinions are sorted in ascending order.\n\nFull size image\n\nFigure 5\n\nHorizontal stacked bar chart presenting the Common Language Effect Size (CLES)\nestimates for the significant linguistic features that survived post-hoc\ncorrections in the Memory dataset. The CLES estimates represent the\nprobability (ranging from 0 to 1) of finding a specific linguistic feature in\ntruthful memories (sky blue) than in deceptive ones (salmon). The CLES for\ntruthful memories are sorted in descending order, while the CLES for deceptive\nmemories are sorted in ascending order.\n\nFull size image\n\nFigure 6\n\nHorizontal stacked bar chart presenting the Common Language Effect Size (CLES)\nestimates for the significant linguistic features that survived post-hoc\ncorrections in the Intention dataset. The CLES estimates represent the\nprobability (ranging from 0 to 1) of finding a specific linguistic feature in\ntruthful intentions (sky blue) than in deceptive ones (salmon). The CLES for\ntruthful intentions are sorted in descending order, while the CLES for\ndeceptive intentions are sorted in ascending order.\n\nFull size image\n\n### Performance on the lie-detection classification task\n\nThis section presents the performance, in terms of averaged accuracy (and\nstandard deviation) of the 10-folds, on the test sets after the last epoch of\nthe small and base model in all the Scenarios.\n\n#### Scenario 1\n\nIn Table 6 are depicted the test accuracies for the FLAN-T5 model, categorized\nby dataset and model size in Scenario 1. In each case, the base model, on\naverage, outperformed the small model, with the Memory dataset showing the\nlargest improvement of 4% and the Intention dataset showing just a 0.06%\nincrease in average accuracy. These results indicate that the larger model\nsize generally leads to improved performance across the three datasets, with\nhigher accuracy observed in the base version.\n\n#### Scenario 2\n\nThis scenario aimed to investigate our fine-tuned LLM\u2019s generalization\ncapability across different deception domains. As presented in Table 5, the\ntest accuracy for the three experiments in this scenario significantly dropped\nto the chance level, showing that the model, in any case, was able to learn a\ngeneral rule to detect lies coming from different contexts.\n\nTable 5 Test accuracy of FLAN-5 models in scenario 2 (three combination of\ntrain sets).\n\nFull size table\n\n#### Scenario 3\n\nIn Scenario 3, we tested the accuracy of the FLAN-T5 small and base version on\nthe aggregated Opinion, Memory, and Intention datasets. The small-sized\nFLAN-T5 achieved an average test accuracy of 75.45% (st. dev. \u00b1 1.6), while\nthe base-sized FLAN-T5 exhibited a higher average test accuracy of 79.31% (st.\ndev. \u00b1 1.3). In other words, the base-sized model outperformed the small model\nby approximately four percentage points.\n\nResults in Table 6 show the disaggregated performance on individual datasets\nbetween the small and base FLAN-T5 models in Scenario 3, with a comparison to\ntheir counterparts in Scenario 1. These comparisons show that FLAN-T5-small in\nScenario 3 exhibited worse performance than in Scenario 1. Instead, in\nScenario 3, the base model barely outperformed its counterparts of Scenario 1\non the Opinion and Intention datasets by less than 1% and slightly\nunderperformed its counterpart of Scenario 1 on the Memory dataset.\n\nTable 6 Test acccuracy of the FLAN-T5 models in Scenarios 1 and 3 for the\nthree datasets.\n\nFull size table\n\nWe identified the top-performing model as the FLAN-T5 base in Scenario 3\nbecause of its higher accuracy in the overall performance. The averaged\nconfusion matrix of the 10 folds for this model is depicted in Fig. 7.\n\nFigure 7\n\nAveraged confusion matrix of the top-performing model identified as FLAN-T5\nbase in Scenario 3. In each square, the results obtained represent the average\n(and standard deviation) from the test set of each iteration of the 10-fold\ncross-validation.\n\nFull size image\n\nNotably, in any case, we were able to outperform both the bag of word +\nlogistic regression classifier baseline and the performance achieved on the\nsame datasets in previous studies^49,50,51.\n\n### Explainability analysis\n\nThis section aims to gain a deeper understanding of the top-performing model\nidentified in Scenario 3 (FLAN-T5 base) through a DeCLaRatiVE stylometric\nanalysis of statements correctly classified and misclassified by the model.\nThe purpose of this analysis was to examine whether the linguistic style of\nthe input statements exerted an influence on the resulting output of the model\nand to provide explanations for the wrong classification outputs. For this\nanalysis, we compared:\n\n  1. a.\n\nTruthful statements misclassified as deceptive (False Negatives), with\ndeceptive statements misclassified as truthful (False Positives);\n\n  2. b.\n\nStatements correctly classified as deceptive (True Negatives) vs. truthful\nstatements misclassified as deceptive (False Negatives);\n\n  3. c.\n\nStatements correctly classified as truthful (True Positives) vs. deceptive\nstatements misclassified as truthful (False Positives).\n\n  4. d.\n\nTruthful vs. deceptive statements correctly classified by the model (True\nPositives vs. True Negatives).\n\nThe statistically significant features reported survived post-hoc correction\nfor multiple comparisons in each fold. Overall, for comparison a), b), and c),\nwe observed no statistically significant differences (p < 0.05) in any\nlinguistic features for most of the splits with the only exception of:\n\n  1. 1)\n\n\u2018fk_read\u2019 in fold 1 (t = 5.30; p = 0.04, CLES = 0.63 [0.55, 0.71], d = 0.46\n[0.18, 0.75]) and \u2018Reality Monitoring\u2019in fold 6 (t = 4.74; p = 0.047, CLES =\n0.62 [0.54, 0.70], d = 0.46 [0.17, 0.75]) for the a) comparison;\n\n  2. 2)\n\n\u2018Reality Monitoring\u2019in fold 6 (t = \u22123.39, p = 0.04, CLES = 0.40 [0.34, 0.46],\nd = \u22120.34 [\u22120.55, \u22120.13]) and \u2018Reality Monitoring\u2019 (t = \u22123.16 p = 0.04, CLES =\n0.41 [0.34, 0.47], d = \u22120.34 [\u22120.56, \u22120.12]) and \u2018Contextual Embedding\u2019 (t =\n\u22122.11; p = 0.01, CLES = 0.39 [0.33, 0.45], d = \u22120.42 [\u22120.63, \u22120.2]) in fold 7\nthe b) comparison;\n\n  3. 3)\n\n\u2018num_syllables\u2018(t = 76.87, p = 0.01, CLES = 0.64 [0.57, 0.7], d = 0.46 [0.27,\n0.7]) and \u2018word_counts\u2019(t = 59.63, p = 0.01, CLES = 0.64 [0.57, 0.71], d =\n0.46 [0.21, 0.7]) in fold 9 for the c) comparison.\n\nConversely, for the d) comparison, several significant features emerged in all\nthe folds and survived corrections for multiple comparisons. Figure 8 depicts\nthe CLES effect size scores of linguistic features, sorted according to the\nnumber of times they were found to be significant among the ten folds. The top\nsix features in Fig. 8 represented a cluster of linguistic features related to\nthe Cognitive Load framework.\n\nFigure 8\n\nLinguistic features in Truthful and Deceptive statements that were accurately\nclassified by FLAN-T5 base in Scenario 3. The bar plot shows the averaged\nCommon Language Effect Size among the ten folds of linguistic features that\nsurvived post-hoc corrections. Linguistic features are sorted in descending\norder according to the number of times they were found to be significant among\nthe 10 folds (displayed at the side of each bar). Linguistic features higher\non average in truthful texts are shown in sky blue, while those higher on\naverage in deceptive texts are shown in salmon.\n\nFull size image\n\n## Discussion\n\nIn the present research, we investigated the efficacy of a Large Language\nModel, specifically FLAN-T5 in its small and base version, in learning and\ngeneralizing the intrinsic linguistic representation of deception across\ndifferent contexts. To accomplish this, we employed three datasets\nencompassing genuine or fabricated statements regarding personal opinions,\nautobiographical experiences, and future intentions.\n\n### Descriptive linguistic analysis\n\nDescriptive linguistic analysis was performed to compare the three datasets on\nlinguistic features by exploring the differences in the DeCLaRatiVE style,\ni.e., analyzing 26 linguistic features extracted from the psychological\nframeworks of Distancing, Cognitive Load, Reality monitoring, and\nVErifiability approach. This analysis aimed to test Hypothesis 5a, which\npostulates a variation in the linguistic style that differentiates truthful\nfrom deceptive statements across varying contexts (i.e., personal opinions vs.\nautobiographical memories vs. future intentions). The results from this\nanalysis confirmed our hypothesis, showing that the linguistic features\nexhibiting statistically significant differences between truthful and\ndeceptive statements indeed varied across datasets. This variation was\nobserved in terms of the total number and type of features, the magnitude of\nthe effect size (from very small to medium), and the direction of the effect.\nIn the following paragraphs, the interpretation of the significant linguistic\nfeatures of each dataset will be discussed.\n\n### Opinions\n\nAfter analyzing truthful and deceptive opinions using the DeCLaRatiVE\nstylometry, different linguistic features\u2014related to the theoretical\nframeworks of CL, RM, and Distancing\u2014were found to be significant.\n\nIn line with the CL framework, we observed that truthful opinions were\ncharacterized by greater complexity, verbosity, and more authenticity in\nlinguistic style^14,31.\n\nFor features related to the RM framework, truthful opinions were characterized\nby a lesser number of concrete words and a greater number of cognitive words,\nas also previously shown^55; in contrast, deceptive opinions showed higher\nscores in the concreteness of words, contextual details, and reality\nmonitoring. These differences may reflect on one side the reasoning processes\nthat truth-tellers engage in evaluating the pros and cons of abstract and\ncontroversial concepts (e.g., abortion), while for deceivers, it may be\nindicative of difficulty in abstraction, resulting in faked opinions that\nsound more grounded in reality.\n\nFinally, in line with previous literature on distancing framework^29,65 and\ndeceptive opinions^20,55, deceivers utilized more other-related word classes\n(\u2018Other-reference\u2019) and fewer self-related words (\u2018Self-reference\u2019),\nconfirming that individuals may tend to avoid personal involvement when\nexpressing deceptive statements.\n\n### Memories\n\nFollowing the analysis of truthful and deceptive narratives of\nautobiographical memories through DeCLaRatiVE stylometry, various linguistic\nfeatures associated with the theoretical frameworks of CL, RM, VA, and\nDistancing were found to be significant.\n\nAs for opinions, according to the CL framework, truthful narratives of\nautobiographical memories exhibited higher levels of complexity and verbosity\nand appeared to be more analytical in style^14,31.\n\nIn accordance with the RM framework^32,33,34,35,36,37, posing that truthful\nmemory accounts tend to reflect the perceptual processes involved while\nexperiencing the event while fabricated accounts are constructed through\ncognitive operations, we found genuine memories exhibiting higher scores in\nmemory-related words and the number of words associated with spatial and\ntemporal information (\u2018Contextual Embedding\u2019), as well as an overall higher RM\nscore. Conversely, we found deceptive memories showing higher scores in words\nrelated to cognitive processes (e.g., reasoning, insight, causation).\nFurthermore, in line with Kleinberg\u2019s truthful concreteness hypothesis ^39,\ntruthful memories were overall characterized by words with higher scores of\nconcreteness.\n\nAlong with the VA, truthful memories contained more verifiable details, as\nindicated by the greater number of named entities about times and\nlocations^23,48. Notably, we found this effect although participants lied in a\nlow-stake scenario. However, deceptive memories were unexpectedly\ncharacterized by a higher number of self-references and named entities of\n\u2018People\u2019. This result is in contrast with previous literature on distancing\nframework^14,29. One possible explanation of this significant but small effect\nis that liars may try to increase their credibility by fostering a sense of\nsocial connection.\n\n### Intentions\n\nUpon examining truthful and deceptive statements of future intentions through\nDeCLaRatiVE stylometry, several linguistic features were found to be\nsignificant. Our findings are consistent with previous research claiming that\ngenuine intentions contain more \u2018how-utterances\u2019, i.e., indicators of careful\nplanning and concrete descriptions of activities. In contrast, false\nintentions are characterized by \u2018why-utterances\u2019, i.e., explanations and\nreasons for why someone planned an activity or for doing something in a\ncertain way^48. Indeed, we found true intentions were more likely to provide\nconcrete and distinct information about the intended action, grounding their\nstatements in real-world experiences and providing temporal and spatial\nreferences. Additionally, true intentions were characterized by a more\nanalytical style and a greater presence of numerical entities. In contrast,\nfalse intentions exhibited a higher number of cognitive words and expressions\nand were temporally oriented toward the present and past.\n\nFurthermore, we found evidence in line with the claim that liars may over-\nprepare their statements^48, as indicated by higher verbosity. Finally, in\ncontrast with the distancing framework^14,29, we found a significantly higher\nproportion of self-references and mentions of people in deceptive statements.\nHowever, the effect size for this finding was small. As for deceptive\nmemories, one possible interpretation is that liars may attempt to appear more\ncredible by creating a sense of social connection.\n\n### Lie detection task\n\nIn order to test the capacity of the FLAN-T5 model to be fine-tuned on a Lie\nDetection task, we developed three scenarios.\n\nIn Scenario 1, we tested whether fine-tuning LLMs can effectively classify the\nveracity of short statements based on raw texts with performance highly above\nthe chance level (Hypothesis 1a). To this aim, we fine-tuned FLAN-T5 in its\nsmall version to perform lie detection as a classification task. We repeated\nthis procedure for the three datasets (i.e., opinions vs. memories vs.\nintentions). This fine-tuning process yielded promising results confirming our\nhypothesis, with an average accuracy of 80.64% (st. dev. \u00b1 2.03%) for the\nOpinion dataset, 76.87% (st. dev. \u00b1 2.06%) for the Memory dataset, and 71.46%\n(st. dev. \u00b1 3.65%) for the Intention dataset.\n\nIn Scenario 2, we tested whether fine-tuning an LLM on deceptive narratives\nenables the model to detect new types of deception (Hypothesis 2). To verify\nthis hypothesis, we fine-tuned FLAN-T5 (small version) on two datasets and\ntested on the third one (e.g., train: opinion + memory; test: intention). Our\nfindings show that the model performed at chance level in all three\ncombinations of this Scenario, suggesting that there are no universal rules\nthe model can learn to distinguish truthful from deceptive statements,\nenabling a generalization of the task across different contexts. Indeed, as\nshown in the Descriptive Linguistic Analysis section, the three datasets\ndiffered significantly in terms of the content and the linguistic style by\nwhich truthful and deceptive narratives are delivered. Therefore, the model\nstruggled to identify a specific pattern of linguistic deception and appeared\nto engage a domain-specific learning, tailoring its classification\ncapabilities to that specific domain of deception.\n\nIn Scenario 3, we tested whether fine-tuning an LLM on a multiple-context\ndataset enables the model to obtain successful predictions on a multi-context\ntest set (Hypothesis 3). At this aim, we fine-tuned and tested FLAN-T5 (small\nversion) with the three aggregated datasets (i.e., opinion + memory +\nintention). The small-sized FLAN-T5 achieved an average accuracy of 75.45%\n(st. dev. \u00b1 1.6). Additionally, the disaggregated performance on individual\ndatasets compared to their counterpart in Scenario 1 exhibited solely a small\ndecrease in accuracy (around 1%). These findings confirmed our hypothesis,\nproviding evidence of LLMs\u2019 ability to generalize when fine-tuned and texted\non a multi-context dataset, in contrast to previous empirical evidence showing\na decline in performance in machine learning models on the same\nscenarios^53,54,55.\n\nTo test whether the model performance increases when employing larger models\n(Hypothesis 4), we repeated the same experiments in Scenarios 1, 2, and 3 with\nthe base version of FLAN-T5.\n\nIn Scenario 1, we found that the base version of FLAN-T5 provided higher\naccuracy than the small version. In Scenario 3, the base version of the model\nachieved an average accuracy of 79.31% (st. dev. \u00b1 1.3), outperforming the\nsmall model by approximately four percentage points. Additionally, this\nincrease in the general accuracy did not compromise the performance on any\nindividual dataset when compared to what achieved by the smaller model or by\nthe FLAN-T5 base in Scenario 1. In contrast, the base version of FLAN-T5 in\nScenario 2 still obtained performance around the chance level.\n\nOn one hand, the findings obtained from the base model in Scenarios 1 and 3\nconfirmed the hypothesis that the model size does influence the performance,\nlikely because a bigger model is able to learn a better representation of\nlinguistic patterns of genuine and deceptive narratives. Specifically, in\nScenario 3, the FLAN-T5 base, with its larger size, possessed the capability\nto comprehend and integrate the features of the three distinct datasets\naltogether, thereby maintaining consistent performance across all individual\ndatasets. In contrast, the smaller FLAN-T5 in Scenario 3 seemed to relinquish\ncertain specialized abilities that are beneficial for specific datasets to\nclassify deception across different contexts.\n\nOn the other hand, findings from Scenarios 2 and 3 (with small and base\nFLAN-T5) showed that LLMs, despite having acquired a comprehensive\nunderstanding of language patterns, still require exposure to prior examples\nto accurately classify deceptive texts within different domains.\n\nFinally, to test whether our approach outperforms classical machine learning\nand deep learning approaches in verbal lie detection (Hypothesis 1b), we\ncompared the results obtained from FLAN-T5 in its small and base versions with\nthe performance of a simpler baseline of a logistic regressor based on BoW\nembedding^62 and of Transformer models previously employed in the literature\non the Opinion^51 and Intention datasets^49,50.\n\nSpecifically, when comparing the Memory dataset to the logistic regression\nbaseline, there was a 32% increase in performance. This improvement might be\nattributed to the longer and more complex nature of the stories in the Memory\ndataset, which challenges the effectiveness of more straightforward methods\nlike logistic regression based on BoW in a lie detection task. In contrast,\nLLMs already possess a robust language representation; thus, fine-tuning LLMs\nleverages this representation, tailoring their NLP proficiency specifically\nfor a lie detection task, yielding higher accuracy.\n\nThe performance gained by fine-tuning LLMs was less pronounced for the Opinion\nand Intention datasets. For the Opinion dataset, this could be due to the\nrelative ease of classification in these datasets, where simpler models can\nalready achieve good performance, leaving a smaller margin for improvement.\nNonetheless, the difference between our approach and the baselines is not\nnegligible. In the Opinion dataset, we outperformed the literature baseline of\na Transformer model trained from scratch by 17% accuracy and surpassed our\nlogistic regression baseline by six percentage points. For the Intention\ndataset, our approach showed a 5-percentage point improvement over the\nlogistic regression baseline and around 1\u20132% improvement over the best\nliterature baseline. Notably, the best literature baseline for the Intention\ndataset (averaged accuracy: 70.61 \u00b1 2.58%) used a similar approach to ours in\nterms of the type of model used, involving a Transformer-based model (BERT +\nCo-attention), which may explain the narrower performance gap.\n\nBesides the differences in performance, the main advantage of our approach is\nits simplicity and flexibility compared to those used in previous\nstudies^49,50,51. Fine-tuning an LLM leverages an existing encoding of\nlanguage that effortlessly handles any type of statement, unlike logistic\nregression based on BoW or training a new Transformer-based model from\nscratch. Taking all these aspects together, fine-tuning LLMs resulted in being\nmore advantageous in terms of feasibility, flexibility, and performance\naccuracy.\n\n### Explainability analysis\n\nTo improve the explainability of the performance collected, we investigated\nwhether the linguistic style that characterizes truthful and deceptive\nnarratives could have a role in the model\u2019s final predictions (Hypothesis 5b).\nFor this aim, we applied a DeCLaRatiVE stylometric analysis on statements that\nwere correctly classified and misclassified by the top-performing model\nidentified in Scenario 3 (i.e., FLAN-T5 base).\n\nIn the misclassified sample, truthful and deceptive statements did not differ\nsignificantly for any linguistic feature extracted with the DeCLaRatiVE\nstylometry technique. The only exception was fold 1, which showed significant\ndifferences in the text\u2019s readability score, and fold 6, which showed\nsignificant differences in 'Reality Monitoring' scores. No significant\ndifferences were detected in each fold in linguistic features between\ndeceptive statements that were correctly classified as deceptive (True\nNegatives) and truthful statements that were misclassified as deceptive (False\nNegatives), with the exception of \u2018Reality Monitoring\u2019 in folds 6 and 7 and\n\u2018Contextual Embedding\u2019 score in fold 7. Finally, truthful statements that were\ncorrectly classified as truthful (True Positives) and deceptive statements\nthat were misclassified as truthful (False Positives) exhibited no significant\ndifferences, except for the number of syllables and number of words in the\nfold 9. We argue that the observation of significant differences in selected\nlinguistic features across specific folds is more indicative that these\nfindings may not be generalizable and are likely influenced by the particular\nfold under analysis. When taken together, most of the analyzed folds showed a\nsubstantial overlap in linguistic style. Consequently, the model might have\nexhibited poor classification performance for those statements because, while\ndeceptive, they showed a linguistic style resembling truthful statements and\nvice-versa.\n\nIn contrast, correctly classified statements displayed several significant\ndifferences between truthful and deceptive statements. Notably, the top six\nlinguistic features in Fig. 8 resulted in statistical significance in at least\n6 out of 10 folds. The fact that we found a consistent pattern of linguistic\nfeatures in correctly classified statements but not in misclassified\nstatements provides evidence for our hypothesis, suggesting that the\nlinguistic style of statements does have a role in the model\u2019s final\npredictions. More in detail, the top-six linguistic features depicted in Fig.\n8 represent a cluster of linguistic cues associated with the CL framework^31,\nspecifically low-level features related to the length, complexity, and\nanalytical style of the texts that may have enabled the distinction between\ntruthful and deceptive statements. The fact that linguistic cues of CL\nsurvived among the several features available -in a mixed dataset of\nutterances reflecting opinions, memories, and intentions- raises the question\nof whether CL cues may be more generalizable than other cues that are, in\ncontrast, more specific to a particular type of deception.\n\n## Conclusion, limitations, and further work\n\nAt the time of writing and to the best of our knowledge, this is the first\nstudy involving the use of an LLM for a lie-detection task.\n\nLLMs are Transformer-based models trained on large corpora of text that have\nproven to generate coherent text in human natural language and have extreme\nflexibility in a wide range of NLP tasks^28. In addition, these models can be\nfurther fine-tuned on specific tasks using smaller task-specific datasets,\nachieving state-of-the-art results^28. In this study, we tested the ability of\na fine-tuned LLM (FLAN-T5) on lie-detection tasks.\n\nFirst, given the extreme flexibility of LLM, we tested whether fine-tuning a\nLLM is a valid procedure to detect deception from raw texts above chance level\nand outperform the classical machine and deep learning approaches. We found\nthat fine-tuning FLAN-T5 on a single dataset is a valid procedure to obtain a\nstate-of-the-art accuracy, as proved by the fact that this procedure\noutperformed the baseline model (BoW + logistic regression) and previous works\nthat applied machine and deep learning techniques on the same\ndatasets^49,50,51,62.\n\nSecond, we wanted to investigate whether fine-tuning an LLM on deceptive\nnarratives enables the model to also detect new types of deceptive narratives.\nFindings from Scenario 2 disconfirms this hypothesis, suggesting that the\nmodel requires previous examples of different deceptive narratives to provide\nadequate accuracy in this classification task.\n\nThird, we investigated whether it is possible to successfully fine-tune an LLM\non a multiple-context dataset. Results from Scenario 3 confirm that fine-tuned\nLLM may provide adequate accuracy in detecting deception from different\ncontexts. We also found that fine-tuning on multiple datasets can increase the\nperformance with respect to when fine-tuned on a single dataset.\n\nFurthermore, we hypothesized that the model performance may depend on the\nmodel size, given that the larger the model, the better the model forms its\ninner representation of language. Results from Scenario 1 and 3 confirmed that\nthe base-sized model of FLAN-T5 provides higher accuracy than the small-sized\nversion.\n\nFinally, with our experiments, we introduced the DeCLaRatiVE stylometry\ntechnique, a new theory-based stylometric approach to investigate deception in\ntexts from four psychological frameworks (Distancing, Cognitive Load, Reality\nMonitoring, and Verifiability approach). We employed the DeCLaRatiVE\nstylometry technique to compare the three datasets on linguistic features and\nwe found that fabricated statements from different contexts exhibit different\nlinguistic cues of deception. We also employed the DeCLaRatiVE stylometry\ntechnique to conduct an explainability analysis and investigate whether the\nlinguistic style by which truthful or deceptive narratives are delivered is a\nfeature that the model takes into account for its final prediction. At this\naim, we compared correctly classified and misclassified statements by the top-\nperforming model (FLAN-T5 base in Scenario 3), finding that correctly\nclassified statements share linguistic features related to the cognitive load\ntheory. In contrast, truthful and deceptive misclassified statements do not\npresent significant differences in linguistic style.\n\nGiven the results achieved, we highlight the importance of a diversified\ndataset to achieve a generalized good performance. We also considered crucial\nthe balance between the diversity of the dataset and the size of the LLM,\nsuggesting that the more diverse the dataset is, the bigger the model required\nto achieve higher-level accuracy. The main advantage of our approach consists\nof its applicability to raw text without the need for extensive training or\nhandcrafted features.\n\nDespite the demonstrated success of our model, three significant limitations\nimpact the ecological validity of our findings and their practical application\nin real-life scenarios.\n\nThe first notable limitation pertains to the narrow focus of our study, which\nconcentrated solely on lie detection within three specific contexts: personal\nopinions, autobiographical memories, and future intentions. This restricted\nscope limits the possibility of accurately classify deceptive texts within\ndifferent domains. A second limitation is that we exclusively considered\ndatasets developed in experimental set-ups designed to collect genuine and\ncompletely fabricated narratives. However, individuals frequently employ\nembedded lies in real-life scenarios, in which substantial portions of their\nnarratives are true, rather than fabricating an entirely fictitious story.\nFinally, the datasets employed in this study were collected in experimental\nlow-stake scenarios where participants had low incentives to lie and appear\ncredible. Because of all the above issues, the application of our model in\nreal-life contexts may be limited, and caution is advised when interpreting\nthe results in such situations.\n\nThe limitations addressed in this study underscore the need for future\nresearch to expand the applicability and generalizability of lie-detection\nmodels for real-life settings. Future works may explore the inclusion of new\ndatasets, trying different LLMs (e.g., the most recent GPT-4), different sizes\n(e.g., FLAN-T5 XXL version), and different fine-tuning strategies to\ninvestigate the variance in performance within a lie-detection task.\nFurthermore, our fine-tuning approach completely erased the previous\ncapabilities possessed by the model; therefore, future works should also focus\non new fine-tuning strategies that do not compromise the model\u2019s original\ncapabilities.\n\n## Data availability\n\nFor the Opinion dataset, we obtained full access after contacting the\ncorresponding author. The Memory dataset is downloadable at the link:\nhttps://msropendata.com/datasets/0a83fb6f-a759-4a17-aaa2-fbac84577318. The\nintention dataset is publicly available at the link: https://osf.io/45z7e/.\n\n## Code availabitity\n\nAll the Colab Notebooks to perform linguistic analysis on the three datasets,\nfine-tune the model in the three Scenarios, and conduct explainability\nanalysis is available at\nhttps://github.com/robecoder/VerbalLieDetectionWithLLM.git.\n\n## References\n\n  1. Walczyk, J. J., Harris, L. L., Duck, T. K. & Mulay, D. A social-cognitive framework for understanding serious lies: Activation-decision-construction-action theory. New Ideas Psychol. 34, 22\u201336. https://doi.org/10.1016/j.newideapsych.2014.03.001 (2014).\n\nArticle Google Scholar\n\n  2. Amado, B. G., Arce, R. & Fari\u00f1a, F. Undeutsch hypothesis and criteria based content analysis: A meta-analytic review. Eur J Psychol Appl Legal Context 7, 3\u201312. https://doi.org/10.1016/j.ejpal.2014.11.002 (2015).\n\nArticle Google Scholar\n\n  3. Vrij, A. et al. Verbal lie detection: Its past, present and future. Brain Sciences 12, 1644. https://doi.org/10.3390/brainsci12121644 (2022).\n\nArticle PubMed PubMed Central Google Scholar\n\n  4. Vrij, A. & Fisher, R. P. Which lie detection tools are ready for use in the criminal justice system?. J. Appl. Res. Mem. Cognit. 5, 302\u2013307. https://doi.org/10.1016/j.jarmac.2016.06.014 (2016).\n\nArticle Google Scholar\n\n  5. DePaulo, B. M. et al. Cues to deception. Psychol. Bull. 129, 74\u2013118. https://doi.org/10.1037/0033-2909.129.1.74 (2003).\n\nArticle PubMed Google Scholar\n\n  6. Bond, C. F. Jr. & DePaulo, B. M. Accuracy of deception judgments. Personal. Soc. Psychol. Rev. 10, 214\u2013234. https://doi.org/10.1207/s15327957pspr1003_2 (2006).\n\nArticle Google Scholar\n\n  7. Levine, T. R., Park, H. S. & McCornack, S. A. Accuracy in detecting truths and lies: Documenting the \u201cveracity effect\u201d. Commun. Monogr. 66, 125\u2013144. https://doi.org/10.1080/03637759909376468 (1999).\n\nArticle Google Scholar\n\n  8. Levine, T. R. Truth-default theory (TDT). J. Lang. Soc. Psychol. 33, 378\u2013392. https://doi.org/10.1177/0261927x14535916 (2014).\n\n  9. Street, C. N. H. & Masip, J. The source of the truth bias: Heuristic processing?. Scand. J. Psychol. 56, 254\u2013263. https://doi.org/10.1111/sjop.12204 (2015).\n\nArticle PubMed Google Scholar\n\n  10. Verschuere, B., et al. The use-the-best heuristic facilitates deception detection. Nat. Hum. Behav. 7, 718\u2013728. https://doi.org/10.1038/s41562-023-01556-2 (2023)\n\n  11. Chen, X., Hao, P., Chandramouli, R., and Subbalakshmi, K. P. Authorship similarity detection from email messages. In International Workshop On Machine Learning and Data Mining In Pattern Recognition. Editor P. Perner (New York, NY: Springer), 375\u2013386. https://doi.org/10.1007/978-3-642-23199-5_28 (2011).\n\n  12. Chen, H. Dark web: Exploring and mining the dark side of the web. In 2011 European Intelligence and Security Informatics Conference, 1\u20132. IEEE (2011).\n\n  13. Daelemans, W. Explanation in computational stylometry. In Computational Linguistics and Intelligent Text Processing, 451\u2013462. Springer, Berlin. https://doi.org/10.1007/978-3-642-37256-8_37 (2013).\n\n  14. Hauch, V., Bland\u00f3n-Gitlin, I., Masip, J. & Sporer, S. L. Are computers effective lie detectors? A meta-analysis of linguistic cues to deception. Personal. Soc. Psychol. Rev. 19, 307\u2013342. https://doi.org/10.1177/1088868314556539 (2015).\n\nArticle Google Scholar\n\n  15. Tomas, F., Dodier, O., & Demarchi, S. Computational measures of deceptive language: Prospects and issues. Front. Commun. 7 https://doi.org/10.3389/fcomm.2022.792378 (2022).\n\n  16. Conroy, N. K., Rubin, V. L. & Chen, Y. Automatic deception detection: Methods for finding fake news. Proc. Assoc. Inf. Sci. Technol. 52, 1\u20134. https://doi.org/10.1002/pra2.2015.145052010082 (2015).\n\nArticle Google Scholar\n\n  17. P\u00e9rez-Rosas, V., Kleinberg, B., Lefevre, A., & Mihalcea, R. Automatic detection of fake news. arXiv preprint arXiv:1708.07104 (2017).\n\n  18. Fornaciari, T. & Poesio, M. Automatic deception detection in Italian court cases. Artif. Intell. Law 21, 303\u2013340. https://doi.org/10.1007/s10506-013-9140-4 (2013).\n\nArticle Google Scholar\n\n  19. Yancheva, M., & Rudzicz, F. Automatic detection of deception in child-produced speech using syntactic complexity features. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics 1, 944\u2013953, (2013).\n\n  20. P\u00e9rez-Rosas, V., & Mihalcea, R. Experiments in open domain deception detection. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. https://doi.org/10.18653/v1/d15-1133 (2015).\n\n  21. Ott, M., Choi, Y., Cardie, C., & Hancock, J. T. Finding deceptive opinion spam by any stretch of the imagination. arXiv preprint arXiv:1107.4557 (2011).\n\n  22. Fornaciari, T., & Poesio, M. Identifying fake Amazon reviews as learning from crowds. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. https://doi.org/10.3115/v1/e14-1030n (2014).\n\n  23. Kleinberg, B., Mozes, M., Arntz, A. & Verschuere, B. Using named entities for computer-automated verbal deception detection. Journal of forensic sciences 63, 714\u2013723. https://doi.org/10.1111/1556-4029.13645 (2017).\n\nArticle PubMed Google Scholar\n\n  24. Mbaziira, A. V., & Jones, J. H. Hybrid text-based deception models for native and Non-Native English cybercriminal networks. In Proceedings of the International Conference on Compute and Data Analysis. https://doi.org/10.1145/3093241.3093280 (2017).\n\n  25. Levitan, S. I., Maredia, A., & Hirschberg, J. Linguistic cues to deception and perceived deception in interview dialogues. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1. https://doi.org/10.18653/v1/n18-1176 (2018).\n\n  26. Kleinberg, B., Nahari, G., Arntz, A., & Verschuere, B. An investigation on the detectability of deceptive intent about flying through verbal deception detection. Collabra: Psychol. 3. https://doi.org/10.1525/collabra.80 (2017).\n\n  27. Const\u00e2ncio, A. S., Tsunoda, D. F., Silva, H. de F. N., Silveira, J. M. da, & Carvalho, D. R. Deception detection with machine learning: A systematic review and statistical analysis. PLOS ONE, 18, e0281323. https://doi.org/10.1371/journal.pone.0281323 (2023).\n\n  28. Zhao, W. X., et al. A survey of large language models. arXiv preprint arXiv:2303.18223. (2023).\n\n  29. Newman, M. L., Pennebaker, J. W., Berry, D. S. & Richards, J. M. Lying words: Predicting deception from linguistic styles. Personal. Soc. Psychol. Bull. 29, 665\u2013675. https://doi.org/10.1177/0146167203029005010 (2003).\n\nArticle Google Scholar\n\n  30. Monaro, M. et al. Covert lie detection using keyboard dynamics. Sci Rep 8, 1976. https://doi.org/10.1038/s41598-018-20462-6 (2018).\n\nArticle CAS PubMed PubMed Central ADS Google Scholar\n\n  31. Vrij, A., Fisher, R. P. & Blank, H. A cognitive approach to lie detection: A meta-analysis. Legal Criminol. Psychol. 22(1), 1\u201321. https://doi.org/10.1111/lcrp.12088 (2015).\n\nArticle Google Scholar\n\n  32. Johnson, M. K. & Raye, C. L. Reality monitoring. Psychol. Rev. 88, 67\u201385. https://doi.org/10.1037/0033-295x.88.1.67 (1981).\n\nArticle Google Scholar\n\n  33. Sporer, S. L. The less travelled road to truth: Verbal cues in deception detection in accounts of fabricated and self-experienced events. Appl. Cognit. Psychol. 11(5), 373\u2013397. https://doi.org/10.1002/(SICI)1099-0720(199710)11:5%3c373::AID-ACP461%3e3.0.CO;2-0 (1997).\n\nArticle Google Scholar\n\n  34. Sporer, S. L. Reality monitoring and detection of deception in The Detection of Deception in Forensic Contexts (Cambridge University Press.), 64\u2013102. https://doi.org/10.1017/cbo9780511490071.004 (2004).\n\n  35. Masip, J., Sporer, S. L., Garrido, E. & Herrero, C. The detection of deception with the reality monitoring approach: A review of the empirical evidence. Psychol. Crime Law 11(1), 99\u2013122. https://doi.org/10.1080/10683160410001726356 (2005).\n\nArticle Google Scholar\n\n  36. Amado, B. G., Arce, R., Fari\u00f1a, F. & Vilari\u00f1o, M. Criteria-Based Content Analysis (CBCA) reality criteria in adults: A meta-analytic review. Int. J. Clin. Health Psychol. 16(2), 201\u2013210. https://doi.org/10.1016/j.ijchp.2016.01.002 (2016).\n\nArticle PubMed PubMed Central Google Scholar\n\n  37. Gancedo, Y., Fari\u00f1a, F., Seijo, D., Vilari\u00f1o, M. & Arce, R. Reality monitoring: A meta-analytical review for forensic practice. Eur. J. Psychol. Appl. Legal Context 13(2), 99\u2013110. https://doi.org/10.5093/ejpalc2021a10 (2021).\n\nArticle Google Scholar\n\n  38. Vrij, A. et al. Verbal lie detection: its past, present and future. Brain Sci. 12(12), 1644. https://doi.org/10.3390/brainsci12121644 (2022).\n\nArticle PubMed PubMed Central Google Scholar\n\n  39. Kleinberg, B., van der Vegt, I., & Arntz, A. Detecting deceptive communication through linguistic concreteness. Center for Open Science. https://doi.org/10.31234/osf.io/p3qjh (2019).\n\n  40. Nahari, G., Vrij, A. & Fisher, R. P. Exploiting liars\u2019 verbal strategies by examining the verifiability of details. Legal Criminol. Psychol. 19, 227\u2013239. https://doi.org/10.1111/j.2044-8333.2012.02069.x (2012).\n\nArticle Google Scholar\n\n  41. Vrij, A., & Nahari, G. The verifiability approach. In Evidence-Based Investigative Interviewing (pp. 116\u2013133). Routledge. https://doi.org/10.4324/9781315160276-7 (2019).\n\n  42. Pennebaker, J. W., Francis, M. E., & Booth, R. J. Linguistic inquiry and word count: LIWC 2001. Mahway: Lawrence Erlbaum Associates, 71, 2001 (2001).\n\n  43. Boyd, R. L., Ashokkumar, A., Seraj, S., & Pennebaker, J. W. The development and psychometric properties of LIWC-22. Austin, TX: University of Texas at Austin, 1\u201347. (2022).\n\n  44. Bond, G. D. & Lee, A. Y. Language of lies in prison: Linguistic classification of prisoners\u2019 truthful and deceptive natural language. Appl. Cognit. Psychol. 19(3), 313\u2013329. https://doi.org/10.1002/acp.1087 (2005).\n\nArticle Google Scholar\n\n  45. Bond, G. D. et al. \u2018Lyin\u2019 Ted\u2019, \u2018crooked hillary\u2019, and \u2018Deceptive Donald\u2019: Language of lies in the 2016 US presidential debates. Appl. Cognit. Psychol. 31(6), 668\u2013677. https://doi.org/10.1002/acp.3376 (2017).\n\nArticle Google Scholar\n\n  46. Bond, G. D., Speller, L. F., Cockrell, L. L., Webb, K. G., & Sievers, J. L. \u2018Sleepy Joe\u2019 and \u2018Donald, king of whoppers\u2019: Reality monitoring and verbal deception in the 2020 U.S. presidential election debates. Psychol. Rep. 003329412211052. https://doi.org/10.1177/00332941221105212 (2022).\n\n  47. Schutte, M., Bogaard, G., Mac Giolla, E., Warmelink, L., Kleinberg, B., & Verschuere, B. Man versus Machine: Comparing manual with LIWC coding of perceptual and contextual details for verbal lie detection. Center for Open Science. https://doi.org/10.31234/osf.io/cth58 (2021).\n\n  48. Kleinberg, B., van der Toolen, Y., Vrij, A., Arntz, A. & Verschuere, B. Automated verbal credibility assessment of intentions: The model statement technique and predictive modeling. Appl. Cognit. Psychol. 32, 354\u2013366. https://doi.org/10.1002/acp.3407 (2018).\n\nArticle Google Scholar\n\n  49. Kleinberg, B., & Verschuere, B. How humans impair automated deception detection performance. Acta Psychol., 213, https://doi.org/10.1016/j.actpsy.2020.103250 (2021).\n\n  50. Ilias, L., Soldner, F., & Kleinberg, B. Explainable verbal deception detection using transformers. arXiv preprint arXiv:2210.03080 (2022).\n\n  51. Capuozzo, P., Lauriola, I., Strapparava, C., Aiolli, F., & Sartori, G. DecOp: A multilingual and multi-domain corpus for detecting deception in typed text. In Proceedings of the 12th Language Resources and Evaluation Conference, 1423\u20131430 (2020).\n\n  52. Sap, M. et al. Quantifying the narrative flow of imagined versus autobiographical stories. Proc. Natl. Acad. Sci. 119(45), e2211715119. https://doi.org/10.1073/pnas.2211715119 (2022).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  53. Hern\u00e1ndez-Casta\u00f1eda, \u00c1., Calvo, H., Gelbukh, A. & Flores, J. J. G. Cross-domain deception detection using support vector networks. Soft Comput. 21, 585\u2013595. https://doi.org/10.1007/s00500-016-2409-2 (2016).\n\nArticle Google Scholar\n\n  54. P\u00e9rez-Rosas, V., & Mihalcea, R. Cross-cultural deception detection. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics 2. https://doi.org/10.3115/v1/p14-2072 (2014).\n\n  55. Mihalcea, R., & Strapparava, C. The lie detector: Explorations in the automatic recognition of deceptive language. In Proceedings of the ACL-IJCNLP 2009 conference short papers 309\u2013312. https://doi.org/10.3115/1667583.1667679 (2009).\n\n  56. R\u00edssola, E. A., Aliannejadi, M., & Crestani, F. Beyond modelling: Understanding mental disorders in online social media. In Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14\u201317, 2020, Proceedings, Part I 42 (pp. 296\u2013310). Springer (2020).\n\n  57. Chung, H. W., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. (2022).\n\n  58. Zhou, L., Burgoon, J. K., Nunamaker, J. F. & Twitchell, D. Automating linguistics-based cues for detecting deception in text-based asynchronous computer-mediated communications. Group Decis. Negot. 13, 81\u2013106. https://doi.org/10.1023/b:grup.0000011944.62889.6f (2004).\n\nArticle Google Scholar\n\n  59. Sol\u00e0-Sales, S., Alzetta, C., Moret-Tatay, C. & Dell\u2019Orletta, F. Analysing deception in witness memory through linguistic styles in spontaneous language. Brain Sci. 13, 317. https://doi.org/10.3390/brainsci13020317 (2023).\n\nArticle PubMed PubMed Central Google Scholar\n\n  60. Sarzynska-Wawer, J., Pawlak, A., Szymanowska, J., Hanusz, K. & Wawer, A. Truth or lie: Exploring the language of deception. PLOS ONE 18, e0281179. https://doi.org/10.1371/journal.pone.0281179 (2023).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  61. Brysbaert, M., Warriner, A. B. & Kuperman, V. Concreteness ratings for 40 thousand generally known English word lemmas. Behav Res 46, 904\u2013911. https://doi.org/10.3758/s13428-013-0403-5 (2014).\n\nArticle Google Scholar\n\n  62. Lin, Y. C., Chen, S. A., Liu, J. J., & Lin, C. J. Linear Classifier: An Often-Forgotten Baseline for Text Classification. arXiv preprint arXiv:2306.07111 (2023).\n\n  63. Moore, J. H. Bootstrapping, permutation testing and the method of surrogate data. Phys. Med. Biol. 44(6), L11 (1999).\n\nArticle CAS PubMed ADS Google Scholar\n\n  64. McGraw, K. O. & Wong, S. P. A common language effect size statistic. Psychol. Bull. 111, 361. https://doi.org/10.1037/0033-2909.111.2.361 (1992).\n\nArticle Google Scholar\n\n  65. Hancock, J. T., Curry, L. E., Goorha, S. & Woodworth, M. On lying and being lied to: A linguistic analysis of deception in computer-mediated communication. Discourse Process. 45, 1\u201323. https://doi.org/10.1080/01638530701739181 (2007).\n\nArticle Google Scholar\n\nDownload references\n\n## Acknowledgements\n\nWe would like to thank Bruno Verschuere and Bennett Kleinberg for sharing the\nfull version of their Intention dataset with us.\n\n## Author information\n\n### Authors and Affiliations\n\n  1. Molecular Mind Lab, IMT School for Advanced Studies Lucca, Piazza San Francesco 19, 55100, Lucca, LU, Italy\n\nRiccardo Loconte & Pietro Pietrini\n\n  2. Department of Mathematics \u201cTullio Levi-Civita\u201d, University of Padova, Padova, Italy\n\nRoberto Russo\n\n  3. Department of General Psychology, University of Padova, Padova, Italy\n\nPasquale Capuozzo & Giuseppe Sartori\n\nAuthors\n\n  1. Riccardo Loconte\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  2. Roberto Russo\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  3. Pasquale Capuozzo\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  4. Pietro Pietrini\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  5. Giuseppe Sartori\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n### Contributions\n\nG.S. conceptualized the research. R.L., R.R., P.C., and G.S. designed the\nresearch. P.C. shared the updated version of the Deceptive Opinion Dataset.\nR.L. performed the descriptive linguistic analysis and explainability\nanalysis. R.R. developed and implemented the fine-tuning strategy. R.L. and\nR.R. wrote the paper. P.P. and G.S. supervised all aspects of whole the\nresearch and provided critical revisions.\n\n### Corresponding author\n\nCorrespondence to Riccardo Loconte.\n\n## Ethics declarations\n\n### Competing interests\n\nThe authors declare no competing interests.\n\n## Additional information\n\n### Publisher's note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional affiliations.\n\n## Supplementary Information\n\n### Supplementary Information.\n\n## Rights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0\nInternational License, which permits use, sharing, adaptation, distribution\nand reproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's\nCreative Commons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article's Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this licence, visit\nhttp://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\n## About this article\n\n### Cite this article\n\nLoconte, R., Russo, R., Capuozzo, P. et al. Verbal lie detection using Large\nLanguage Models. Sci Rep 13, 22849 (2023).\nhttps://doi.org/10.1038/s41598-023-50214-0\n\nDownload citation\n\n  * Received: 29 June 2023\n\n  * Accepted: 16 December 2023\n\n  * Published: 21 December 2023\n\n  * DOI: https://doi.org/10.1038/s41598-023-50214-0\n\n### Share this article\n\nAnyone you share the following link with will be able to read this content:\n\nSorry, a shareable link is not currently available for this article.\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\n### Subjects\n\n  * Computer science\n  * Human behaviour\n  * Psychology\n\n## This article is cited by\n\n  * ### Smascherare le bugie con l'intelligenza artificiale\n\n    * Viola Rita\n\nNature Italy (2024)\n\n  * ### Spotting lies with artificial intelligence\n\n    * Viola Rita\n\nNature Italy (2024)\n\n## Comments\n\nBy submitting a comment you agree to abide by our Terms and Community\nGuidelines. If you find something abusive or that does not comply with our\nterms or guidelines please flag it as inappropriate.\n\nDownload PDF\n\n## Associated content\n\nCollection\n\n### Deep learning models in cognitive sciences\n\nAdvertisement\n\nScientific Reports (Sci Rep) ISSN 2045-2322 (online)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n  * Cancel contracts here\n\n\u00a9 2024 Springer Nature Limited\n\nSign up for the Nature Briefing: AI and Robotics newsletter \u2014 what matters in\nAI and robotics research, free to your inbox weekly.\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing: AI and Robotics\n\n", "frontpage": false}
