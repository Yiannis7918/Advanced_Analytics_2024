{"aid": "40031962", "title": "How fast can grammar-structured LLM generation be?", "url": "https://blog.dottxt.co/how-fast-cfg.html", "domain": "dottxt.co", "votes": 1, "user": "btwillard", "posted_at": "2024-04-14 15:46:55", "comments": 0, "source_title": "How fast can grammar-structured generation be?", "source_text": "How fast can grammar-structured generation be?\n\nHOME | .TXT WEBSITE\n\n# How fast can grammar-structured generation be?\n\n## Table of Contents\n\n  * 1\\. Introduction\n  * 2\\. An Embedded SQL Grammar\n  * 3\\. C Grammar Comparisons\n  * 4\\. Going Forward\n\n## 1 Introduction\n\nHello, this is Brandon Willard from .txt. Since we started .txt a few months\nback, we've been hard at work researching all the ways that structured\ngeneration can be used to improve the results obtained from LLMs. An important\npart of making sure that structured generation is truly useful in practice is\nguaranteeing that its costs are low enough for widespread use. Alongside that\nconcern is the general \"flexibility\" of the structure being imposed.\n\nUsually, performance and flexibility are a very tricky trade-off. A while back\nwe wrote a paper describing how minimal run-time latency can be achieved for\nregular language/regex-structured generation (see here), but, in practice, the\nflexibility of context-free languages is often needed. In the paper we also\nhinted at an extension of the regular language approach to context-free\nlanguages.\n\nI devised one complete implementation of such an extension that I'll briefly\ndemonstrate in the following. This extension to context-free grammar-\nstructured generation carries theoretical performance and scaling guarantees\nsimilar to the normal language case, and we'll see this in the form of a\nnegligible sub-millisecond average impact on generation.\n\nFor a preview, here's a side-by-side using llama.cpp:\n\n## 2 An Embedded SQL Grammar\n\nLet's say we want output that takes the following form:\n\n    \n    \n    The following is a SQL statement that returns values from the \"id\" column of the \"users\" table: ```sql select id from users ```\n\nWe'll use a model that we know is already capable of producing this kind of\noutput due to its training data:\n\n    \n    \n    import torch from outlines import models, generate model = models.transformers(\"Salesforce/codegen-350M-mono\", device=\"cuda\")\n\nFirst, we'll ask it to complete this prompt using unstructured generation\nstarting from a select statement:\n\n    \n    \n    prompt = r\"\"\"The following is a SQL statement that returns values from the \"id\" column of the \"users\" table: ```sql select\"\"\" text_generator = generate.text(model) rng = torch.Generator(device=\"cuda\") rng.manual_seed(789001) res = text_generator(prompt, max_tokens=100, rng=rng) print(prompt + res) # The following is a SQL statement that returns values from the \"id\" column of the \"users\" table: # ```sql # select id, first_name, last_name, age from users # ``` # OR as follows: # ```sql # INSERT INTO users # VALUES (95518,'Brad', 'Patel',30) # ``` # # Note: An INSERT statement which already has an autoincrement column will have this # as the column value but will instead be ``generative`` in the method definition. # This can be most any sequence_of_values_appearing\n\nThe results are not bad, but also not great. Unstructured generation adds a\nlot more to the output than we implicitly needed or wanted.\n\nWe can do better with context-free grammar (CFG) structured generation by\nclarifying exactly the kind of output we want, while also leaving enough room\nfor the model to generate useful results. More specifically, let's say we only\nwant to produce one fenced SQL code block that fulfills the implication of the\ntext preceding it.\n\nHere's a formal grammar that expresses those constraints:\n\n    \n    \n    from guided_generation.parsing import PartialLark cfg_str = r\"\"\" %import .org.partial_sql.start -> start_sql_code start: PROMPT code_block code_block : \"\\n```sql\\n\" start_sql_code \"\\n```\\n\" PROMPT : /.+/ %import common.WS %ignore WS \"\"\" lp = PartialLark(cfg_str, parser=\"lalr\", start=\"start\", deterministic=True)\n\nWhat we did in that grammar was state that we wanted arbitrary prompt text\nimmediately followed by a single fenced Markdown-style code block containing\nonly valid SQL. We accomplished the latter by importing a larger SQL grammar\nbased on this.\n\nThis example demonstrates how grammars can be composed and how doing so can\ncover the requirements of a desired prompt and output format as well as\ncontextual syntax constraints on specific parts of the output.\n\nIn order to sample according to this grammar, we create a CFG Guide that\nfollows the outlines API. While a version of CFG-structured generation with\nthe same class name already exists in outlines , we'll be using our in-house\nimplementation here. At some point, we plan to open source an implementation\nof this approach, and the interface will likely be the same.\n\n    \n    \n    from guided_generation.guides import CFGGuide cfg_guide = CFGGuide.from_cfg_string(cfg_str, model.tokenizer)\n\nHere's an example confirming that we get the expected output:\n\n    \n    \n    from outlines.samplers import multinomial cfg_generator = CFGSequenceGenerator( cfg_guide, model, multinomial(), device=model.device ) rng = torch.Generator(device=\"cuda\") rng.manual_seed(789001) res = cfg_generator(prompt, max_tokens=100, rng=rng) print(prompt + res) # The following is a SQL statement that returns values from the \"id\" column of the \"users\" table: # ```sql # select id, first_name, last_name, age from users # ``` # The parse checks out! assert lp.parse_from_state(lp.parse(prompt + res), is_end=True)\n\nWe need to emphasize that the multinomial sampling being performed here has\nnot been altered to account for the grammar constraints. The sampling is\nperformed after the entire support (i.e. non-zero \"score\" tokens) has been\ndetermined via our approach. This means that all of the results shown here\napply to any type of sampling step.\n\nWe're going to perform some adhoc profiling by monkey patching the method used\nto determine which tokens are allowed next (i.e. the support) during\nstructured generation. This is the step that introduces all the latency in\nstructured generation. The method is Guide.get_next_instruction, and you can\nreference the outlines source code to see that very little is done in this\nstep during unstructured generation.\n\n    \n    \n    import time from types import MethodType def make_timed(generator, override_class=False): times = [] _get_next_instruction = type(generator.fsm).get_next_instruction def timed_next_instruction(self, *args, **kwargs): t1 = time.perf_counter() res = _get_next_instruction(self, *args, **kwargs) t2 = time.perf_counter() times.append(t2 - t1) return res if override_class: type(generator.fsm).get_next_instruction = timed_next_instruction else: generator.fsm.get_next_instruction = MethodType(timed_next_instruction, generator.fsm) return times text_times = make_timed(text_generator) cfg_times = make_timed(cfg_generator, override_class=True)\n\nHere are the unstructured timings:\n\n    \n    \n    import numpy as np rng = torch.Generator(device=\"cuda\") rng.manual_seed(789001) text_times.clear() while len(text_times) < 500: _ = text_generator(prompt, max_tokens=100, rng=rng) len(text_times) # 500 np.max(text_times) # 0.0007046809769235551 np.mean(text_times) # 0.00037260419636731966 np.std(text_times) # 3.184645737295283e-05 %timeit text_generator(prompt, max_tokens=100, rng=rng) # 3.47 s \u00b1 161 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nHere are the structured timings:\n\n    \n    \n    rng = torch.Generator(device=\"cuda\") rng.manual_seed(789001) cfg_times.clear() while len(cfg_times) < 500: res = cfg_generator(prompt, max_tokens=100, rng=rng) assert lp.parse(prompt + res) len(text_times) # 500 np.max(cfg_times) # 0.0011627330095507205 np.mean(cfg_times) # 0.0003676540171549465 np.std(cfg_times) # 0.00022955151532709527 %timeit cfg_generator(prompt, max_tokens=100, rng=rng) # 2.25 s \u00b1 621 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nAs we can see, our CFG-structured generation adds on average sub-millisecond\nlatencies, and it barely breaks a millisecond at maximum.\n\nIn the following, we'll take the latency considerations of our approach a bit\nfurther by using the C grammar and comparing the results with llama.cpp's\ngrammar-structured generation.\n\n## 3 C Grammar Comparisons\n\nWe start by getting a sense of the latency added by llama.cpp's grammar-\nstructured generation.\n\n    \n    \n    from llama_cpp import Llama, LlamaGrammar from llama_cpp.llama_grammar import C_GBNF llm = Llama.from_pretrained( repo_id=\"cosmo3769/starcoderbase-1b-GGUF\", filename=\"*Q4_K_M.gguf\", verbose=True ) llama_c_grammar = LlamaGrammar.from_string(C_GBNF) prompt = \"\"\" int main() { int n, i, flag = 0; printf(\"Enter a positive integer: \"); scanf(\"%d\", &n); // 0 and 1 are not prime numbers // change flag to 1 for non-prime number if (n == 0 || n == 1) flag = 1; for (i = 2; i <= n / 2;\"\"\"\n\nHere we're using a very small subset of C provided by the llama.cpp library\nitself. While this grammar probably won't work to produce a good completion of\nour prompt, it should still serve to demonstrate something like a relative\nlower bound on the cost of llama.cpp's structured generation for a syntax like\nC's.\n\nRunning without grammar-structured generation:\n\n    \n    \n    res = llm(prompt, max_tokens=100, seed=2309) # Llama.generate: prefix-match hit # # llama_print_timings: load time = 3238.88 ms # llama_print_timings: sample time = 49.30 ms / 74 runs ( 0.67 ms per token, 1501.01 tokens per second) # llama_print_timings: prompt eval time = 0.00 ms / 1 tokens ( 0.00 ms per token, inf tokens per second) # llama_print_timings: eval time = 2800.12 ms / 74 runs ( 37.84 ms per token, 26.43 tokens per second) # llama_print_timings: total time = 3121.48 ms / 75 tokens print(prompt + res[\"choices\"][0][\"text\"]) # # int main() { # # int n, i, flag = 0; # printf(\"Enter a positive integer: \"); # scanf(\"%d\", &n); # # // 0 and 1 are not prime numbers # // change flag to 1 for non-prime number # if (n == 0 || n == 1) # flag = 1; # # for (i = 2; i <= n / 2; ++i) { # # //if (n % i == 0 && flag != 1) # printf(\"%d\", i); # # // if n is divisible by both 0 and 1, it remains 1 # if ((n % i) == 0) # flag = 1; # # } # return 0; # } # #\n\nRunning with llama.cpp's grammar-structured generation:\n\n    \n    \n    res = llm(prompt, max_tokens=74, grammar=llama_c_grammar, seed=2309) # Llama.generate: prefix-match hit # # llama_print_timings: load time = 1189.41 ms # llama_print_timings: sample time = 2880.14 ms / 74 runs ( 38.92 ms per token, 25.69 tokens per second) # llama_print_timings: prompt eval time = 0.00 ms / 1 tokens ( 0.00 ms per token, inf tokens per second) # llama_print_timings: eval time = 2709.23 ms / 74 runs ( 36.61 ms per token, 27.31 tokens per second) # llama_print_timings: total time = 6047.86 ms / 75 tokens print(prompt + res[\"choices\"][0][\"text\"]) # # int main() { # # int n, i, flag = 0; # printf(\"Enter a positive integer: \"); # scanf(\"%d\", &n); # # // 0 and 1 are not prime numbers # // change flag to 1 for non-prime number # if (n == 0 || n == 1) # flag = 1; # # for (i = 2; i <= n / 2;int j10x10(int x){//for each prime number (from 3 to sqrt(n))... # // if (x % 3 != 0 && x % 5 != 0) continue;//if the divisors of a number are odd or even then skip it; # // for (\n\nWhile it appears as though llama.cpp's grammar-structured generation is not\nsuited to complete the prompt with valid C code, it does spin off into C code\nproductions, which is all we need for a relative latency measurement.\n\nThe relevant results under \"sample time\" indicate that llama.cpp's structured\ngeneration\u2013for this very small subset of C\u2013adds approximately 30ms to each\nstep.\n\nTo get a better idea of the latency behind llama.cpp's structured generation,\nwe can observe the statistics for a sample sequence of length 300 under a much\nsimpler prompt:\n\n    \n    \n    res = llm( \"// The following is\", max_tokens=300, grammar=llama_c_grammar, seed=23209, ) # Llama.generate: prefix-match hit # # llama_print_timings: load time = 194438.18 ms # llama_print_timings: sample time = 10556.69 ms / 300 runs ( 35.19 ms per token, 28.42 tokens per second) # llama_print_timings: prompt eval time = 0.00 ms / 1 tokens ( 0.00 ms per token, inf tokens per second) # llama_print_timings: eval time = 10865.96 ms / 300 runs ( 36.22 ms per token, 27.61 tokens per second) # llama_print_timings: total time = 23248.72 ms / 301 tokens print(\"// The following is\" + res[\"choices\"][0][\"text\"]) # // The following isint of(){}float functions_like_object_methods(float f){}int function_like_object_method(int i){}float function_like_object_method(float f){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int i(){}float f(){}float f(){}int\n\nNow, we'll try the same model using our approach, but with a nearly complete C\ngrammar adapted to lark from this Yacc specification.\n\n    \n    \n    lp = PartialLark.open( \"org/c.lark\", parser=\"lalr\", start=\"translation_unit\", deterministic=True, )\n\nFor comparison, the C grammar provided by llama.cpp has about 21 rules; the C\ngrammar we're using has 211. This effectively means that our parser needs to\ndo considerably more work.\n\nOur approach is applied with the same settings via a c_logits_processor\ninstance:\n\n    \n    \n    from llama_cpp import LogitsProcessorList c_logits_processor = construct_logits_processor(lp, \"bigcode/starcoderbase-1b\", \"org\") res = llm( prompt, max_tokens=100, seed=2309, logits_processor=LogitsProcessorList([c_logits_processor]), ) # Llama.generate: prefix-match hit # # llama_print_timings: load time = 1189.41 ms # llama_print_timings: sample time = 50.57 ms / 74 runs ( 0.68 ms per token, 1463.23 tokens per second) # llama_print_timings: prompt eval time = 0.00 ms / 1 tokens ( 0.00 ms per token, inf tokens per second) # llama_print_timings: eval time = 2850.81 ms / 74 runs ( 38.52 ms per token, 25.96 tokens per second) # llama_print_timings: total time = 3468.24 ms / 75 tokens print(prompt + res[\"choices\"][0][\"text\"]) # # int main() { # # int n, i, flag = 0; # printf(\"Enter a positive integer: \"); # scanf(\"%d\", &n); # # // 0 and 1 are not prime numbers # // change flag to 1 for non-prime number # if (n == 0 || n == 1) # flag = 1; # # for (i = 2; i <= n / 2; ++i) { # # //if (n % i == 0 && flag != 1) # printf(\"%d\", i); # # // if n is divisible by both 0 and 1, it remains 1 # if ((n % i) == 0) # flag = 1; # # } # return 0; # } # #\n\nThe performance results and output both look familiar: sample time is around\n0.6 ms per token in both the unstructured and structured cases and the output\nis identical\u2013as it should be, since the unstructured results already consisted\nof syntactically valid C code, and we're using the same seeds.\n\nIn order to get a clearer picture of the latency introduced by our approach\nonly, we also measured the support-computing steps directly in\nc_logits_processor.__call__:\n\n    \n    \n    np.mean(c_logits_processor.times) # 0.0004697099612786661\n\nIn other words, we're introducing a latency around the ~0.6 ms of unstructured\ngeneration.\n\nUsing a sample rate estimate for our method composed of the approximately 0.5\nms average plus llama.cpp's ~0.7 ms measurement, we have the following summary\nof tokens per second:\n\nLet's use the same long-sequence parameters we used above for llama.cpp's\nstructured generation, but this time using our approach:\n\n    \n    \n    c_logits_processor.reset() res = llm( \"// The following is\", max_tokens=300, seed=23209, logits_processor=LogitsProcessorList([c_logits_processor]), ) # Llama.generate: prefix-match hit # # llama_print_timings: load time = 194438.18 ms # llama_print_timings: sample time = 217.70 ms / 300 runs ( 0.73 ms per token, 1378.02 tokens per second) # llama_print_timings: prompt eval time = 0.00 ms / 1 tokens ( 0.00 ms per token, inf tokens per second) # llama_print_timings: eval time = 11309.21 ms / 300 runs ( 37.70 ms per token, 26.53 tokens per second) # llama_print_timings: total time = 14009.49 ms / 301 tokens print(\"// The following is\" + res[\"choices\"][0][\"text\"]) # // The following is just a workaround to avoid the following error: # // /usr/bin/ld: cannot find -lgomp # // and it fails with \"Cannot load dynamic library 'libgomp.so.1'\" # // If you want to compile a statically linked version, use: # // LDFLAGS=\"-static\" make # // # // The following is just a workaround for the fact that on Mac OS X, # // GCC does not install libstdc++ unless it's also installed in a # // separate location (e.g. /usr/local). This is because you have to # // set LDFLAGS=-static -L/usr/local/lib first and then run gcc, which # // complains about conflicting symbols. Therefore, we specify the path # // to the libstdc++ that's installed in a separate location here, but # // note that it must be linked in statically (e.g. LDFLAGS=\"-static\"). # // # // The following is just a workaround for the fact that on Mac OS X, # // GCC does not install libstdc++ unless it's also installed in a # // separate location (e.g. /usr/local). This is because you have to # // set LDFLAGS=-static -L/usr/local/lib first and then run gcc, which # // complains about conflicting symbols. Therefore, we specify the path # // to the libstdc++\n\nJust to confirm that our approach is properly structuring the output, we'll\nmake the unstructured model produce results that aren't valid C according to\nour test grammar. After that, we'll show that our structured approach attempts\nto produce the same results but ultimately produces something in accordance\nwith the grammar. More specifically, our test grammar doesn't support #define\nstatements, so we'll make the model generate something with such a statement.\n\n    \n    \n    prompt = \"\"\" // The following C file multiplies\"\"\" seed = 9900 res = llm(prompt, max_tokens=100, seed=seed) # llama_print_timings: load time = 194438.18 ms # llama_print_timings: sample time = 69.89 ms / 100 runs ( 0.70 ms per token, 1430.78 tokens per second) # llama_print_timings: prompt eval time = 294.33 ms / 16 tokens ( 18.40 ms per token, 54.36 tokens per second) # llama_print_timings: eval time = 4253.96 ms / 99 runs ( 42.97 ms per token, 23.27 tokens per second) # llama_print_timings: total time = 5015.61 ms / 115 tokens print(prompt + res[\"choices\"][0][\"text\"]) # # // The following C file multiplies two 16-bit values and returns the result. # // It is defined as: # // __declspec(naked) void Multiply(__asm(\"movl %%eax, %%ecx\") unsigned short x, __asm(\"movl %%ebx, %%edx\") unsigned short y); # # #define MUL_ASM \\ # __asm { \\ # push %ecx; \\ # mov %ecx, %eax; \\ # push %edx; \\ # mov lp.parse(prompt + res[\"choices\"][0][\"text\"]) # ... # UnexpectedCharacters: No terminal matches '#' in the current parser context, at line 6 col 1 # # #define MUL_ASM \\ # ^ # ...\n    \n    \n    c_logits_processor.reset() res = llm( prompt, max_tokens=100, seed=seed, logits_processor=LogitsProcessorList([c_logits_processor]), ) # llama_print_timings: load time = 194438.18 ms # llama_print_timings: sample time = 67.86 ms / 100 runs ( 0.68 ms per token, 1473.62 tokens per second) # llama_print_timings: prompt eval time = 0.00 ms / 1 tokens ( 0.00 ms per token, inf tokens per second) # llama_print_timings: eval time = 4252.20 ms / 100 runs ( 42.52 ms per token, 23.52 tokens per second) # llama_print_timings: total time = 5158.61 ms / 101 tokens print(prompt + res[\"choices\"][0][\"text\"]) # # // The following C file multiplies two 16-bit values and returns the result. # // It is defined as: # // __declspec(naked) void Multiply(__asm(\"movl %%eax, %%ecx\") unsigned short x, __asm(\"movl %%ebx, %%edx\") unsigned short y); # # void Multiply(unsigned short x, unsigned short y) { # __asm__(\"movl %%eax, %%ecx\" # // The result is in eax. # // We don't need ebx\n\n## 4 Going Forward\n\nOur approach is able to structure generation according to non-trivial context-\nfree grammars while maintaining performance parity with unstructured\ngeneration. Likewise, these examples were performed using a larger ~49k\nvocabulary instead of the smaller ~32k vocabularies commonly used to\ndemonstrate grammar-structured generation. Our approach specifically addresses\nthe issues of scaling in vocabulary and grammar sizes. For example, we've\ntested pathological vocabularies all the way up to 125k tokens and observed\nthat they still do not exceed milliseconds in latency.\n\nFinally, it's time to explain another important detail behind this\nimplementation: it's unoptimized pure Python. We've been focusing on\ncorrectness and breadth of grammar support, so the implementation hasn't been\ndesigned for performance yet. We have numerous optimizations lined up that\nwill bring that millisecond latency down even further. We also have natural\nextensions to non-deterministic context-free grammars, non-trivial lexer-level\nlogic, and efficient semantic constraints.\n\nIn summary, expect to see even more from .txt!\n\n", "frontpage": false}
