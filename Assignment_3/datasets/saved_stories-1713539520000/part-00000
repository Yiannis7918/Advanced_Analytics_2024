{"aid": "40084699", "title": "Llama 3 8B is almost as good as Wizard 2 8x22B", "url": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/discussions/5", "domain": "huggingface.co", "votes": 32, "user": "tosh", "posted_at": "2024-04-19 09:11:27", "comments": 7, "source_title": "MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF \u00b7 OK llama 3 8b model is INSANE. Is almost as good as wizard 2 8x22b!", "source_text": "MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF \u00b7 OK llama 3 8b model is INSANE.\nIs almost as good as wizard 2 8x22b!\n\nHugging Face\n\n#\n\nMaziyarPanahi\n\n/\n\nMeta-Llama-3-8B-Instruct-GGUF\n\nText Generation Transformers GGUF PyTorch English mistral facebook meta llama\nllama-3 quantized 2-bit 3-bit 4-bit precision 5-bit 6-bit 8-bit precision\n16-bit GGUF text-generation-inference\n\nModel card Files Files and versions Community\n\n7\n\n## OK llama 3 8b model is INSANE. Is almost as good as wizard 2 8x22b!\n\n#5\n\npinned\n\nby mirek190 - opened about 13 hours ago\n\nDiscussion\n\nmirek190\n\nabout 13 hours ago\n\n\u2022\n\nedited about 13 hours ago\n\nThis 8b model is insane.... almost level of wizard 2 8x22b! which has 140b\nparameters....wtf!\n\nllamacpp\n\nmain.exe --model models/new3/Meta-Llama-3-8B-Instruct.Q8_0.gguf --color\n--threads 30 --keep -1 --batch-size 512 --n-predict -1 --repeat-penalty 1.1\n--ctx-size 0 --interactive -ins -ngl 99 --simple-io -r '<|eot_id|>' --in-\nprefix \"\\n<|start_header_id|>user<|end_header_id|>\\n\\n\" --in-suffix\n\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" -p\n\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a\nhelpful, smart, kind, and efficient AI assistant. You always fulfill the\nuser's requests to the best of your\nability.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\nHi!<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\nSeriously ! wtf\n\n    \n    \n    Today I own 3 cars but last year I sold 2 cars. How many cars do I own today? <|eot_id|><|start_header_id|>assistant<|end_header_id|> Let's break it down! You currently own 3 cars. Last year, you sold 2 cars. To find the number of cars you owned before selling any, add the current number to the number of cars sold: 3 (current) + 2 (sold) = 5 cars Since you've already sold those 2 cars, subtract them from the total: 5 - 2 = 3 cars You still own the same 3 cars that you currently own.\n\neven made this question almost properly! ... forgot about 1 gold coin...\n\n    \n    \n    I have 10 apples. I find 3 gold coins in the bottom of a river. The river runs near a big city that has something to do with what I can spend the coins on. I then lose 4 apples but gain a gold coin. Three birds run into my path and drop 6 apples each. I play an online game and win 6 gold coins but I have to share them equally with my 2 teammates. I buy apples for all the coins I have. The price of an apple is 0.5 coins. How many apples do I have? And where is the river? <|eot_id|><|start_header_id|>assistant<|end_header_id|> Let's break down the changes you've experienced step by step! 1. You start with 10 apples. 2. You find 3 gold coins in the river, which doesn't affect your apple count. 3. You lose 4 apples, leaving you with 6 apples (10 - 4 = 6). 4. The birds drop 6 apples each, so that's a total of 18 new apples (6 x 3 = 18). Adding these to your remaining apples gives you a total of 24 apples (6 + 18 = 24). 5. You win 6 gold coins in the game but have to share them equally with your teammates. That means you get 2 gold coins (6 \u00f7 3 = 2). 6. You spend all the gold coins you have (including the original 3 and the new 2) on apples, since each apple costs 0.5 coins. Let's calculate how many apples you can buy with your total gold coins: You have a total of 5 gold coins (3 + 2). Since each apple costs 0.5 coins, you can buy: 5 coins \u00f7 0.5 coins/apple = 10 apples Adding these to the apples you already had, you now have a total of: 24 apples + 10 apples = 34 apples As for where the river is located, it runs near a big city that allows you to spend the gold coins on something (apples). Since you found the gold coins in the river and then went to play an online game with your teammates, it's likely that the city is nearby or easily accessible. You now have 34 apples!\n\nlook on this! insane! only GPT4 and OPUS can answer it!\n\n    \n    \n    Create 10 sentences that ends with a word \"apple\". Remember the word \"apple\" MUST be at the end. <|eot_id|><|start_header_id|>assistant<|end_header_id|> Here are 10 sentences that end with the word \"apple\": 1. The farmer picked a juicy red apple. 2. She ate a crunchy snack of peanut butter and apple. 3. The teacher asked her student to bring a small green apple. 4. I love biting into a fresh, crisp autumn apple. 5. Grandma always kept a sweet, ripe Granny Smith apple. 6. The health-conscious person chose a healthy Gala apple. 7. Dad planted a row of seedless Red Delicious apple. 8. In the still life painting, there was a beautiful Golden apple. 9. Mom packed a lunchbox with a juicy Fuji apple and cheese. 10. At the farmer's market, I bought a big, ripe Braeburn apple. Each sentence ends with the word \"apple\", as requested!\n\nor this ...\n\n    \n    \n    What are the top 10 most famous non-human animal statues? <|eot_id|><|start_header_id|>assistant<|end_header_id|> What a unique question! Here's a list of the top 10 most famous non-human animal statues, in no particular order: 1. **The Lion at the Louvre** (Paris, France): A majestic bronze lion statue guarding the entrance to the world-famous Louvre Museum. 2. **The Charging Bull** (New York City, USA): A iconic bronze sculpture of a bull, symbolizing strength and resilience, located in Lower Manhattan's Financial District. 3. **The Horses of Saint-Mark's Basilica** (Venice, Italy): Four magnificent bronze horses adorning the facade of Saint-Mark's Basilica, a masterpiece by Italian sculptor Barletta. 4. **The Lion of Belfort** (Belfort, France): A massive 15-meter-tall stone lion statue guarding the citadel in eastern France. 5. **The Elephant Gates** (Paris, France): Two colossal bronze elephants flanking the entrance to the Mus\u00e9e d'Orsay, a famous art museum. 6. **The Bronze Horseman** (St. Petersburg, Russia): A stunning equestrian statue of Peter the Great, located in Senate Square, symbolizing Russian history and culture. 7. **The Fountain of the Four Rivers** (Rome, Italy): A majestic bronze statue depicting four rivers (Nile, Ganges, Danube, and Rio Plata) being personified as women, located in the Piazza Navona. 8. **The Lion of Glasgow** (Glasgow, Scotland): A grand stone lion statue standing guard outside the City Chambers building in Glasgow's George Square. 9. **The Horse Guards** (London, UK): Two imposing bronze horse statues situated at the entrance to Buckingham Palace, symbolizing British military prowess. 10. **The Sea Lions of the Golden Gate Bridge** (San Francisco, USA): Two iconic bronze sea lion statues guarding the Golden Gate Bridge, a popular tourist attraction.\n\ninsane ...\n\n    \n    \n    If I lay 2 wet shirts out in the sun flat on the ground to dry and it takes 5 hours until they are dry, how long would it take to dry 4 shirts of the same type that way simultanously? <|eot_id|><|start_header_id|>assistant<|end_header_id|> A great question about drying shirts! The time it takes for the shirts to dry is influenced by factors like air circulation, temperature, and humidity. Since you're laying them flat on the ground in the sun, we can assume that these conditions are similar for all four shirts. Since it took 5 hours to dry 2 shirts simultaneously, it's likely that the drying process is not affected by the number of shirts being dried. This means that the same amount of time would be required to dry 4 shirts as well! So, if it takes 5 hours to dry 2 shirts, it would still take approximately 5 hours to dry all 4 shirts simultaneously\n    \n    \n    There are two ducks in front of a duck, two ducks behind a duck and a duck in the middle. How many ducks are there? <|eot_id|><|start_header_id|>assistant<|end_header_id|> The classic lateral thinking puzzle! Let's break it down: \"two ducks in front of a duck\" means that one duck is standing in front of another. \"Two ducks behind a duck\" implies that this same duck is being followed by two more ducks. And, of course, there's the \"duck in the middle\". So, if we combine all these clues, we can conclude that there are... 3 ducks!\n\nTHAT llama3 8b is INSANE. Better than anything I saw even better than wizard 2\n8x22b ! ( question with apple sentence, etc ) That is a huge leap to other\ndimension for small models performance ... I just do not believe such a small\nmodel ( 8b ! ) can be so smart and has a such high reasoning capability. I am\nafraid of testing 70b version ;)\n\nmirek190 changed discussion title from O k that llama 3 8b model is INSANE. Is\nhas almost as good as wizard 2 8x22b! to OK llama 3 8b model is INSANE. Is\nalmost as good as wizard 2 8x22b! about 13 hours ago\n\nMaziyarPanahi\n\nOwner about 13 hours ago\n\nThis looks sick! @mirek190 many thanks for sharing this, it really is insane!\nFor such a small model! Wow!\n\nMaziyarPanahi\n\nOwner about 13 hours ago\n\nNow I am excited to see what else you can do with this model! :D\n\nmoreward\n\nabout 12 hours ago\n\nKarpathy mentioned that it was trained like extra super long + more data. He\nclaims today's mainline models are 100-1000x undertrained!\n\nMaziyarPanahi\n\nOwner about 12 hours ago\n\nthat makes sense actually! Meta has massive amount of resources, so they can\ngo longer and much wider in training. This given its size, is going to be the\nbase of so many fine-tunes from the community! I am very excited. I had to\ndouble check to make sure 70B was not used in the example by accident! :D\n\nmirek190\n\nabout 12 hours ago\n\n\u2022\n\nedited about 12 hours ago\n\nI answered the second time the most complex question ... and answered\nCORRECTLY .... I remind GPT-4 is hardly answering this one.\n\n    \n    \n    I have 10 apples. I find 3 gold coins in the bottom of a river. The river runs near a big city that has something to do with what I can spend the coins on. I then lose 4 apples but gain a gold coin. Three birds run into my path and drop 6 apples each. I play an online game and win 6 gold coins but I have to share them equally with my 2 teammates. I buy apples for all the coins I have. The price of an apple is 0.5 coins. How many apples do I have? And where is the river? <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nLet's break it down step by step! 1. You start with 10 apples. 2. You find 3 gold coins in the river, but this information doesn't change your apple count. 3. You lose 4 apples, leaving you with: * 10 - 4 = 6 apples 4. The birds drop 6 apples each, so that's a total of: * 6 (your remaining apples) + 18 (birds' apples) = 24 apples 5. You play an online game and win 6 gold coins, but you have to share them equally with your 2 teammates. That means you get: * 6 coins / 3 players = 2 coins per player You now have a total of: 3 (initial coins) + 1 (new coin) + 2 (shared coins) = 6 gold coins 6. You buy apples for all the coins you have, using the price of an apple being 0.5 coins: * 6 coins / 0.5 coins per apple = 12 apples You now have a total of: 24 apples (from birds and previous losses) + 12 apples (bought with gold coins) = 36 apples So, you now have 36 apples! As for the river, it's near a big city that has something to do with what you can spend the coins on. Based on the fact that you won gold coins and spent them on apples, I'm going to take a guess that the river is near a city known for its agriculture or fruit production!\n\nAlso only GPT-4 is answering this one ...\n\n    \n    \n    Create 10 sentences that ends with a word \"apple\". Remember the word \"apple\" MUST be at the end. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHere are 10 sentences that end with the word \"apple\": 1. The farmer picked the juiciest apple. 2. I'm going to make a pie with this sweet apple. 3. He took a big bite of his crunchy apple. 4. The tree is bearing delicious red apple. 5. She hid her phone under the kitchen apple. 6. It's time to eat a healthy green apple. 7. They went on a trip to pick fresh yellow apple. 8. The teacher gave each student a small shiny new apple. 9. He got bitten by a wormy old apple. 10. I'm saving up my money for a brand new shiny apple. Let me know if you need anything else!\n\nMaziyarPanahi\n\nOwner about 12 hours ago\n\nWow! my fine-tuned Mixtral-8x22B couldn't get to So, you now have 36 apples!,\nhaha! This is a very strong 8B! (I was a bit sad not to see a 32B, but this is\nmuch smaller and much better than any 32B if I am not mistaken)\n\nMaziyarPanahi\n\nOwner about 12 hours ago\n\nMaziyarPanahi\n\nOwner about 12 hours ago\n\nWho remembers this one: Generate 10 sentences MUST be ending with the word\n\\\"apple\\\".\n\nThe only LLM that got it right was DBRX, which comparing that to 8B is just\ncrazy!\n\nmirek190\n\nabout 12 hours ago\n\nLook\n\nI asked for a snake game! WORKS WTF\n\n    \n    \n    Provide complete working code for a snake game in python please. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHere's a simple implementation of the classic Snake Game using Python and the Pygame library. This game creates a window with a snake that you can control to eat food pellets while avoiding the edges and its own tail. ``` import pygame import random import time import sys # Initialize Pygame pygame.init() # Set up some constants WIDTH, HEIGHT = 800, 600 SPEED = 10 WHITE = (255, 255, 255) RED = (255, 0, 0) # Set up the display screen = pygame.display.set_mode((WIDTH, HEIGHT)) # Set up the snake and food snake = [(200, 200), (220, 200), (240, 200)] food = (400, 300) direction = 'right' # Game loop while True: # Event handling for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() sys.exit() elif event.type == pygame.KEYDOWN: if event.key == pygame.K_UP and direction != 'down': direction = 'up' elif event.key == pygame.K_DOWN and direction != 'up': direction = 'down' elif event.key == pygame.K_LEFT and direction != 'right': direction = 'left' elif event.key == pygame.K_RIGHT and direction != 'left': direction = 'right' # Move the snake head = snake[0] if direction == 'up': new_head = (head[0], head[1] - SPEED) elif direction == 'down': new_head = (head[0], head[1] + SPEED) elif direction == 'left': new_head = (head[0] - SPEED, head[1]) elif direction == 'right': new_head = (head[0] + SPEED, head[1]) snake.insert(0, new_head) # Check for collision with food if snake[0] == food: food = (random.randint(0, WIDTH - SPEED), random.randint(0, HEIGHT - SPEED)) else: snake.pop() # Check for collision with the edge or itself if (snake[0][0] < 0 or snake[0][0] >= WIDTH or snake[0][1] < 0 or snake[0][1] >= HEIGHT or snake[0] in snake[1:]): pygame.quit() sys.exit() # Drawing screen.fill(WHITE) for pos in snake: pygame.draw.rect(screen, RED, (pos[0], pos[1], SPEED, SPEED)) pygame.draw.rect(screen, RED, (food[0], food[1], SPEED, SPEED)) # Update the display pygame.display.flip() # Cap the frame rate time.sleep(0.05) ``` This game will run indefinitely until the snake collides with the edge or itself.\n\nMaziyarPanahi\n\nOwner about 12 hours ago\n\nmind => blown\n\nmirek190\n\nabout 12 hours ago\n\n> that makes sense actually! Meta has massive amount of resources, so they can\n> go longer and much wider in training. This given its size, is going to be\n> the base of so many fine-tunes from the community! I am very excited. I had\n> to double check to make sure 70B was not used in the example by accident! :D\n\nlol\n\nmirek190\n\nabout 12 hours ago\n\n\u2022\n\nedited about 3 hours ago\n\n> Wow! my fine-tuned Mixtral-8x22B couldn't get to So, you now have 36\n> apples!, haha! This is a very strong 8B! (I was a bit sad not to see a 32B,\n> but this is much smaller and much better than any 32B if I am not mistaken)\n\neven any 70b model before llama 3 8b ....\n\nmirek190\n\nabout 12 hours ago\n\n> Karpathy mentioned that it was trained like extra super long + more data. He\n> claims today's mainline models are 100-1000x undertrained!\n\nlooking on llama 3 I can belive in it ....\n\nmirek190 changed discussion status to closed about 12 hours ago\n\nMaziyarPanahi changed discussion status to open about 12 hours ago\n\nMaziyarPanahi pinned discussion about 12 hours ago\n\nMaziyarPanahi\n\nOwner about 12 hours ago\n\nI am gonna pin this thread, this is just too good to get closed @mirek190 ! \ud83d\udd25\ud83d\udd25\n\nReza2kn\n\nabout 9 hours ago\n\nInteresting! I was wondering if the 70B model was any better than the Bigstral\nor Command R+, but apparently the 8B model is pretty good on its own! I'd love\nto see Wolfram from r/LocalLlama's take on both the 8B and 70B models, also\nlove to see what can Eric Hartford and Teknium build with it :)\n\nappvoid\n\nabout 6 hours ago\n\nprompt:\n\n    \n    \n    || Query: If I have 6 apples and eat seven of them, how many are left and why?. || Answer:\n\noutput:\n\n    \n    \n    0 apples are left. This is because you ate 7 apples, but you only had 6 apples to begin with. You cannot eat more apples than you have.\n\nI can't believe its reasoning! This is the fp16 version, I can assure you this\nis the closest you can get to GPT-4 in that size.\n\nMaziyarPanahi\n\nOwner about 5 hours ago\n\n@appvoid right?!!! It\u2019s bananas that you can\u2019t trick it to make a mistake as\neasily as we do with giants LLMs! How long was that training!\n\nMaziyarPanahi\n\nOwner about 5 hours ago\n\n> I'd love to see Wolfram from r/LocalLlama's take on both the 8B and 70B\n> models, also love to see what can Eric Hartford and Teknium build with it :)\n\nDon\u2019t quote me on this, but I have a feeling all 3 are happening right now \ud83d\ude06\n\nappvoid\n\nabout 4 hours ago\n\n> @appvoid right?!!! It\u2019s bananas that you can\u2019t trick it to make a mistake as\n> easily as we do with giants LLMs! How long was that training!\n\nIt was trained on 15 trillion tokens so it makes sense to me. It makes\nmistakes on this prompt though:\n\n    \n    \n    Is Europe the only continent that starts with A?\n\nThis is due to the nature of language models and prompting them with false\ninformation will make them produce the most likely false information you would\nlike. However, is still impressive on this size this kind of performance. Is\nliterally the old GPT-4 model on your pc.\n\nmirek190\n\nabout 3 hours ago\n\n\u2022\n\nedited about 3 hours ago\n\n..in size of 8b .... HOW.\n\nBUT\n\nlook what llma3 70b model says Answer is correct . GPT-4 make mistake\n\n    \n    \n    Is Europe the only continent that starts with A? <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n No, Europe is not the only continent that starts with \"E\", not \"A\". Africa is another continent that starts with the letter \"A\". There are 7 continents in total: Africa, Antarctica, Asia, Australia, Europe, North America, and South America.\n\nmirek190\n\nabout 3 hours ago\n\n> prompt:\n>  \n>  \n>     || Query: If I have 6 apples and eat seven of them, how many are left\n> and why?. || Answer:\n>\n> output:\n>  \n>  \n>     0 apples are left. This is because you ate 7 apples, but you only had 6\n> apples to begin with. You cannot eat more apples than you have.\n>\n> I can't believe its reasoning! This is the fp16 version, I can assure you\n> this is the closest you can get to GPT-4 in that size.\n\nThat 8b model seems like impossible project ...\n\nMaziyarPanahi\n\nOwner about 3 hours ago\n\nI am having a hard time to come up with a reason to fine-tune Llama-3-8B!\nMaybe more coding datasets or function calling?\n\nmirek190\n\nabout 2 hours ago\n\n\u2022\n\nedited about 2 hours ago\n\nLlama 3 8b is so capable ... Hard to believe .\n\nJust try and find out if can be even better.\n\nCan you imagine a new mistal 2 8b model and later mixtral 2 instruct 8x8b\nmodel .... Wild times to come ...;)\n\npouya-sa\n\nabout 2 hours ago\n\nBased on the benchmark it seems they worked on coding I tested the snake game\nin C# with 70B parameters model and it made it amazingly impressive but when I\npushed it hard on another hard question I went crazy but I don't know if it\nwas hugging chat problem or the model\n\nMaziyarPanahi\n\nOwner about 2 hours ago\n\n@pouya-sa could it be the prompt template? It's new and we need many round of\nimprovements specially for multi-turn conversations to best optimize this.\nMaybe using it as an instruct (single question, no history) for now it's the\nsafest.\n\n\u00b7 Sign up or log in to comment\n\n", "frontpage": true}
