{"aid": "39961282", "title": "Anticipatory Music Transformer", "url": "https://crfm.stanford.edu/2023/06/16/anticipatory-music-transformer.html", "domain": "stanford.edu", "votes": 2, "user": "tarr11", "posted_at": "2024-04-07 15:11:34", "comments": 0, "source_title": "Stanford CRFM", "source_text": "Stanford CRFM\n\n## Anticipatory Music Transformer: A Controllable Infilling Model for Music\n\n#### Authors: John Thickstun and David Hall and Chris Donahue and Percy Liang\n\nThe Anticipatory Music Transformer is a controllable generative model that\nfacilitates co-composition of music with AI. Paper Code Release Model Release\nModel Card Colab Notebook Correspondence to: jthickstun@cs.stanford.edu\n\n  * Input\n\n  * Output\n\nInput: Melody, Output: Accompaniment (option 1)\n\nInput: Melody, Output: Accompaniment (option 2)\n\nInput: Beginning and Ending, Output: A Bridged Span\n\nInput: Chords and Drums, Output: Melody and Bassline\n\nFigure 1. The Anticipatory Music Transformer generates infilling completions\nof music. Hover (click on mobile) to see the generated completions.\n\nThe Anticipatory Music Transformer is a generative model of symbolic music. We\ncan control this model by writing parts of a musical composition, and asking\nthe model to fill in the rest. For example, suppose you wrote a melody: you\ncan ask the model to suggest accompaniments to this melody. Or suppose you\nwrote a complete composition and are unsatisfied with a particular section:\nyou can ask the model to suggest alternative variations for this section.\nThese are examples of infilling control: the Anticipatory Music Transformer\ngenerates (i.e., infills) completed musical compositions, given an incomplete\ncomposition as input (see Figure 1).\n\nPeople value agency and control over creative collaborations with generative\nAI; we want to build models that promote interactive, human-centered\nengagements. The Anticipatory Music Transformer facilitates co-creation of\nmusic via infilling operations. Infilling gives a composer fine-grained\ncontrol over this collaboration, choosing what to write themselves and what to\ndelegate to AI. Infilling control also facilitates an interactive co-creation\nprocess, whereby the composer iteratively queries the model and keeps\nfragments of the generated music that they like while discarding the rest.\n\nThe Anticipatory Music Transformer models symbolic music rather than musical\naudio. We model symbolic music because we hope to build interactive tools for\ncomposers, analogous to a writing assistant. For the audio examples below, we\nsynthesize artificial performances of symbolic music. This mechanical\nsynthesis generates audio with a particular quality reminiscent of 90's video\ngame music. We are enthusiastic about collaborating with musicians and\nelectronic music producers to create more expressive and sonically rich\nperformances: if you are interested in artistic collaborations, please reach\nout!\n\n  * Tonal controls\n\n  * Percussive controls\n\n  * Tonal completion\n\n  * Percussive completion\n\nMelody + Vamp Input Generated Accompaniment (option 1) Generated Accompaniment\n(option 2)\n\n1 / 1\n\nExample 1. Alternative accompaniments to a given melody, proposed by the\nAnticipatory Music Transformer.\n\nUsing infilling control, we can generate and revise variations of the output\nof an Anticipatory Music Transformer to suit our preferences. In Example 1, we\nshow two different accompaniments generated by the model (visualized in purple\nand orange) given a generated melody and a brief vamp as input (visualized in\nblue and green). After choosing to accept the first accompaniment (option 1)\nwe may continue to revise this composition. For example, we can delete a span\nof notes from the composition, and query the Anticipatory Music Transformer to\noffer alternative infilling revisions for this missing span. This interaction\npattern is exhibited in Example 2.\n\n  * Tonal controls\n\n  * Percussive controls\n\n  * Tonal completion\n\n  * Percussive completion\n\nMasked Input Generated Revision (option 1) Generated Revision (option 2)\nGenerated Revision (option 3)\n\n1 / 1\n\nExample 2. Revisions of a given span of music, proposed by the Anticipatory\nMusic Transformer.\n\nWe release the 360M parameter Anticipatory Music Transformer used to create\nthe examples on this page. This model was trained for 800k steps on the Lakh\nMIDI dataset. We also release code for training and querying the model. This\ncode is a crude interface for interacting with the Anticipatory Music\nTransformer: we encourage the community to integrate this model into more\nstandard music sequencing workflows such as Ableton or Logic. Please reach out\nif we are able to help with these integrations.\n\n# Foundation Models for Music\n\nThe Anticipatory Music Transformer is built using the generative pretrained\nTransformer architecture (GPT) that powers language models like GPT-4 and\ntools like ChatGPT. Analogous to language models that are trained to predict\nthe next word in a sentence, previous GPT-powered music models--including\nGoogle's Music Transformer and OpenAI's MuseNet--are trained to predict the\nnext note in a sequence given the past. These models linearly generate a\ncomposition from start to finish. In contrast, the human process of music\ncomposition is iterative and non-linear: a composer will sketch, edit, go\nback, and revise a draft many times.\n\nA foundation model for music should support the human composition process. In\ncontrast to standard GPT-based models that generate music from start to\nfinish, the Anticipatory Music Transformer is trained to infill parts of a\npre-existing draft composition. We believe that infilling control complements\nthe human composition and editing process. The infilling capabilities of the\nAnticipatory Music Transformer are made possible by a modeling principle that\nwe call anticipation. Whereas standard GPT-based models are trained to predict\nthe next note given the past, anticipatory models are trained to predict the\nfuture given both the past and foreknowledge of certain upcoming notes in the\nfuture. We say that the anticipatory model anticipates these upcoming notes.\nSee the paper for a technical description of anticipation.\n\nLike sequence-to-sequence modeling (Seq2Seq) anticipation combines a pair of\ninput and output sequences into a single training sequence. Whereas Seq2Seq\nprepends the input sequence to the output sequence, anticipation interleaves\ninputs with outputs, so that an input note at time t is located close to\noutputs near time t in the combined sequence. This preserves locality in the\ntraining sequence, an important inductive bias for many models. Crucially, we\nshow in the paper that\u2014to be able to condition on the inputs of an\nanticipatory model\u2014the inputs must appear following stopping times in the\nouput sequence. This property is not satisfied by natural interleaving\nstrategies (e.g., sort-ordering) which motivates a more subtle definition of\nanticipation.\n\nInfilling makes the Anticipatory Music Transformer more controllable than\nprevious generative models of music. Music infilling control is analogous to\nthe inpainting capabilities of generative image models like DALL\u00b7E and Stable\nDiffusion. In contrast to these image models, the Anticipatory Music\nTransformer does not use language to control the model's outputs. Whereas\nlanguage is an expressive medium for describing visual imagery, colloquial\nlanguage is often a poor tool for expressing musical ideas: writing about\nmusic is like dancing about architecture. Instead of providing a language\ninterface, we focus on infilling capabilities as an interaction mechanism for\ncontrolling the Anticipatory Music Transformer.\n\n# Interactive Control\n\nWe control the Anticipatory Music Transformer with asynchronous control inputs\n(e.g., a melody). These controls are asynchronous because they can appear at\ndifferent times than the generated notes (e.g., an accompaniment to the\nmelody). In addition to control inputs, we can exercise editorial control by\nchoosing to accept, reject, or revise the generated notes proposed by the\nmodel. Accepted notes are appended to the note history, becoming feedback to\nsubsequent generation. We can think of the Anticipatory Music Transformer as a\nnon-linear, asynchronous feedback controller. An example of an interactive\ncontrol flow for symbolic music generation is illustrated by Figure 2.\n\nFigure 2. An interactive feedback loop for music co-creation with an\nAnticipatory Music Transformer.\n\nThe accompaniments in Example 3 are created using this interactive feedback\nloop, using a melody as the control inputs. We interact with the model by\nincrementally generating a short sequence of notes (2-5 seconds at a time) and\nchoosing whether to accept or reject this proposed continuation of the music.\nWe did not manually revise the model's outputs to create the music in Example\n3. Generating music with this incremental, interactive process allows us to\ncontrol its evolution, steering the Anticipatory Music Transformer in\ndirections that we find musically interesting. Samples from the model\ngenerated without human interaction are included at the end of this post.\n\nWe sample from the model using nucleus sampling. By default (and for the non-\ninteractive examples at the end of this post) we use Nucleus p=0.95. But for\ninteractive generation, we found it helpful to vary this parameter. Setting\nhigh values of p encourages diverse generation: sometimes this creates\nexciting music, but the results can also be too experimental. Setting low\nvalues of p encourages more conservative generation. By adaptively choosing p,\nwe are able to exert further control over the model, adjusting between more\nexploratory musical ideation (high p) and more conservative resolution of\nearlier musical ideas. See the Colab notebook for an implementation of this\ninteractive control flow.\n\n  * Tonal input\n\n  * Percussive input\n\n  * Tonal output\n\n  * Percussive output\n\nMelody + Vamp Input Co-Composed Accompaniment Original Composition\n\n1 / 3\n\nDua Lipa - Levitating\n\nExample 3. Musical accompaniments to popular melodies, generated by the\nAnticipatory Music Transformer with interactive editing by John Thickstun.\n\n# Releasing the Anticipatory Music Transformer\n\nWe release the model featured on this page, as well as models trained for the\nablation study in Table 1 of the paper. Intermediate training checkpoints\n(saved every 10k steps) are also available on request. All code and models are\nreleased under the Apache License, Version 2.0.\n\n  * Paper: the paper describing anticipation, the application of anticipation to implementing and training an Anticipatory Music Transformer, evaluation of these models, and discussion of the potential use and deployment of these models.\n  * Code: code related to the Anticipatory Music Transformer, including infrastructure for pre-processing anticipatory dataset, sampling from the model, and performing human evaluation. Instructions are also included for training an Anticipatory Music Transformer; the Levanter codebase used to train these models is available separately.\n  * Model Weights: the final checkpoints for each of the 9 models trained in the paper are hosted by the Stanford CRFM organization on the HuggingFace Hub. Here are links to specific models (Row numbers reference Table 1 of the Anticipatory Music Transformer paper):\n    * Small Autoregressive - 100k Steps - Interarrival-Time Encoded (Row 1)\n    * Small Autoregressive - 100k Steps (Row 2)\n    * Small Anticipatory - 100k Steps (Row 3)\n    * Small Autoregressive - 800k Steps (Row 4)\n    * Small Anticipatory - 800k Steps (Row 5) [Good Model; More Efficient Inference]\n    * Medium Anticipatory - 100k Steps (Row 6)\n    * Medium Anticipatory - 200k Steps (Row 7)\n    * Medium Anticipatory - 800k Steps (Row 8) [Best Model]\n    * Large Anticipatory - 100k Steps (Row 9)\n  * Google Colab notebook: an interactive demo of the Anticipatory Music Transformer.\n\nThe Lakh MIDI dataset used to train the Anticipatory Music Transformer is\nlicensed under the Creative Commons CC-BY 4.0 terms, which ostensibly allows\nfree redistribution, reuse, remixing, and transformation of its content for\nany purpose. However, in many cases, the MIDI files contained in this dataset\nare derivative work, transcribed from source material (e.g., pop music) that\nis subject to copyright; we presume that the copyright status of Lakh MIDI is\nmore restrictive than its license would suggest. The copyright status of\nmodels trained on copyrighted data is an open legal question, and therefore\nthe Apache 2.0 license may not fully reflect the legal status of the\nAnticipatory Music Transformer.\n\nGenerative models are known to memorize parts of their training data. The\nAnticipatory Music Transformer provide no technical guarantees against\nreproducing music from the Lakh MIDI training dataset: any music generated by\nthe Anticipatory Music Transformer risks infringing on copyrighted material.\nIndeed, we observe that the third example of the generated accompaniments is\nnearly identical to the corresponding human-composed accompaniment; it is\nlikely that a near-duplicate version of this composition appears in the Lakh\nMIDI training data split and has been memorized. We therefore recommend\ncaution in deployment of these models for commercial activities.\n\nWe see many opportunities for generative models to support human creativity\nand expression. But insofar as these models increase productivity or automate\naspects of the creative process, they may also be disruptive for people who\nmake a living creating commercial art and music. See Section 7 of the paper\nfor a more thorough discussion of ethical considerations regarding the\ndeployment of generative models of music. We have worked to construct a model\nwith control capabilities that make it a useful, empowering tool for\ncomposers. We welcome feedback on our decision to pursue this line of\nresearch; we hope to foster a discussion of how we can steer future research\nin this area towards methods that serve and support composers and musicians.\n\n# Acknowledgments\n\nWe thank Jennifer Brennan, Andy Chen, and Josh Gardner for feedback on this\nblog post.\n\nThe members of this research team belong to the Stanford Center for Research\non Foundation Models (CRFM), the Stanford Natural Language Processing Group,\nand Google DeepMind. We thank the CRFM and the Stanford Institute for Human-\nCentered Artificial Intelligence (HAI) for supporting this work. Toyota\nResearch Institute (TRI) and Schmidt Futures provided funds to support this\nwork. The models released with this work were trained on Cloud TPU VMs,\nprovided by Google\u2019s TPU Research Cloud (TRC).\n\n    \n    \n    @article{thickstun2023anticipatory, title={Anticipatory Music Transformer}, author={Thickstun, John and Hall, David and Donahue, Chris and Liang, Percy}, journal={arXiv preprint arXiv:2306.08620}, year={2023} }\n\n## Uncurated Music Examples\n\nInputs for the following examples were randomly selected from the Lakh MIDI\ntest set. Outputs of the Anticipatory Music Transformer were generated with\nnucleus sampling (p = 0.95). Outputs are not cherry-picked.\n\nThese examples were used for human evaluation of the Anticipatory Music\nTransformer. Model outputs were generated in two settings, controlled by two\ndifferent types of input:\n\n  * Accompaniment: the model generates an infilling completion of music given a melody as input.\n  * Continuation: the model generates a temporal continuation of music given the beginning as input.\n\nSee the paper for results of human evaluation using these examples.\n\n### Accompaniment\n\nFor the accompaniment task, generation methods are given a prompt consisting\nof 5 seconds of a full ensemble and an additional 15 seconds of a single\nmelodic instrument from that ensemble. Prompts are randomly sampled from the\nLakh MIDI test set. The model is prompted to infill 15 seconds of musical\ncontent in a manner that is musically compatible with both the (past) ensemble\nand (contemperaneous) solo instrument prompt. We compare anticipation to two\nbaseline infilling methods (Autoregressive and Retrieval). The generated\ncompletions are evaluated in Table 3 of the paper.\n\n  * Tonal controls\n\n  * Percussive controls\n\n  * Tonal completion\n\n  * Percussive completion\n\nInput Anticipatory Autoregressive (baseline) Retrieval (baseline) Human\n\n1 / 50\n\n### Continuation\n\nFor the continuation task, models are given a prompt consisting of 3 bars of a\ncomposition (4-6 seconds). Prompts are randomly sampled from the beginnings of\ncompositions in the Lakh MIDI test set. The model is prompted to generate the\nremainder of a 20-second clip (approximately 15 seconds of musical content) in\na manner that is musically compatible with the first 3 bars. We compare our\nbest model (Medium - 360M parameters) to a smaller version of the same model\n(Small - 128M parameters) as well as the FIGARO Music Transformer and an\nautoregressive model trained without anticipation that uses a more standard\n(interarrival) tokenization of music. The generated completions are evaluated\nin Table 2 of the paper.\n\n  * Tonal controls\n\n  * Percussive controls\n\n  * Tonal completion\n\n  * Percussive completion\n\nInput Anticipatory (Medium) FIGARO Anticipatory (Small) Autoregressive (Small\ninterarrival) Human\n\n1 / 50\n\nSign up to get email updates on the Center for Research on Foundation Models\n(CRFM) or email us at contact-crfm@stanford.edu.\n\nCRFM is grateful to our supporters.\n\n\u00a9 2021. Stanford Center for Research on Foundation Models. Designed by Joon\nSung Park.\n\n", "frontpage": false}
