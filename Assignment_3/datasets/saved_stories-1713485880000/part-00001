{"aid": "40079103", "title": "LLM costs per MTok (updated 4/18/2024)", "url": "https://venki.dev/notes/llm-costs", "domain": "venki.dev", "votes": 1, "user": "venkii", "posted_at": "2024-04-18 18:16:32", "comments": 0, "source_title": null, "source_text": "LLM Costs per MTok\n\n# LLM Costs per MTok\n\nProvider| Model| Input ($/MTok)| Output ($/MTok)  \n---|---|---|---  \nReplicate / Meta| llama-3-8b| $0.05| $0.25  \nTogether / Meta| llama-3-8b| $0.2| $0.2  \nAnthropic| claude-3 haiku| $0.25| $1.25  \nOpenAI| gpt-3.5-turbo| $0.5| $1.5  \nTogether / Mistral| mixtral-8x7b| $0.6| $0.6  \nAnthropic| claude-2-instant| $0.8| $2.4  \nTogether / Meta| llama-3-70b| $0.9| $0.9  \nOpenAI| finetuned gpt-3.5-turbo| $3| $6  \nAnthropic| claude-3 sonnet| $3| $15  \nAnthropic| claude-2| $8| $24  \nOpenAI| gpt-4-turbo| $10| $30  \nAnthropic| claude-3 opus| $15| $75  \nOpenAI| gpt-4-8k| $30| $60  \nOpenAI| gpt-4-32k| $60| $120  \n  \nLast updated: Apr 18 2024\n\n###\n\nFAQ\n\nQ: What is a MTok?\n\nA: 1 million tokens\n\nQ: Are tokens the same across providers? Does this comparison make sense?\n\nA: No, tokens are different, but they should be close enough.\n\n  * Comparing Anthropic vs OpenAI, Anthropic has no public tokenizer, but claims their tokens are ~3.5 English chars per token. OpenAI claims their tokens are ~4 English chars per token. So perhaps Anthropic prices in the above table are an underestimate, and should be increased by 14%.\n\n  * Comparing Mixtral vs OpenAI - the Mixtral vocabulary size is 32000 vs OpenAI\u2019s 100,256. This might imply that Mixtral costs too are underestimated, but I\u2019m not entirely sure how big the difference is.\n\n", "frontpage": false}
