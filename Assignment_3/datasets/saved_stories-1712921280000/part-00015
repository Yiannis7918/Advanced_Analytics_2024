{"aid": "40008458", "title": "An Introduction to Flow Matching", "url": "https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html", "domain": "cam.ac.uk", "votes": 15, "user": "sebg", "posted_at": "2024-04-12 01:04:06", "comments": 0, "source_title": "An introduction to Flow Matching \u00b7 Cambridge MLG Blog", "source_text": "An introduction to Flow Matching \u00b7 Cambridge MLG Blog\n\nCambridge MLG\n\n#\n\nAn introduction to Flow Matching\n\ndiffusion model generative modelling normalising flows\n\nBy Tor Fjelde, Emile Mathieu, Vincent Dutordoir\n\n#\n\nTable of contents\n\n  1. Introduction\n  2. Normalising Flows\n\n    1. Learning flow parameters by maximum likelihood\n    2. Residual flow\n    3. Continuous time limit\n  3. Flow matching\n\n    1. Conditional Flows\n    2. Gaussian probability paths\n    3. But is CFM really all rainbows and unicorns?\n    4. Coupling\n  4. Quick Summary\n  5. Citation\n  6. Acknowledgments\n  7. References\n\n#\n\nIntroduction\n\nFlow matching (FM) is a recent generative modelling paradigm which has rapidly\nbeen gaining popularity in the deep probabilistic ML community. Flow matching\ncombines aspects from Continuous Normalising Flows (CNFs) and Diffusion Models\n(DMs), alleviating key issues both methods have. In this blogpost we\u2019ll cover\nthe main ideas and unique properties of FM models starting from the basics.\n\n##\n\nGenerative Modelling\n\nLet\u2019s assume we have data samples from a distribution of interest , which\ndensity is unknown. We\u2019re interested in using these samples to learn a\nprobabilistic model approximating . In particular, we want efficient\ngeneration of new samples (approximately ) distributed from . This task is\nreferred to as generative modelling.\n\nThe advancement in generative modelling methods over the past decade has been\nnothing short of revolutionary. In 2012, Restricted Boltzmann Machines, then\nthe leading generative model, were just about able to generate MNIST digits.\nToday, state-of-the-art methods are capable of generating high-quality images,\naudio and language, as well as model complex biological and physical systems.\nUnsurprisingly, these methods are now venturing into video generation.\n\nFigure 1: Protein generated by RFDiffusion (Watson et al., 2023).\n\nFigure 2: Image from DALL-E 3 (Betker et al., 2023).\n\n##\n\nOutline\n\nFlow Matching (FM) models are in nature most closely related to (Continuous)\nNormalising Flows (CNFs). Therefore, we start this blogpost by briefly\nrecapping the core concepts behind CNFs. We then continue by discussing the\ndifficulties of CNFs and how FM models address them.\n\n#\n\nNormalising Flows\n\nLet be a continuously differentiable function which transforms elements of ,\nwith a continously differentiable inverse . Let be a density on and let be the\ndensity induced by the following sampling procedure\n\nwhich corresponds to transforming the samples of by the mapping . Using the\nchange-of-variable rule we can compute the density of as\n\nwhere the last equality can be seen from the fact that and a simple\napplication of the chain rule^1. The quantity is the Jacobian of the inverse\nmap. It is a matrix of size containing . Depending on the task at hand,\nevaluation of likelihood or sampling, the formulation in or is preferred\n(Friedman, 1987; Chen & Gopinath, 2000).\n\nTransforming a base distribution into another via a transformation is\ninteresting, yet its direct application in generative modelling is limited. In\ngenerative modelling, the aim is to approximate a distribution using only the\navailable samples. Therefore, this task requires the transformation to map\nsamples from a \u201csimple\u201d distribution, such as , to approximately the data\ndistribution. However, a straightforward linear transformation, as in the\nprevious example, is inadequate due to the highly non-Gaussian nature of the\ndata distribution. This brings us to a neural network as a flexible\ntransformation . The key task then becomes optimising the neural net\u2019s\nparameters .\n\n###\n\nLearning flow parameters by maximum likelihood\n\nLet\u2019s denote the induced parametric density by the flow as .\n\nA natural optimisation objective for learning the parameters is to consider\nmaximising the probability of the data under the model:\n\nParameterising as a deep neural network leads to several constraints:\n\n  * How do we enforce invertibility?\n  * How do we compute its inverse?\n  * How do we compute the jacobian efficiently?\n\nDesigning flows therefore requires trading-off expressivity (of the flow and\nthus of the probabilistic model) with the above mentioned considerations so\nthat the flow can be trained efficiently.\n\n###\n\nResidual flow\n\nIn particular, computing the determinant of the Jacobian is in general very\nexpensive (as it would require automatic differentation passes in the flow) so\nwe impose structure in ^2.\n\nFull-rank residual (Behrmann et al., 2019; Chen et al., 2010)\n\nExpressive flows relying on a residual connection have been proposed as an\ninteresting middle-ground between expressivity and efficient determinant\nestimation. They take the form:\n\nwhere unbiased estimate of the log likelihood can be obtained^3. As opposed to\nauto-regressive flows (Huang et al., 2018, Larochelle and Murray, 2011,\nPapamakarios et al., 2017), and low-rank residual normalising flows (Van Den\nBerg et al. 2018), the update in has full rank Jacobian, typically leading to\nmore expressive transformations.\n\nFigure 4: Jacobian structure of different normalising flows.\n\nWe can also compose such flows to get a new flow:\n\nThis can be a useful way to construct move expressive flow! The model\u2019s log-\nlikelihood is then given by summing each flow\u2019s contribution\n\nwith .\n\n###\n\nContinuous time limit\n\nAs mentioned previously, residual flows are transformations of the form for\nsome and Lipschitz residual connection . We can re-arrange this to get\n\nwhich is looking awfully similar to being a derivative. In fact, letting and\ntaking the limit under certain conditions^4, a composition of residual flows\nis given by an ordinary differential equation (ODE):\n\nwhere the flow of the ODE is defined such that\n\nThat is, maps initial condition to the ODE solution at time :\n\n#### Continuous change-in-variables\n\nOf course, this only defines the map ; for this to be a useful normalising\nflow, we still need to compute the log-abs-determinant of the Jacobian!\n\nAs it turns out, the density induced by (or equivalently ) can be computed via\nthe following equation^5\n\nThis statement on the time-evolution of is generally known as the Transport\nEquation. We refer to as the probability path induced by .\n\nComputing the total derivative (as also depends on ) in log-space yields^6\n\nresulting in the log density\n\nParameterising a vector field neural network therefore induces a parametric\nlog-density\n\nIn practice, to compute one can either solve both the time evolution of and\nits log density jointly\n\nor solve only for and then use quadrature methods to estimate .\n\nFeeding this (joint) vector field to an adaptive step-size ODE solver allows\nus to control both the error in the sample and the error in the .\n\nOne may legitimately wonder why should we bother with such time-continuous\nflows versus discrete residual flows. There are a couple of benefits:\n\n  1. CNFs can be seen as an automatic way of choosing the number of residual flows to use, which would otherwise be a hyperparameter we would have to tune. In the time-continuous setting, we can choose an error threshold and the adapative solver would give us a the discretisation step size , effectively yielding steps. Using an explicit first order solver, each step is of the form , akin to a residual flow, where the residual connection parameters are shared for each discretisation step, since is amortised over , instead of having a different for each layer.\n  2. In residual flows, during training we need to ensure that is Lipschitz; otherwise the resulting flow will not be invertible and thus not a valid normalising flow. With CNFs, we still require the vector field to be Lipschitz in , but we don\u2019t have to worry about exactly what this Lipschitz constant is, which is obviously much easier to satisfy and enforce in. the neural architecture.\n\nNow that you know why CNFs are cool, let\u2019s have a look at what such a flow\nwould be for a simple example.\n\n#### Training CNFs\n\nSimilarly to any flows, CNFs can be trained by maximum log-likelihood\n\nwhere the expectation is taken over the data distribution and is the\nparameteric distribution. This involves integrating the time-evolution of\nsamples and log-likelihood , both terms being a function of the parametric\nvector field . This requires\n\n  * \u26a0\ufe0f Expensive numerical ODE simulations at training time!\n  * \u26a0\ufe0f Estimators for the divergence to scale nicely with high dimension. ^7\n\nCNFs are very expressive as they parametrise a large class of flows, and\ntherefore of probability distribution. Yet training can be extremely slow due\nto the ODE integration at each iteration. One may wonder whether a\n\u2018simulation-free\u2019, i.e. not requiring any integration, training procedure\nexists for training these CNFs.\n\n#\n\nFlow matching\n\nAnd that is exactly where Flow Matching (FM) comes in!\n\nFlow matching is a simulation-free way to train CNF models where we directly\nformulate a regression objective w.r.t. the parametric vector field of the\nform\n\nIn the equation above, would be a vector field inducing a probability path (or\nbridge) interpolating the reference to , i.e.\n\nIn words: we\u2019re just performing regression on for all .\n\nOf course, this requires knowledge of a valid , and if we already have access\nto , there\u2019s no point in learning an approximation in the first place! But as\nwe will see in the next section, we can leverage this formulation to construct\na useful target for witout having to compute explicitly .\n\nThis is where Conditional Flow Matching (CMF) comes to the rescue.\n\n###\n\nConditional Flows\n\nFirst, let\u2019s remind ourselves that the transport equation relates a vector\nfield to (the time evolution of) a probability path\n\nthus constructing or is equivalent. One key idea (Lipman et al., 2023 and\nAlbergo & Vanden-Eijnden, 2022) is to express the probability path as a\nmarginal over a joint involving a latent variable : . The term being a\nconditional probability path, satisfying some boundary conditions at and so\nthat be a valid path interpolating between and . In addition, as opposed to\nthe marginal , the conditional could be available in closed-form.\n\nIn particular, as we have access to data samples , it sounds pretty reasonable\nto condition on , leading to the following marignal probabilithy path\n\nIn this setting, the conditional probability path need to satisfy the boundary\nconditions\n\nwith small, and for whatever reference we choose, typically something \u201csimple\u201d\nlike , as illustrated in the figure below.\n\nFigure 8: Two conditional flows for two univariate Gaussians.\n\nThe conditional probability path also satisfies the transport equation with\nthe conditional vector field :\n\nLipman et al. (2023) introduced the notion of Conditional Flow Matching (CFM)\nby noticing that this conditional vector field can express the marginal vector\nof interest via the conditional probability path as\n\nTo see why this the same the vector field as the one defined earlier, i.e. the\none generating the (marginal) pribability path , we need to show that the\nexpression above for the marginal vector field satisfies the transport\nequation\n\nWriting out the left-hand side, we have\n\nwhere in the we used and in the we used the expression of in .\n\nThe relation between , and their induced densities are illustrated in the\nFigure 9 below. And since and are solutions corresponding to the vector fields\nand with , Figure 9 is equivalent to Figure 10, but note the difference in the\nexpectation taken to go from compared to .\n\nFigure 9: Diagram illustrating the relation between the paths , , and their\ninduced marginal and conditional densities.\n\nFigure 10: Diagram illustrating the relation between the vector fields , , and\ntheir induced marginal and conditional densities.\n\nMoreover, equipped with the knowledge of , we can replace\n\nwhere , with an equivalent loss regressing the conditional vector field and\nmarginalising instead:\n\nThese losses are equivalent in the sense that\n\nwhich implies that we can use instead to train the parametric vector field .\nThe defer the full proof to the footnote^9, but show the key idea below. By\ndeveloping the squared norm in both losses, we can easily show that the\nsquared terms are equal or independent of . Let\u2019s develop inner product term\nfor and show that it is equal to the inner product of :\n\nwhere in the we used the expression of in .\n\nThe benefit of the CFM loss being that once we define the conditional\nprobability path , we can construct an unbiased Monte Carlo estimator of the\nobjective using samples from the data target !\n\nThis estimator can be efficiently computed as it involves an expectation over\nthe joint , of the conditional vector field both being available as opposed to\nthe marginal vector field which involves an expectation over the posterior .\n\nWe note that, as opposed to the log-likelihood maximisation loss of CNFs which\ndoes not put any preference over which vector field can be learned, the CFM\nloss does specify one via the choice of a conditional vector field, which will\nbe regressed by the neural vector field .\n\n###\n\nGaussian probability paths\n\nLet\u2019s now look at practical example of conditional vector field and the\ncorresponding probability path. Suppose we want conditional vector field which\ngenerates a path of Gaussians, i.e.\n\nfor some mean and standard deviation .\n\nOne conditional vector field inducing the above-defined conditional\nprobability path is given by the following expression:\n\nas shown in the proof below.\n\n###\n\nBut is CFM really all rainbows and unicorns?\n\nUnfortunately not, no. There are two issues arising from crossing conditional\npaths. We will explain this just after, but now we stress that this lead to\n\n  1. Non-straight marginal paths ODE hard to integrate slow sampling at inference.\n  2. Many possible for a noised high CFM loss variance slow training convergence.\n\nTo get a better understanding of what these two points above, let\u2019s revisit\nthe example once more. As we see in the figures below, realizations of the\nconditional vector field , i.e. sampling from the process\n\nresult in paths that are quite different from the marginal paths as\nillustrated in the figures below.\n\nFigure 16: Realizations of conditional paths from for two different with\nconditional vector field given by .\n\nFigure 17: Paths from to following the true marginal vector field . Paths are\nhighlighted by the sign of the 2nd vector component.\n\nIn particular, we can see that the marginal paths do not cross; this is indeed\njust the uniqueness property of ODE solutions. A realization of the\nconditional vector field also exhibits the \u201cnon-crossing paths\u201d property,\nsimilar to the marginal flows , however paths corresponding to different\nrealizations may intersect, as highlighted in the figure above.\n\nConsider two highlighted paths in the visualization of , with data samples and\n. When learning a parameterized vector field via stochastic gradient descent\n(SGD), we approximate the CFM loss as:\n\nwhere , , and . We compute the gradient with respect to for a gradient step.\n\nIn such a scenario, we\u2019re attempting to align with two different vector fields\nwhose corresponding paths are impossible under the marginal vector field that\nwe\u2019re trying to learn! This fact can lead to increased variance in the\ngradient estimate, and thus slower convergence.\n\nIn slightly more complex scenarios, the situation becomes even more striking.\nBelow we see a nice example from Liu et al. (2022) where our reference and\ntarget are two different mixture of Gaussians in 2D differing only by the sign\nof the mean in the x-component. Specifically,\n\nwhere we set , unless otherwise specified.\n\nFigure 18: Realizations of conditional paths following conditional vector\nfield from . Paths are highlighted by the sign of the 2nd vector component.\n\nFigure 19: Realizations of marginal paths following the marginal vector field\nfrom . Paths are highlighted by the sign of the 2nd vector component.\n\nHere we see that marginal paths (bottom figure) end up looking very different\nfrom the conditional paths (top figure). Indeed, at training time paths may\nintersect, whilst at sampling time they cannot (due to the uniqueness of the\nODE solution). As such we see on the bottom plot that some (marginal) paths\nare quite curved and would therefore require a greater number of\ndiscretisation steps from the ODE solver during inference.\n\nWe can also see how this leads to a significant variance of the CFM loss\nestimate for in the figure below. More generally, samples from the reference\ndistribution which are arbitrarily close to eachothers can be associated with\neither target modes, leading to high variance in the vector field regression\nloss.\n\nFigure 20: Realizations of conditional paths following the conditional vector\nfield for .\n\nFigure 21: Variance of conditional vector field over for both blue and red\ntrajectories for .\n\nAn intuitive solution would be to associate data samples with reference\nsamples which are close instead of some arbitrary pairing. We\u2019ll detail this\nidea next via the concept of couplings and optimal transport.\n\n###\n\nCoupling\n\nSo far we have constructed the vector field by conditioning and marginalising\nover data points . This is referred as a one-sided conditioning, where the\nprobability path is constructed by marginalising over :\n\ne.g. .\n\nFigure 22: One sided interpolation. Source: Figure (2) in Albergo & Vanden-\nEijnden (2022).\n\nYet, more generally, we can consider conditioning and marginalising over\nlatent variables , and minimising the following loss:\n\nAs suggested in Liu et al. (2023), Tong et al. (2023), Albergo & Vanden-\nEijnden (2022) and Pooladian et al. (2023) one can condition on both endpoints\nof the process, referred as two-sided conditioning. The marginal probability\npath is defined as:\n\nThe following boundary condition on : and is required so that the marginal has\nthe proper conditions and .\n\nFor instance, a deterministic linear interpolation gives and the simplest\nchoice regarding the coupling is the consider independent samples: .\n\nFigure 23: Two sided interpolation. Source: Figure (2) in Albergo & Vanden-\nEijnden (2022).\n\nOne main advantage being that this allows for non Gaussian reference\ndistribution . Choosing a standard normal as noise distribution we recover the\nsame one-sided conditional probability path as earlier:\n\n#### Optimal Transport (OT) coupling\n\nNow let\u2019s go back to the idea of not using an independent coupling (i.e.\npairing) but instead to correlate pairs with a joint . Tong et al. (2023) and\nPooladian et al. (2023) suggest using the optimal transport coupling\n\nwhich minimises the optimal transport (i.e. Wasserstein) cost (Monge, 1781,\nPeyr\u00e9 and Cuturi 2020). The OT coupling associates samples and such that the\ntotal distance is minimised.\n\nThis OT coupling is illustrated in the right hand side of the figure below,\nadapted from Tong et al. (2023). In contrast to the middle figure which an\nindependent coupling, the OT one does not have paths that cross. This leads to\nlower training variance and faster sampling^10.\n\nFigure 24: One-sided conditioning (Lipman et al., 2022)\n\nFigure 25: Two-sided conditioning (Tong et al., 2023)\n\nFigure 26: OT coupling (Tong et al., 2023)\n\nIn practice, we cannot compute the optimal coupling between and , as\nalgorithms solving this problem are only known for finite distributions. In\nfact, finding a map from to is the generative modelling problem that we are\ntrying to solve in the first place!\n\nTong et al. (2023) and Pooladian et al. (2023) propose to approximate the OT\ncoupling by computing such optimal coupling only over each mini-batch of data\nand noise samples, coined mini-batch OT (Fatras et al., 2020). This is\nscalable as for finite collection of samples the OT problem can be computed\nwith quadratic complexity via the Sinkhorn algorithm (Peyre and Cuturi, 2020).\nThis results in a joint distribution over \u201cinputs\u201d and \u201coutputs\u201d such that the\nexpected distance is (approximately) minimised. Finally, to construct a mini-\nbatch from this which we can subsequently use for training, we can either\ncompute the expectation wrt. by considering all pairs (in practice, this can\noften boil down to only needing to consider disjoint pairs^11) or sample a new\ncollection of training pairs with ^12.\n\nFor example, we can apply this to the example from before, which almost\ncompletely removes the crossing paths behaviour described earlier, as can be\nseen in the figure below.\n\nFigure 27: with uniformly sampled pairings (left) and with OT pairings\n(right).\n\nWe also observe similar behavior when applying this the more complex example ,\nas can be seen in the figure below.\n\nFigure 28: with uniformly sampled pairings (left) and with OT pairings\n(right).\n\nAll in all, making use of mini-batch OT seems to be a strict improvement over\nthe uniform sampling approach to constructing the mini-batch in the above\nexamples and has been shown to improve practical performance in a wide range\nof applications (Tong et al., 2023; Klein et al., 2023).\n\nIt\u2019s worth noting that in we only considered choosing the coupling such that\nwe minimize the expected squared Euclidean distance. This works well in the\nexamples and , but we could also replace squared Euclidean distance with some\nother distance metric when constructing the coupling . For example, if we were\nmodeling molecules using CNFs, it might also make sense to pick such that and\nare also rotationally aligned as is done in the work of Klein et al. (2023).\n\n#\n\nQuick Summary\n\nIn short, we\u2019ve shown that flow matching is an efficient approach to training\ncontinuous normalising flows (CNFs), by directly regressing over the vector\nfield instead of explicitly training by maximum likelihood. This is enabled by\nconstructing the target vector field as the marginalisation of simple\nconditional vector fields which (marginally) interpolate between the reference\nand data distribution, but crucially for which we can evaluate and integrate\nover time. A neural network parameterising the vector field can then be\ntrained by regressing over these conditional vector fields. Similarly to CNFs,\nsampled can be obtained at inference time by solving the ODE associated with\nthe neural vector field.\n\nIn this post we have not talked about diffusion (i.e. score based) models on\npurpose as they are not necessary for understanding flow matching. Yet these\nare deeply related and even exactly the same in some setting. We are planning\nto explore these connections, along with generalisations in a follow-up post!\n\n#\n\nCitation\n\nPlease cite us as:\n\n    \n    \n    @misc{mathieu2024flow, title = \"An Introduction to Flow Matching\", author = \"Fjelde, Tor and Mathieu, Emile and Dutordoir, Vincent\", journal = \"https://mlg.eng.cam.ac.uk/blog/\", year = \"2024\", month = \"January\", url = \"https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html\" }\n\n#\n\nAcknowledgments\n\nWe deeply thank Michael Albergo, Valentin Debortoli and James Thornton for\ngiving insightful feedback!\n\n#\n\nReferences\n\n  * Albergo, Michael S. & Vanden-Eijnden, Eric (2023) Building Normalizing Flows with Stochastic Interpolants.\n\n  * Behrmann, Jens and Grathwohl, Will and Chen, Ricky T. Q. and Duvenaud, David and Jacobsen, Joern-Henrik (2019). Invertible Residual Networks.\n\n  * Betker, James, Gabriel Goh, Li Jing, TimBrooks, Jianfeng Wang, Linjie Li, LongOuyang, JuntangZhuang, JoyceLee, YufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu, YunxinJiao and Aditya Ramesh (2023). Improving Image Generation with Better Captions.\n\n  * Chen & Gopinath (2000). Gaussianization.\n\n  * Chen & Lipman (2023). Riemannian Flow Matching on General Geometries.\n\n  * Chen, Ricky T. Q. and Behrmann, Jens and Duvenaud, David K and Jacobsen, Joern-Henrik (2019). Residual flows for invertible generative modeling.\n\n  * De Bortoli, Mathieu & Hutchinson et al. (2022). Riemannian Score-Based Generative Modelling.\n\n  * Dupont, Doucet & Teh (2019). Augmented Neural Odes.\n\n  * Friedman (1987). Exploratory projection pursuit.\n\n  * George Papamakarios, Theo Pavlakou, Iain Murray (2018). Masked Autoregressive Flow for Density Estimation.\n\n  * Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron (2018). Neural Autoregressive Flows.\n\n  * Klein, Kr\u00e4mer & No\u00e9 (2023). Equivariant Flow Matching.\n\n  * Lipman, Yaron and Chen, Ricky T. Q. and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt (2022). Flow Matching for Generative Modeling.\n\n  * Liu, Xingchao and Gong, Chengyue and Liu, Qiang (2022). Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow.\n\n  * Monge, Gaspard (1781). M\u00e9moire Sur La Th\u00e9orie Des D\u00e9blais et Des Remblais.\n\n  * Peyr\u00e9, Gabriel and Cuturi, Marco (2020). Computational Optimal Transport.\n\n  * Pooladian, Aram-Alexandre and {Ben-Hamu}, Heli and {Domingo-Enrich}, Carles and Amos, Brandon and Lipman, Yaron and Chen, Ricky T. Q. (2023). Multisample Flow Matching: Straightening Flows With Minibatch Couplings.\n\n  * Song, Sohl-Dickstein & Kingma et al. (2020). Score-Based Generative Modeling Through Stochastic Differential Equations.\n\n  * Tong, Alexander and Malkin, Nikolay and Fatras, Kilian and Atanackovic, Lazar and Zhang, Yanlei and Huguet, Guillaume and Wolf, Guy and Bengio, Yoshua (2023). Simulation-Free Schrodinger Bridges via Score and Flow Matching.\n\n  * Tong, Malkin & Huguet et al. (2023). Improving and Generalizing Flow-Based Generative Models With Minibatch Optimal Transport.\n\n  * Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana V{'a}zquez and Lauko, Anna and De Bortoli, Valentin and Mathieu, Emile and Ovchinnikov, Sergey and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David (2023). De Novo Design of Protein Structure and Function with RFdiffusion.\n\n  1. The property implies, by the chain rule, \u21a9\n\n  2. Autoregressive (Papamakarios et al., 2018; Huang et al., 2018) One strategy is to factor the flow\u2019s Jacobian to have a triangular structure by factorising the density as with each conditional being induced via a flow. Low rank residual (Van Den Berg et al., 2018) Another approach is to construct a flow via a residual connection: with parameters , and . Leveraging Sylvester\u2019s determinant identity , the determinant computation can be reduced to one of a matrix which is advantageous if . \u21a9\n\n  3. A sufficient condition for to be invertible is for to be -Lipschitz [Behrmann et al., 2019]. The inverse can be approximated via fixed-point iteration (Chen et al., 2019). \u21a9\n\n  4. A sufficient condition for to be invertible is for to be Lipschitz and continuous by Picard\u2013Lindel\u00f6f theorem. \u21a9\n\n  5. The Fokker\u2013Planck equation gives the time evolution of the density induced by a stochastic process. For ODEs where the diffusion term is zero, one recovers the transport equation. \u21a9\n\n  6. Expanding the divergence in the transport equation we have: Yet since also depends on , to get the total derivative we have Where the last step comes from . Hence, \u21a9\n\n  7. The Skilling-Hutchinson trace estimator is given by with isotropic and centred. In our setting we are interested in which can be approximated with a Monte-Carlo estimator, where the integrand is computed via automatic forward or backward differentiation. \u21a9\n\n  8. The top row is with reference and target , and the bottom row is the example. The left column shows the straight-line solutions for the marginals and the right column shows the marginal solutions induced by considering the straight-line conditional interpolants. \u21a9\n\n  9. Developing the square in both losses we get: and Taking the expectation over the last inner product term: Then we see that the neural network squared norm terms are equal since: \u21a9\n\n  10. Dynamic optimal transport [Benamou and Brenier, 2000] \u21a9\n\n  11. In mini-batch OT, we only work with the empirical distributions over and , i.e. they all have weights , where is the size of the mini-batch. This means that we can find a matching the in by solving what\u2019s referred to as a linear assignment problem. This results in a sparse matrix with exactly entries, each then with a weight of . In such a scenario, computing the expectation over the joint , which has entries but in this case only non-zero entries, can be done by only considering training pairs where every is involved in exactly one pair and similarly for every . This is usally what\u2019s done in practice. When solving the assignment problem is too computationally intensive, using Sinkhorn and a sampling from the coupling might be the preferable approach. \u21a9\n\n  12. Note the size of the resulting mini-batch sampled from does not necessarily have to be of the same size as the mini-batch size used to construct the mini-batch OT approximation as we can sample from with replacement, but using the same size is typically done in practice, e.g. Tong et al. (2023). \u21a9\n\nPublished on 20 January 2024.\n\nThis is the blog of the Cambridge Machine Learning Group. Follow us on Twitter\n@CambridgeMLG and check out our GitHub cambridge-mlg.\n\n\u00a9 Cambridge Machine Learning Group. Design by wesselb.github.io. Powered by\nJekyll. Icons by Icons8. Feed.\n\n", "frontpage": true}
