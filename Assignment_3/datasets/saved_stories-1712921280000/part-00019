{"aid": "40008520", "title": "How I Got into Deep Learning", "url": "https://www.vikas.sh/post/how-i-got-into-deep-learning", "domain": "vikas.sh", "votes": 5, "user": "sebg", "posted_at": "2024-04-12 01:12:57", "comments": 0, "source_title": "How I got into deep learning", "source_text": "How I got into deep learning - Vikas Paruchuri\n\n# How I got into deep learning\n\nApril 11, 2024 \u2022 8 min read\n\nI ran an education company, Dataquest, for 8 years. Last year, I got the itch\nto start building again. Deep learning was always interesting to me, but I\nknew very little about it. I set out to fix that problem.\n\nSince then, I\u2019ve trained dozens of models (several state of the art for open\nsource), built 2 libraries that have 5k+ Github stars, and recently accepted\nan offer from answer.ai, a research lab started by Jeremy Howard.\n\nI say this to establish the very rough outline of my learning journey. In this\npost, I\u2019m going to cover more detail about how I learned deep learning.\nHopefully it helps on your journey.\n\n## My background\n\nI didn\u2019t learn this stuff in school. I majored in American History for\nundergrad, and failed quite a few classes.\n\nI did machine learning and Python work in 2012, but convinced myself that deep\nlearning was too complicated for me. One reason for this was because I learned\nby doing Kaggle competitions. Kaggle competitions are amazing for learning\nquickly, but can leave you with gaps in the fundamentals - like how algorithms\nwork mathematically.\n\nWhen deep learning started to become popular, it was very math-heavy, and I\nfelt like I\u2019d never be able to understand it. Of course, this was false, as I\nproved to myself 10 years later, but the angle at which you approach something\nmakes all the difference. I approached deep learning top-down the first time,\nby gluing models together without understanding how they worked. I eventually\nhit a wall, and couldn\u2019t get past it.\n\n### Useful skills\n\nWhen I studied deep learning last year, I already had useful skills. The first\nwas strong Python programming ability. Despite efforts to the contrary, Python\nis still the universal language of AI. If you want to get into AI, start by\ngetting really good at programming.\n\nNo matter what era of AI I\u2019ve been in, data cleaning has been >70% of my work.\nIt\u2019s possible if you\u2019re doing pure research or working on toy problems you can\navoid working with data, but otherwise data skills are essential.\n\nThere\u2019s a slightly more nebulous skill I\u2019ll call pragmatism. Deep learning has\na lot of rabbit holes - ranging from \u201cwhat\u2019s the perfect base model?\u201d, to\n\u201cwhat if I get rid of the sigmoid here?\u201d Some of these rabbit holes are\nuseful, but most of them will eat a lot of time. Being able to recognize when\nto go deep, and when to just do the fast/easy solution is important.\n\n## Book learning\n\nThis time, I decided to learn bottom-up, fundamentals first. I read The Deep\nLearning Book. It\u2019s a few years old, but still a fantastic resource. Read it\nslowly. A lot of the terminology and math will be unfamiliar - look them up.\nYou may need to sketch some things out or code them to get them - give\nyourself the space to do that. If the math is unfamiliar, a good complementary\nresource is Math for Machine Learning. Although I haven\u2019t taken them, fast.ai\nand the Karpathy videos are high quality.\n\nEven though architectures like CNN or RNN might seem out of date in a world\nthat is moving towards transformers for everything, CNNs are still widely\nused, and everything old is new again with RNNs.\n\nWhen you\u2019re done with the first 2 parts of the book (you can skip part 3), you\nshould be at a point where you can code up any of the main neural networks\narchitectures in plain numpy (forward and backward passes).\n\nOne thing that will really help you get to that point is teaching the skills\nwhile you learn them. I started putting together a course, Zero to GPT, as I\nread the deep learning book. Teaching is the ultimate way to solidify concepts\nin your head, and I found myself falling into a nice cycle of learning,\nlooking up/sketching what I didn\u2019t understand, then teaching it.\n\n### Papers\n\nThe book will take you up to 2015-era deep learning. After reading the book, I\nread some of the foundational deep learning papers from the 2015-2022 era and\nimplemented them in PyTorch. You can use Google Colab for free/cheap GPUs, and\nWeights and Biases to track your training runs.\n\nA noncomprehensive list is:\n\n  * RNN attention\n  * Transformers\n  * Switch transformer\n  * LoRA\n  * Vision Transformer\n  * AdamW\n  * GPT-2\n\nAfter this, you should be able to understand most conversations people have\nabout deep learning model architectures.\n\n## Fine-tuning and Discord\n\nThe easiest entrypoint for training models these days is finetuning a base\nmodel. Huggingface transformers is great for finetuning because it implements\na lot of models already, and uses PyTorch.\n\nThere are Discord communities, like Nous Research and EleutherAI where people\ndiscuss the latest models and papers. I\u2019d recommend joining them, seeing\nwhat\u2019s state of the art at the moment, and trying some finetuning.\n\nThe easiest way to finetune is to pick a small model (7B or fewer params), and\ntry finetuning with LoRA. You can use Google Colab, or something like Lambda\nLabs if you need more VRAM or multiple GPUs.\n\nI wanted to train models to code better, so I put together datasets and\nfinetuned a few different base models on data from StackOverflow and other\nplaces. It really helped me understand the linkage between model architecture,\ndata, compute, and output. However, finetuning is a very crowded space, and\nit\u2019s hard to make an impact when the state of the art changes every day.\n\n## Problem Discovery\n\nAs I was working on finetuning, I realized that some of the highest quality\ndata was in textbook form, and locked away in pdfs. One way I tried to solve\nthis was to generate synthetic data.\n\nAnother way was to extract the data from pdfs and turn it into good training\ndata (markdown). There was an approach called nougat that worked well in many\ncases, but was slow and expensive to run. I decided to see if I could build\nsomething better by leveraging the data already in the pdf (avoiding OCR), and\nonly using models when needed. I chained together several different models,\nalong with heuristics in between. This approach, marker, is 10x faster than\nnougat, works with any language, and is usually more accurate.\n\nWorking on marker led me to want to solve several more problems, and I\u2019ve also\ntrained an equation to LaTeX model, a text detection model, an OCR model\nthat\u2019s competitive with Google Cloud, and a layout model.\n\nFor all of these models, I took existing architectures, changed the layers,\nloss, and other elements, then generated/found the right datasets. For\nexample, for the OCR model, I started with the Donut architecture, added GQA,\nan MoE layer, UTF-16 decoding (1-2 tokens for any character), and changed some\nof the model shapes.\n\nSince OCR models are typically small (less than 300M params), I was able to\ntrain all of these models on 4x A6000s. I probably could have gotten away with\n2x A6000s if I was a bit more efficient.\n\nHopefully this illustrates 3 things for you:\n\n  * Understanding the fundamentals is important to training good models\n  * Finding interesting problems to solve is the best way to make an impact with what you build\n  * You don\u2019t need a lot of GPUs\n\nThere are many niches in AI where you can make a big impact, even as a\nrelative outsider.\n\n### Open source\n\nAs you may have noticed, I open source all of my AI projects. The data stack\nis a very underinvested area of AI relative to impact. I feel strongly that\nthe more widely you can distribute high quality training data, the lower the\nrisk of 1-2 organizations having a monopoly on good models.\n\nOpen source also has a side effect of being a good way to get exposure. Which\nleads me to the last part of my story.\n\n## Getting a research job\n\nI was thinking about building a business around my open source tools. Working\nsomewhere wasn\u2019t on my radar at all. But when Jeremy reached out about\nanswer.ai, I felt like it was an opportunity I had to take. The chance to work\nwith talented people, make a positive impact, and learn a lot is hard to pass\nup.\n\nMy open source work directly led to the job opportunity, both in the obvious\nway (it gave me exposure), and in a subtler way (it significantly improved my\nskills). Hopefully, you\u2019ll open source something as you learn, too.\n\n## Next steps\n\nI suspect my work at answer.ai will look very similar to my open source work.\nI\u2019ll keep training models, improving the data stack, and releasing publicly.\n\nIf you\u2019re trying to break into deep learning, I hope this post was useful. If\nnot, I hope it was somewhat entertaining (you made it to the end, so it\nprobably was, right?).\n\nAs for me, I\u2019m going back to training some models (watching 2 of them converge\nright now).\n\nI\u2019m Vikas, a developer based in Oakland. I'm a lifelong learner currently\ndiving deep into AI. I've started an education company, won ML competitions,\nand served as a US diplomat.\n\n  * My background\n  * Useful skills\n  * Book learning\n  * Papers\n  * Fine-tuning and Discord\n  * Problem Discovery\n  * Open source\n  * Getting a research job\n  * Next steps\n\n", "frontpage": true}
