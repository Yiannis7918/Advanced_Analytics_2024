{"aid": "40012739", "title": "From Words to Numbers: Your LLM Is a Capable Regressor", "url": "https://huggingface.co/papers/2404.07544", "domain": "huggingface.co", "votes": 1, "user": "Anon84", "posted_at": "2024-04-12 13:40:34", "comments": 0, "source_title": "Paper page - From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples", "source_text": "Paper page - From Words to Numbers: Your Large Language Model Is Secretly A\nCapable Regressor When Given In-Context Examples\n\nHugging Face\n\nPapers\n\narxiv:2404.07544\n\n# From Words to Numbers: Your Large Language Model Is Secretly A Capable\nRegressor When Given In-Context Examples\n\nPublished on Apr 11\n\n\u00b7 Featured in Daily Papers on Apr 12\n\nUpvote\n\n8\n\nAuthors:\n\n,\n\n,\n\n,\n\n## Abstract\n\nWe analyze how well pre-trained large language models (e.g., Llama2, GPT-4,\nClaude 3, etc) can do linear and non-linear regression when given in-context\nexamples, without any additional training or gradient updates. Our findings\nreveal that several large language models (e.g., GPT-4, Claude 3) are able to\nperform regression tasks with a performance rivaling (or even outperforming)\nthat of traditional supervised methods such as Random Forest, Bagging, or\nGradient Boosting. For example, on the challenging Friedman #2 regression\ndataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM,\nRandom Forest, KNN, or Gradient Boosting. We then investigate how well the\nperformance of large language models scales with the number of in-context\nexemplars. We borrow from the notion of regret from online learning and\nempirically show that LLMs are capable of obtaining a sub-linear regret.\n\nView arXiv page View PDF Add to collection\n\n### Community\n\n\u00b7 Sign up or log in to comment\n\nUpvote\n\n8\n\n## Models citing this paper 0\n\nNo model linking this paper\n\nCite arxiv.org/abs/2404.07544 in a model README.md to link it from this page.\n\n## Datasets citing this paper 0\n\nNo dataset linking this paper\n\nCite arxiv.org/abs/2404.07544 in a dataset README.md to link it from this\npage.\n\n### Spaces citing this paper 0\n\nNo Space linking this paper\n\nCite arxiv.org/abs/2404.07544 in a Space README.md to link it from this page.\n\n## Collections including this paper 1\n\n", "frontpage": false}
