{"aid": "40084465", "title": "RDSA: An intelligent tool for data science assignments", "url": "https://link.springer.com/article/10.1007/s11042-022-14053-x", "domain": "springer.com", "votes": 2, "user": "teleforce", "posted_at": "2024-04-19 08:18:34", "comments": 0, "source_title": "rDSA : an intelligent tool for data science assignments - Multimedia Tools and Applications", "source_text": "rDSA : an intelligent tool for data science assignments | Multimedia Tools and Applications\n\nLoading [MathJax]/jax/output/HTML-CSS/config.js\n\nSkip to main content\n\nLog in\n\n## Search\n\n## Navigation\n\n  * Find a journal\n  * Publish with us\n  * Track your research\n\n# rDSA : an intelligent tool for data science assignments\n\n  * 1224: New Frontiers in Multimedia-based and Multimodal HCI\n  * Open access\n  * Published: 05 November 2022\n\n  * Volume 82, pages 12879\u201312905, (2023)\n  * Cite this article\n\nDownload PDF\n\nYou have full access to this open access article\n\nMultimedia Tools and Applications Aims and scope Submit manuscript\n\nrDSA : an intelligent tool for data science assignments\n\nDownload PDF\n\n  * Pierpaolo Vittorini ORCID: orcid.org/0000-0002-6975-8958^1 &\n  * Alessandra Galassi^1\n\n  * 941 Accesses\n\n  * 2 Citations\n\n  * 1 Altmetric\n\n  * Explore all metrics\n\n## Abstract\n\nTools supporting the teaching and learning of programming may help professors\nin correcting assignments and students in receiving immediate feedback, thus\nimproving the solution before the final submission. This paper describes the\nrDSA tool, which was designed, developed, and evaluated to support students in\ncompleting assignments concerning (i) the execution of statistical analyses in\nthe R language and (ii) commenting on the results in natural language. The\npaper focuses on the feedback provided by the tool to students and how it was\ndesigned/evaluated/improved over the years. The paper also presents the\nresults of two studies that indicate the advantages of using the tool in terms\nof engagement and learning outcomes. To conclude, we provide a discussion on\nthe characteristics of the tool, its use in similar courses, and the scope for\nfuture work.\n\n### Similar content being viewed by others\n\n### The Promises and Challenges of Artificial Intelligence for Teachers: a\nSystematic Review of Research\n\nArticle Open access 25 March 2022\n\nIsmail Celik, Muhterem Dindar, ... Sanna J\u00e4rvel\u00e4\n\n### Word problems in mathematics education: a survey\n\nArticle 13 January 2020\n\nLieven Verschaffel, Stanislaw Schukajlow, ... Wim Van Dooren\n\n### Systematic Reviews in Educational Research: Methodology, Perspectives and\nApplication\n\nChapter \u00a9 2020\n\nUse our pre-submission checklist\n\nAvoid common mistakes on your manuscript.\n\n## 1 Introduction\n\nVarious tools that support professors and students in the teaching and\nlearning of programming have been developed since the 1960s [15, 22]. Such\ntools may provide simplified development environments, use visualisations or\nanimations to provide better insight into running a program, automatically\ngrade student solutions, and guide students towards the correct solution\nthrough hints and feedback messages [24]. The manual correction of solutions\nis a complex, crucial, tedious, and error-prone task and the problem\nparticularly aggravates when such an assessment involves many students. By\nautomating the grading process, we assist teachers in making corrections,\nmaking it possible for students to receive immediate feedback, and improving\non their solutions before submission.\n\nIn our previous research, we approached the problem of automated grading of\nassignments made up of a list of commands, their respective output, and\ncomments (written in natural language) on the meaning of the output. We\ninitially introduced a distance, expressed in terms of grade, between the\ncorrect solution (given by a professor) and the solution given by a student.\nThen, we focused on specific data science assignments, whose solutions\nrequired commands written in R language [10], the respective output and\ncomments written in the Italian language. We then developed the rDSA (r Data\nScience Assignments) tool within the UnivAQ Test Suite (UTS) system^Footnote\n1, which implemented the aforementioned distance for this kind of assignments\n[4, 7, 12, 18, 45].\n\nMore recently, we focused our attention on the feedback that the rDSA tool\nreturns to students after automated grading. As known, feedback is a crucial\nelement in learning [21, 30], and can be defined as \u201cthe process in which\nlearners obtain information about their work [...], the qualities of the work\nitself, to generate improved work\u201d [8]. Feedback is useful for both formative\nand summative assessments, i.e., in the execution of a course to verify the\nlearning progress (by the teacher or by the students themselves), and at the\nend of the course to determine the learning outcomes [20]. Within a design-\nbased research approach [6], the paper presents the three\ndesign/evaluation/improvement cycles about the feedback returned to students\nand reports on the possible improvements in the learning outcomes.\n\nTo present the research, we structured the paper as follows. Section 2\ndiscusses the application scenario and foreseen educational impact. Section 3\nsummarises the related work. Section 4 describes the rDSA tool development and\nevaluation, in terms of three iterations and the respective results. Section 5\ndiscusses the overarching findings of the paper and compares the results with\npreviously published work. Section 6 concludes the paper by summarising the\nmain results and presenting future work.\n\nThis study extends the results reported in [17] as follows. First, we analysed\nthe state-of-the-art in more detail by providing more evidence on the\neffectiveness of the feedback during the learning process. Second, we\npresented all our contributions in terms of the three iterations that led to\nthe current design and implementation of the rDSA tool. Third, we added a\nfurther evaluation cycle (i.e., online interviews), the novel implementation\nof the tool, and the learning outcomes.\n\n## 2 Application scenario and educational impact\n\nHealth Informatics as a course in Medicine and Surgery, as well as Information\nSystems in Nursing Sciences and Prevention Sciences in the University of\nL\u2019Aquila (Italy), have a specific topic on how to execute statistical analyses\nin R and how to correctly interpret the results into the corresponding\nclinical findings. The related exercises and the final exam have the same\nstructure: they start with the definition of the dataset and then list the\nanalyses and technical/clinical interpretations that should be performed. The\nanalyses must be executed through R commands and can be both descriptive\n(e.g., mean, sd), or inferential (e.g., t.test, wilcox.test), and test\nnormality (e.g., shapiro.test). To interpret the results, students must\nunderstand, for example, if a test on normality suggests that the distribution\nshould be normal or not, or if a test on a hypothesis is statistically\nsignificant.\n\nFor instance, see Fig. 1. The assignment regards a simple experiment about the\neffects of an antihypertensive drug. Let us consider the fourth point in the\nassignment. Since the systolic blood pressure was quantitative and the same\npatients were measured before and after the treatment, the student (after a\nnormality test) should use a paired t-test. Such a test is executed in R using\nthe command\n\nFig. 1\n\nA sample exercise\n\nFull size image\n\nThe p-value (see line 4 of the output) is 0.01274, less than 0.05. The student\nshould then conclude that the difference in systolic blood pressure is\nstatistically significant and therefore it should be caused by the effect of\nthe drug. This conclusion solves point 5 of the assignment.\n\nThe experience gained during the years in correcting such assignments enabled\nthe authors to identify the most common mistakes, which are of varying types\nand are sometimes difficult to spot. Given this educational scenario, we began\nthe design and implementation of an automated grading tool that supports both\nformative and summative assessment activities as follows. As a formative\nassessment instrument, it can provide students with both textual feedback and\nan estimated evaluation of the quality of the submitted solution, and it\nenables teachers to monitor students\u2019 progress with their homework. As a\nsummative assessment instrument, the tool can be used by the teacher to\nshorten and improve manual correction activities. Accordingly, this may cause\nseveral expected educational benefits. For students, these include the ability\nto support their understanding of the commands and interpretation of the\nresults. Students should be able to improve their final learning outcomes.\nProfessors should expect fewer returning students. Moreover, in [45] we\nalready showed the tool reduced professors\u2019 workload in terms of both\nevaluation time and errors.\n\nAppendix A summarises the most relevant considerations about the didactic\norganisation of Italian courses in general and the specific courses examined\nin this work (e.g., degree system, exam sessions), which are useful for\nreaders unfamiliar with the Italian Academic system.\n\n## 3 Related work\n\nWe structured the related work as follows. First, we discussed the specific\nproblem in the automatic grading of short-text answers (in our context, the\ncomments given to interpret the commands). Then, we discussed the literature\non automated assessment systems (AASs) and intelligent tutoring systems\n(ITSs), with specific regard to the system focusing on programming assignments\n(in our context, the analysis of the commands and output, as well as the\nreturned feedback). Then, we summarised our research on the effectiveness of\ndifferent releases of automated feedback. Lastly, we summarised previous work\nfrom the researchers in this field.\n\n### 3.1 Automated grading of short-text answers\n\nSeveral solutions have been proposed to perform automated grading of short-\ntext answers [9]. In such a context, the common task is to assign either a\nreal-valued score (e.g., from \u201c0\u201d to \u201c1\u201d) or to assign a label (e.g.,\n\u201ccorrect\u201d or \u201cirrelevant\u201d) to a student response. The approaches are manifold.\nSome rely on knowledge bases and syntactic analyses [31], which are available\nonly for a limited set of languages. More recent studies have exploited the\npotential of vector-based similarity metrics to compare students\u2019 answers\nagainst a gold standard [39, 45]. Such features were further explored in [29]\nto include several corpus-based, knowledge-based, word-based, and vector-based\nsimilarity indices. Recently, few studies have been conducted using\ntransformer-based architectures [14] and neural network classifiers to perform\nshort-answer grading [2, 28, 40]. Although neural approaches have demonstrated\nacceptable performance and generalisation capabilities across domains and\nlanguages, they typically require large amounts of data to learn an accurate\nclassification model.\n\n### 3.2 AASs and ITSs\n\nNowadays, the available learning systems have different capabilities,\nincluding the specific characteristics traditionally assigned to AASs and\nITSs. As known, AASs focus on assessing a student\u2019s ultimate solution to an\nexercise or exam, with a grade or feedback report, to ease instructors from\nmanually assessing many students. ITSs focus on helping students arrive at the\nsolution by offering help at each step.\n\nAutomated assessment of student programming assignments was first attempted in\nthe sixties [22], and an extensive set of tools has been developed to date\n[25, 38]. Worth summarising is a literature review by Keuning et al. [25],\nwhere the authors reviewed over 100 AASs and ITSs in the STEM field^Footnote 2\nand categorised them in terms of (i) the nature of the feedback and (ii) which\ntechniques are used to generate the feedback. In what follows, we summarise\nthe most important results and introduce acronyms for all system\ncharacteristics to easily relate our tool and its features to them.\n\nAs for the nature of feedback, the reviewed tools offer either a simple or an\nelaborate one. Simple feedback (SF) is just a report on the achieved\nperformance level (e.g., 75% of correct answers), or feedback that\ncommunicates whether a solution is correct or not, or that provides a\ndescription of a correct solution. By contrast, the elaborated feedback (EF),\nprovides detailed support to students. It can provide hints on pre-\nrequirements or how to approach the exercise, explanations on subject matters\nand examples illustrating concepts, knowledge about mistakes, and how to\nproceed. With specific regard to the mistakes, the available systems report to\nthe student if the program does not produce the expected results (EF-ER), if\nit contains syntactic errors (EF-SY), if crashes (EF-CR), if they use\ndifferent algorithms that are not accepted (EF-AL), if has style (EF-ST), or\nperformance issues (EF-PE). In terms of supporting students on how to proceed,\nthe summarised tools give hints on what the student should do to correct a\nmistake (EF-CM), describe the next step a student must take to come closer to\na solution (EF-NS), and provide hints on how to improve an already correct\nsolution (EF-IS).\n\nDifferent techniques are also used to generate feedback. The general\napproaches are: (i) model tracing (TEC-MT); the tool analyses the process by\nwhich the student is solving a problem; (ii) constraint-based modelling (TEC-\nCBM); the tool only considers the current solution against pre-defined\nsolution constraints (e.g. the solution provided by the professor, the\npresence of a for-loop); (iii) tutoring based on data analysis (TEC-TDA); the\ntool generates hints by using extensive sets of past student solutions. The\ndomain-specific techniques, among the many, are: (i) static code analysis; the\nprogram is analysed without running it, to detect misunderstood concepts, the\nabsence or presence of certain code structures, and to give hints on fixing\nthese mistakes (TEC-SCA); and (ii) dynamic code analysis; running a program\nand comparing the output to the expected output (TEC-DCA).\n\nFocusing on the context of computer science education, many tools support\ndifferent language paradigms and provide elaborated feedback to enhance\nlearning [36]. For example, recent AASs/ITSs support functional programming\nsuch as learning procedural programming [19] and object-oriented languages,\nthat include Java [41] and C [33], as well as (web) scripting languages such\nas PHP, JavaScript, and Python [11, 26].\n\n### 3.3 Effectiveness of (automated) feedback\n\nFeedback plays an important role in the learning process because it helps\nstudents identify their strengths and weaknesses and target the areas that\nneed more work, encourage self-evaluation, and increase engagement [16, 34,\n43]. Feedback from automatic assessment of students\u2019 solutions is an important\naid for students\u2019 learning processes in self-study and distance learning, and\nhas a positive effect on learning outcomes. [21, 30].\n\nA recent meta-analysis of experiments on the effectiveness of human tutoring,\ncomputer tutoring, and no tutoring [44], reported the following: The author\ncategorised feedback by computer tutoring systems in terms of its detail\nlevel, that is: (i) answer-based, a single and unique feedback about the wrong\nanswer; (ii) step-based, a feedback provided for each step of the solution;\nand (iii) substep-based feedback, similar to the step-based case, but with a\nfiner-grained level of details. Accordingly, the author tested the hypothesis\nthat the effect size of human tutoring > substep-based > step-based > answer-\nbased > no tutoring. The effect size measures the extent of association\nbetween two variables (in our case, the different tutoring method and its\neffectiveness): the larger the effect size, the stronger the association\nbetween the two variables. With respect to no tutoring, the main conclusions\nwere: the effect size of human tutoring is d = 0.79 more effective and step-\nbased tutoring is almost as good as human tutoring (d = 0.76). The substep-\nbased tutoring, given that there are only a few statistically reliable studies\n\u2013 seems to have an effect size of 0.75 < d < 0.80, and answer-based tutoring\nis only d = 0.31 more effective.\n\n### 3.4 Previous work from the authors\n\nIn the last few years, we started the design, development, and testing of the\nUTS system, with the objective of providing a state-of-the-art system to\nsupport both formative and summative assessment in terms of: (i) classical\ntesting [13] such as quizzes with multiple-choice questions, (ii) computerised\nadaptive testing [46] such as sequence of multiple-choice questions that\n\u201cadapts\u201d to the student, and (iii) automated grading for both code snippets\nand open-ended answers [4, 7, 12, 45].\n\nIn regard to automated grading, as explained in the previous section, we\nfocused on assignments comprising commands, output, and short answers.\nStarting from the aforementioned literature, in [45], we introduced a general\napproach valid for assignments whose solutions are represented as a list of\ntriples containing the command, its output, and a possible comment. In the\nproposed approach, the solution provided by a student is compared to the\ncorrect solution provided by the professor. Then, we defined the distance\nbetween the two solutions, which represents the final grade: the larger the\ndistance, the lower the grade, and vice versa. The distance is based on the\nfollowing possibilities: a student issued (i) a correct command that returned\nthe correct output, (ii) a command that seems correct but returned an output\ndifferent from that of the professor, (iii) missed the command, or (iv)\ncorrectly or incorrectly interpreted the result of the analysis. This approach\nwas implemented in the rDSA tool, which focuses on the R language for the\nprogramming part and the Italian language for the open-ended answers. In\nparticular, the tool uses static source code analysis for the R code snippets,\na supervised neural classifier fed by sentence embedding and distance-based\nfeatures for the open-ended answers, and assembles all results in structured\nfeedback [2, 45]. It is worth remarking that the approach, besides its\ncontextualisation of the R language and the health setting, can be applied to\nany course that includes data science assignments, whose solutions are made up\nof a sequence of commands, the output, interleaved with comments written in\nnatural language that explain the results.\n\nNevertheless, the research reported in this paper focuses on the automated\nfeedback returned to students, how it was cyclically designed, implemented and\nevaluated, and which were the possible improvements in the learning outcomes.\n\n## 4 rDSA tool: development and evaluation\n\nThe objective of this section is to show the improvements of the rDSA tool,\nspecifically, the automatic feedback, in terms of three iterations (see Fig.\n2). The first iteration (Section 4.1) describes the initial feedback, its\nimplementation, and its evaluation through direct observations. The feedback\nwas then redesigned from the first implementation to provide complete, more\nstructured, and detailed messages. The second iteration (Section 4.2) reports\non the implementation of the improved design and on the three studies. The\nfirst two studies collected quantitative and qualitative data:\n\n  * through questionnaires, the first study measured: (i) the engagement, (ii) the usefulness, quality and relevance of the available exercises, (iii) the expectation/experience with the system, and (iv) the impact of the feedback in the learning process (Section 4.2.2);\n\n  * by means of structured interviews, the second study, collected useful suggestions \u2013 directly from students \u2013 on how to further improve the feedback (Section 4.2.3);\n\nFig. 2\n\nStructure of the iterative process\n\nFull size image\n\nThe third study focused on the learning outcomes of the students that used the\ninitial and the improved feedback (Section 4.2.4). The third iteration\n(Section 4.3) shows the current feedback and the system architecture. The\nimproved feedback was based on the suggestions and hints collected through the\nquestionnaires and interviews completed by the students during the second\niteration.\n\n### 4.1 First iteration\n\nThe first iteration is the initial step concerning the implementation and\nevaluation of the tool. The main results (Sections 4.1.1 and 4.1.2) were\nalready published in [4, 18], and are briefly summarised here to help the\nreaders understand all the improvements made to the rDSA tool.\n\n#### 4.1.1 First implementation\n\nThe first implementation of the rDSA tool [4] focuses on providing an\nautomated evaluation of the solutions to the assignments, in terms of the\nfinal grade and basic feedback. The tool was mostly an AAS, returning both a\nsimple feedback through final grade and an elaborate one with: (i) the correct\ncommands, marked in green with a \u201cCorrect\u201d statement; (ii) the commands that\nappeared correct but returned a wrong output, marked in blue with a \u201cThe\ncommand seems correct, but the output differs from the solution\u201d; (iii) the\nmissed commands, marked in red; and (iv) the student\u2019s comments in green or\nred (if right or wrong, respectively), with a commenting statement. For the\nadopted techniques, we used (i) constraint-based modelling and static code\nanalysis to analyse the commands and the related output, and (ii) the\nLevenshtein string similarity distance [27], divided by the length of the\nlongest string^Footnote 3.\n\nAs an example, let us consider the following solution to the exercise\ndiscussed in Fig. 1^Footnote 4:\n\nIn the solution, the student omitted the commands to solve point 4 (because\nhe/she did not issue the t.test command), executed correctly the mean s and\nonly one of the shapiro.test s (the second one returned a wrong p-value), and\ndid not give any interpretation of the tests. Fig. 3 shows the returned\nfeedback, structured as described above, with the estimated grade ranging from\n0 to 1.\n\nFig. 3\n\nStudent feedback \u2013 first implementation\n\nFull size image\n\n#### 4.1.2 First evaluation\n\nBy watching and listening to twelve students working with the tool, we\ncollected all the highlighted issues and classified them into two categories\n[18]: (i) technical improvements and (ii) feedback structure. Focusing on the\nfeedback, the students reported that the feedback was more technical than\ndidactic because it only reported that the command was right or wrong, and\nfailed to explain the error. Furthermore, the system automated grade leads to\nfalse pass/fail outcomes, especially when the grade is sufficiently close. To\naddress these issues, we redesigned the feedback to cover all the possible\ncases of mistakes, as detailed in the following section.\n\n#### 4.1.3 Improved feedback design\n\nThe improved feedback was then designed as follows:\n\n  1. 1.\n\nfor each command given by the student\n\n    1. 1.a.\n\nif the command and its output are equal to a certain command and output\ncontained in the correct solution, return a \u201cCorrect\u201d feedback;\n\n    2. 1.b.\n\nif the command is in the correct solution, but its output differs from the\noutput of the correct solution, we first return the generic message \u201cThe\ncommand seems correct, but the output differs from the right solution\u201d, then\nwe investigate the following two scenarios:\n\n      1. 1.b.1.\n\nthe student made a mistake in the command call, by checking if the student\nused:\n\n        1. 1.b.1.a.\n\na wrong number of parameters: we either return the sentence \u201cThe parameter ...\nseems missing\u201d, or the sentence \u201cThe parameter ... seems not needed\u201d;\n\n        2. 1.b.1.b.\n\na wrong variable: we return the sentence \u201cThe variable ... does not seem\ncorrect\u201d;\n\n        3. 1.b.1.c.\n\na wrong Boolean predicate for selecting a subset of rows: we return the\nsentence \u201cThe Boolean predicate ... does not seem correct\u201d.\n\nDepending on the case, a message is added to suggest solutions for the error\nto the student. For instance, in the case of a t.test without the expected\npaired=TRUE parameter, we add the message \u201cYou should have used a paired\ntest\u201d;\n\n      2. 1.b.2.\n\nif nothing above applies, we assume that the student incorrectly imported the\ndataset and the message \u201cPlease check if the data was imported correctly\u201d is\nreturned.\n\n    3. 1.c.\n\nthe command is not in the correct solution. In this case, we first return the\ngeneric message \u201cWrong command\u201d Then, we try to find in the correct solution a\n\u201csimilar\u201d command, i.e., an inappropriate choice of command to calculate the\ncentral tendency or dispersion (referring to descriptive statistics) or the\nhypothesis testing (referring to inferential statistics). Depending on the\ncase, a different message is returned. For example, if the student uses the\nmedian instead of the mean, we return the message \u201cAnother command to\ncalculate the central tendency is in the correct solution. Did you\nmisunderstand the question or variable type?\u201d.\n\n    4. 1.d.\n\nif the command requires a comment:\n\n      1. 1.d.1.\n\nif the comment is not present, we return the message \u201cNo comment was found\u201d;\n\n      2. 1.d.2.\n\nif the comment is present:\n\n        1. 1.d.2.a.\n\nif the comment is considered correct, we return the message \u201cThe\ninterpretation of the analysis seems correct\u201d;\n\n        2. 1.d.2.b.\n\nelse, we return the message \u201cThe interpretation of the analysis seems\nincorrect\u201d.\n\n  2. 2.\n\nall commands of the correct solution that were not identified in the previous\nanalysis, are listed as \u201cMissed commands\u201d.\n\n### 4.2 Second iteration\n\n#### 4.2.1 Second implementation\n\nThe second implementation of the tool yields several improvements from both\nthe technical and feedback viewpoints. We corrected the technical issues\nevident in the first evaluation and exploited novel methods for grading the\ncomments as right or wrong, moving from the Levenshtein string similarity\ndistance to supervised classification and sentence embedding [45]. For\nfeedback, we implemented the previous design. In terms of the adopted\ntechniques, together with constraint-based modelling and static code analysis\nfor analysing the commands and the related output, we use tutoring based on\ndata analysis to evaluate the comments.\n\nAs an example, let us consider the following solution to the exercise\ndiscussed in Fig. 1^Footnote 5:\n\nIn the solution, the student omitted commands to solve point 1 (i.e., he/she\ndid not issue the two required mean commands), executed correctly the\nshapiro.test, did not give an interpretation to the normality tests, forgot\nthe paired=TRUE option for the hypothesis testing, but correctly interprets\nthe (wrong) result.\n\nFigure 4 shows the new feedback. The tool recognised the two correct normality\ntests (first two green blocks), but was unable to find their interpretation\n(the subsequent red block). It then found the t.test, but \u2013 given that the\ncalculated p-value was different than that in the correct solution \u2013 the tool\ninspected the command call and found the missing paired=TRUE parameter. Hence,\nthe tool reported such a problem in terms of three lines that close the blue\nblock, the last two (highlighted in the figure) suggesting how to get to the\ncorrect solution. The tool automatically classified the comment given to the\nhypothesis testing as incorrect, as reported in the subsequent red block. In\nthe last block, the tool reported the missing commands. The feedback is then\ncompleted with an estimation of the final grade, this time using the Italian\ngrading system (i.e., from 0 to 30 cum laude).\n\nFig. 4\n\nStudent feedback \u2013 second implementation\n\nFull size image\n\n#### 4.2.2 Second evaluation (questionnaires)\n\n### Objective\n\nThe objective of the evaluation was to use standardised and ad-hoc\nquestionnaires to verify the following: (i) the system improved the student\nengagement; (ii) the feedback was useful in helping students understand how to\nsolve the exercises and deepen the grasp of the subject, how the students\nhandled the system in preparation for the exam, that is, how they read through\nthe exercises, or to check and submit the solution, or iteratively to refine\nthe solution before the final submission; and (iii) if students would like to\nuse similar tools in other subjects.\n\n### Materials and methods\n\nWe conducted a study using data collected from two different cohorts, made up\nof students of Medicine and Surgery course, from the 2019/20 and 2020/21\nacademic years (see Fig. 5) and used the first and second implementation of\nthe tool, respectively.\n\nFig. 5\n\nStudy design\n\nFull size image\n\nBoth cohorts compiled the User Engagement Assessment Scale (UEAS [3]) and a\nquestion containing a general opinion on the feedback (see Appendix B). The\n2020/21 cohort completed two further questionnaires. The first was structured\nto capture expectations and experiences on the following: (i) the usefulness\nof the feedback as a whole, (ii) the clarity of the explanations given for\nincorrect commands, (iii) clarity of the explanation given for the partially\nwrong commands, and (iv) usefulness in solving the exercise (see Appendix C).\nThe second contained questions on the impact of the feedback, how feedback was\nimplemented, and if they would recommend similar systems in similar subjects\n(see Appendix D).\n\nThe questionnaires were analysed:\n\n  * for UEAS, we scored the engagement as discussed in [3], then calculated the average engagement for both cohorts;\n\n  * as for expectation and experience questionnaire, we followed the approach proposed by Albert & Dixon [1]. We first calculated the mean of the expectations and experiences for each element. Then, we placed the results in a scatterplot (expectation on the x-axis, experience on the y-axis). As suggested in [1], elements in the top-right quadrant (i.e. good expectation and good experience) can be considered satisfactory; elements in the bottom-right quadrant such as good expectation and low experience need to be prioritised; elements in the top-left quadrants representing low expectation and good experience show a surprisingly good user experience; and elements in the bottom-left quadrant for low expectation and experience should be addressed as well, but with a lower priority.\n\n  * finally, all questions that required a Likert-scale answer were analysed through averages. For the multiple-choice questions, we used frequency tables.\n\nInferential analyses were performed using t-tests or Wilcoxon tests (depending\non the type of variable \u2013 qualitative or quantitative \u2013 and if normally\ndistributed or not), paired or not (in case of paired or independent samples)\n[35]. In the results, when reporting the p-value, we added an index that\nclarifies the adopted method, w for the Wilcoxon test, t for the t-test, and p\nfor the paired version.\n\n### Results\n\n40 students from the 2019/20 cohort and 16 students from the 2020/21 cohort\nanswered the UEAS questionnaire. We observed an increased engagement, from\n3.6/5 for the 2019/20 cohort, to 4.2/5 for the 2020/21 cohort, a statistically\nsignificant difference (p_w = 0.002).\n\nThe general opinion question investigated on usefulness, quality, and\nrelevance of the available exercises. It was answered by 26 students in the\n2019/20 cohort and 46 students in the 2020/21 cohort. In both cases, the\ngeneral opinion was rated as 4.7/5.\n\nA total of 63 students of the 2020/21 cohort answered the\nexpectation/experience questionnaire. Figure 6 summarises the analysis of all\nquestions, based on general usefulness (USEFULNESS), clarity of the feedback\nfor the completely wrong commands (CLARITY C.W.), clarity for the partially\nwrong commands (CLARITY P.W.), and the usefulness for solving the exercise\n(SOLVE). The USEFULNESS, CLARITY C.W., and SOLVE elements are in the top-right\nquadrant and considered satisfactory. However, CLARITY P.W. \u2013 even if\nborderline \u2013 did not meet the expectations (p_wp = 0.00043).\n\nFig. 6\n\nResult summary of the expectation/experience questionnaire administered to the\n2020/21 cohort\n\nFull size image\n\nFinally, the answers on the impact of the feedback showed that the system\u2019s\nautomatic feedback was useful for students to understand their errors (30\nstudents), to understand the correct statistical method to solve the problem\n(37 students), and to verify the preparation for the final exam (36 students).\nMost of the students used the tool iteratively to improve their solutions (48\nstudents). Few used the tool only before submitting the solution (12 students)\nor just to see the exercises (two students). Finally, students suggest using\nsimilar tools with a rate of 4.7/5.\n\nAccording to the results, usefulness in general, clarity of the explanation\nfor the completely wrong commands, and usefulness for solving the exercise had\nvery positive expectations and experiences. On the other hand, the explanation\nfor \u201cthe partially wrong commands\u201d had an unsatisfactory experience. Improving\nthis feedback element became a priority in our research: we therefore planned\nand conducted a further evaluation (Section 4.2.3), focusing on the\nexplanatory feedback for partially wrong commands.\n\n#### 4.2.3 Second evaluation (interviews)\n\n### Objective\n\nThe objective of this evaluation was to collect suggestions from students on\nhow to improve the explanatory feedback, especially for the partially wrong\ncommands.\n\n### Materials and methods\n\nTo collect the suggestions from students, we used semi-structured interviews.\n\nInterviews had different levels of structure. More structured interviews\nfollowed a strict and well-defined sequence of questions, whereas the\nunstructured interviews were more conversational, without following a fixed\nscheme. Structured interviews are usually easier to conduct and collect more\nconsistent data than unstructured ones. On the other hand, unstructured\ninterviews \u2013 at the expense of being more challenging and gathering less\nconsistent data \u2013 may elicit suggestions or ideas coming from the interviewee\nthat the interviewer may not have thought of.\n\nWe conducted our interviews online (due to the COVID-19 outbreak), as follows\n(see Appendix E):\n\n  * the interview was conducted with a conversational tone, one student at a time;\n\n  * before starting the interview, we asked for consent to record the interview and take pictures;\n\n  * during the interview we showed slides by sharing our computer screen;\n\n  * we structured our interview based on these seven elements; six identified cases of mistakes, and an \u201cunknown\u201d case;\n\n  * for each case, we first explained the case with an example (except for the \u201cunknown\u201d case), then we showed the feedback that the tool would have returned; hence, we asked:\n\n    * a close-ended question asking if the returned feedback was clear or needed improvements;\n\n    * an open-ended question asking how to improve the feedback.\n\nThe methods for the analysis of the open-ended questions were both narrative;\nmaking sense of the individual answers and thematic content analysis;\nidentifying common themes between the different interviews.\n\n### Results\n\nSix students volunteered for interviews. The results were as follows:\n\n  1. 1.\n\nin case of a command given by a student, but is not explicitly required for\nthe exercise and it is not a mistake, then the system should: (i) highlight it\nin orange, rather than in red (which is used for completely wrong commands),\n(ii) not consider it in the estimation of the final grade;\n\n  2. 2.\n\nfew students asked to directly receive the solution as feedback, rather than\nas suggestions on how to correct the error;\n\n  3. 3.\n\none student suggested structuring the feedback as a two-step process: in the\nfirst step, the feedback should include the suggestions as in the current\nversion; in the second step \u2013 if the same error is repeated \u2013 the feedback\nshould show the correct solution;\n\n  4. 4.\n\nall students asked to improve the feedback to have more precise and punctual\nexplanations.\n\nBased on the suggestions reported above, we decided to improve the feedback as\nfollows.\n\nWe accepted the first and the third suggestions, in terms of the two-step\nprocess. We therefore expanded the feedback design by adding a further step\n(that is, 1.b.3) before concluding the processing for partially wrong\ncommands:\n\n  *     * 1.b ...\n\n      * 1.b.3 if a mistake is repeated for the same command, then return the correct command and the correct output.\n\nFinally, for the fourth suggestion, we decided to take into account the\npropaedeuticity between commands (e.g., before issuing a parametric test, one\nshould check the normality of the distributions) and if the used command is\nfor a different type of study (e.g., a t-test used for more than two samples).\nFor this suggestion, we expanded point 1.c of Section 4.1.3 as follows:\n\n  *     * 1.c. the command is not in the correct solution. In this case, we first return the generic message \u201cWrong command\u201d Then, we try to find in the correct solution a \u201csimilar\u201d command, i.e., an improper choice of the command for calculating the central tendency or dispersion (referring to descriptive statistics) or the hypothesis testing (referring to inferential statistics). If a \u201csimilar\u201d command is found, then\n\n      * depending on the case, return an appropriate message. For example, if the student used the median instead of the mean, we return the message \u201cAnother command to calculate the central tendency is in the correct solution.\u201d;\n\n      * if propaedeuticities exist and are not respected, then, we return the message \u201cSome preparatory commands seem to be missing\u201d;\n\n      * if the used command is appropriate for a different type of study, then, we return a message that explains the type of study the student is facing. For instance, if the student used a t.test instead of the aov (i.e. analysis of variance), we return the message \u201cPlease note that the study includes more than two samples.\u201d;\n\n      * else, we return the generic message \u201cDid you misunderstand the question or the variable type?\u201d\n\n#### 4.2.4 Learning outcomes\n\n### Objective\n\nIn this section, we report on the hypothetical effect of the tool in improving\nthe didactic outcomes. We enrolled students only from the courses of Nursing\nSciences and Prevention Sciences, because they both followed the course online\n(because of the COVID-19 pandemic), whereas the students from medicine and\nsurgery followed the course either in presence (the 2019/20 academic year) or\npartially online (the 2020/21 academic year). In particular, we tested the\nfollowing hypotheses:\n\n  * RQ.D.1 : Are the grades of students that attended the first session of exams of the 2020/21 academic year (on average) higher than the grades of the students that attended the first session of exams of the 2019/20 academic year?\n\n  * RQ.D.2 : Independently from the academic year, are the grades of students that used the tool for formative assessment ^Footnote 6 (on average) higher than the grades of students who did not use the tool?\n\n  * RQ.D.3 : Focusing only on the students that used the tool for formative assessment, are the grades of those that attended the first session of exams of the 2020/21 academic year (on average) higher than the grades of those that attended the first session of exams of the 2019/20 academic year?\n\n### Materials and methods\n\nWe collected the grades obtained by the students through the same set of eight\ndifferent assignments; these were randomly assigned to each student by the UTS\nsystem during the aforementioned exam sessions. The analyses were both\ndescriptive and inferential: means and standard deviations for descriptive\nstatistics, t-tests, or Wilcoxon tests for inferential statistics. To choose\nbetween the t-test or the Wilcoxon test, we tested the normality of the\ndistributions through a Shapiro-Wilk test [35]. Consequently, the p-value of\nthe hypothesis testing is accompanied by either a t or w subscript to indicate\nthat the reported value comes from a t-test or a Wilcoxon test, respectively.\n\n### Results\n\nThe results concerning RQ.D.1 are summarised in Table 1(a). The 2019/20 cohort\ncomprises 67 students, whereas the 2020/21 cohort comprises 45 students. The\naverage grade for the 2019/20 cohort is 23.88, the average grade for 2020/21\nis 26.38. The difference was statistically significant (p value _w = 0.012).\nRegarding RQ.D.2 (see Table 1(b)), only 35 students did not use the tool, 77\ndid. On average, the grades increased from 22.65 to 25.90.\n\nTable 1 Results about the didactic outcomes\n\nFull size table\n\n### 4.3 Third iteration\n\n#### 4.3.1 Current implementation\n\nStarting from both suggestions and opinions collected from the interviews\nSection 4.2.3 and questionnaires Section 4.2.2, the new feedback was\nimplemented as follows. Figure 7(a) shows Case 1.b.3, discussed in Section\n4.2.3. In particular, the dotted box highlights the part resulting from two\nsubsequent calls of feedback (compared with Fig. 4, which does not contain\nthis further explanation). Finally, Fig. 7(b) depicts one of the cases\ndiscussed in 1.c, and in particular, the case in which a \u201csimilar\u201d command is\nissued that, however, should be used for a different study design. In the\nspecific example, the student used the aov command instead of the t.test\ncommand.\n\nFig. 7\n\nStudent feedback \u2013 current implementation\n\nFull size image\n\n#### 4.3.2 Current architecture\n\nFigure 8 depicts the rDSA tool architecture. A solution is analysed as\nfollows. First, the code is parsed, and the commands, outputs, and comments\nare extracted. Then, the comments are first processed by a Natural Language\nProcessing (NLP) module to extract all relevant features, then classified as\neither right or wrong through a supervised classifier (see [2, 45] for\ndetails). Hence, the distance between the student\u2019s and the teacher\u2019s solution\nis calculated by the \u201cDistance calculator\u201d module. The distance is then\nconverted to the estimated grade. Unlike the previous implementations, now the\ntool has a specific module (called \u201cFeedback builder\u201d) that takes care of\nassembling the feedback according to the aforementioned design. Technically,\nthe system is developed in Java, uses Java-Server Faces [37] for implementing\nthe user interfaces, Java Persistence API [23] for storing the data into a\nMariaDB database [5], Python scripts for the NLP analysis [42], and R as\nbackend for the classification task [10].\n\nFig. 8\n\nTool architecture and feedback implementation\n\nFull size image\n\n## 5 Discussion\n\nThe tool evolved during the three iterations as summarised in Table 2, with\nrespect to the characteristics introduced in Section 3.2. The first release of\nthe rDSA tool provided both the final grading (SF) and a short report about\nthe wrong/correct commands/outputs/comments (EF-ER, GR-AB) by comparing the\nstudent\u2019s solution with the correct one (TEC-CBM). From a technical viewpoint,\nrDSA uses static code analysis to evaluate the code (TEC-SCA). In the second\niteration, the rDSA tool provided elaborated feedback to correct a mistake\n(EF-CM) and the next step towards the correct solution (EF-NS) by means of\nhints for each command containing a mistake (GR-SB). The second implementation\nalso involved an automated classification of the comments as right/wrong,\nbased on natural language processing and machine learning techniques (TEC-\nTDA). Finally, the last iteration added a more detailed feedback (EF-AL, GR-\nSSB).\n\nTable 2 Summary of the characteristics of the rDSA tool\n\nFull size table\n\nThe analyses presented in the previous section yield several interesting\nresults. The first regards the increased engagement by the 2020/21 cohort with\nvis-a-vis the previous cohort. This result supports our hypothesis, that more\ndetailed and explanatory feedback could raise more attention and participation\nin students. Nevertheless, the 2019/20 cohort followed the lectures in class,\nwhereas the 2020/21 was online, creating a clear bias. However, at this point,\nwe do not have two cohorts that can be compared without biases. We therefore\nconsidered this result preliminary, as it required verification, nevertheless\nencouraging.\n\nFurthermore, the fact that a majority of the students used the tool\niteratively as a guide (confirming or suggesting changes), refining the\nsolution until the final submission, suggesting using similar tools, showed\nthe key role played by the tool in exam preparation.\n\nThe analysis of the expectation/experience was instrumental to highlight the\nsuccessful three elements of the automated feedback (i.e., usefulness in\ngeneral, clarity of the explanation for the completely wrong commands, and\nusefulness for solving the exercise) and the unsatisfactory one (i.e., the\nexplanation for \u201cthe partially wrong commands\u201d). It was also helpful in\ndefining the priority and suggesting a further evaluation focusing on the\nexplanatory feedback for partially wrong commands. It is worth noting, these\ntypes of errors are the most deceptive and ambiguous. The student knew which\ncommand had to be used, but introduced \u201csomething\u201d wrong in the call (e.g., a\nwrong variable or wrong data). Therefore, an explanation that solves these\ntypes of errors must be precise and specific to be effective. In other words,\nexplaining a completely wrong command is somewhat easy: for example, the use\nof a median instead of a mean can easily be explained in terms of an incorrect\nchoice of the central tendency indicator, while a command that appears correct\nbut returns a different output may be caused by a multitude of factors,\nsometimes difficult even for a teacher to spot. Therefore, the initial\nidentification of the possible cause, the generation of automated feedback\nthat explains that specific mistake and suggests a way to solve the issue is\nactually a difficult task to implement.\n\nThe results of the didactic outcomes are very positive. All comparisons were\nstatistically significant, suggesting a positive effect of the tool and\nautomated feedback. However, there are at least two limitations. First, the\n2019/20 and 2020/21 cohorts included different students that followed the\ncourse in the classroom or online (due to COVID-19 pandemic), respectively.\nNevertheless, the students in the different years came from the same type of\ncourses, and were admitted to the university after passing the same type of\nadmission test, following the same teaching program in the same modality.\nSecond, the students who decided to use the tool might be more motivated and\nthus, in advance, more likely to get higher grades than the others.\n\n## 6 Conclusions\n\nThis paper summarises our latest research on the design, development, and\nevaluation of the feedback provided by the rDSA tool. In previous research, we\nfocused on the more technical aspects of the tool, i.e., code analysis,\nautomated grading of short-text answers, and their implementation. Only\nrecently, we started focusing on the feedback and how to structure it, to be\neffective in improving the comprehension of the subject and \u2013 as a consequence\n\u2013 the solutions given to assignments.\n\nTo this aim, we conducted several studies that yielded interesting results.\nIncreased engagement and learning outcomes were observed in students who used\nthe second release instead of the first release. In addition to the previously\ndiscussed limitations, the results are in line with the literature summarised\nin Section 3.3 and suggest the effectiveness of adopting the rDSA tool as a\nformative assessment instrument. Continuing this research line, we are\nplanning a further evaluation step concerning the latest implementation and\nits effectiveness in explaining the most ambiguous and insidious mistakes.\n\nSeveral features can still be added with respect to the state-of-the-art. The\nmost pertinent ones are the analysis of syntax errors (EF-SY) and the\npossibility of offering personalised hints based on a student model (TEC-MT)\n[32]. Less relevant are the performance analysis (EF-PE) and how to improve an\nalready correct solution (EF-IS). So far, performance is not a key factor in\nthe assignments we are currently providing to students. If we introduce topics\nrequiring it in our lectures (e.g. deep learning, dealing with large\ndatasets), this characteristic will become a priority. A similar consideration\nstands for how to improve an already correct solution. Thus far, given our\nassignments and the practical aim of the course, a correct solution can be\npotentially improved only by using a different command. Not pertinent to our\ntype of assignments are the remaining ones, that is, dealing with crashes (EF-\nCR), style (EF-ST), and using dynamic code analysis (TEC-DCA).\n\nHowever, it is worth mentioning the effort devoted to the implementation.\nDeveloping the static code analysis module, collecting the gold standards and\nshort answers given by students, setting up the natural language module as\nwell as the supervised classifier, requires significant effort. Therefore, a\nresearcher/professor wishing to introduce a similar tool within his/her\nteaching activities must ponder and reflect on these points.\n\nTo help researchers, the rDSA source code and datasets used in this research\nwere made publicly available on the following website:\nhttps://vittorini.univaq.it/uts/\n\n## Notes\n\n  1. Available at URL https://vittorini.univaq.it/uts/.\n\n  2. STEM is a well-known acronym for the fields of science, technology, engineering, and mathematics. It is commonly used as a general container for all scientific and technological subjects.\n\n  3. This division is adopted for the comments to return a distance in the range [0,1].\n\n  4. English for Italian sentences:\n\n(i) \u201cPressione sistolica\u201d : Systolic blood pressure (ii) \u201cprima\u201d : before;\n(iii) \u201cdopo\u201d : after.\n\n  5. English for Italian words/sentences: (i) \u201cpressione\u201d : blood pressure; (ii) \u201cprima\u201d : before; (iii) \u201cdopo\u201d : after; (iv) \u201csiccome il p-value e\u2019 maggiore di 0.05, la differenza non e\u2019 statisticamente significativa\u2019 : given that the p-value is larger than 0.05, the difference is not statistically significant.\n\n  6. We considered that a student used the tool or not if he/she completed at least half of the available exercises.\n\n## References\n\n  1. Albert W, Dixon E (2003) Is this what you expected? The use of expectation measures in usability testing. In: Proceedings of the Usability professionals association 2003 Conference, Scottsdale, AZ\n\n  2. Angelone AM, Galassi A, Vittorini P (2021) Improved automated classification of sentences in data science exercises. In: Methodologies and intelligent systems for technology enhanced learning, 11th international conference. M.S4TEL 2021, p 12\u201321. Springer, Cham\n\n  3. Angelone AM, Vittorini P (2019) A report on the application of adaptive testing in a first year university course. In: nLTEC 2019: learning technology for education challenges. Springer, vol 1011, pp 439\u2013449\n\n  4. Angelone AM, Vittorini P (2019) The automated grading of R code snippets: preliminary results in a course of health informatics. In: Proc. of the 9th international conference in methodologies and intelligent systems for technology enhanced learning. Springer\n\n  5. Bartholomew D (2015) Getting started with maria DB: explore the powerful features of mariaDB with practical examples. Packt Publishing\n\n  6. Bell P, Hoadley CM, Linn MC (2004) Design-based research in education. In: Internet environments for Science Education. Routledge, pp 73\u201385\n\n  7. Bernardi A, Innamorati C, Padovani C, Romanelli R, Saggino A, Tommasi M, Vittorini P (2019) On the design and development of an assessment system with adaptive capabilities. In: Advances in intelligent systems and computing. Springer, vol 804, Cham\n\n  8. Boud D, Molloy E (2012), Feedback in higher and professional education: understanding it and doing it well. Routledge\n\n  9. Burrows S, Gurevych I, Stein B (2015) The eras and trends of automatic short answer grading. Int J Artif Intell Educ 25(1):60\u2013117\n\nArticle Google Scholar\n\n  10. Core R, Team R (2018) A language and environment for statistical computing\n\n  11. Croft D, England M (2020) Computing with CodeRunner at Coventry University Automated summative assessment of Python and C++ code. Proceedings of the 4th Conference on Computing Education Practice 2020\n\n  12. De Gasperis G, Menini S, Tonelli S, Vittorini P (2019) Automated grading of short text answers: preliminary results in a course of health informatics. In: ICWL 2019 : 18th international conference on web-based learning, Magdeburg. Springer. LNCS\n\n  13. DeVellis RF (2006) Classical test theory. Med Care 44(11):S50\u2013S59\n\nArticle Google Scholar\n\n  14. Devlin J, Chang M-W, Lee K, Toutanova K (2019) BERT: pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 conference of the north american chapter of the association for computational linguistics: human language technologies, vol 1 (Long and Short Papers), pp 4171\u20134186, Minneapolis, Minnesota. Association for computational linguistics\n\n  15. Douce C, Livingstone D, Orwell J (2005) Automatic test-based assessment of programming a review. J Educ Resources Comput 5(3):09\n\nArticle Google Scholar\n\n  16. Erhel S, Jamet E (2013) Digital game-based learning: impact of instructions and feedback on motivation and learning effectiveness. Comput Educ 9:156\u2013167\n\nArticle Google Scholar\n\n  17. Galassi A, Vittorini P (2021) Automated feedback to students in data science assignments: improved implementation and results. In: CHItaly 2021: 14th biannual conference of the Italian SIGCHI chapter (CHItaly \u201921). ACM, Bolzano, New York\n\n  18. Galassi A, Vittorini P (2021) Improved feedback in automated grading of data science assignments. In: Advances in intelligent systems and computing, volume AISC. Springer, vol 1236, pages 296\u2013300\n\n  19. Gerdes A, Jeuring J, Heeren B (2012) An interactive functional programming tutor. Ann Innov Technol Comput Sci Educ, ITiCSE 05:250\u2013255\n\nGoogle Scholar\n\n  20. Harlen W, James M (1997) Assessment and learning: differences and relationships between formative and summative assessment. Assess Educ Princ Pract 4 (3):365\u2013379\n\nArticle Google Scholar\n\n  21. Hattie J, Timperley H (2007) The power of feedback. Rev Edu Res 77(1):81\u2013112\n\nArticle Google Scholar\n\n  22. Hollingsworth J (1960) Automatic graders for programming classes. Commun ACM 3(10):528\u2013529\n\nArticle Google Scholar\n\n  23. Keith M, Schincariol M, Nardone M (2018) Pro JPA 2 in Java EE 8 : an in-depth guide to Java persistence APIs Apress\n\n  24. Kelleher C, Pausch R (2005) Lowering the barriers to programming: a taxonomy of programming environments and languages for novice programmers. ACM Comput Surveys 37(2):83\u2013137\n\nArticle Google Scholar\n\n  25. Keuning H, Jeuring J, Heeren B (2018) A systematic literature review of automated feedback generation for programming exercises. ACM Trans Comput Educ (TOCE) 19(1):9\n\nGoogle Scholar\n\n  26. Knutas A, Savchenko D, Hynninen T, Gr\u00f6 nberg N (2019) Constructive alignment of web programming assignments and automated assessment with unit testing. Proceedings of the 19th Koli calling international conference on computing education research\n\n  27. Levenshtein VI (1966) Binary codes capable of correcting deletions, insertions and Reversals. Soviet Physics Doklady 10:707\n\nMathSciNet Google Scholar\n\n  28. Liu T, Ding W, Wang Z, Tang J, Huang GY, Liu Z (2019) Automatic short answer grading via multiway attention networks. In: International conference on artificial intelligence in education, pp 169\u2013173\n\n  29. Magooda A, Zahran MA, Rashwan MA, Raafat H, Fayek M (2016) Vector based techniques for short answer grading. In: FLAIRS conference\n\n  30. Malmi L, Korhonen A (2004) Automatic feedback and resubmissions as learning aid IEEE. In: International conference on advanced learning technologies, 2004. Proceedings, pp 186\u2013190\n\n  31. Mohler M, Bunescu R, Mihalcea R (2011) Learning to grade short answer questions using semantic similarity measures and dependency graph alignments. In: HLT \u201911: proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pp 752\u2013762\n\n  32. Nkambou R, Bourdeau J, Mizoguchi R (eds) (2010) Advances in intelligent tutoring systems, volume 308 of studies in computational intelligence. Springer, Berlin\n\nGoogle Scholar\n\n  33. Odekirk-Hash E, Zachary JL (2001) Automated feedback on programs means students need less help from teachers. In: Proceedings of the thirty-second SIGCSE technical symposium on computer science education, SIGCSE \u201901, Association for computing machinery, pp 55\u201359, New York\n\n  34. Poulos A, Mahony MJ (2008). Assess Evaluat Higher Educ 33 (2):143\u2013154\n\nArticle Google Scholar\n\n  35. Riffenburgh RH (2012) statistics in medicine. Elsevier/Academic Press\n\n  36. Rivers K, Koedinger K (2013) Automatic generation of programming feedback A data-driven approach. AIED Workshops 50:06\n\nGoogle Scholar\n\n  37. Scholtz B (2018) The definitive guide to JSF in Java EE 8 : building web applications with Java Server faces Apress\n\n  38. Souza DM, Felizardo KR, Barbosa EF (2016) A systematic literature review of assessment tools for programming assignments in2016. In: IEEE 29th international conference on software engineering education and training (CSEET), pp 147\u2013156. IEEE\n\n  39. Sultan MA, Salazar C, Sumner T (2016) Fast and easy short answer grading with high accuracy. In: Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, Stroudsburg, pp 1070\u20131075. Association for computational linguistics, PA\n\n  40. Sung C, Dhamecha TI, Mukhi N (2019) Improving short answer grading using transformer-based pre-training. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 11625 LNAI:469\u2013481\n\nGoogle Scholar\n\n  41. Sykes E, Franek F (2004) A prototype for an intelligent tutoring system for students learning to program in javatm. Adv Technol Learning 01(1):402\u2013317\n\nArticle Google Scholar\n\n  42. Thanaki J (2017) Python natural language processing : explore NLP with machine learning and deep learning techniques Packt Publishing\n\n  43. Urquiza-Fuentes J, Vel\u00e1zquez-Iturbide JA (2013) Toward the effective use of educational program animations: The roles of student\u2019s engagement and topic complexity. Comput Educ 67:178\u2013192\n\nArticle Google Scholar\n\n  44. VanLehn K (2011) The relative effectiveness of human tutoring. Intelligent tutoring systems, and other tutoring systems. Educ Psychol 46(4):197\u2013221\n\nArticle Google Scholar\n\n  45. Vittorini P, Menini S, Tonelli S (2020) An AI-based system for formative and summative assessment in data science courses. Int J Artif Intell Educ 12:1\u201327\n\nGoogle Scholar\n\n  46. Wainer H, Dorans NJ, Flaugher R, Green BF, Mislevy RJ, Dorans NJ, Flaugher R, Green BF, Mislevy RJ (2000) Computerized adaptive testing. Routledge\n\nDownload references\n\n## Funding\n\nOpen access funding provided by Universit\u00e0 degli Studi dell\u2019Aquila within the\nCRUI-CARE Agreement.\n\n## Author information\n\n### Authors and Affiliations\n\n  1. Department of Life, Health and Environmental Sciences, University of L\u2019Aquila, P.le S. Tommasi, 1, Coppito, L\u2019Aquila, 67100, Italy\n\nPierpaolo Vittorini & Alessandra Galassi\n\nAuthors\n\n  1. Pierpaolo Vittorini\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  2. Alessandra Galassi\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n### Corresponding author\n\nCorrespondence to Pierpaolo Vittorini.\n\n## Ethics declarations\n\n### Conflicts of interests/Competing interests\n\nThe authors declare that they did not receive support from any organisation\nfor the submitted work.\n\n## Additional information\n\n### Publisher\u2019s note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional affiliations.\n\n## Appendices\n\n### Appendix A: Didactic organisation\n\nTeaching is organised as follows:\n\n  * the Academic year begins in Autumn and lasts 12 months;\n\n  * the first day is October 1^st and the last day is September 30^th of the successive year;\n\n  * the Academic year is divided into two semesters;\n\n  * usually, the first semester starts in October and ends in February, the second starts in March and ends in July;\n\n  * a course can be held in a semester or during the entire year.\n\nExams are organised as follows:\n\n  * there are multiple exam sessions during the Academic year;\n\n  * usually, there are 3 sessions per year: one at the end of the first semester (February), another at the end of the second semester (July), and the last one before the beginning of the successive Academic year (September);\n\n  * a student can take the same exam multiple times until he/she passes it;\n\n  * exam grades range from 0 to 30 cum laude, customarily considered as 31;\n\n  * an exam is passed with a grade \u2265 18.\n\nReferring to the specific courses of \u201cHealth Informatics\u201d in Medicine and\nSurgery, and \u201cInformation Systems\u201d in Nursing Sciences and Prevention\nSciences:\n\n  * both courses are numerus clausus. The following table summarises the size of each cohort per degree course:\n\nDegree course| 2019/20| 2020/21  \n---|---|---  \nMedicine and Surgery| 137| 137  \nNursing Sciences| 49| 49  \nPreventive Sciences| 30| 30  \nTotal| 216| 216  \n  \n  * the course of Health Informatics is in the first semester, the course of Information System is in the second semester;\n\n  * the use of the rDSA tool is recommended for formative assessment;\n\n  * the rDSA tool is used for summative assessment, without the automated feedback;\n\n  * the exercises solved as homework are not considered for the final grade.\n\n### Appendix B: General opinion\n\n1| 2| 3| 4| 5  \n---|---|---|---|---  \n1=Useless, 2=Not very useful, 3=Neutral,  \n4=Useful, 5=Extremely useful  \nHow do you evaluate the usefulness, quality, and relevance of the exercises\navailable on the formative assessment tool for exam preparation?  \n  \n### Appendix C: Expectation/experience questionnaire\n\n1| 2| 3| 4| 5  \n---|---|---|---|---  \n1=Useless, 2=Not very useful, 3=Neutral,  \n4=Useful, 5=Extremely useful  \nHow did you expect the automatic feedback provided by the platform to be in\ngeneral?  \nHow do you generally rate the automatic feedback provided by the platform?  \n1=Not at all, 2=A little, 3=Neutral, 4=Very, 5=Completely  \nFor completely wrong commands, that is, those highlighted in red, how much did\nyou expect the feedback provided by the platform to be clear?  \nFor completely wrong commands, that is, those highlighted in red, how clear\ndid you find the feedback provided by the platform?  \nFor partially wrong commands, that is, those highlighted in blue, how much did\nyou expect the feedback provided by the platform to be clear?  \nFor the partially wrong commands, that is, those highlighted in blue, how\nclear did you find the feedback provided by the platform?  \n1=Surely not, 2=No, 3=Maybe, 4=Surely yes, 5=Yes  \nDid you expect that the explanation of the feedback would allow you to solve\nthe exercise correctly?  \nDid the explanation of the feedback later allow you to solve the exercise\ncorrectly?  \n  \n### Appendix D: Impact\n\nCheck all those that apply  \n---  \nThe automated feedback provided by the platform allowed me to:  \n1\\. Understand my mistakes  \n2\\. Understand the correct statistical method for solving the problem  \n3\\. Verification of my preparation for the final exam  \nSelect only one  \nHow did you use the system?  \n1\\. I only used it to see the exercises  \n2\\. I only use it before submitting the solution  \n3\\. I used it iteratively, to improve my solution before the final submission  \n1| 2| 3| 4| 5  \n1=Surely not, 2=No, 3=Maybe, 4=Surely yes, 5=Yes  \nWould you recommend the use of automatic feedback systems of this type to\nprepare for similar examinations?  \n  \n### Appendix E: Interview\n\nA) Wrong variable  \n---  \nExample. Suppose we ask you to calculate the average weight of a few patients\nand run the following command:  \n> mean(es$height)  \nFeedback. The system would reply to you: \u201cThe command looks correct, but the\noutput is different from the right solution; the variable height does not seem\ncorrect\u201d  \nQuestions. (a) According to your opinion, can feedback improve? (b) In the\naffirmative case, how do you suggest improving it?  \nB) Wrong boolean predicate  \nExample. Suppose we ask you to calculate the weight of female patients and run\nthe command:  \n> mean(es$weight[es$sex==\"M\"])  \nFeedback. The system would reply to you: \u201cThe command looks correct, but the\noutput is different from the right solution; the Boolean predicate sex==\"M\"\ndoes not seem correct\u201d  \nQuestions. (a) According to your opinion, can feedback improve? (b) In the\naffirmative case, how do you suggest improving it?  \nC) Missing parameter - pt 1  \nExample. We asked you to calculate if there was a statistically significant\ndrop in cholesterol after taking a new statin. You run the command:  \n> t.test(es$col.pre, es$col.post)  \nFeedback. The system would reply: \u201cThe command looks correct, but the output\nis different from the right solution; the parameter paired=TRUE appears to be\nmissing; you should have used a paired data test\u201d  \nQuestions. (a) According to your opinion, can feedback improve? (b) In the\naffirmative case, how do you suggest improving it?  \nD) Missing parameter - pt 2  \nExample. Suppose we ask you to calculate if there is a statistically\nsignificant difference in cholesterol between men and women. You run the\ncommand:  \n> t.test(es$col, es$hdl)  \nFeedback. The system would reply: \u201cThe command looks correct, but the output\nis different from the right solution, the parameter col[sex==\"M\"] appears to\nbe missing; the parameter col[sex==\"F\"] appears to be missing; the hdl\nvariable does not seem correct\u201d  \nQuestions. (a) According to your opinion, can feedback improve? (b) In the\naffirmative case, how do you suggest improving it?  \nE) Too many parameters  \nExample. Suppose we ask you to calculate if there is a statistically\nsignificant difference in cholesterol between men and women. You run the\ncommand:  \n> t.test(es$col[es$sex==\"M\"], es$col[es$sex==\"F\"], paired=TRUE)  \nFeedback. The system would reply: \u201cThe command looks correct, but the output\nis different from the right solution, the parameter paired=TRUE does not seem\nnecessary; you shouldn\u2019t have used a paired data test\u201d  \nQuestions. (a) According to your opinion, can feedback improve? (b) In the\naffirmative case, how do you suggest improving it?  \nF) Similar command  \nExample. Suppose we ask you to calculate the average weight of some patients.\nYou run the command:  \n> median(es$height)  \nFeedback. The system would reply to you: \u201cThe command looks correct, but the\noutput is different from the right solution. In the correct solution, a\ndifferent command was used to calculate the centre tendency. Please check the\nquestion and type of variable. \u201d  \nFurther note for the student. Similar feedback is returned in the case of\nconfidence intervals (e.g. ci.mu.t instead of ci.median) and for the\nhypothesis test (e.g. t.test instead of wilcox.test)  \nQuestions. (a) According to your opinion, can feedback improve? (b) In the\naffirmative case, how do you suggest improving it?  \nG) Unknown case  \nFeedback. When the system cannot find a better explanation for a command that\nlooks correct but gives a wrong result, the system responds as follow: \u201cThe\ncommand looks correct, but the output is different from the right solution.\nPlease check if the data have been imported correctly\u201d  \nQuestions. (a) According to your opinion, can feedback improve? (b) In the\naffirmative case, how do you suggest improving it?  \n  \n## Rights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0\nInternational License, which permits use, sharing, adaptation, distribution\nand reproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's\nCreative Commons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article's Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this licence, visit\nhttp://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\n## About this article\n\n### Cite this article\n\nVittorini, P., Galassi, A. rDSA : an intelligent tool for data science\nassignments. Multimed Tools Appl 82, 12879\u201312905 (2023).\nhttps://doi.org/10.1007/s11042-022-14053-x\n\nDownload citation\n\n  * Received: 23 November 2021\n\n  * Revised: 20 May 2022\n\n  * Accepted: 06 October 2022\n\n  * Published: 05 November 2022\n\n  * Issue Date: April 2023\n\n  * DOI: https://doi.org/10.1007/s11042-022-14053-x\n\n### Share this article\n\nAnyone you share the following link with will be able to read this content:\n\nSorry, a shareable link is not currently available for this article.\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\n### Keywords\n\n  * Interactive learning environments\n  * Automated feedback generation\n  * Computer-assisted instruction\n  * Automated assessment systems\n  * Intelligent tutoring systems\n  * Technology-enhanced learning\n\nUse our pre-submission checklist\n\nAvoid common mistakes on your manuscript.\n\nAdvertisement\n\n### Discover content\n\n  * Journals A-Z\n  * Books A-Z\n\n### Publish with us\n\n  * Publish your research\n  * Open access publishing\n\n### Products and services\n\n  * Our products\n  * Librarians\n  * Societies\n  * Partners and advertisers\n\n### Our imprints\n\n  * Springer\n  * Nature Portfolio\n  * BMC\n  * Palgrave Macmillan\n  * Apress\n\n128.140.102.183\n\nNot affiliated\n\n\u00a9 2024 Springer Nature\n\n", "frontpage": false}
