{"aid": "40089265", "title": "Diagnosing precision loss on Nvidia graphics cards", "url": "https://icode4.coffee/?p=566", "domain": "icode4.coffee", "votes": 1, "user": "dev_tty01", "posted_at": "2024-04-19 17:02:44", "comments": 0, "source_title": "Diagnosing Precision Loss on NVIDIA Graphics Cards", "source_text": "Diagnosing Precision Loss on NVIDIA Graphics Cards \u2013 I Code 4 Coffee\n\nI Code 4 Coffee\n\nWhere coffee turns into code!\n\nMenu\n\n  * Downloads\n  * Donate\n  * About Me\n\n### Recent Posts\n\n  * Halo 2 in HD: Pushing the Original Xbox to the Limit\n  * Light Gun Hacking Part 1: Using Namco light guns in Unity\n  * Diagnosing Precision Loss on NVIDIA Graphics Cards\n  * Fixing Rendering Bugs in Dead Rising PC\n  * Frogger\u2019s Adventure: The Rescue \u2013 Windows 7/10 Fix\n\n### github & socials\n\nBrowse: Home / Diagnosing Precision Loss on NVIDIA Graphics Cards\n\n## Diagnosing Precision Loss on NVIDIA Graphics Cards\n\nRyan Miceli / January 28, 2022 / 4 Comments / DirectX, PC\n\nThis is a short write up about how I diagnosed and debugged some precision\nloss issues I was encountering on NVIDIA graphics cards. The goal of this\nwrite up is to give some insight into what debugging deeply rooted rendering\nissues is like.\n\n## Background\n\nI encountered this issue while working on a Halo 2 modding tool I have been\ndeveloping for the past year. The goal of this tool is to be on par, if not\nbetter, than the tools that Bungie would have used to originally develop the\ngame. Part of that is being able to render the level geometry and game models\nexactly as they would appear in game. For this I had written my own rendering\nengine which included a multi-layer shading system, a well as a JIT system to\ndynamically translate and recompile Xbox shaders into PC equivalents. The\nrendering engine was based on the actual rendering engine for the game, using\nseveral months of reverse engineering research to support it. While this would\nallow me to render things almost identical to how they appear in game, this\nalso put me into an \u201call or nothing\u201d situation. Since I\u2019m using the actual\ngame shaders for rendering and the render states are setup entirely from game\ndata, even the smallest bug would cause havoc for what gets rendered on\nscreen. I knew this would be an uphill battle but the end result was well\nworth it.\n\n## Initial investigation\n\nAfter getting the initial set of bugs worked out I was able to render the\nfirst two shading layers pretty reliably with only a couple minor issues. One\nof which being some z-fighting that occurred on certain level mesh pieces.\n\nAt first glance I suspected the depth stencil state was incorrect or that a\ndepth bias was not being applied, which are the typical causes of z-fighting.\nBecause the game data I was using to setup the render state was for the Xbox\nimplementation of DirectX 8, all of the d3d8 render states had to be recreated\nor emulated for d3d11 which is what my tool was based on. More than likely I\nwas not supporting some render state correctly which leads to this issue.\n\nWhen debugging graphical issues you need a graphics debugger, something like\nPIX, RenderDoc, or Intel Graphics Frame Analyzer. My go to is RenderDoc. From\nmy experience it has the most fluid UI, great compatibility with different\nrendering APIs, and presents the information in a very organized and easy to\nfollow way. Every graphical issue I debug starts with taking a frame snapshot\nin RenderDoc so naturally that was the next step.\n\nOnce I had the frame snapshot I immediately began to analyze it and inspect\nthe render state for one of the mesh pieces that had z-fighting issues. At\nthis time I only supported two shading layers: texaccum and lightmap. The\nlightmap layer is pretty self explanatory but the texaccum layer is actually a\nworkaround for limitations in the shader pipeline on the original Xbox and\nwill need some explaining.\n\n#### The texaccum layer\n\nHalo uses a multi-layer rendering system with a number of layers such as\ntexaccum, lightmap, fog, transparency, overlays, and water, to name a few. On\nthe original Xbox console you could only sample 4 textures per pixel shader.\nIn order to work around this and sample more textures Bungie created a\ntexaccum, or \u201ctexture accumulator\u201d layer. This layer has the specific purpose\nof sampling 2 or more textures into an output texture that can be fed into\nsubsequent shading layers. Anything that uses more than 4 textures will first\nbe drawn on the texaccum layer. This usually includes objects with complex\nbase maps, detail maps, and bump maps. Once all applicable objects are drawn\nto the texaccum layer the output texture looks something like this:\n\nNext the lightmap layer will be rendered which will use the texaccum texture\nas an input. Shaders for this layer use the transformed vertex position to\nsample the texaccum texture and combine that pixel color with the color from\nthe lightmap texture, and any other textures needed. Once the lightmap layer\nis fully rendered, the scene now looks like this (does not include z-fighting\nissue):\n\nPC ports and later versions of Halo are able to sample more than 4 textures\nper pixel shader and don\u2019t need the texaccum layer. But because I was working\nwith Halo 2 for the original Xbox, I needed to support this functionality in\norder to render things correctly.\n\n## Analyzing the frame snapshot\n\nWith the snapshot open I began to analyze the render states for all the mesh\npieces that were z-fighting and found they were all using the same technique\nbetween the texaccum and lightmap layers.\n\n#### Pass 1: Texaccum\n\nThe mesh pieces were first drawn to the texaccum layer with a depth test of\nD3D11_COMPARISON_GREATER_EQUAL (we are using a reverse depth buffer, 0f is far\nplane and 1f is near plane) and depth write enabled. This pass would fill the\ndepth buffer with the depth of each vertex and combine multiple textures into\nthe texaccum output texture.\n\n#### Pass 2: Lightmap\n\nNext the same mesh pieces would be drawn to the output texture using the\ntexaccum texture as input. The depth test for this layer was set to\nD3D11_COMPARISON_EQUAL and depth write was disabled. The idea here is only\nkeep vertices which match whats in the depth buffer exactly. If you render the\nsame mesh using the same transformation matrix and shader constants you should\nget the same output as before, the depth test will pass, and a new pixel value\nwill be written to the output image. However this was not the case as we are\ngetting z-fighting after rendering the second layer. It\u2019s also important to\nnote that if the depth test fails the pixel is discarded and not written to\nthe output image, and whatever color value is currently there is what gets\ndisplayed. Since the lightmap layer renders to an output image that was\ncleared to cornflower blue, the resulting pixel value when the depth test\nfails will be cornflower blue, exactly what we see in the video.\n\nBetween the two draw calls for each mesh piece the vertex buffer is never\ntouched, but the world-view-projection matrix and shader constants are updated\nand rewritten to constants buffer. I thought that maybe there was some weird\nprecision loss occurring between calculations and decided to compare the\nconstants buffer for both layers for a single mesh piece. Using RenderDoc I\ndumped the constants buffers to two files and did a binary comparison on them.\nFloating point comparisons can be misleading if the numbers being displayed\nare truncated so binary comparison is the way to go. But the buffers were the\nsame. Next I used RenderDoc to check the depth buffer history for one of the\nz-fighting pixels. The pixel history shows me every depth test performed for a\nsingle pixel, the pixel color before and after the test, and the test result.\nThis would let me see the depth comparison result for our z-fighting pixels\nand why they failed.\n\nIn the image above we can see the mesh piece being rendered (purple) on the\nlightmap layer, and the history for the one of the pixels that was z-fighting\n(on the right). Event #1 is the output image being cleared to cornflower blue,\nevent #2, 3, and 4 are pixels that were discarded because they were either\nfurther away from the camera than what was already in the depth buffer, or the\ntriangle was backfacing and culled (the triangle was facing the wrong\ndirection). Event #5 is the one for the highlighted mesh piece that is\nz-fighting. We can see the output pixel color from the shader is what we would\nexpect the mesh piece to look like, but the depth test failed and it was\ndiscarded. The other thing we can see is the depth value from the shader,\n0.00156, and the value that was already in the depth buffer, also 0.00156. It\nappeared as though the depth values were equal and with a depth test of\nD3D11_COMPARISON_EQUAL this should have passed, yet it seemingly failed which\nshould be impossible.\n\nI thought there must have been something wrong and decided to take another\nsnapshot, and another, and another, and another... until I realized that every\nsnapshot and every z-fighting pixel was showing the same thing. The depth\nvalue calculated by the shader seemed to match what was in the depth buffer\nand yet the equals comparison was failing. At this point I didn\u2019t know if\nRenderDoc was rounding the numbers being displayed (I\u2019ll get to this in a\nbit), but even if it was the shaders were designed to work with this exact\nrender state and works in the real game. I was fairly confident the render\nstate was setup correctly as other geometry pieces were using the same render\nstates on the texaccum and lightmap layers without z-fighting. So it seemed as\nthough there was something else going on.\n\nIn today\u2019s world of game design this is probably a seldomly used technique to\nrender the same mesh twice using a depth comparison test of\nD3D11_COMPARISON_EQUALS for the second pass. The only reason the original game\ndid this was because they could only sample 4 textures per pass and needed to\nrender the mesh piece twice in order to sample more textures. This made me\nwonder if this could actually be hardware related, I was running on a NVIDIA\nRTX 3080 but what would happen if I ran this on an AMD card?\n\nI pinged a friend on discord who had an AMD graphics card and sent him a copy\nof my tool. As the level geometry loaded and he panned the camera to the same\narea there was no z-fighting at all, absolutely none. He switched from his AMD\nRX 580 to a NVIDIA RTX 2060 and sure enough the z-fighting was there. That\nconfirmed it, this was an issue with NVIDIA graphics cards. But it didn\u2019t\nexplain what the actual issue was or give me any idea on how to implement a\nfix for this.\n\n## Diagnosing hardware precision loss\n\nThe next step was to figure out what was actually going on. I assumed that\nRenderDoc was truncating the depth values which is why they appeared equal\neven though the comparison test failed. I looked in the settings window and\nfound a setting for the number of decimal places to show for floating point\nvalues. I cranked that bad boy up to 15 and checked the depth values again.\nSure enough the precision loss was clear now:\n\nIt appears as though after the 9th decimal place the values start to differ\nwith an approximate 0.000000002 difference. Now we confirmed the precision\nloss but where was this manifesting from? The vertex buffer and shader\nconstants are the same between both calls, so I figured it must be something\nbetween the shaders being used. I pulled out the lines for calculating the\noutput vertex position (ultimately what the depth value is based on) from both\nshaders and compared side by side:\n\nC\n\n12345678910111213141516171819202122232425262728| // mul r10.xyz, v0.xyz,\nc74.xyzr10.xyz = (va_position.xyz *VertexCompressionPositionMin.xyz).xyz;//\nadd r10.xyz, r10.xyz, c75.xyzr10.xyz = r10.xyz\n+VertexCompressionPositionMax.xyz;// dph r0.x, r10, c-46r0.x = dph(r10,\nc[50]);// dph r0.y, r10, c-45r0.y = dph(r10, c[51]);// dph r0.z, r10, c-44r0.z\n= dph(r10, c[52]);// dph oPos.x, r0, c-96oPos.x = dph(r0, WorldTransform1);//\ndph oPos.y, r0, c-95oPos.y = dph(r0, WorldTransform2);// dph oPos.z, r0,\nc-94oPos.z = dph(r0, WorldTransform3);// dph oPos.w, r0, c-93oPos.w = dph(r0,\nWorldTransform4);  \n---|---  \n  \nC\n\n12345678910111213141516171819202122232425262728| // mul r10.xyz, v0.xyz,\nc74.xyzr10.xyz = (va_position.xyz *VertexCompressionPositionMin.xyz).xyz;//\nadd r10.xyz, r10.xyz, c75.xyzr10.xyz = r10.xyz\n+VertexCompressionPositionMax.xyz;// dph r0.x, r10, c-46r0.x = dph(r10,\nc[50]);// dph r0.y, r10, c-45r0.y = dph(r10, c[51]);// dph r0.z, r10, c-44r0.z\n= dph(r10, c[52]);// dph oPos.x, r0, c-96oPos.x = dph(r0, WorldTransform1);//\ndph oPos.y, r0, c-95oPos.y = dph(r0, WorldTransform2);// dph oPos.z, r0,\nc-94oPos.z = dph(r0, WorldTransform3);// dph oPos.w, r0, c-93oPos.w = dph(r0,\nWorldTransform4);  \n---|---  \n  \nThis code above is transforming the vertex position into the scene\u2019s \u201cworld\u201d\nview. Halo does this in two steps. First the vertex position is transformed by\nthe model\u2019s transformation matrix contained in c[50], c[51], and c[52]. This\nmoves and rotates the model into the correct position in the world. Then the\nresult is multiplied by the world-view-projection matrix contained in\nWorldTransform1-4 to put it into the final position relative to the camera.\nThe \u201cworld\u201d part of the world-view-projection matrix is just the identity\nmatrix in this case.\n\nAs we can see both shaders are performing the same operations for calculating\nthe vertex position, or are they? One thing that came to mind was that all of\nthe shaders are set to be optimized when run through the HLSL compiler. Over\nthe years I have seen optimizers do some really dirty things to code,\neverything from dereferencing bad addresses, to omitting return values, to\njust completely removing code that has important functionality. As a quick\ntest I changed the HLSL compiler flags so that no optimizations are applied,\nand to my surprise that made the issue infinitely worse:\n\nWhile this wasn\u2019t the result I was hoping for this at least confirms that\noptimizations do play a role in how this issue manifests. I reverted the\nchange and took another RenderDoc snapshot, this time instead of comparing the\nHLSL source code I compared the microcode instructions that were emitted for\nthe compiled shaders. On the left is the output from the texaccum shader and\nthe right is the output from the lightmap shader:\n\nC\n\n12345678910111213141516171819202122232425| common.fx:216 - dph()return (a.x *\nb.x) + (a.y * b.y) +(a.z * b.z) + b.w;14: dp3 r0.w, r0.xyzx, c[51].xyzx15: add\nr0.w, r0.w, c[51].w16: mul r1.x, r0.w, c[0].y17: dp3 r1.y, r0.xyzx,\nc[50].xyzx18: dp3 r0.x, r0.xyzx, c[52].xyzx19: add r0.x, r0.x, c[52].w20: add\nr0.y, r1.y, c[50].w21: mad r0.z, r0.y, c[0].x, r1.x22: mad r0.z, r0.x, c[0].z,\nr0.z23: add r1.x, r0.z, c[0].w24: mul r0.z, r0.w, c[1].y25: mad r0.z, r0.y,\nc[1].x, r0.z26: mad r0.z, r0.x, c[1].z, r0.z27: add r1.y, r0.z, c[1].w28: mul\nr0.z, r0.w, c[2].y29: mul r0.w, r0.w, c[3].y30: mad r0.w, r0.y, c[3].x,\nr0.w31: mad r0.y, r0.y, c[2].x, r0.z32: mad r0.y, r0.x, c[2].z, r0.y33: mad\nr0.x, r0.x, c[3].z, r0.w34: add r2.w, r0.x, c[3].w35: add r1.z, r0.y, c[2].w  \n---|---  \n  \nC\n\n12345678910111213141516| common.fx:216 - dph()return (a.x * b.x) + (a.y * b.y)\n+(a.z * b.z) + b.w;32: dp3 r0.w, r0.xyzx, c[50].xyzx33: add r1.x, r0.w,\nc[50].w34: dp3 r0.w, r0.xyzx, c[51].xyzx35: dp3 r0.x, r0.xyzx, c[52].xyzx36:\nadd r1.z, r0.x, c[52].w37: add r1.y, r0.w, c[51].w38: dp3 r0.x, r1.xyzx,\nc[0].xyzx39: add r0.x, r0.x, c[0].w40: dp3 r1.w, r1.xyzx, c[1].xyzx41: add\nr0.y, r1.w, c[1].w42: dp3 r1.w, r1.xyzx, c[2].xyzx43: add r0.z, r1.w,\nc[2].w44: dp3 r1.w, r1.xyzx, c[3].xyzx  \n---|---  \n  \nThe above instructions are the output from all the \u201cdph\u201d (dot product\nhomogeneous) function calls in the shader code above. As we can see the\ntexaccum shader used mad/add pairs and the lightmap shader used dp3/add pairs.\nIn order to transform the vertex position into the scene\u2019s \u201cworld\u201d we must\nmultiply it by the world-view-projection matrix. This is done with a series of\n4 dot product calculations. But the vertex position is a vector3 and the rows\nof the world-view-projection matrix are vector4\u2019s so they are not compatible.\nTo work around this you can either create a vector4 for the vertex position\nand set the W component to 1, or you can do a homogeneous dot product which\nwill ignore the missing component on one of the vectors:\n\nC\n\n1234| float dph(float3 a, float4 b){return (a.x * b.x) + (a.y * b.y) + (a.z *\nb.z) + b.w;}  \n---|---  \n  \nBy looking at the microcode that was emitted we can start to see the issue.\nOne transformation is using different instructions than the other which leads\nto some very very tiny precision loss between the two results. Given the same\ninputs these instruction pairs produce different outputs. Since a dot product\nis just a combination of mul/add/mad instructions I would think under the hood\ndp3 is just a combo of those and should produce the same result. But for all I\nknow there is some special hardware path specifically for dp3, or, to add yet\nanother layer of complexity, the microcode we see here is not what the GPU\nactually executes. This microcode is generated by the DirectX shader compiler\nand is hardware agnostic. Under the hood the graphics card drivers will\nactually JIT this into some hardware specific machine code, so we have no idea\nwhat this actually looks like to the graphics card itself.\n\nI don\u2019t know which instruction has the precision loss but I was curious to see\nwhat the emitted code looked like on a machine with an AMD graphics card, if\nit would be any different. I suspected not since the DirectX shader compiler\noperates without any communication to the graphics card drivers. Being curious\nI had my friend take a RenderDoc snapshot on his PC and I checked the emitted\ncode for both shaders. Sure enough it was roughly the same code as what was\nemitted on my machine. The texaccum shader was using mul/add pairs and the\nlightmap shader was using dp3/add pairs. So this confirmed that the issue was\nactually precision loss on NVIDIA graphics cards in one of the add/mul/mad\ninstructions.\n\n### *Update*\n\nAfter writing this post a coworker linked me to an NVIDIA developer doc that\ncovers precision loss for dot product calculations. It covers a few different\nalgorithms for computing dot products and states \u201ceach algorithm yields a\ndifferent result, and they are all slightly different from the correct\nmathematical dot product\u201d. So this behavior of different instruction sequences\nfor dot product calculations producing different results for a single input is\nby-design for NVIDIA graphics cards. The interesting part is this isn\u2019t\nentirely true for AMD graphics cards. As I said in the previous section, I\nconfirmed that the shader compiler was emitting two different combinations of\ninstructions for the dph function in both shaders, same as I encountered on my\nNVIDIA setup. So AMD must have some technique they use under the hood to make\nthe precision loss uniform between the two combinations of instructions. I\nwould be curious to know what this technique is, but couldn\u2019t find anything in\nthe AMD developer documents I found on their website. If anyone has any info\non this I would be interested in hearing more about it.\n\n## Optimizing the optimizer\n\nNow that the issue is understood I need to implement a fix for this. The\nobvious answer might be to just add a depth bias and change the depth test to\nD3D11_COMPARISON_GREATER_EQUAL. However, I need to accurately detect the\nrender state for the lightmap shader in an elegant way. Because these shaders\nare part of the game data that users can modify when using this modding tool,\nI didn\u2019t want to implement the fix based on the shader file name. The user\ncould just rename or clone the shader, or even create their own shader that\nhas the same behavior. I tried to write some logic to accurately detect this\nrender state and set a depth bias but this ended up causing issues in other\nshaders using the same state that didn\u2019t have an issue. The next thing I tried\nwas just chopping everything off after 5 decimal places hoping that it would\neliminate the precision loss, but this didn\u2019t have any affect. At least not\nuntil chopping everything off after the second decimal place, at which point\nthe z-fighting was gone but the depth buffer was destroyed and objects just\nstarted bleeding through each other.\n\nAfter sleeping on this for a few days I came back with a fresh mindset and\napproached the issue from a new angle. While the issue was ultimately on the\nhardware, the optimizer was what was pushing us down that path. If I could\nforce the optimizer to choose one of the two paths consistently then that\nshould mitigate the issue. Now the solution became clear, instead of\ncalculating the dot product the long way I would use the dp3 intrinsic and a\nsingle add. This should optimize the dph function to the point where the\nshader optimizer wouldn\u2019t be able to perform any further optimizations on it.\nIf all the shaders that get compiled emit the same instructions for the dph\nfunction then they should all compute the same result for a given input. This\nwas the new dph function:\n\nC\n\n1234| float dph(float3 a, float4 b){return dot(a.xyz, b.xyz) + b.w;}  \n---|---  \n  \nWith this implemented I loaded up the level files for coagulation and voila,\nthe issue is now \u201cfixed\u201d:\n\n## Conclusion\n\nDiagnosing graphical issues can be quite tedious but with tools like RenderDoc\nand a proper work flow for diagnosis they can become much more manageable.\nHaving worked through a number of rendering issues while working on this tool\nand others I have been able to develop a set of methodologies to quickly\ndiagnose a number of rendering issues. This one threw me for a loop as I was\nexpecting the issue to be something I was doing. After all, I was using a\ncustom rendering engine which relied on JIT to translate Xbox shaders into PC\nformat, as well as emulating DirectX 8 specific render states on DirectX 11,\nsome of which were Xbox only extensions. I was sure the issue was something I\nwas doing but it actually turned out to be a precision loss issue when\ncomputing dot products on NVIDIA graphics cards.\n\nOne might think this means AMD graphics cards have higher precision than\nNVIDIA, but that is not necessarily true. I don\u2019t know what either card is\ndoing under the hood. For all I know the AMD card could be truncating values\nbefore the precision loss manifests as an issue. While I wanted to do further\nresearch into this I was pretty burnt out from spending almost a weeks worth\nof evenings dealing with this issue. Now that I had mitigated it I decided to\nmove on.\n\n#### Minimized proof of concept\n\nDuring my investigation I created a minimized poc of the issue by modifying\none of the DirectX sample applications. While the poc is successful in\nreproducing the issue it does not have nearly the same magnitude of z-fighting\nas I encountered in my modding tool. The z-fighting in the poc happens very\nseldomly, though it is present. When I experienced it in my modding tool the\nz-fighting was present 98% of the time. Running the poc on a NVIDIA card will\nrender a green cube that will periodically flicker red when the precision loss\noccurs. Running it on AMD cards gives no flickering as the precision loss\nissue is not present.\n\nFrom the tests I ran these were the results:\n\n  * NVIDIA RTX 3080 \u2013 cube flickers (precision loss)\n  * NVIDIA RTX 2060 \u2013 cube flickers\n  * AMD RX 580 \u2013 no flickering (no precision loss)\n  * AMD RX 5700 XT \u2013 no flickering\n\nI uploaded the source + bins to github for anyone who wants to give it a try\nor play around with it.\n\nTags: amd, DirectX, loss, nvidia, precision\n\n### 4 Responses\n\n  1. Anthony / 2-1-2022 / \u00b7\n\nHi,\n\nIf I\u2019m following along correctly, this isn\u2019t a hardware bug but instead a case\nwhere the original guest microcode produces different floating-point results\nthan the host microcode generated by the host compiler from the HLSL that\nyou\u2019ve dynamically generated.\n\nGiven fadd/fmul vs fmad are going to produce different results, it seems like\nthe fundamental issue is that you need to ensure that the compiler generates\nhost microcode that produces the same results as the guest microcode. What\ndoes the original guest microcode look like? Is it using individual fadd/fmul\nor fmad ops?\n\nIn GLSL there is a precise keyword to force the compiler to treat operations\nconsistently (so x = a * b + c would produce a fadd/fmul rather than an fmad).\nPerhaps there\u2019s something equivalent in HLSL that could be used if you need to\nprevent the use of fused ops.\n\n  2. rmiceli / 2-2-2022 / \u00b7\n\nThe issue isn\u2019t that the JIT\u2019d shaders produce different floating point values\nthan the original xbox shaders, it\u2019s the dp3/mad instructions do not produce\nthe same results as add/mul. On the original xbox there is actually a \u201cdph\u201d\ninstruction which I emulate via the dph HLSL function I wrote. Any place in\nthe JIT\u2019d shaders that call the dph function would have had a dph instruction\nin the original xbox microcode. I actually tried marking the variables and all\nassociated operations using the precise keyword but this had no noticeable\naffect and the emitted microcode was the same as if the keyword was never\nused.\n\nI definitely believe this is a GPU bug as dp3 and mad are just a combination\nof add/multiply operations, so regardless of what instructions are emitted the\ndph function should always produce the same output for a given input. Even\nwhen compiling the shaders with the \u201cskip optimizations\u201d flag the z-fighting\nwas 10x worse on NVIDIA cards while there was no noticeable affect on AMD\ncards. If add/mul do not produce the same result as dp3 or mad then the shader\ncompiler would never be able to optimize add/mul into dp3/mad instructions\nsince there is no guarantee the result produced would be correct.\n\n  3. Anthony / 2-2-2022 / \u00b7\n\n>as dp3 and mad are just a combination of add/multiply operations, so\nregardless of what instructions are emitted the dph function should always\nproduce the same output for a given input.\n\nThis is where the confusion is coming from. This isn\u2019t true unfortunately.\nWhen you perform say, a 32-bit floating-point multiply, a > 32-bit result will\nbe computed. This result will normalized and rounded back down to 32-bits\nbefore getting placed in the destination register. Fused operations (aka fmad\nand dph) are going to perform the subsquent add with this higher-precision\nintermediate value and only round the final result.\n\nFloating-point operations are generally not associative nor distributive.\nHowever, higher-level languages generally treat them as such for optimization\npurposes. In the GLSL manual you can find where they discuss floating-point\nprecision \u2013\n\nhttps://i.imgur.com/hhBRKWE.png\n\nYou\u2019ll notice that for a * b + c it\u2019s acceptable to be rounded once (fmad) or\ntwice (fmul/fadd).\n\nWith this all in mind, this is why converting to using the built-in dot\nroutine fixed the issue. You\u2019re starting off with the guest microcode which\nused a higher-precision fused operation (dph) and you need to ensure that in\nthe end, the host microcode is using something with (hopefully) similar\nprecision. By using dot you\u2019re helping the compiler do so. I\u2019m not sure of how\nexactly dph is implemented, but I\u2019d be concerned with your ultimate\nimplementation of \u2013\n\ndot(a.xyz, b.xyz) + b.w\n\nThis may end up producing a dp3/fadd (aka two rounding operations). You may\ninstead want \u2013\n\ndot(vec4(a.xyz, 1.0), b)\n\n  4. Ryan Miceli / 2-7-2022 / \u00b7\n\n> You\u2019ll notice that for a * b + c it\u2019s acceptable to be rounded once (fmad)\n> or twice (fmul/fadd).\n\nThis does seem to be the case. A coworker linked me to a NVIDIA developer doc\nthat describes precision loss when computing dot products using different\nalgorithms. They state that \u201ceach algorithm yields a different result, and\nthey are all slightly different from the correct mathematical dot product\u201d. So\nit seems that different instruction sequences used will have different\nresults.\n\nI still find it interesting that this doesn\u2019t seem to affect AMD graphics\ncards as neither the minimized poc or the larger tool I\u2019m developing show any\nz-fighting on AMD cards. They must have some technique they use under the hood\nto keep any precision loss uniform, but I couldn\u2019t find anything about it in\nthe developer docs I found on their website.\n\n### Leave a Reply Click here to cancel reply.\n\n\u2190 Fixing Rendering Bugs in Dead Rising PC\n\nLight Gun Hacking Part 1: Using Namco light guns in Unity \u2192\n\n### Random Image\n\nCopyright \u00a9 2024\n\nPowered by Oxygen Theme.\n\n", "frontpage": false}
