{"aid": "40098574", "title": "An Entire Social Network in 1.6GB (GraphD Part 2)", "url": "https://jazco.dev/2024/04/20/roaring-bitmaps/", "domain": "jazco.dev", "votes": 1, "user": "todsacerdoti", "posted_at": "2024-04-20 16:37:35", "comments": 0, "source_title": "An entire Social Network in 1.6GB (GraphD Part 2)", "source_text": "An entire Social Network in 1.6GB (GraphD Part 2) \u00b7 Jaz's Blog\n\n### Jaz's Blog A space where I rant about computers\n\n# An entire Social Network in 1.6GB (GraphD Part 2)\n\n20 Apr 2024\n\nIn Part 1 of this series, we tried to answer the question \u201cwho do you follow\nwho also follows user B\u201d in Bluesky, a social network with millions of users\nand hundreds of millions of follow relationships.\n\nAt the conclusion of the post, we\u2019d developed an in-memory graph store for the\nnetwork that uses HashMaps and HashSets to keep track of the followers of\nevery user and the set of users they follow, allowing bidirectional lookups,\nintersections, unions, and other set operations for combining social graph\ndata.\n\nI received some helpful feedback after that post where several people pointed\nme towards Roaring Bitmaps as a potential improvement on my implementation.\n\nThey were right, Roaring Bitmaps would be an excellent fit for my Graph\nservice, GraphD, and could also provide me with a much needed way to quickly\npersist and load the Graph data to and from disk on startup, hopefully\nreducing the startup time of the service.\n\n## What are Bitmaps?\n\nIf you just want to dive into the Roaring Bitmap spec, you can read the paper\nhere, but it might be easier to first talk about bitmaps in general.\n\nYou can think of a bitmap as a vector of one-bit values (like booleans) that\nlet you encode a set of integer values.\n\nFor instance, say we have 10,000 users on our website and want to keep track\nof which users have validated their email addresses. We could do this by\ncreating a list of the uint32 user IDs of each user, in which case if all\n10,000 users have validated their emails we\u2019re storing 10k * 32 bits = 40KB.\n\nOr, we could create a vector of single-bit values that\u2019s 10,000 bits long (10k\n/ 8 = 1.25KB), then if a user has confirmed their email we can set the value\nat the index of their UID to 1.\n\nIf we want to create a list of all the UIDs of validated accounts, we can walk\nthe vector and record the index of each non-zero bit. If we want to check if\nuser n has validated their email, we can do a O(1) lookup in the bitmap by\nloading the bit at index n and checking if it\u2019s set.\n\n## When Bitmaps get Big and Sparse\n\nNow when talking about our social network problem, we\u2019re dealing with a few\nmore than 10,000 UIDs. We need to keep track of 5.5M users and whether or not\nthe user follows or is followed by any of the other 5.5M users in the network.\n\nTo keep a bitmap of \u201cPeople who follow User A\u201d, we\u2019re going to need 5.5M bits\nwhich would require (5.5M / 8) ~687KB of space.\n\nIf we wanted to keep bitmaps of \u201cPeople who follow User A\u201d and \u201cPeople who\nUser A follows\u201d, we\u2019d need ~1.37MB of space per user using a simple bitmap,\nmeaning we\u2019d need 5,500,000 * 1.37MB = ~7.5 Terabytes of space!\n\nClearly this isn\u2019t an improvement of our strategy from Part 1, so how can we\nmake this more efficient?\n\nOne strategy for compressing the bitmap is to take consecutive runs of 0\u2019s or\n1\u2019s (i.e. 00001110000001) in the bitmap and turn them into a number.\n\nFor instance if we had an account that followed only the last 100 accounts in\nour social network, the first 5,499,900 indices in our bitmap would be 0\u2019s and\nso we could represent the bitmap by saying: 5,499,900 0's, then 100 1's which\nyou notice I\u2019ve written here in a lot fewer than 687KB and a computer could\nencode using two uint32 values plus two bits (one indicator bit for the state\nof each run) for a total of 66 bits.\n\nThis strategy is called Run Length Encoding (RLE) and works pretty well but\nhas a few drawbacks: mainly if your data is randomly and heavily populated,\nyou may not have many consecutive runs (imagine a bitset where every odd bit\nis set and every even bit is unset). Also lookups and evaluation of the bitset\nrequires walking the whole bitset to figure out where the index you care about\nlives in the compressed format.\n\nThankfully there\u2019s a more clever way to compress bitmaps using a strategy\ncalled Roaring Bitmaps.\n\nA brief description of the storage strategy for Roaring Bitmaps from the\nofficial paper is as follows:\n\n> We partition the range of 32-bit indexes ([0, n)) into chunks of 2^16\n> integers sharing the same 16 most significant digits. We use specialized\n> containers to store their 16 least significant bits. When a chunk contains\n> no more than 4096 integers, we use a sorted array of packed 16-bit integers.\n> When there are more than 4096 integers, we use a 2^16-bit bitmap.\n>\n> Thus, we have two types of containers: an array container for sparse chunks\n> and a bitmap container for dense chunks. The 4096 threshold insures that at\n> the level of the containers, each integer uses no more than 16 bits.\n\nThese bitmaps are designed to support both densely and sparsely distributed\ndata and can provide high performance binary set operations (and/or/etc.)\noperating on the containers within two or more bitsets in parallel.\n\nTo be honest I haven\u2019t dug too deep into the paper and am not familiar enough\nwith their datastructure yet to explain it with diagrams and such, but if\nthat\u2019s something you\u2019d be interested in, please let me know!\n\nSo, how does this help us build a better graph?\n\n## GraphD, Revisited with Roaring Bitmaps\n\nLet\u2019s get back to our GraphD Service, this time in Go instead of Rust.\n\nFor each user we can keep track of a struct with two bitmaps:\n\n    \n    \n    type FollowMap struct { followingBM *roaring.Bitmap followingLk sync.RWMutex followersBM *roaring.Bitmap followersLk sync.RWMutex }\n\nOur FollowMap gives us a Roaring Bitmap for both the set of users we follow,\nand the set of users who follow us.\n\nAdding a Follow to the graph just requires we set the right bits in both\nuser\u2019s respective maps:\n\n    \n    \n    // Note I've removed locking code and error checks for brevity func (g *Graph) addFollow(actorUID, targetUID uint32) { actorMap, _ := g.g.Load(actorUID) actorMap.followingBM.Add(targetUID) targetMap, _ := g.g.Load(targetUID) targetMap.followersBM.Add(actorUID) }\n\nEven better if we want to compute the intersections of two sets (i.e. the\npeople User A follows who also follow User B) we can do so in parallel:\n\n    \n    \n    // Note I've removed locking code and error checks for brevity func (g *Graph) IntersectFollowingAndFollowers(actorUID, targetUID uint32) ([]uint32, error) { actorMap, ok := g.g.Load(actorUID) targetMap, ok := g.g.Load(targetUID) intersectMap := roaring.ParAnd(4, actorMap.followingBM, targetMap.followersBM) return intersectMap.ToArray(), nil }\n\nStoring the entire graph as Roaring Bitmaps in-memory costs us around 6.5GB of\nRAM and allows us to perform set intersections between moderately large sets\n(with hundreds of thousands of set bits) in under 500 microseconds while\nserving over 70k req/sec!\n\nAnd the best part of all? We can use Roaring\u2019s serialization format to write\nthese bitmaps to disk or transfer them over the network.\n\n## Storing 164M Follows in 1.6GB\n\nIn the original version of GraphD, on startup the service would read a CSV\nfile with an adjacency list of the (ActorDID, TargetDID) pairs of all follows\non the network.\n\nThis required creating a CSV dump of the follows table, pausing writes to the\nfollows table, then bringing up the service and waiting 5 minutes for it to\nread the CSV file, intern the DIDs as uint32 UIDs, and construct the in-memory\ngraph.\n\nThis process is slow, pauses writes for 5 minutes, and every time our service\nrestarts we have to do it all over again!\n\nWith Roaring Bitmaps, we\u2019re now given an easy way to effectively serialize a\nversion of the in-memory graph that is many times smaller than the adjacency\nlist CSV and many times faster to load.\n\nWe can serialize the entire graph into a SQLite DB on the local machine where\neach row in a table contains:\n\n    \n    \n    (uid, DID, followers_bitmap, following_bitmap)\n\nLoading the entire graph from this SQLite DB can be done in around ~20\nseconds:\n\n    \n    \n    // Note I've removed locking code and error checks for brevity rows, err := g.db.Query(`SELECT uid, did, following, followers FROM actors;`) for rows.Next() { var uid uint32 var did string var followingBytes []byte var followersBytes []byte rows.Scan(&uid, &did, &followingBytes, &followersBytes) followingBM := roaring.NewBitmap() followingBM.FromBuffer(followingBytes) followersBM := roaring.NewBitmap() followersBM.FromBuffer(followersBytes) followMap := &FollowMap{ followingBM: followingBM, followersBM: followersBM, followingLk: sync.RWMutex{}, followersLk: sync.RWMutex{}, } g.g.Store(uid, followMap) g.setUID(did, uid) g.setDID(uid, did) }\n\nWhile the service is running, we can also keep track of the UIDs of actors who\nhave added or removed a follow since the last time we saved the DB, allowing\nus to periodically flush changes to the on-disk SQLite only for bitmaps that\nhave updated.\n\nSyncing our data every 5 seconds while tailing the production firehose takes\n2ms and writes an average of only ~5MB to disk per flush.\n\nThe crazy part of this is, the on-disk representation of our entire follow\nnetwork is only ~1.6GB!\n\nBecause we\u2019re making use of Roaring\u2019s compressed serialized format, we can\nturn the ~6.5GB of in-memory maps into 1.6GB of on-disk data. Our largest\nbitmap, the followers of the bsky.app account with over 876k members, becomes\n~500KB as a blob stored in SQLite.\n\nSo, to wrap up our exploration of Roaring Bitmaps for first-degree graph\ndatabases, we saw:\n\n  * A ~20% reduction in resident memory size compared to HashSets and HashMaps\n  * A ~84% reduction in the on-disk size of the graph compared to an adjacency list\n  * A ~93% reduction in startup time compared to loading from an adjacency list\n  * A ~66% increase in throughput of worst-case requests under load\n  * A ~59% reduction in p99 latency of worst-case requests under low\n\nMy next iteration on this problem will likely be to make use of DGraph\u2019s in-\nmemory Serialized Roaring Bitmap library that allows you to operate on fully-\ncompressed bitmaps so there\u2019s no need to serialize and deserialize them when\nreading from or writing to disk. It also probably results in significant\nmemory savings as well!\n\nIf you\u2019re interested in solving problems like these, take a look at our open\nBackend Developer Job Rec.\n\nYou can find me on Bluesky here.\n\n### Related posts\n\n  * Your Data Fits in Memory (GraphD Part 1) 15 Apr 2024\n  * Scaling Go to 192 Cores with Heavy I/O 10 Jan 2024\n  * Solving Thundering Herds with Request Coalescing in Go 28 Sep 2023\n\n\u00a9 2024. All rights reserved.\n\n", "frontpage": false}
