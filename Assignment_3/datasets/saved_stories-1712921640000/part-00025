{"aid": "40009416", "title": "Plan for Hyper-V scalability in Windows Server", "url": "https://learn.microsoft.com/en-us/windows-server/virtualization/hyper-v/plan/plan-hyper-v-scalability-in-windows-server", "domain": "microsoft.com", "votes": 1, "user": "taubek", "posted_at": "2024-04-12 04:35:30", "comments": 0, "source_title": "Plan for Hyper-V scalability in Windows Server", "source_text": "Plan for Hyper-V scalability in Windows Server | Microsoft Learn\n\nSkip to main content\n\n## Microsoft Build\n\nMay 21\u201323, 2024\n\nAI has disrupted the industry. Join us as we disrupt it some more.\n\nRegister now\n\nThis browser is no longer supported.\n\nUpgrade to Microsoft Edge to take advantage of the latest features, security\nupdates, and technical support.\n\nDownload Microsoft Edge More info about Internet Explorer and Microsoft Edge\n\nLearn\n\nSign in\n\nLearn\n\nSign in\n\nWindows Server\n\nRead in English\n\n# Plan for Hyper-V scalability in Windows Server\n\n  * Article\n  * 04/10/2024\n\nImportant\n\nWindows Server 2025 is in PREVIEW. This information relates to a prerelease\nproduct that may be substantially modified before it's released. Microsoft\nmakes no warranties, expressed or implied, with respect to the information\nprovided here.\n\nThis article gives you details about the maximum configuration for components\nyou can add and remove on a Hyper-V host or its virtual machines, such as\nvirtual processors or checkpoints. As you plan your deployment, consider the\nmaximums that apply to each virtual machine, and those that apply to the\nHyper-V host. Maximums continue to grow in Windows Server versions, in\nresponse to requests to support newer scenarios such as machine learning and\ndata analytics.\n\nNote\n\nFor information about System Center Virtual Machine Manager (VMM), see Virtual\nMachine Manager. VMM is a Microsoft product for managing a virtualized data\ncenter that is sold separately.\n\n## Maximums for virtual machines\n\nThese maximums apply to each virtual machine when the host is run the selected\nproduct version. The guest operating system might support less than the\nvirtual machine maximum. Not all components are available in both generations\nof virtual machines. For a comparison of the generations, see Should I create\na generation 1 or 2 virtual machine in Hyper-V?\n\nComponent| Maximum| Notes  \n---|---|---  \nCheckpoints| 50| The actual number might be lower, depending on the available\nstorage. Each checkpoint is stored as an .avhd file that uses physical\nstorage.  \nMemory|\n\n  * 240 TB for generation 2\n  * 1 TB for generation 1\n\n| Review the requirements for the specific operating system to determine the\nminimum and recommended amounts.  \nSerial (COM) ports| 2| None.  \nSize of physical disks attached directly to a virtual machine| Varies| Maximum\nsize is determined by the guest operating system.  \nVirtual Fibre Channel adapters| 4| As a best practice, we recommended that you\nconnect each virtual Fibre Channel Adapter to a different virtual SAN.  \nVirtual floppy devices| 1 virtual floppy drive| None.  \nVirtual hard disk capacity|\n\n  * 64 TB for VHDX format\n  * 2,040 GB for VHD format\n\n| Each virtual hard disk is stored on physical media as either a .vhdx or a\n.vhd file, depending on the format used by the virtual hard disk.  \nVirtual IDE disks| 4| The startup disk (sometimes called the boot disk) must\nbe attached to one of the IDE devices. The startup disk can be either a\nvirtual hard disk or a physical disk attached directly to a virtual machine.  \nVirtual processors|\n\n  * 2048 for generation 2\n  * 64 for generation 1\n\n| The number of virtual processors supported by a guest operating system might\nbe lower. For details, see the information published for the specific\noperating system.  \nVirtual SCSI controllers| 4| Use of virtual SCSI devices requires integration\nservices, which are available for supported guest operating systems. For\ndetails on which operating systems are supported, see Supported Linux and\nFreeBSD virtual machines and Supported Windows guest operating systems.  \nVirtual SCSI disks| 256| Each SCSI controller supports up to 64 disks, which\nmeans that each virtual machine can be configured with as many as 256 virtual\nSCSI disks. (4 controllers x 64 disks per controller)  \nVirtual network adapters| 68 adapters total:\n\n  * 64 Hyper-V specific network adapters\n  * 4 legacy network adapters;\n\n| The Hyper-V specific network adapter provides better performance and\nrequires a driver included in integration services. For more information, see\nPlan for Hyper-V networking in Windows Server.  \n  \nTip\n\nThis tables also applies to Azure Stack HCI version 21H2, 22H2, and 23H2.\n\nComponent| Maximum| Notes  \n---|---|---  \nCheckpoints| 50| The actual number might be lower, depending on the available\nstorage. Each checkpoint is stored as an .avhd file that uses physical\nstorage.  \nMemory|\n\n  * 240 TB for generation 2\n  * 1 TB for generation 1\n\n| Review the requirements for the specific operating system to determine the\nminimum and recommended amounts.  \nSerial (COM) ports| 2| None.  \nSize of physical disks attached directly to a virtual machine| Varies| Maximum\nsize is determined by the guest operating system.  \nVirtual Fibre Channel adapters| 4| As a best practice, we recommended that you\nconnect each virtual Fibre Channel Adapter to a different virtual SAN.  \nVirtual floppy devices| 1 virtual floppy drive| None.  \nVirtual hard disk capacity|\n\n  * 64 TB for VHDX format\n  * 2,040 GB for VHD format\n\n| Each virtual hard disk is stored on physical media as either a .vhdx or a\n.vhd file, depending on the format used by the virtual hard disk.  \nVirtual IDE disks| 4| The startup disk (sometimes called the boot disk) must\nbe attached to one of the IDE devices. The startup disk can be either a\nvirtual hard disk or a physical disk attached directly to a virtual machine.  \nVirtual processors|\n\n  * 1,024 for generation 2\n  * 64 for generation 1\n\n| The number of virtual processors supported by a guest operating system might\nbe lower. For details, see the information published for the specific\noperating system.  \nVirtual SCSI controllers| 4| Use of virtual SCSI devices requires integration\nservices, which are available for supported guest operating systems. For\ndetails on which operating systems are supported, see Supported Linux and\nFreeBSD virtual machines and Supported Windows guest operating systems.  \nVirtual SCSI disks| 256| Each SCSI controller supports up to 64 disks, which\nmeans that each virtual machine can be configured with as many as 256 virtual\nSCSI disks. (4 controllers x 64 disks per controller)  \nVirtual network adapters| 68 adapters total:\n\n  * 64 Hyper-V specific network adapters\n  * 4 legacy network adapters;\n\n| The Hyper-V specific network adapter provides better performance and\nrequires a driver included in integration services. For more information, see\nPlan for Hyper-V networking in Windows Server.  \n  \nComponent| Maximum| Notes  \n---|---|---  \nCheckpoints| 50| The actual number might be lower, depending on the available\nstorage. Each checkpoint is stored as an .avhd file that uses physical\nstorage.  \nMemory|\n\n  * 12 TB for generation 2\n  * 1 TB for generation 1\n\n| Review the requirements for the specific operating system to determine the\nminimum and recommended amounts.  \nSerial (COM) ports| 2| None.  \nSize of physical disks attached directly to a virtual machine| Varies| Maximum\nsize is determined by the guest operating system.  \nVirtual Fibre Channel adapters| 4| As a best practice, we recommended that you\nconnect each virtual Fibre Channel Adapter to a different virtual SAN.  \nVirtual floppy devices| 1 virtual floppy drive| None.  \nVirtual hard disk capacity|\n\n  * 64 TB for VHDX format\n  * 2,040 GB for VHD format\n\n| Each virtual hard disk is stored on physical media as either a .vhdx or a\n.vhd file, depending on the format used by the virtual hard disk.  \nVirtual IDE disks| 4| The startup disk (sometimes called the boot disk) must\nbe attached to one of the IDE devices. The startup disk can be either a\nvirtual hard disk or a physical disk attached directly to a virtual machine.  \nVirtual processors|\n\n  * 250 for generation 2\n  * 64 for generation 1\n\n| The number of virtual processors supported by a guest operating system might\nbe lower. For details, see the information published for the specific\noperating system.  \nVirtual SCSI controllers| 4| Use of virtual SCSI devices requires integration\nservices, which are available for supported guest operating systems. For\ndetails on which operating systems are supported, see Supported Linux and\nFreeBSD virtual machines and Supported Windows guest operating systems.  \nVirtual SCSI disks| 256| Each SCSI controller supports up to 64 disks, which\nmeans that each virtual machine can be configured with as many as 256 virtual\nSCSI disks. (4 controllers x 64 disks per controller)  \nVirtual network adapters| 68 adapters total:\n\n  * 64 Hyper-V specific network adapters\n  * 4 legacy network adapters;\n\n| The Hyper-V specific network adapter provides better performance and\nrequires a driver included in integration services. For more information, see\nPlan for Hyper-V networking in Windows Server.  \n  \nComponent| Maximum| Notes  \n---|---|---  \nCheckpoints| 50| The actual number might be lower, depending on the available\nstorage. Each checkpoint is stored as an .avhd file that uses physical\nstorage.  \nMemory|\n\n  * 12 TB for generation 2\n  * 1 TB for generation 1\n\n| Review the requirements for the specific operating system to determine the\nminimum and recommended amounts.  \nSerial (COM) ports| 2| None.  \nSize of physical disks attached directly to a virtual machine| Varies| Maximum\nsize is determined by the guest operating system.  \nVirtual Fibre Channel adapters| 4| As a best practice, we recommended that you\nconnect each virtual Fibre Channel Adapter to a different virtual SAN.  \nVirtual floppy devices| 1 virtual floppy drive| None.  \nVirtual hard disk capacity|\n\n  * 64 TB for VHDX format\n  * 2,040 GB for VHD format\n\n| Each virtual hard disk is stored on physical media as either a .vhdx or a\n.vhd file, depending on the format used by the virtual hard disk.  \nVirtual IDE disks| 4| The startup disk (sometimes called the boot disk) must\nbe attached to one of the IDE devices. The startup disk can be either a\nvirtual hard disk or a physical disk attached directly to a virtual machine.  \nVirtual processors|\n\n  * 240 for generation 2\n  * 64 for generation 1\n\n| The number of virtual processors supported by a guest operating system might\nbe lower. For details, see the information published for the specific\noperating system.  \nVirtual SCSI controllers| 4| Use of virtual SCSI devices requires integration\nservices, which are available for supported guest operating systems. For\ndetails on which operating systems are supported, see Supported Linux and\nFreeBSD virtual machines and Supported Windows guest operating systems.  \nVirtual SCSI disks| 256| Each SCSI controller supports up to 64 disks, which\nmeans that each virtual machine can be configured with as many as 256 virtual\nSCSI disks. (4 controllers x 64 disks per controller)  \nVirtual network adapters| 12 adapters total:\n\n  * 8 Hyper-V specific network adapters\n  * 4 legacy network adapters\n\n| The Hyper-V specific network adapter provides better performance and\nrequires a driver included in integration services. For more information, see\nPlan for Hyper-V networking in Windows Server.  \n  \n## Maximums for Hyper-V hosts\n\nThese maximums apply to each Hyper-V host running the selected product\nversion.\n\nComponent| Maximum| Notes  \n---|---|---  \nLogical processors| 2,048| Both of these features must be enabled in the\nfirmware:\n\n  * Hardware-assisted virtualization\n  * Hardware-enforced Data Execution Prevention (DEP)\n\n  \nMemory|\n\n  * 4 PB for hosts that support 5-level paging\n  * 256 TB for hosts that support 4-level paging\n\n| None.  \nNetwork adapter teams (NIC Teaming)| No limits imposed by Hyper-V.| None.  \nPhysical network adapters| No limits imposed by Hyper-V.| None.  \nRunning virtual machines per server| 1024| None.  \nStorage| Limited by what is supported by the host operating system. No limits\nimposed by Hyper-V.| Note: Microsoft supports network-attached storage (NAS)\nwhen using SMB 3.0. NFS-based storage isn't supported.  \nVirtual network switch ports per server| Varies; no limits imposed by\nHyper-V.| The practical limit depends on the available computing resources.  \nVirtual processors available the host| 2,048| The limit is applied to the host\noperating system (root partition)  \nVirtual processors per logical processor| No ratio imposed by Hyper-V.| None.  \nVirtual processors per server| 2048| None.  \nVirtual storage area networks (SANs)| No limits imposed by Hyper-V.| None.  \nVirtual switches| Varies; no limits imposed by Hyper-V.| The practical limit\ndepends on the available computing resources.  \n  \nComponent| Maximum| Notes  \n---|---|---  \nLogical processors| 1,024| Both of these features must be enabled in the\nfirmware:\n\n  * Hardware-assisted virtualization\n  * Hardware-enforced Data Execution Prevention (DEP)\n\n  \nMemory|\n\n  * 4 PB for hosts that support 5-level paging\n  * 256 TB for hosts that support 4-level paging\n\n| None.  \nNetwork adapter teams (NIC Teaming)| No limits imposed by Hyper-V.| None.  \nPhysical network adapters| No limits imposed by Hyper-V.| None.  \nRunning virtual machines per server| 1024| None.  \nStorage| Limited by what is supported by the host operating system. No limits\nimposed by Hyper-V.| Note: Microsoft supports network-attached storage (NAS)\nwhen using SMB 3.0. NFS-based storage isn't supported.  \nVirtual network switch ports per server| Varies; no limits imposed by\nHyper-V.| The practical limit depends on the available computing resources.  \nVirtual processors available the host| 1,024| The limit is applied to the host\noperating system (root partition)  \nVirtual processors per logical processor| No ratio imposed by Hyper-V.| None.  \nVirtual processors per server| 2048| None.  \nVirtual storage area networks (SANs)| No limits imposed by Hyper-V.| None.  \nVirtual switches| Varies; no limits imposed by Hyper-V.| The practical limit\ndepends on the available computing resources.  \n  \nComponent| Maximum| Notes  \n---|---|---  \nLogical processors| 512| Both of these features must be enabled in the\nfirmware:\n\n  * Hardware-assisted virtualization\n  * Hardware-enforced Data Execution Prevention (DEP)\n\n  \nMemory| 24 TB| None.  \nNetwork adapter teams (NIC Teaming)| No limits imposed by Hyper-V.| None.  \nPhysical network adapters| No limits imposed by Hyper-V.| None.  \nRunning virtual machines per server| 1024| None.  \nStorage| Limited by what is supported by the host operating system. No limits\nimposed by Hyper-V.| Note: Microsoft supports network-attached storage (NAS)\nwhen using SMB 3.0. NFS-based storage isn't supported.  \nVirtual network switch ports per server| Varies; no limits imposed by\nHyper-V.| The practical limit depends on the available computing resources.  \nVirtual processors available the host| 320| The limit is applied to the host\noperating system (root partition)  \nVirtual processors per logical processor| No ratio imposed by Hyper-V.| None.  \nVirtual processors per server| 2048| None.  \nVirtual storage area networks (SANs)| No limits imposed by Hyper-V.| None.  \nVirtual switches| Varies; no limits imposed by Hyper-V.| The practical limit\ndepends on the available computing resources.  \n  \nComponent| Maximum| Notes  \n---|---|---  \nLogical processors| 512| Both of these features must be enabled in the\nfirmware:\n\n  * Hardware-assisted virtualization\n  * Hardware-enforced Data Execution Prevention (DEP)\n\n  \nMemory| 24 TB| None.  \nNetwork adapter teams (NIC Teaming)| No limits imposed by Hyper-V.| None.  \nPhysical network adapters| No limits imposed by Hyper-V.| None.  \nRunning virtual machines per server| 1024| None.  \nStorage| Limited by what is supported by the host operating system. No limits\nimposed by Hyper-V.| Note: Microsoft supports network-attached storage (NAS)\nwhen using SMB 3.0. NFS-based storage isn't supported.  \nVirtual network switch ports per server| Varies; no limits imposed by\nHyper-V.| The practical limit depends on the available computing resources.  \nVirtual processors available the host| 320| The limit is applied to the host\noperating system (root partition)  \nVirtual processors per logical processor| No ratio imposed by Hyper-V.| None.  \nVirtual processors per server| 2048| None.  \nVirtual storage area networks (SANs)| No limits imposed by Hyper-V.| None.  \nVirtual switches| Varies; no limits imposed by Hyper-V.| The practical limit\ndepends on the available computing resources.  \n  \n## Failover Clusters and Hyper-V\n\nThis table lists the maximums that apply when using Hyper-V and Failover\nClustering. It's important to do capacity planning to ensure that there's\nenough hardware resources to run all the virtual machines in a clustered\nenvironment.\n\nComponent| Maximum| Notes  \n---|---|---  \nNodes per cluster| 64| Consider the number of nodes you want to reserve for\nfailover, and maintenance tasks such as applying updates. We recommend that\nyou plan for enough resources to allow for 1 node to be reserved for failover.\nMeaning it remains idle until another node is failed over to it, sometimes\nreferred to as a passive node. You can increase this number if you want to\nreserve more nodes. There's no recommended ratio or multiplier of reserved\nnodes to active nodes; the only requirement is that the total number of nodes\nin a cluster can't exceed the maximum of 64.  \nRunning virtual machines per cluster and per node| 8,000 per cluster| Several\nfactors can affect the real number of virtual machines you can run at the same\ntime on one node, such as: - Amount of physical memory being used by each\nvirtual machine. - Networking and storage bandwidth. - Number of disk\nspindles, which affects disk I/O performance.  \n  \n## Feedback\n\nComing soon: Throughout 2024 we will be phasing out GitHub Issues as the\nfeedback mechanism for content and replacing it with a new feedback system.\nFor more information see: https://aka.ms/ContentUserFeedback.\n\nSubmit and view feedback for\n\nThis product This page\n\nView all page feedback\n\n## Additional resources\n\nTraining\n\nModule\n\nConfigure and manage Hyper-V virtual machines - Training\n\nConfigure and manage Hyper-V virtual machines\n\nCertification\n\nMicrosoft Certified: Windows Server Hybrid Administrator Associate -\nCertifications\n\nAs a Windows Server hybrid administrator, you integrate Windows Server\nenvironments with Azure services and manage Windows Server in on-premises\nnetworks.\n\nEvents\n\nMigrate to Innovate - Be AI Ready, Be Secure\n\nApr 16, 4 PM - Apr 16, 6 PM\n\nJoin us on April 16 to learn how migrating Windows Server and SQL Server to\nAzure fuels innovation.\n\nJoin now\n\nEnglish (United States)\n\nYour Privacy Choices\n\n  * Previous Versions\n  * Blog\n  * Contribute\n  * Privacy\n  * Terms of Use\n  * Trademarks\n  * \u00a9 Microsoft 2024\n\n## Additional resources\n\nEvents\n\nMigrate to Innovate - Be AI Ready, Be Secure\n\nApr 16, 4 PM - Apr 16, 6 PM\n\nJoin us on April 16 to learn how migrating Windows Server and SQL Server to\nAzure fuels innovation.\n\nJoin now\n\nTraining\n\nModule\n\nConfigure and manage Hyper-V virtual machines - Training\n\nConfigure and manage Hyper-V virtual machines\n\nCertification\n\nMicrosoft Certified: Windows Server Hybrid Administrator Associate -\nCertifications\n\nAs a Windows Server hybrid administrator, you integrate Windows Server\nenvironments with Azure services and manage Windows Server in on-premises\nnetworks.\n\nEnglish (United States)\n\nYour Privacy Choices\n\n  * Previous Versions\n  * Blog\n  * Contribute\n  * Privacy\n  * Terms of Use\n  * Trademarks\n  * \u00a9 Microsoft 2024\n\n", "frontpage": false}
