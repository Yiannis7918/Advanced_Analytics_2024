{"aid": "40076788", "title": "State of DevSecOps", "url": "https://www.datadoghq.com/state-of-devsecops/", "domain": "datadoghq.com", "votes": 1, "user": "ssully", "posted_at": "2024-04-18 14:40:29", "comments": 0, "source_title": "State of DevSecOps", "source_text": "State of DevSecOps | Datadog\n\n# State of DevSecOps\n\nresearch / security / appsec / cloud security / aws / google cloud / azure /\nsecurity research\n\nShipping secure code rapidly and at scale is a challenge across the software\nindustry, as evidenced by continued news of high-profile data breaches and\ncritical vulnerabilities. To address this challenge, organizations are\nincreasingly adopting DevSecOps, a practice in which application developers\nwork closely alongside operations and security teams throughout the\ndevelopment life cycle. DevSecOps looks at application security holistically,\nrecognizing that code must be secure not only in how it\u2019s written but also in\nhow it\u2019s deployed and run in production.\n\nWe analyzed tens of thousands of applications and container images and\nthousands of cloud environments to assess the security posture of applications\ntoday and evaluate the adoption of best practices that are at the core of\nDevSecOps\u2014infrastructure as code, automated cloud deployments, secure\napplication development practices, and the usage of short-lived credentials in\nCI/CD pipelines.\n\nOur findings demonstrate that modern DevOps practices go hand-in-hand with\nstrong security measures\u2014in fact, security helps drive operational excellence.\nOur research also shows that while security starts with visibility, securing\napplications is only realistic when practitioners are given enough context and\nprioritization to focus on what matters, without getting lost in the noise.\n\nFact 1\n\n## Java services are the most impacted by third-party vulnerabilities\n\nBy analyzing the security posture of a range of applications, written in a\nvariety of programming languages, we have identified that Java services are\nthe most affected by third-party vulnerabilities: 90 percent of Java services\nare vulnerable to one or more critical or high-severity vulnerabilities\nintroduced by a third-party library, versus an average of 47 percent for other\ntechnologies.\n\nJava services are also more likely to be vulnerable to real-world exploits\nwith documented use by attackers. The US Cybersecurity and Infrastructure\nSecurity Agency (CISA) keeps a running list of vulnerabilities that are\nexploited in the wild by threat actors, the Known Exploited Vulnerabilities\n(KEV) catalog. This continuously updated list is a good way to identify the\nmost impactful vulnerabilities that attackers are actively exploiting to\ncompromise systems. From the vulnerabilities in that list, we found that Java\nservices are overrepresented: 55 percent of Java services are affected, versus\n7 percent of those built using other languages.\n\nThe pattern of overrepresentation holds true even when focusing on specific\ncategories of vulnerabilities\u2014for example, 23 percent of Java services are\nvulnerable to remote code execution (RCE), impacting 42 percent of\norganizations. The prevalence of impactful vulnerabilities in popular Java\nlibraries\u2014including Tomcat, the Spring Framework, Apache Struts, Log4j, and\nActiveMQ\u2014may partly explain these high numbers. The most common\nvulnerabilities are:\n\n  * Spring4Shell (CVE-2022-22965)\n  * Log4Shell (CVE-2021-45046 and CVE-2021-44228)\n  * An RCE in Apache ActiveMQ (CVE-2023-46604)\n\nThe hypothesis is reinforced when we examine where these vulnerabilities\ntypically originate. In Java, 63 percent of high and critical vulnerabilities\nderive from indirect dependencies\u2014i.e., third-party libraries that have been\nindirectly packaged with the application. These vulnerabilities are typically\nmore challenging to identify, as the additional libraries in which they appear\nare often introduced into an application unknowingly.\n\nIt\u2019s therefore essential to consider the full dependency tree, not only direct\ndependencies, when scanning for application vulnerabilities. It\u2019s also\nessential to know whether any new dependency added to an application is well-\nmaintained and frequently upgrades its own dependencies. Frameworks such as\nthe OpenSSF Scorecard are helpful to quickly assess the health of open source\nlibraries.\n\n> \u201cThe attack surface of your organization isn't just the public-facing code\n> you've written\u2014it\u2019s also your applications\u2019 dependencies, both direct and\n> indirect. How are you tracking vulnerabilities in these dependencies?\n> Furthermore, are you able to alert early upon compromise? How do you limit\n> the damage? While most vulnerabilities aren't worth prioritizing, those\n> vulnerabilities with strong exploitation potential are often found in\n> overprivileged applications with long-lived credentials, increasing their\n> potential severity.\u201d\n\nJeff McJunkin Founder at Rogue Valley Information Security and SANS Instructor\n\nFact 2\n\n## Attack attempts from automated security scanners are mostly unactionable\nnoise\n\nWe analyzed a large number of exploitation attempts against applications\nacross various languages. We found that attacks coming from automated security\nscanners represent by far the largest number of exploitation attempts. These\nscanners are generally open source tools that attackers attempt to run at\nscale, scanning the whole internet to identify vulnerable systems. Popular\nexamples of these tools include Nuclei, ZGrab, and SQLmap.\n\nWe identified that the vast majority of attacks performed by automated\nsecurity scanners are harmless and only generate noise for defenders. Out of\ntens of millions of malicious requests that we identified coming from such\nscanners, only 0.0065 percent successfully triggered a vulnerability. This\nshows that it\u2019s critical to have a strong framework for alert prioritization\nin order to enable defenders to effectively monitor raw web server logs or\nperimeter web application firewall (WAF) alerts. Integrating threat\nintelligence and application runtime context into security detections can help\norganizations filter for the most critical threats.\n\nFact 3\n\n## Only a small portion of identified vulnerabilities are worth prioritizing\n\nIn 2023, over 4,000 high and 1,000 critical vulnerabilities were identified\nand inventoried in the Common Vulnerabilities and Exposures (CVE) project.\nThrough our research, we\u2019ve found that the average service is vulnerable to 19\nsuch vulnerabilities. However, according to past academic research, only\naround 5 percent of vulnerabilities are exploited by attackers in the wild.\n\nGiven these numbers, it\u2019s easy to see why practitioners are overwhelmed with\nthe amount of vulnerabilities they face, and why they need prioritization\nframeworks to help them focus on what matters. We analyzed a large number of\nvulnerabilities and computed an \u201cadjusted score\u201d based on several additional\nfactors to determine the likelihood and impact of a successful exploitation:\n\n  * Is the vulnerable service publicly exposed to the internet?\n  * Does it run in production, as opposed to a development or test environment?\n  * Is there exploit code available online, or instructions on how to exploit the vulnerability?\n\nWe also considered the Exploit Prediction Scoring System (EPSS) score, giving\nmore weight to vulnerabilities that scored higher on this metric. We applied\nthis methodology to all vulnerabilities to assess how many would remain\ncritical based on their adjusted score. We identified that, after applying our\nadjusted scoring, 63 percent of organizations that had vulnerabilities with a\ncritical CVE severity no longer have any critical vulnerabilities. Meanwhile,\n30 percent of organizations see their number of critical vulnerabilities\nreduced by half or more.\n\nWhen determining which vulnerabilities to prioritize, organizations should\nadopt a framework that enables them to consistently evaluate issue severity.\nGenerally, a vulnerability is more serious if:\n\n  1. The impacted service is publicly exposed\n  2. The vulnerability is running in production\n  3. There is exploit code publicly available\n\nWhile other vulnerabilities might still carry risk, they should likely be\naddressed only after issues that meet these three criteria.\n\nFact 4\n\n## Lightweight container images lead to fewer vulnerabilities\n\nIn software development and security alike, less is often more. This is\nparticularly true in the context of third-party dependencies, such as\ncontainer base images. There are typically multiple options for choosing a\nbase image, including:\n\n  * Using a large image based on a classic Linux distribution, such as Ubuntu\n  * Using a slimmer image based on a lightweight distribution, such as Alpine Linux or BusyBox\n  * Using a distroless image, which contains only the minimum runtime necessary to run the application\u2014and sometimes, nothing other than the application itself\n\nBy analyzing thousands of container images, we identified that the smaller a\ncontainer image is, the fewer vulnerabilities it is likely to have\u2014likely\nbecause it contains fewer third-party libraries. On average, container images\nsmaller than 100 MB have 4.4 high or critical vulnerabilities, versus 42.2 for\nimages between 250 and 500 MB, and almost 80 for images larger than that.\n\nThis demonstrates that in containerized environments, using lightweight images\nis a critical practice for minimizing the attack surface, as it helps reduce\nthe number of third-party libraries and operating system packages that an\napplication depends on. In addition, thin images lead to reduced storage needs\nand network traffic, as well as faster deployments. Finally, lightweight\ncontainer images help minimize the tooling available to an attacker\u2014including\nsystem utilities such as curl or wget\u2014which makes exploiting many types of\nvulnerabilities more challenging.\n\n> \u201cWhen I\u2019m playing the role of the threat actor in container environments\n> like Kubernetes, container images built on distroless make my day harder.\u201d\n\nJay Beale CEO at consulting firm InGuardians\n\nFact 5\n\n## Adoption of infrastructure as code is high, but varies across cloud\nprovider\n\nThe concept of infrastructure as code (IaC) was initially introduced in the\n1990s with projects like CFEngine, Puppet, and Chef. After public cloud\ncomputing gained popularity, IaC quickly became a de facto standard for\nprovisioning cloud environments. IaC brings considerable benefits for\noperations, including version control, traceability, and reproducibility\nacross environments. Its declarative nature also helps DevOps teams understand\nthe desired state, as opposed to reading neverending bash scripts that\ndescribe how to get there.\n\nIaC is also considered a critical practice when securing cloud production\nenvironments, as it helps ensure that:\n\n  * All changes are peer-reviewed\n  * Human operations have limited permissions on production environments, since deployments are handled by a CI/CD pipeline\n  * Organizations can scan IaC code for weak configurations, helping them identify issues before they reach production\n\nWe identified that in AWS, over 71 percent of organizations use IaC through at\nleast one popular IaC technology such as Terraform, CloudFormation, or Pulumi.\nThis number is lower in Google Cloud, at 55 percent.\n\nNote that we cannot report on Azure, since Azure Activity Logs don\u2019t log HTTP\nuser agents.\n\nAcross AWS and Google Cloud, Terraform is the most popular technology, coming\njust before cloud-specific IaC tools, namely CloudFormation and Google\nDeployment Manager.\n\nFact 6\n\n## Manual cloud deployments are still widespread\n\nHumans, thankfully, are not machines\u2014and this means we are bound to make\nmistakes. A major element of quality control, and by extension security, is to\nautomate repetitive tasks that can be taken out of human hands.\n\nIn cloud production environments, a CI/CD pipeline is typically responsible\nfor deploying changes to infrastructure and applications. The automation that\ntakes place in this pipeline can be done with IaC tools or through scripts\nusing cloud provider\u2013specific tooling.\n\nAutomation ensures that engineers don\u2019t need constant privileged access to the\nproduction environment, and that deployments are properly tracked and peer-\nreviewed. The opposite of this best practice\u2014taking actions manually from the\ncloud console\u2014is often referred to as click operations, or ClickOps.\n\nBy analyzing CloudTrail logs, we identified that at least 38 percent of\norganizations in AWS had used ClickOps in all their AWS accounts within a\n14-day window preceding the writing of this study. According to our\ndefinition, this means that these organizations had deployed workloads or\ntaken sensitive actions manually through the AWS Management Console\u2014including\nin their production environments\u2014during this period of time.\n\nFact 7\n\n## Usage of short-lived credentials in CI/CD pipelines is still too low\n\nIn cloud environments, leaks of long-lived credentials are one of the most\ncommon causes for data breaches. CI/CD pipelines increase this attack surface\nbecause they typically have privileged permissions and their credentials could\nleak through excessive logging, compromised software dependencies, or build\nartifacts\u2014similar to the codecov breach. This makes using short-lived\ncredentials for CI/CD pipelines one of the most critical aspects of securing a\ncloud environment.\n\nHowever, we identified that a substantial number of organizations continue to\nrely on long-lived credentials in their AWS environments, even in cases where\nshort-lived ones would be both more practical and more secure. Across\norganizations using GitHub Actions\u2014representing over 31 percent of\norganizations running in AWS\u2014we found that only 37 percent exclusively use\n\u201ckeyless\u201d authentication based on short-lived credentials and OpenID Connect\n(OIDC). Meanwhile, 63 percent used IAM users (a form of long-lived credential)\nat least once to authenticate GitHub Actions pipelines, while 42 percent\nexclusively used IAM users.\n\nUsing keyless authentication in CI/CD pipelines is easier to set up and more\nsecure. Once again, this shows that good operational practices also tend to\nlead to better security outcomes.\n\n## Secure your applications and cloud resources with Datadog\n\n## Methodology\n\nFindings are based on data collected from February 2024 to April 2024.\n\n### Fact 1\n\nFor this fact, we analyzed vulnerabilities in third-party libraries of\napplications across various languages and runtimes (Java, .NET, PHP, Python,\nRuby, Javascript, and Go) and that use Datadog Application Security\nManagement\u2019s Software Composition Analysis feature.\n\nKnown exploited vulnerabilities are sourced from the CISA KEV catalog, which\nwe extracted on April 10, 2024.\n\nWe classified each vulnerability as coming from a direct or transitive\ndependency. Note that this fact only focuses on Java applications, because we\ncurrently only support making the distinction between direct and transitive\ndependencies for JVM-based services.\n\n### Fact 2\n\nWe analyzed suspicious and malicious requests identified by Datadog ASM Threat\nManagement coming from security scanners, as defined by our out-of-the-box\nrule. Based on dynamic instrumentation, we then determined which\nvulnerabilities were successfully exploited. We only considered malicious\nrequests that we could definitively determine were either successfully\nexploited or were harmless.\n\n### Fact 3\n\nSimilarly to fact 1, we analyzed vulnerabilities in third-party libraries of\napplications that use Datadog Application Security Management\u2019s Software\nComposition Analysis feature.\n\nWe considered vulnerabilities with a \u201ccritical\u201d CVSSv3 base score and, based\non the context available, used the following methodology to compute the\n\u201ctemporal\u201d and \u201cenvironmental\u201d CVSSv3 metrics:\n\n  * When the service was running in a non-production environment, we adjusted the \u201cmodified confidentiality, integrity, and availability impact\u201d to \u201cLow.\u201d\n  * When the service was not publicly exposed on the internet and the exploit vector was \u201cNetwork,\u201d we set the \u201cmodified attack vector\u201d to \u201cLocal.\u201d\n  * When the EPSS Score was below 1 percent, we set the \u201cmodified attack complexity\u201d to \u201cHigh.\u201d\n  * When a public exploit was available, we set the \u201cexploit code maturity\u201d to \u201cProof of concept\u201d or to \u201cUnproven\u201d otherwise.\n\nWe then computed the adjusted score based on the CVSS v3.1 methodology and\nconsidered the ratio of vulnerabilities whose adjusted score was still\n\u201ccritical\u201d (above 9.0).\n\n### Fact 4\n\nWe analyzed data of containers scanned through Datadog Cloud Security\nManagement (CSM) Vulnerability Management and reviewed any identified OS-level\nvulnerabilities. This includes both publicly available images and images from\nprivate registries.\n\n### Fact 5\n\nFor this fact, we analyzed cloud activity logs from AWS and Google Cloud\nenvironments (AWS CloudTrail and Google Cloud Admin Activity Logs) and\ndetermined which IaC technology was used based on the HTTP user agent.\n\nThis fact contains no data for Azure because Azure Activity Logs do not\ninclude the HTTP user agent.\n\nNote: The data window for this fact is from January 1 to February 18, 2024. If\nan organization did not use a known IaC technology during this period, we\ncounted it as \u201cnot using IaC.\u201d\n\n### Fact 6\n\nFor this fact, we focused on AWS and analyzed AWS CloudTrail logs.\nSpecifically, we defined that an organization is \u201cperforming manual cloud\ndeployments through the AWS Console\u201d if we found at least one of the events\nfrom the list below in all the AWS accounts they monitor with Datadog. Because\nwe assume that every organization monitors at least one production account\nwith Datadog, we\u2019re able to determine that organizations that meet this\ncriteria use ClickOps in their production environment.\n\n    \n    \n    RunInstances AuthorizeSecurityGroupIngress CreateVpc CreateCluster CreateDBCluster CreateDBInstance CreateInstances CreateKeyPair RegisterTaskDefinition\n\nWe then filtered to identify when these events were performed manually from\nthe AWS Console using the methodology described by Arkadiy Tetelman.\n\n### Fact 7\n\nTo identify organizations that use GitHub Actions with OIDC authentication, we\nqueried AWS CloudTrail logs using the following Datadog logs query:\n@eventName:AssumeRoleWithWebIdentity\n@userIdentity.identityProvider:*token.actions.githubusercontent.com*\n-status:error.\n\nTo identify organizations that use GitHub Actions with IAM users, we:\n\n  1. Queried AWS CloudTrail logs using the following Datadog logs query: @userIdentity.accessKeyId:AKIA* @userIdentity.type:IAMUser -status:error\n  2. Filtered the results on source IPs known to be used by GitHub Actions, as determined by GitHub\u2019s API endpoint\n\n### Licensing\n\nReport: CC BY-ND\n\nImages: CC BY\n\n\u00a9 Datadog 2024\n\nTERMS\n\nPRIVACY\n\nCookie Preferences\n\nGet Started with Datadog\n\nWE VALUE YOUR PRIVACY\n\nWe use cookies and similar technologies (\"Cookies\") to provide and improve our\nservices and for marketing, as outlined in our Privacy Policy and Cookie\nPolicy. By clicking \"Accept All,\" you agree that you consent to our use of\nCookies.\n\nPrivacy Policy\n\nPowered by:\n\n", "frontpage": false}
