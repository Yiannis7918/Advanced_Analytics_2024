{"aid": "40089658", "title": "First impressions of early-access GPT-4 fine-tuning", "url": "https://www.supersimple.io/blog/gpt-4-fine-tuning-early-access", "domain": "supersimple.io", "votes": 17, "user": "gmays", "posted_at": "2024-04-19 17:31:40", "comments": 0, "source_title": "First Impressions of Early-Access GPT-4 Fine-Tuning | Supersimple", "source_text": "First Impressions of Early-Access GPT-4 Fine-Tuning | Supersimple\n\nLog inGet started\n\n# First Impressions of Early-Access GPT-4 Fine-Tuning\n\nMarko Klopets\n\nCo-Founder & CEO, Supersimple\n\nMarch 19, 2024\n\nA few weeks ago we finally got access to the GPT-4 fine-tuning API (in limited\nearly access), and were super excited to check out how well it works. We\u2019d\nbeen a user of OpenAI\u2019s fine-tuned models since fine-tuning the original GPT-3\ndavinci model first became available.\n\nUnsurprisingly, a fine-tuned GPT-4 greatly outperforms fine-tuned GPT-3.5\n(more than 50% improvement for our use case!). We\u2019ll go deeper into how we use\nthese models below, but let\u2019s start by comparing several of the OpenAI-based\nmodels we use at Supersimple:\n\n  * Fine tuned (FT) GPT-3 Davinci model, which we used at the beginning\n  * GPT-3.5 and GPT-4 base models\n  * GPT-3.5 and GPT-4, fine-tuned models using a custom proprietary data set\n\nRelative model performance based on our test set; FT signifies a fine-tuned\nmodel\n\nThe models in question were fine-tuned for a domain-specific use case of\nasking data questions in natural language. The LLMs are tasked with putting\ntogether the appropriate reports and underlying queries for the user\u2019s\nquestion.\n\nThese evaluations were performed using a proprietary test data set we use at\nSupersimple; Davinci\u2019s (GPT-3) performance is taken as a baseline. To give\nmore insight into where we see these models working well, let\u2019s give a bit\nmore context on what we actually do. We\u2019ll also include more comparative data,\nincluding cost and latency, below!\n\n## Context & our LLM usage at Supersimple\n\nSupersimple is a data analytics platform that allows users to dive deep into\ntheir data, to answer complex ad-hoc questions in seconds and to uncover deep\ndata insights.\n\nOur users can ask natural language (plain English) questions about their data,\nand get answers in the form of tables and visualizations. The AI explains its\nwork using a few no-code steps, and the user can always continue to go even\ndeeper \u2013 either using natural language or a few clicks on our data platform.\n\nWe use LLMs to answer our users\u2019 natural language questions, with the goal of\nproviding a great starting point for further deep dives into the data. The LLM\ntakes the user's question, relevant parts of their semantic data model,\nexisting reports and dashboards, and generates an exploration that answers the\nquestion with the most context possible.\n\nThis feature is also particularly useful as an onboarding tool, which\nshowcases the capabilities of our platform and teaches users to use our data\nexploration UI. For long-time users, plain English is at times still the\nfastest way to answer questions, but at times it\u2019s just as quick for them to\njust click a few buttons.\n\n## Implementation\n\nTechnically speaking, the LLM outputs a custom high-level markup language\n(DSL) that we designed for our specific use case, that\u2019s token-efficient and\nwell-aligned with pre-trained language models. This DSL gets compiled into\nJSON, that we in turn compile into one or more database queries to be\nexecuted. Finally, we render an interactive data exploration to the user based\non the report structure dictated by the DSL.\n\nIt's important to highlight that this task is quite different from the\ntraditional text-to-SQL problem for a number of reasons:\n\n  * The output is a no-code exploration, which is transparent and explainable to the user. This means that the model directly interacts with our data platform. We believe that generating & showing code or SQL is a terrible way to prove/explain reports to users \u2013 even if those users are technical, but especially if they aren\u2019t.\n  * The resulting report/exploration is easily editable in the UI, and is meant to serve as a starting point for further deep dives. When exploring data, you won\u2019t want to think about refactoring SQL queries.\n  * The complex output is broken into individual blocks that represent logical steps in the reasoning process.\n  * We offload the complexity of constructing correct SQL queries with many joins, CTEs and subqueries to the platform\u2019s \u201cquery engine\u201d, which deterministically compiles the semantic steps output by the model into valid and performant SQL. This means the LLM doesn\u2019t need to worry about an entire class of problems, allowing it to perform better overall.\n  * When generating output, the model takes into account existing dashboards and user-defined concepts.\n\nDue to the complexity of the task and the need for a custom output format (a\nDSL that the AI uses to effectively construct app UI state), we found that\nfine-tuned models tend to perform significantly better than the base ones.\n\nTo reiterate: we never generate SQL using LLMs \u2013 our LLMs only interact with\nour app directly.\n\n## Fine-tuning and benchmarking\n\nFor fine-tuning, we use a proprietary dataset that contains tens of millions\nof tokens worth of question-answering examples, with combinations of data\nmodels, questions and the perfect output reports to answer these questions.\n\nGPT-3.5 (gpt-3.5-turbo-0125) and GPT-4 (gpt-4-0613, the only fine-tunable\nGPT-4 model available at the moment) were fine-tuned for 3 epochs.\n\nFor both of the base models here (gpt-3.5-turbo-0125 and gpt-4-0125-preview),\nwe used the same prompt, containing 8-shot examples. Performance of all models\nexcept davinci was evaluated in the beginning of March 2024 while the\nperformance of davinci was last evaluated back in August 2023.\n\nThe performance comparison is shown in the next table. For our test set, fine-\ntuned (FT) GPT-4 outperformed GPT-3.5 by 56%, a significant, albeit slightly\nsmaller improvement than the jump from Davinci FT to GPT-3.5 FT (96%).\n\nFine-tuned GPT-4 is slightly slower than fine-tuned GPT-3.5 (21.6 tokens/s vs\n31 tokens/s).\n\nNote: the original version of this article showed significantly worse latency\nstats for GPT-4: our first benchmarks only achieved 5.8 tokens/s, but OpenAI\nhas since greatly improved the service's stability and speed.\n\nWhile cost is significantly higher for GPT-4 (15x higher for inference, 11x\nhigher for training, compared to GPT-3.5), it might not be an important factor\nfor you depending on your use case.\n\nWe benchmarked models on our in-house test set of 100 diverse questions.\nExamples of questions and model performance are shown in the table below:\n\nQuestion| Davinci FT| GPT-3.5| GPT-4| GPT-3.5 FT| GPT-4 FT  \n---|---|---|---|---|---  \nUser with id 342122| \u2705| \u2705| \u2705| \u2705| \u2705  \nCompanies that are using the analytics dashboard?| \u2705| \u274c| \u2705| \u2705| \u2705  \nWhat's our ARR and how is it changing over time?| \u274c| \u274c| \u274c| \u2705| \u2705  \nCompanies whose subscriptions are expiring this month| \u274c| \u274c| \u274c| \u2705| \u2705  \nQoQ change in feature usage on account level| \u274c| \u274c| \u274c| \u274c| \u2705  \nChurn trends this year vs last year| \u274c| \u274c| \u274c| \u274c| \u2705  \nWhat are the main blockers in our onboarding funnel?| \u274c| \u274c| \u274c| \u274c| \u274c  \n  \nDespite the performance enhancements, models continue to struggle with broad\nand open-ended queries when trying to answer with a single completion.\n\nWorryingly, there is an observable trend of diminishing returns from fine-\ntuning. While fine-tuned Davinci showed marked improvement over its base\nmodel, fine-tuned GPT-3.5 offered lesser gains, and the progress achieved by\nfine-tuning GPT-4 was even smaller.\n\nTo address these limitations, in production, we seldom rely on a single model\ncall. Instead, we employ a mix of various specialized models, prompts, and\nheuristics to improve both accuracy and response time, achieving better\nperformance than any individual model.\n\n## Conclusion\n\nHaving used fine-tuned LLMs in production from the early days of GPT-3-davinci\nbeing publicly available, we were extremely excited to get to try out fine-\ntuning GPT-4. On the one hand, we were blown away by the performance gains\nover the previously-best GPT-3.5-FT.\n\nOn the other hand, we actually expected this, as the gap between the base\nversions of GPT-3.5 and GPT-4 was already immense.\n\nFine-tuned GPT-4 significantly outperforms both fine-tuned GPT-3.5 and vanilla\nGPT-4 on our test set. We saw comparable improvements to those that we saw\nwhen first switching from GPT-3-FT to GPT-3.5-FT last year.\n\nThe main issue at the moment with the largest models is higher latency. As our\ngoal is to make human interactions with data seamless and effortless, we can't\nhave users waiting behind an AI for more than a few seconds.\n\nBecause of the higher latency (and cost), we only use GPT-4-FT for a certain\nsubset of questions and for some of the most critical reasoning steps. For the\nrest, we use various other models, including GPT-3.5-FT.\n\nWith current state-of-the-art models, we believe that:\n\n  1. A single model with a single completion (or API call if using hosted LLMs) is not sufficient for many real-world applications that require non-trivial levels of reasoning capabilities\n  2. In a work context, it\u2019s critical for any AI to accurately explain its work to users (people don\u2019t even believe other humans before seeing proof!)\n\nRead more about our natural language question-answering AI on our website, or\nget started with our next-generation business intelligence platform today.\n\nFree guide\n\n## Get the free definitive guide to Business Intelligence for Startups\n\nWe interviewed hundreds of companies about why and how their data setups have\nfailed them and compiled their learnings into a single guide.\n\nLearn from their experience: \u2022 What a SaaS startup's data stack should look\nlike \u2022 How product, sales and customer success teams use data at leading\ncompanies \u2022 The difference between successful and failed self-service data\ninitiatives \u2022 How the modern data stack can make things harder, not easier \u2022\nAvoiding mistakes that make people mistrust their data \u2022 Enabling rapid\ncollaboration between data teams and everyone else\n\nWe'll email you the guide as soon as it's out!\n\nOops! Something went wrong while submitting the form.\n\nSupersimple helps B2B SaaS teams make better decisions using data.\n\nhi@supersimple.io\n\nQuick Links\n\nHomeData SecurityPricing and plansBlogBook a demo\n\nPlatform\n\nOverviewNatural Language AIAI Insight EngineEmbedded AnalyticsStatus\n\nCompany\n\nCareersContact usLinkedInPrivacy policyTerms & conditions\n\n", "frontpage": true}
