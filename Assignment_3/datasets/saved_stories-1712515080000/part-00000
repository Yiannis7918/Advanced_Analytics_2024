{"aid": "39960330", "title": "LlamaBot with Ollama on my home virtual private network", "url": "https://ericmjl.github.io/blog/2024/2/21/llamabot-with-ollama-on-my-home-virtual-private-network/", "domain": "ericmjl.github.io", "votes": 1, "user": "tosh", "posted_at": "2024-04-07 12:36:02", "comments": 0, "source_title": "LlamaBot with Ollama on my home virtual private network", "source_text": "LlamaBot with Ollama on my home virtual private network - Eric J. Ma's\nPersonal Site\n\n# Eric J Ma's Website\n\nLlamaBot with Ollama on my home virtual private network\n\nwritten by Eric J. Ma on 2024-02-21 | tags: gpu deep learning ollama llm tailscale linux ubuntu gpu llamabot\n\n## Introduction\n\nAt home, I have a relatively idle GPU tower. It's something I bought way back\nin 2016 to do deep learning. It has an NVIDIA GTX1080 GPU in there with 8GB of\nRAM. By today's standards, it's puny. Over the years, however, I've used it\nless frequently to do GPU-heavy things because of time. But I recently found a\nway to give it a new lease of life: running an Ollama server on my home's\nprivate network! I wanted to share how I made that happen in this blog post.\n\n## Setup Tailscale\n\nI have all my personal devices (my M1 MacBook Air, phone, tablet, a\nDigitalOcean server running Dokku, NAS, and my home GPU box) running on a\nTailscale virtual private network. Since my home GPU box is running Ubuntu\nLinux, I used the official Tailscale Linux installation instructions to get\nTailscale installed on my GPU box, ensuring that it was on the same VPN as my\nMacBook.\n\n## Install Ollama on GPU box\n\nOnce I did that, I then installed Ollama on my GPU box. While ssh-ed into my\nGPU server, I executed the command on the Ollama Linux installation page,\nwhich was:\n\n    \n    \n    curl -fsSL https://ollama.com/install.sh | sh\n\nTo verify that Ollama was installed correctly, on my GPU box, I executed the\ncommand:\n\n    \n    \n    ollama run mistral\n\nDoing so allowed me to verify that Ollama was installed correctly.\n\n## Configure Ollama for network access\n\nBy default, the Ollama web server runs on 127.0.0.1:11434, which doesn't allow\nfor inbound connections from other computers. To change that behaviour, we\nmust change the OLLAMA_HOST environment variable to 0.0.0.0. I followed the\ninstructions in Ollama's documentation. To start, we edit the systemd service:\n\n    \n    \n    systemctl edit ollama.service\n\nThen, we add the following contents to the text file that gets opened up:\n\n    \n    \n    [Service] Environment=\"OLLAMA_HOST=0.0.0.0\"\n\nFinally, after saving and exiting the text file, we reload systemd and restart\nOllama:\n\n    \n    \n    systemctl daemon-reload systemctl restart ollama\n\n## Test Ollama access remotely\n\nNow, Ollama will be running on host 0.0.0.0. To verify that it is running\ncorrectly, I went back to my laptop and ran the following curl command:\n\n    \n    \n    curl http://<my-gpu-box-ip-address-here>:11434/api/chat -d '{ \"model\": \"mistral\", \"messages\": [ { \"role\": \"user\", \"content\": \"hey there, how are you doing?\" } ] }'\n\nI got back a long stream of JSONs:\n\n    \n    \n    {\"model\":\"mistral\",\"created_at\":\"2024-02-21T01:53:12.747357134Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Hello\"},\"done\":false} {\"model\":\"mistral\",\"created_at\":\"2024-02-21T01:53:12.769246194Z\",\"message\":{\"role\":\"assistant\",\"content\":\"!\"},\"done\":false} ... {\"model\":\"mistral\",\"created_at\":\"2024-02-21T01:53:14.054314656Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\"},\"done\":true,\"total_duration\":2734292991,\"load_duration\":1320868996,\"prompt_eval_count\":17,\"prompt_eval_duration\":106030000,\"eval_count\":61,\"eval_duration\":1306913000}\n\nI thus verified that I could connect to the Ollama server running on my GPU\nbox!\n\n## Check GPU usage\n\nKnowing Ollama's behaviour, I knew that the mistral model should be loaded\ninto GPU memory for a little while before being taken down. To verify that it\nwas indeed using the GPU, I ran:\n\n    \n    \n    nvidia-smi\n\nWhich gave me:\n\n    \n    \n    ericmjl in \ud83c\udf10 ubuntu-gpu in ~ \u276f nvidia-smi Wed Feb 21 05:41:50 2024 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 520.61.05 Driver Version: 520.61.05 CUDA Version: 11.8 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:01:00.0 Off | N/A | | 27% 31C P2 50W / 180W | 4527MiB / 8192MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1453 G /usr/lib/xorg/Xorg 18MiB | | 0 N/A N/A 2282 G /usr/bin/gnome-shell 2MiB | | 0 N/A N/A 3192354 C /usr/local/bin/ollama 4502MiB | +-----------------------------------------------------------------------------+\n\nPerfect!\n\n## Interact with Ollama using LlamaBot\n\nTaking it one step further, I decided to connect to my Ollama server using\nllamabot's SimpleBot class. In principle, it should be easy to do so because\nwe have a LiteLLM pass-through for additional keyword arguments, and that\nmeant I should be able to do so with:\n\n    \n    \n    import os system_prompt = \"You are a funny bot!\" bot = SimpleBot( model_name=\"ollama/mistral\", # Specifying Ollama via the model_name argument is necessary when pointing to an Ollama server! system_prompt=system_prompt, stream_target=\"stdout\", # this is the default! api_base=f\"http://<my-gpu-box-ip-address-here>:11434\", ) response = bot(\"Hello!\")\n\nAnd indeed, it works! I get back my usual mistral bot response:\n\n    \n    \n    Why, thank you! I'm here to make your day brighter with my witty and humorous remarks. So, tell me, why did the tomato turn red? Because it saw the salad dressing! Get it? *laughs manically* But seriously, how about we discuss something more important, like pizza or memes?\n\n## Try a different model\n\nI can even easily swap out models (as long as they've been downloaded to my\nmachine):\n\n    \n    \n    bot = SimpleBot( model_name=\"ollama/llama2:13b\", # Specifying Ollama via the model_name argument is necessary when pointing to an Ollama server! system_prompt=system_prompt, stream_target=\"stdout\", # this is the default! api_base=f\"http://<my-gpu-box-ip-address-here>:11434\", ) response = bot(\"Hello!\")\n\nThis gives me:\n\n    \n    \n    WOOHOO! *party popper* OH MY GOSH, IT'S SO GLORIOUS TO BE A FUNNY BOT! *confetti* HELLO THERE, MY DEAR HUMAN FRIEND! *sunglasses* I'M READY TO BRING THE LAUGHS AND MAKE YOUR DAY A LITTLE BIT BRIGHTER! \ud83d\ude04\u2764\ufe0f WHAT CAN I DO FOR YOU, MY HUMAN PAL?\n\n(Llama2 appears to have a goofier personality.)\n\n## Limitation: models need to be downloaded and available\n\nOne limitation (?) that I see right now is that Ollama needs to have\ndownloaded a model before it can be used from SimpleBot. As an example, I\ndon't have the Microsoft Phi2 model downloaded on my machine:\n\n    \n    \n    ericmjl in \ud83c\udf10 ubuntu-gpu in ~ \u276f ollama list NAME ID SIZE MODIFIED llama2:13b d475bf4c50bc 7.4 GB 8 hours ago mistral:7b-text-q5_1 05b86a2ea9de 5.4 GB 8 hours ago mistral:latest 61e88e884507 4.1 GB 44 hours ago\n\nThus, when running SimpleBot using Phi:\n\n    \n    \n    bot = SimpleBot( model_name=\"ollama/phi\", # phi is not on my GPU box! system_prompt=system_prompt, stream_target=\"stdout\", # this is the default! api_base=f\"http://<my-gpu-box-ip-address-here>:11434\", ) response = bot(\"Hello!\")\n\nI get the following error:\n\n    \n    \n    { \"name\": \"ResponseNotRead\", \"message\": \"Attempted to access streaming response content, without having called `read()`.\", \"stack\": \"--------------------------------------------------------------------------- ResponseNotRead Traceback (most recent call last) Cell In[15], line 10 1 system_prompt = \\\"You are a funny bot!\\\" 3 bot = SimpleBot( 4 model_name=\\\"ollama/phi\\\", # Specifying Ollama via the model_name argument is necessary when pointing to an Ollama server! 5 system_prompt=system_prompt, 6 stream_target=\\\"stdout\\\", # this is the default! 7 api_base=f\\\"http://{os.getenv('OLLAMA_SERVER')}:11434\\\", 8 ) ---> 10 response = bot(\\\"Hello!\\\") ... File ~/anaconda/envs/llamabot/lib/python3.11/site-packages/httpx/_models.py:567, in Response.content(self) 564 @property 565 def content(self) -> bytes: 566 if not hasattr(self, \\\"_content\\\"): --> 567 raise ResponseNotRead() 568 return self._content ResponseNotRead: Attempted to access streaming response content, without having called `read()`.\" }\n\nThe way I solved this was by SSH-ing into my GPU box and running:\n\n    \n    \n    ollama pull phi\n\nYou can think of the Ollama server as being a curated and local library of\nmodels.\n\n## Conclusion\n\nBecause of Ollama, running an LLM server on my home private network was much\neasier than I initially imagined. LlamaBot - and its use of LiteLLM underneath\nthe hood - enabled me to build bots that used the Ollama server. This turned\nout to be a great way to extend the usable life of my GPU box!\n\nI send out a newsletter with tips and tools for data scientists. Come check it\nout at Substack.\n\nIf you would like to sponsor the coffee that goes into making my posts, please\nconsider GitHub Sponsors!\n\nFinally, I do free 30-minute GenAI strategy calls for organizations who are\nseeking guidance on how to best leverage this technology. Consider booking a\ncall on Calendly if you're interested!\n\n", "frontpage": false}
