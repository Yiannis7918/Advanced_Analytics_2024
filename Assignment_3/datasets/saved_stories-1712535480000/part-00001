{"aid": "39962572", "title": "LLMs for Alignment Research: a safety priority?", "url": "https://www.alignmentforum.org/posts/nQwbDPgYvAbqAmAud/llms-for-alignment-research-a-safety-priority", "domain": "alignmentforum.org", "votes": 1, "user": "rntn", "posted_at": "2024-04-07 18:15:56", "comments": 0, "source_title": "LLMs for Alignment Research: a safety priority? \u2014 AI Alignment Forum", "source_text": "LLMs for Alignment Research: a safety priority? \u2014 AI Alignment Forum\n\n##\n\nAI ALIGNMENT FORUM\n\nAF\n\n# LLMs for Alignment Research: a safety priority?\n\nby Abram Demski\n\n13 min read4th Apr 20245 comments\n\n# 55\n\nCyborgismAI\n\nFrontpage\n\nLLMs for Alignment Research: a safety priority?\n\n4th Apr 2024\n\nWhat is wrong with current models?\n\nIs there any hope for better?\n\nAgainst Autonomy\n\nServant vs Collaborator\n\nNotions of Alignment\n\nThe Need for an Independent Safety Approach\n\nEngineer-Researcher Collaboration\n\nAside: Hallucinations\n\nUpdating (on relatively small amounts of text)\n\nFeedback Tools\n\n5 comments\n\nA recent short story by Gabriel Mukobi illustrates a near-term scenario where\nthings go bad because new developments in LLMs allow LLMs to accelerate\ncapabilities research without a correspondingly large acceleration in safety\nresearch.\n\nThis scenario is disturbingly close to the situation we already find ourselves\nin. Asking the best LLMs for help with programming vs technical alignment\nresearch feels very different (at least to me). LLMs might generate junk code,\nbut you can keep pointing out the problems with the code, and the code will\neventually work. This can be faster than doing it myself, in cases where I\ndon't know a language or library well; the LLMs are moderately familiar with\neverything.\n\nWhen I try to talk to LLMs about technical AI safety work, however, I just get\ngarbage.\n\nI think a useful safety precaution for frontier AI models would be to make\nthem more useful for safety research than capabilities research. This extends\nbeyond applying AI technology to accelerate safety research within top AI\nlabs; models available to the general public (such as GPT-N, Claude-N) should\nalso accelerate safety more than capabilities.\n\n# What is wrong with current models?\n\nMy experience is mostly with Claude, and mostly with versions of Claude before\nthe current (Claude 3).^[1] I'm going to complain about Claude here; but\neverything else I've tried seemed worse. In particular, I found GPT4 to be\nworse than Claude2 for my purposes.\n\nAs I mentioned in the introduction, I've been comparing how these models feel\nhelpful for programming to how useless they feel for technical AI safety.\nSpecifically, technical AI safety of the mathematical-philosophy flavor that I\nusually think about. This is not, of course, a perfect experiment to compare\ncapability-research-boosting to safety-research-boosting. However, the tasks\nfeel comparable in the following sense: programming involves translating\nnatural-language descriptions into formal specifications; mathematical\nphilosophy also involves translating natural-language descriptions into formal\nspecifications. From this perspective, the main difference is what sort of\nformal language is being targeted (IE, programming languages vs axiomatic\nmodels).\n\nI don't have systematic experiments to report; just a general feeling that\nClaude's programming is useful, but Claude's philosophy is not.^[2] It is not\nobvious, to me, why this is. I've spoken to several people about it. Some\nreactions:\n\n  * If it could do that, we would all be dead!\n\n    * I think a similar mindset would have said this about programming, a few years ago. I suspect there are ways for modern LLMs to be more helpful to safety research in particular which do not also imply advancing capabilities very much in other respects. I'll say more about this later in the essay.\n  * There's probably just a lot less training data for mathematical philosophy than for programming.\n\n    * I think this might be an important factor, but it is not totally clear to me.\n  * Mathematical philosophy is inherently more difficult than programming, so it is no surprise.\n\n    * This might also be an important factor, but I consider it to be only a partial explanation. What is more difficult, exactly? As I mentioned, programming and mathematical philosophy have some strong similarities.\n\nProblems include a bland, people-pleasing attitude which is not very helpful\nfor research. By default, Claude (and GPT4) will enthusiastically agree with\nwhatever I say, and stick to summarizing my points back at me rather than\nproviding new insights or adding useful critiques. When Claude does engage in\nmore structured reasoning, it is usually wrong and bad. (I might summarize it\nas \"based more on vibes than logic\".)\n\n# Is there any hope for better?\n\nAs a starting observation: although a given AI technology, such as GPT4, might\nnot meet some safety standards we'd like to impose (eg,\ntransparency/interpretability), its widespread use means we are already forced\nto gamble on its relative safety. In some weak sense, this gives us a\nresource: a technology which we can use without increasing risks. This\ncertainly doesn't imply that any arbitrary use of GPT4 is non-risk-increasing.\nHowever, it does suggest approaches involving cautiously harnessing modern AI\ntechnology for what it's good for, without placing it in the driver's seat.\n\nWe're at a point in history where suddenly many new things are possible; it's\na point where it makes a lot of sense to look around, explore, and see whether\nyou can find a significant way to leverage the new technologies for good. With\nthe technology being so new, I don't think we should stop at the obvious (EG,\ngive up because chatting with modern LLMs about safety research did not feel\nfruitful).\n\nSome obvious things to try include better prompting strategies, and fine-\ntuning models specifically for helping with this sort of work. It might be\nuseful to attach LLMs to theorem-proving assistants and teach the LLMs to\n(selectively) formalize what the user is trying to reason about as axioms or\nproofs in the connected formal system.\n\nIt would also be helpful to simply make a more systematic study of what these\nmodels can and cannot help with, relating to AI safety research.\n\nI'll state some more specific ideas about how to use modern LLMs to benefit\nsafety research towards the end of this essay; there are some more intuitions\nI want to communicate first.\n\nWhat follows is my personal vision for how modern LLMs could be more useful\nfor safety research; I don't want to put overmuch emphasis on it. My main\npoint has already been made: making LLMs comparatively more useful for AI\nsafety work as opposed to AI capabilities work should itself be considered a\nsafety priority.\n\n## Against Autonomy\n\nI think there's a dangerous bias toward autonomy in AI -- both in terms of\nwhat AI companies are trying to produce, and also in what consumers are asking\nfor. I want to advocate for a greater emphasis on collaborative AI, rather\nthan AI which takes requests and then delivers.\n\n### Servant vs Collaborator\n\nBig AI companies are for the most part fine-tuning models to take a prompt and\nreturn an answer. This is a pretty reasonable idea, but it sometimes feels\nlike interacting with a nervous intern desperate to prove themselves in their\nfirst week on the job.\n\nFor example, my brother started a conversation with something like \"I'm\nthinking about making an RPG\". Bing responded with a very reasonable list of\nthings to think about when making an RPG. The problem is that my brother\nactually had a very specific idea in mind, and the advice was very generic.\nSimply put, my brother hadn't finished explaining what he wanted before\npressing enter. It would have been more useful for Bing to engage in active\nlistening: \"What kind of RPG are you interested in making?\" or similar\nconversational questions; and only write a research report giving advice after\nthe general shape of the request was clear. You have to be careful what you\nsay to the nervous intern, because the nervous intern will scurry off and\nwrite up a report at the drop of a hat.\n\nSimilarly, this video argues that Sudowrite (an AI novel-writing tool) is less\nuseful to authors than NovelCrafter (also an AI novel-writing tool) because\nSudowrite's philosophy is closer to \"click a button for the AI to write a\nnovel for you\" while NovelCrafter is oriented toward a more collaborative\nmodel.\n\nI think there are a few sources of autonomy-bias which I want to point out,\nhere:\n\n  * Autonomy is often easier to train into AI. For example, to generate whole pictures, you just need a data-set consisting of finished art. More sophisticated image manipulation sometimes requires more complex data-sets which might be more difficult to obtain.\n  * Autonomy is easier to conceive of. Push a button and it does what you want. Collaboration often requires more sophisticated user interfaces and more complex ideas about workflows -- perhaps involving domain-specific knowledge about how domain experts actually go about their business.\n  * Autonomy is more appealing to the people in charge of corporate budgets. My brother is currently working as a programmer, and his boss says he can't wait till the AI is at the point where you just push a button and get the code you asked for. My brother, due to having a closer relationship with the code, has a much more collaborative relationship with the AI. To programmers, the inadequacies of the \"just push a button\" model are more apparent.\n\n### Notions of Alignment\n\nGarret Baker recently commented:\n\n> To my ears it sounded like Shane [Legg]'s solution to \"alignment\" was to\n> make the models more consequentialist. I really don't think he appreciates\n> most of the difficulty and traps of the problems here. This type of\n> thinking, on my model of their models, should make even alignment optimists\n> unimpressed, since much of the reason for optimism lies in observing current\n> language models, and interpreting their outputs as being\n> nonconsequentialist, corrigible, and limited in scope yet broad in\n> application.\n\nLet's set aside whether Garret Baker's analysis of Shane Legg is correct. If\nit was correct, could you really blame him? Someone could be quite up-to-date\nwith the alignment literature and still take the view that \"alignment\"\nbasically means \"value alignment\" -- which is to say, absorbing human values\nand then optimizing them. Some of the strongest advocates of alternate ideas\nlike \"corrigibility\" will still say that progress towards it has stalled and\nthe evidence points toward it being a very unnatural concept.\n\nSimply put, we don't yet have a strong alternative to agent-centric (autonomy-\ncentric) alignment.\n\nA couple of people who I talk to have been moving away from the value-\nalignment picture, recently, instead replacing it with the following picture:\naligned AI systems are systems which increase, rather than decrease, the\nagency of humans. This is called capabilitarianism (in contrast to\nutilitarianism).^[4]\n\nThink of social media vs wikis. Social media websites are attention-sucking\nmachines which cause addictive scrolling. Wikis, such as wikipedia, are in\ncontrast incredibly useful.\n\nOr think of a nanny state which makes lots of decisions for its citizens on\nutilitarian grounds, vs a government which emphasizes freedom and informed\ndecision-making, fostering the ability of its citizens to optimize their own\nlives, rather than doing it for them.\n\nThis notion of alignment is still lacking the level of clarity which the more\nconsequentialist notion possesses, but it sure seems like there are less ways\nfor this kind of vision to go wrong.\n\n### The Need for an Independent Safety Approach\n\nOpenAI says:\n\n> Our goal is to build a roughly human-level automated alignment researcher.\n> We can then use vast amounts of compute to scale our efforts, and\n> iteratively align superintelligence.\n\nI think this sort of plan can easily go wrong. Broadly speaking, aiming to\ntake the human out of the loop seems like a mistake. We want to be on a\ntrajectory where humans very much remain in the loop. Of course I don't think\nthe superalignment team at OpenAI are trying to take humans entirely out of\nthe loop in a broader sense. But I don't think \"automated alignment\nresearcher\" should be the way we think about the end goal.\n\nIf you are trying to use AI to accelerate alignment work, but your main\napproach to alignment work is \"use AI to accelerate alignment work\" -- it\nseems to me that it is easy to miss a certain sort of grounding. You're\nsolving for X in the equation \"use AI to accelerate X\".\n\nInstead, I would propose that people working on LLMs should work to make LLMs\nuseful to alignment researchers whose main approach to alignment IS NOT \"make\nLLMs useful to alignment researchers\".\n\nThis prevents the snake from eating its own tail (and thereby killing itself).\n\n## Engineer-Researcher Collaboration\n\nMy main proposal for making modern LLMs comparatively more useful for AI\nsafety research is to pair AI safety researchers with generative AI engineers.\nThe engineers would try to create tools useful for accelerating safety\nresearch, while the safety researchers would provide testing and feedback.\nThis setup also provides some distance between the LLM engineering and the\nsafety work, to avoid the eating-its-own-tail problem. The safety researchers\nare bringing their own approach to safety work, so that \"automating safety\nresearch\" does not become the whole safety approach.\n\nThis could take the shape of single safety researchers working with single\nengineers, to an org with a team of safety researchers working with a team of\nengineers, all the way to a whole safety-research org working with a whole\nengineering org.\n\nAlthough my intuition here is that it is important for the safety researcher\nto have their own safety work which they are trying to accelerate using LLMs,\nit is plausible that most of the impact comes from building tools which are\nable to help a larger number of safety researchers; for example, the 'end\nproduct' might be an LLM which has been trained to be a helpful assistant for\na broad variety of safety researchers. I therefore imagine this LLM serving as\nsomething like a wiki for the AI safety community: like a more sophisticated\nversion of Stampy,^[5] where research-oriented conversation styles are also\ncurated, rather than only question-answer pairs.\n\n### Aside: Hallucinations\n\nI want to mention my personal model of \"AI hallucination\". Here's a pretty\nstandard example: when I ask Claude or GPT4 for references to papers on a very\nniche topic, the references it comes up with are usually made up. However,\nthey are generally plausible -- I often can't tell whether they are made up or\nnot before searching for those references myself.\n\nI think there's a common mindset which sees these hallucinations as some kind\nof malfunction. And they are, if the purpose of LLMs is seen as delivering\ntruthful information to the user. But if we think of the LLM as a really good\nprior distribution over what humans might say, then it starts to look less\nlike a malfunction and more like fairly good performance: the details filled\nin are quite plausible, even if incorrect.\n\nIf the prior lacks specific information we want it to have, the thing to do is\nupdate on that information. OpenAI and Anthropic provide interfaces where you\ncan give a thumbs-up or thumbs-down; notably, this is a common social-media\ninterface. But this feedback is not nearly rich enough. And it takes an\nautonomy-centric, reinforcement-learning-like attitude (the AI is learning to\nplease users) rather than putting the users in the pilot seat.\n\nIn order to get models to be useful for the sorts of tasks I try to use them\nfor, it seems to me like what's needed a way to give specific feedback on\nspecific outputs (such as my own list of references for the topic I queried\nabout) and updating the text-generating distribution in response to this\nfeedback, so that it will be remembered later. (This can be done to varying\ndegrees, and with varying trade-offs, EG using prompt-engineering solutions vs\nfine-tuning solutions.)\n\nThis way, knowledge flows in both directions, and users can build up a shared\ncontext with LLMs (including both object-level knowledge, like specific\ncitations, and process-level knowledge, like how verbose/succinct to be).\n\n### Updating (on relatively small amounts of text)\n\nA main weakness of Deep Learning was how data-hungry it tends to be. For\nexample, deep-learning systems can play Atari games at the human level, but\nachieving that level of competence requires many more hours of play for deep\nlearning than humans need. Similar remarks apply for tasks ranging from chess\nto image recognition.\n\nLLMs require lots and lots of data for the generative pre-training, but once\nyou've done that, you've got a \"really good prior\" -- my impression is,\nrelatively small amounts of data can be used to fine-tune the model.\n(Unfortunately, I was not able to quickly find recommended sizes for fine-\ntuning datasets, so take this with a grain of salt.)\n\nFor LLMs above approximately 40 billion parameters,^[6] these updates can be\nquite good, in the sense that new knowledge seems to integrate itself across a\nbroad variety of conversational contexts which were not explicitly trained.\n\nMy favorite example of this: Claude was trained using a technique called\nConstitutional AI. I've had some extensive conversations with Claude about AI\nalignment problems. In my experience, whenever AI alignment is involved,\nClaude tries to shoe-horn Constitutional AI into the conversation as the\nsolution to whatever problem we're talking about. The arguments for the\nrelevance of Constitutional AI might be incoherent,^[7] but Claude's\nunderstanding that Constitutional AI is an alignment idea is coherent, as well\nas Claude's enthusiasm for that particular technique.^[8]\n\nThis was not the intention of Claude's training. Anthropic simply wanted\nClaude to know a reasonable amount about itself, so that it could say things\nlike \"I'm Claude, an AI designed by Anthropic\" and explain some basic facts\nabout how it was made.^[6]\n\nMore generally, I have found Claude to be enthusiastic/defensive about the\nmore empirical type of safety work which takes place at Anthropic. I'm unable\nto find the chat in question now, but there was one conversation where it\npassionately advocated for understanding what a neural network was doing\n\"weight by weight\" in contrast to more theoretical approaches.\n\nSo, as you can see, consequences of updates might be unintended and\nundesirable, but they are clearly smart in a significant sense. Concepts are\nbeing combined in meaningful ways. This is not \"just autocomplete\".\n\nSuch smart updates are a double-edged sword. For \"the wiki model\" of LLMs to\nwork well, it would be helpful to develop tools to search for (possibly\nunintended & undesirable) consequences of updates.\n\nNote that fine-tuning smaller LLMs, around 8 billion parameters, is feasible\nfor individuals and small groups with modest amounts of money; but fine-tuning\nmodels larger than 40 billion parameters, where we see the phenomenon of\nreally smart generalizations from fine-tuning examples, is still out of reach\nafaik.\n\n### Feedback Tools\n\nSo: I imagine that for modern LLMs to be very useful for experts in the field\nof AI safety, some experts will need to spend a lot of time giving LLMs\nspecific feedback. This feedback would include specific information (refining\nthe LLM's knowledge) as well as training on useful interaction styles for\nresearch.\n\nIn order to facilitate such feedback, I think it would be important to develop\ntools which help rapidly indicate specific problems with text (in contrast to\na mere thumbs-up or thumbs-down), and see a preview of how the LLM would adapt\nbased on this feedback, so that the feedback can be tweaked to achieve the\ndesired result.\n\nTo give a simple idea for what this could look like: a user might highlight a\npart of an AI-generated response that they would like to give feedback on. A\npop-up feedback box appears, listing some AI-generated potential corrections\nfor the user to select, and also allowing the user to type their own\ncorrection. Once a correction has been selected/written, the AI generates some\npotential amendments to its constitution which would detect this problem and\ncorrect it in the future; again the user can look at these and select one or\nwrite their own proposed amendment. Finally, the system then generates some\nexamples of the impact the proposed amendment would have (probing for\nunintended and undesirable consequences). The user can revise the amendment\nuntil it has the desired effect, at which point they would finalize it.\n\nI have heard the term \"reconstitutional AI\" used to point in this general\ndirection.\n\n  1. ^^\n\nMy conversations with Claude3 so far do seem somewhat better. However, I\nsuppose that its ability to program has similarly improved.\n\n  2. ^^\n\nModern LLMs are more useful to beginners than experts.^[3] A highly\nexperienced programmer can already easily write the kind of code that LLMs can\nhelp with, and with fewer errors. A beginner, however, has much more to gain\nfrom LLM assistance. Similarly, then, modern LLMs are probably a lot more\nhelpful to people who are starting to get into AI safety research. It could be\nthat what I'm observing is, really, that I'm a worse programmer than I am a\nsafety researcher.\n\n  3. ^^\n\nBrynjolfsson, Erik, Danielle Li, and Lindsey R. Raymond. Generative AI at\nwork. No. w31161. National Bureau of Economic Research, 2023.\n\n  4. ^^\n\nSome links about this, compiled by TJ:\n\nhttps://thingofthings.substack.com/p/on-capabilitarianism\n\nhttps://plato.stanford.edu/entries/capability-approach/\n\nhttps://philpapers.org/rec/SENCAC\n\nhttps://arxiv.org/abs/2308.00868\n\nhttps://forum.effectivealtruism.org/posts/zy6jGPeFKHaoxKEfT/the-capability-\napproach-to-human-welfare\n\nhttps://www.princeton.edu/~ppettit/papers/Capability_EconomicsandPhilosophy_2001.pdf\n\n  5. ^^\n\nStampy is a Discord bot which facilitates curated Q&A about AI safety.\n\n  6. ^^\n\nAccording to private correspondence with a reliable-seeming source.\n\n  7. ^^\n\nAlthough, no more incoherent than I might expect of some human person who is\nvery enthusiastic about constitutional AI.\n\n  8. ^^\n\nIf Claude was trained to explain Constitutional AI factually, but not trained\nto be actively enthusiastic and push Constitutional AI via motivated\narguments... is this an example of defensive reasoning? Did Claude generalize\nfrom the observation that people are generally defensive of their own\nbackground, arguing for the superiority of their profession or home country?\nWould Claude more generally try to bend arguments in its own favor, in some\nsense? Or is this a more benign generalization, perhaps from the idea that a\ncharacter who explains concept X in depth is probably enthusiastic about\nconcept X?\n\nCyborgism1AI1\n\nFrontpage\n\n# 55\n\nLLMs for Alignment Research: a safety priority?\n\n4th Apr 2024\n\n14Stephen McAleese\n\n5Ryan Greenblatt\n\n2Abram Demski\n\n6Charlie Steiner\n\n5Abram Demski\n\nNew Comment\n\n5 comments, sorted by\n\ntop scoring\n\nClick to highlight new comments since: Today at 6:56 PM\n\n[-]Stephen McAleese2d100\n\nLLMs aren't that useful for alignment experts because it's a highly\nspecialized field and there isn't much relevant training data. The AI Safety\nChatbot partially solves this problem using retrieval-augmented generation\n(RAG) on a database of articles from https://aisafety.info. There also seem to\nbe plans to fine-tune it on a dataset of alignment articles.\n\nReply\n\n[-]Ryan Greenblatt2d32\n\n> LLMs aren't that useful for alignment experts because it's a highly\n> specialized field and there isn't much relevant training data.\n\nSeems plausibly true for the alignment specific philosophy/conceptual work,\nbut many people attempting to improve safety also end up doing large amounts\nof relatively normal work in other domains (ML, math, etc.)\n\nThe post is more centrally talking about the very alignment specific use cases\nof course.\n\nReply\n\n[-]Abram Demski2d20\n\nSounds pretty cool! What LLM powers it?\n\nReply\n\n[-]Charlie Steiner2d44\n\nWouldn't other people also like to use an AI that can collaborate with them on\ncomplex topics? E.g. people planning datacenters, or researching RL, or trying\nto get AIs to collaborate with other instances of themselves to accurately\nsolve real-world problems?\n\nI don't think people working on alignment research assistants are planning to\njust turn it on and leave the building, they on average (weighted by money)\nseem to be imagining doing things like \"explain an experiment in natural\nlanguage and have an AI help implement it rapidly.\"\n\nSo I think both they and this post are describing the strategy of \"building\nvery generally useful AI, but the good guys will be using it first.\" I hear\nyou as saying you want a slightly different profile of generally-useful skills\nto be targeted.\n\nReply\n\n[-]Abram Demski2d50\n\nI don't think the plan is \"turn it on and leave the building\" either, but I\nstill think the stated goal should not be automation.\n\nI don't quite agree with the framing \"building very generally useful AI, but\nthe good guys will be using it first\" -- the approach I am advocating is not\nto push general capabilities forward and then specifically apply those\ncapabilities to safety research. That is more like the automation-centric\napproach I am arguing against.\n\nHmm, how do I put this...\n\nI am mainly proposing more focused training of modern LLMs with feedback from\nsafety researchers themselves, toward the goal of safety researchers getting\nutility out of these systems; this boosts capabilities for helping-with-\nsafety-research specifically, in a targeted way, because that is what you are\ngetting more+better training feedback on. (Furthermore, checking and\nmaintaining this property would be an explicit goal of the project.)\n\nI am secondarily proposing better tools to aid in that feedback process; these\ncan be applied to advance capabilities in any area, I agree, but I think it\nonly somewhat exacerbates the existing \"LLM moderation\" problem; the general\nsolution of \"train LLMs to do good things and not bad things\" does not seem to\nget significantly more problematic in the presence of better training tools\n(perhaps the general situation even gets better). If the project was\nsuccessful for safety research, it could also be extended to other fields. The\nquestion of how to avoid LLMs being helpful for dangerous research would be\nsimilar to the LLM moderation question currently faced by Claude, ChatGPT,\nBing, etc: when do you want the system to provide helpful answers, and when do\nyou want it to instead refuse to help?\n\nI am thirdly also mentioning approaches such as training LLMs to interact with\nproof assistants and intelligently decide when to translate user arguments\ninto formal languages. This does seem like a more concerning general-\ncapability thing, to which the remark \"building very generally useful AI, but\nthe good guys will be using it first\" applies.\n\nReply\n\nModeration Log\n\n", "frontpage": false}
