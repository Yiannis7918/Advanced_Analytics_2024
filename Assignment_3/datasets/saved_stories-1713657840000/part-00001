{"aid": "40099344", "title": "Financial Market Applications of LLMs", "url": "https://thegradient.pub/financial-market-applications-of-llms/", "domain": "thegradient.pub", "votes": 15, "user": "andreyk", "posted_at": "2024-04-20 18:03:15", "comments": 4, "source_title": "Financial Market Applications of LLMs", "source_text": "Financial Market Applications of LLMs\n\n# Financial Market Applications of LLMs\n\n20.Apr.2024 . 7 min read\n\nThe AI revolution drove frenzied investment in both private and public\ncompanies and captured the public\u2019s imagination in 2023. Transformational\nconsumer products like ChatGPT are powered by Large Language Models (LLMs)\nthat excel at modeling sequences of tokens that represent words or parts of\nwords [2]. Amazingly, structural understanding emerges from learning next-\ntoken prediction, and agents are able to complete tasks such as translation,\nquestion answering and generating human-like prose from simple user prompts.\n\nNot surprisingly, quantitative traders have asked: can we turn these models\ninto the next price or trade prediction [1,9,10]? That is, rather than\nmodeling sequences of words, can we model sequences of prices or trades. This\nturns out to be an interesting line of inquiry that reveals much about both\ngenerative AI and financial time series modeling. Be warned this will get\nwonky.\n\nLLMs are known as autoregressive learners -- those using previous tokens or\nelements in a sequence to predict the next element or token. In quantitative\ntrading, for example in strategies like statistical arbitrage in stocks, most\nresearch is concerned with identifying autoregressive structure. That means\nfinding sequences of news or orders or fundamental changes that best predict\nfuture prices.\n\nWhere things break down is in the quantity and information content of\navailable data to train the models. At the 2023 NeurIPS conference, Hudson\nRiver Trading, a high frequency trading firm, presented a comparison of the\nnumber of input tokens used to train GPT-3 with the amount of trainable tokens\navailable in the stock market data per year HRT estimated that, with 3,000\ntradable stocks, 10 data points per stock per day, 252 trading days per year,\nand 23400 seconds in a trading day, there are 177 billion stock market tokens\nper year available as market data. GPT-3 was trained on 500 billion tokens, so\nnot far off [6].\n\nnumbers courtesy of HRT 2023 NeuRIPS presentation\n\nBut, in the trading context the tokens will be prices or returns or trades\nrather than syllables or words; the former is much more difficult to predict.\nLanguage has an underlying linguistic structure (e.g., grammar) [7]. It\u2019s not\nhard to imagine a human predicting the next word in a sentence, however that\nsame human would find it extremely challenging to predict the next return\ngiven a sequence of previous trades, hence the lack of billionaire day\ntraders. The challenge is that there are very smart people competing away any\nsignal in the market, making it almost efficient (\u201cefficiently inefficient\u201d,\nin the words of economist Lasse Pedersen) and hence unpredictable. No\nadversary actively tries to make sentences more difficult to predict \u2014 if\nanything, authors usually seek to make their sentences easy to understand and\nhence more predictable.\n\nLooked at from another angle, there is much more noise than signal in\nfinancial data. Individuals and institutions are trading for reasons that\nmight not be rational or tied to any fundamental change in a business. The\nGameStop episode in 2021 is one such example. Financial time series are also\nconstantly changing with new fundamental information, regulatory changes, and\noccasional large macroeconomic shifts such as currency devaluations. Language\nevolves at a much slower pace and over longer time horizons.\n\nOn the other hand, there are reasons to believe that ideas from AI will work\nwell in financial markets. One emerging area of AI research with promising\napplications to finance is multimodal learning [5], which aims to use\ndifferent modalities of data, for example both images and textual inputs to\nbuild a unified model. With OpenAI\u2019s DALL-E 2 model, a user can enter text and\nthe model will generate an image. In finance, multi-modal efforts could be\nuseful to combine information classical sources such as technical time series\ndata (prices, trades, volumes, etc.) with alternative data in different modes\nlike sentiment or graphical interactions on twitter, natural language news\narticles and corporate reports, or the satellite images of shipping activity\nin a commodity centric port. Here, leveraging multi-modal AI, one could\npotentially incorporate all these types of non-price information to predict\nwell.\n\nAnother strategy called \u2018residualization\u2019 holds prominence in both finance and\nAI, though it assumes different roles in the two domains. In finance,\nstructural `factor\u2019 models break down the contemporaneous observations of\nreturns across different assets into a shared component (the market return, or\nmore generally returns of common, market-wide factors) and an idiosyncratic\ncomponent unique to each underlying asset. Market and factor returns are\ndifficult to predict and create interdependence, so it is often helpful to\nremove the common element when making predictions at the individual asset\nlevel and to maximize the number of independent observations in the data.\n\nIn residual network architectures such as transformers, there\u2019s a similar idea\nthat we want to learn a function h(X) of an input X, but it might be easier to\nlearn the residual of h(X) to the identity map, i.e., h(X) \u2013 X. Here, if the\nfunction h(X) is close to identity, its residual will be close to zero, and\nhence there will be less to learn and learning can be done more efficiently.\nIn both cases the goal is to exploit structure to refine predictions: in the\nfinance case, the idea is to focus on predicting innovations beyond what is\nimplied by the overall market, for residual networks the focus is on\npredicting innovations to the identity map.\n\nA key ingredient for the impressive performance of LLMs work is their ability\nto discern affinities or strengths between tokens over long horizons known as\ncontext windows. In financial markets, the ability to focus attention across\nlong horizons enables analysis of multi-scale phenomena, with some aspects of\nmarket changes explained across very different time horizons. For example, at\none extreme, fundamental information (e.g., earnings) may be incorporated into\nprices over months, technical phenomena (e.g., momentum) might be realized\nover days, and, at the other extreme, microstructure phenomena (e.g., order\nbook imbalance) might have a time horizon of seconds to minutes.\n\nCapturing all of these phenomena involves analysis of multiple time horizons\nacross the context window. However, in finance, prediction over multiple\nfuture time horizons is also important. For example, a quantitative system may\nseek to trade to profit from multiple different anomalies that are realized\nover multiple time horizons (e.g., simultaneously betting on a microstructure\nevent and an earnings event). This requires predicting not just the next\nperiod return of the stock, but the entire term structure or trajectory of\nexpected returns, while current transformer-style predictive models only look\none period in the future.\n\nAnother financial market application of LLMs might be synthetic data creation\n[4,8]. This could take a few directions. Simulated stock price trajectories\ncan be generated that mimic characteristics observed in the market and can be\nextremely beneficial given that financial market data is scarce relative to\nother sources as highlighted above in the number of tokens available.\nArtificial data could open the door for meta-learning techniques which have\nsuccessfully been applied, for example, in robotics. In the robotic setting\ncontrollers are first trained using cheap but not necessarily accurate physics\nsimulators, before being better calibrated using expensive real world\nexperiments with robots. In finance the simulators could be used to coarsely\ntrain and optimize trading strategies. The model would learn high level\nconcepts like risk aversion and diversification and tactical concepts such as\ntrading slowly to minimize the price impact of a trade. Then precious real\nmarket data could be employed to fine-tune the predictions and determine\nprecisely the optimal speed to trade.\n\nFinancial market practitioners are often interested in extreme events, the\ntimes when trading strategies are more likely to experience significant gains\nor losses. Generative models where it\u2019s possible to sample from extreme\nscenarios could find use. However extreme events by definition occur rarely\nand hence determining the right parameters and sampling data from the\ncorresponding distribution is fraught.\n\nDespite the skepticism that LLMs will find use in quantitative trading, they\nmight boost fundamental analysis. As AI models improve, it\u2019s easy to imagine\nthem helping analysts refine an investment thesis, uncover inconsistencies in\nmanagement commentary or find latent relationships between tangential\nindustries and businesses [3]. Essentially these models could provide a\nCharlie Munger for every investor.\n\nThe surprising thing about the current generative AI revolution is that it\u2019s\ntaken almost everyone \u2013 academic researchers, cutting edge technology firms\nand long-time observers \u2013 by surprise. The idea that building bigger and\nbigger models would lead to emergent capabilities like we see today was\ntotally unexpected and still not fully understood.\n\nThe success of these AI models has supercharged the flow of human and\nfinancial capital into AI, which should in turn lead to even better and more\ncapable models. So while the case for GPT-4 like models taking over\nquantitative trading is currently unlikely, we advocate keeping an open mind.\nExpecting the unexpected has been a profitable theme in the AI business.\n\n### References\n\n  1. \u201cApplying Deep Neural Networks to Financial Time Series Forecasting\u201d Allison Koenecke. 2022\n  2. \u201cAttention is all you need.\u201d A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones... Advances in Neural Information Processing Systems, 2017\n  3. \u201cCan ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models\u201d . Lopez-Lira, Alejandro and Tang, Yuehua, (April 6, 2023) Available at SSRN\n  4. \u201cGenerating Synthetic Data in Finance: Opportunities, Challenges and Pitfalls.\u201d SA Assefa, D Dervovic, M Mahfouz, RE Tillman... - Proceedings of the First ACM International Conference ..., 2020\n  5. \u201cGPT-4V(ision) System Card.\u201d OpenAI. September 2023\n  6. \u201cLanguage models are few-shot learners.\u201d T Brown, B Mann, N Ryder, M Subbiah, JD Kaplan... - Advances in Neural Information Processing Systems, 2020\n  7. \u201cSequence to Sequence Learning with Neural Networks.\u201d I.Sutskever,O.Vinyals,and Q.V.Le in Advances in Neural Information Processing Systems, 2014, pp. 3104\u20133112.\n  8. \u201cSynthetic Data Generation for Economists\u201d. A Koenecke, H Varian - arXiv preprint arXiv:2011.01374, 2020\n  9. C. C. Moallemi, M. Wang. A reinforcement learning approach to optimal execution. Quantitative Finance, 22(6):1051\u20131069, March 2022.\n  10. C. Maglaras, C. C. Moallemi, M. Wang. A deep learning approach to estimating fill probabilities in a limit order book. Quantitative Finance, 22(11):1989\u20132003, October 2022.\n\n### Citation\n\nFor attribution in academic contexts or books, please cite this work as\n\n    \n    \n    Richard Dewey and Ciamac Moallemi, \"Financial Market Applications of LLMs,\" The Gradient, 2024\n    \n    \n    @article{dewey2024financial, author = {Richard Dewey and Ciamac Moallemi}, title = {Financial Market Applications of LLMs}, journal = {The Gradient}, year = {2024}, howpublished = {\\url{https://thegradient.pub/financial-market-applications-of-llms}, }\n\nLLMOverviews\n\n### Richard Dewey\n\nCo-founder and CEO of Proven. Previously worked in financial markets at PIMCO,\nElm Partners, Tudor and Royal Bridge.\n\n### Recent Posts\n\n### A Brief Overview of Gender Bias in AI\n\n08.Apr.2024\n\n### Mamba Explained\n\n27.Mar.2024\n\n### Car-GPT: Could LLMs finally make self-driving cars happen?\n\n08.Mar.2024\n\n### Do text embeddings perfectly encode text?\n\n05.Mar.2024\n\n### Why Doesn\u2019t My Model Work?\n\n24.Feb.2024\n\n### Tags\n\nArt Conference Deep Learning Ethics Explainability Generative Models Graphs\nHealthcare History Human factors Impacts Interpretability Language LLM Machine\nLearning NLP Overviews Perspectives Podcast Policy Quantum ML Reinforcement\nLearning Science Speech Recognition Trends Vision XAI\n\n### You Might Be Interested In\n\nDeep Learning\n\n## Modern AI is Domestification\n\n27.May.2023\n\nTed Xiao\n\nDeep Learning\n\n## In-Context Learning, In Context\n\n29.Apr.2023\n\nDaniel Bashir\n\nHistory\n\n## How Machine Learning Can Help Unlock the World of Ancient Japan\n\n17.Nov.2019\n\nAlex Lamb\n\nTags\n\n  * Art\n  * Conference\n  * Deep Learning\n  * Ethics\n  * Explainability\n  * Generative Models\n  * Graphs\n  * Healthcare\n  * History\n  * Human factors\n  * Impacts\n  * Interpretability\n  * Language\n  * LLM\n  * Machine Learning\n  * NLP\n  * Overviews\n  * Perspectives\n  * Podcast\n  * Policy\n  * Quantum ML\n  * Reinforcement Learning\n  * Science\n  * Speech Recognition\n  * Trends\n  * Vision\n  * XAI\n\n\u00a9 2024 The Gradient \u2013 Published with Ghost & Nubia\n\n", "frontpage": true}
