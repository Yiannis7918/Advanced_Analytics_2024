{"aid": "40018963", "title": "Ferret-v2: An Improved Baseline for Referring and Grounding with LLMs", "url": "https://huggingface.co/papers/2404.07973", "domain": "huggingface.co", "votes": 4, "user": "CharlesW", "posted_at": "2024-04-13 00:04:37", "comments": 0, "source_title": "Paper page - Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models", "source_text": "Paper page - Ferret-v2: An Improved Baseline for Referring and Grounding with\nLarge Language Models\n\nHugging Face\n\nPapers\n\narxiv:2404.07973\n\n# Ferret-v2: An Improved Baseline for Referring and Grounding with Large\nLanguage Models\n\nPublished on Apr 11\n\n\u00b7 Featured in Daily Papers on Apr 12\n\nUpvote\n\n17\n\nAuthors:\n\nHaotian Zhang ,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n## Abstract\n\nWhile Ferret seamlessly integrates regional understanding into the Large\nLanguage Model (LLM) to facilitate its referring and grounding capability, it\nposes certain limitations: constrained by the pre-trained fixed visual encoder\nand failed to perform well on broader tasks. In this work, we unveil\nFerret-v2, a significant upgrade to Ferret, with three key designs. (1) Any\nresolution grounding and referring: A flexible approach that effortlessly\nhandles higher image resolution, improving the model's ability to process and\nunderstand images in greater detail. (2) Multi-granularity visual encoding: By\nintegrating the additional DINOv2 encoder, the model learns better and diverse\nunderlying contexts for global and fine-grained visual information. (3) A\nthree-stage training paradigm: Besides image-caption alignment, an additional\nstage is proposed for high-resolution dense alignment before the final\ninstruction tuning. Experiments show that Ferret-v2 provides substantial\nimprovements over Ferret and other state-of-the-art methods, thanks to its\nhigh-resolution scaling and fine-grained visual processing.\n\nView arXiv page View PDF Add to collection\n\n### Community\n\nnicolay-r\n\nabout 15 hours ago\n\nExcited to running into your findings on training recipes, modality encoders,\nexperiments on resolution scaling. Well done \ud83d\udc4f. A quick question on mentioned\ntransparency of the Ferret-v2: is it already on Github?\n\n\u00b7 Sign up or log in to comment\n\nUpvote\n\n17\n\n## Models citing this paper 0\n\nNo model linking this paper\n\nCite arxiv.org/abs/2404.07973 in a model README.md to link it from this page.\n\n## Datasets citing this paper 0\n\nNo dataset linking this paper\n\nCite arxiv.org/abs/2404.07973 in a dataset README.md to link it from this\npage.\n\n### Spaces citing this paper 0\n\nNo Space linking this paper\n\nCite arxiv.org/abs/2404.07973 in a Space README.md to link it from this page.\n\n## Collections including this paper 8\n\n", "frontpage": true}
