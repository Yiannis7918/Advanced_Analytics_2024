{"aid": "40018785", "title": "OpenEQA: Embodied Question Answering in the Era of Foundation Models", "url": "https://open-eqa.github.io/", "domain": "open-eqa.github.io", "votes": 4, "user": "beefman", "posted_at": "2024-04-12 23:42:08", "comments": 0, "source_title": "OpenEQA: Embodied Question Answering in the Era of Foundation Models", "source_text": "OpenEQA: Embodied Question Answering in the Era of Foundation Models\n\n# OpenEQA: Embodied Question Answering in the Era of Foundation Models\n\nArjun Majumdar^*, Anurag Ajay^*, Xiaohan Zhang^*, Pranav Putta, Sriram\nYenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets,\nSergio Arnaud, Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma, Vincent\nBerges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Batra, Mrinal\nKalakrishnan, Franziska Meier, Chris Paxton, Sasha Sax, Aravind Rajeswaran\n\nWork done at Fundamental AI Research (FAIR), Meta. ^*Equal Contributions\n\nPaper Blog Code Benchmark\n\n## Illustration of an episode history along with questions and answers from\nour OpenEQA benchmark, which contains 1600+ untemplated questions that tests\nseveral aspects of open vocabulary embodied question answering.\n\n## Abstract\n\nWe present a modern formulation of Embodied Question Answering (EQA) as the\ntask of understanding an environment well enough to answer questions about it\nin natural language. An agent can achieve such an understanding by either\ndrawing upon episodic memory, exemplified by agents on smart glasses, or by\nactively exploring the environment, as in the case of mobile robots. We\naccompany our formulation with OpenEQA - the first open-vocabulary benchmark\ndataset for EQA supporting both episodic memory and active exploration use\ncases. OpenEQA contains over 1600 high-quality human generated questions drawn\nfrom over 180 real-world environments. In addition to the dataset, we also\nprovide an automatic LLM-powered evaluation protocol that has excellent\ncorrelation with human judgement. Using this dataset and evaluation protocol,\nwe evaluate several state-of-the-art foundation models like GPT-4V and find\nthat they significantly lag behind human-level performance. Consequently,\nOpenEQA stands out as a straightforward, measurable, and practically relevant\nbenchmark that poses a considerable challenge to current generation of AI\nmodels. We hope this inspires and stimulates future research at the\nintersection of Embodied AI, conversational agents, and world models.\n\n## Performance of State-of-the-Art Models\n\nLLM vs. Multi-Modal LLM Performance on EM-EQA. We evaluated several multi-\nmodal LLMs including Claude 3, Gemini Pro, and GPT-4V on OpenEQA. We find that\nthese models consistently outperform text-only (or blind) LLM baselines such\nas LLaMA-2 or GPT-4. However, performance is substantially worse than the\nhuman baselines.\n\n## OpenEQA Dataset Statistics\n\nExample questions and dataset statistics of OpenEQA. The episode history H\nprovides a human-like tour of a home. EQA agents must answer diverse, human-\ngenerated questions Q from 7 EQA categories, aiming match the ground answers\nA*. Tours are collected from diverse environments including home and office\nlocations (not shown above). Dataset statistics (right) break down the\nquestion distribution by video source (top), question category (middle), and\nepisodic memory vs active setting. Note that, by design, the HM3D questions\nare shared across the EM-EQA and A-EQA settings.\n\n## Automated Evaluation Workflow\n\nIllustration of LLM-Match evaluation and workflow. While the open-vocabulary\nnature makes EQA realistic, it poses a challenge for evaluation due to\nmultiplicity of correct answers. One approach to evaluation is human trials,\nbut it can be prohibitively slow and expensive, especially for benchmarks. As\nan alternative, we use an LLM to evaluate the correctness of open-vocabulary\nanswers produced by EQA agents.\n\n## Performance by Category\n\nCategory-level performance on EM-EQA. We find that agents with access to\nvisual information excel at localizing and recognizing objects and attributes,\nand make better use of this information to answer questions that require world\nknowledge. However, on other categories performance is closer to the blind LLM\nbaseline (GPT-4), indicating substantial room for improvement on OpenEQA.\n\n## BibTeX\n\n    \n    \n    @inproceedings{OpenEQA2023, title = {OpenEQA: Embodied Question Answering in the Era of Foundation Models}, booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)}, author = {Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and Yadav, Karmesh and Li, Qiyang and Newman, Ben and Sharma, Mohit and Berges, Vincent and Zhang, Shiqi and Agrawal, Pulkit and Bisk, Yonatan and Batra, Dhruv and Kalakrishnan, Mrinal and Meier, Franziska and Paxton, Chris and Sax, Sasha and Rajeswaran, Aravind}, year = {2024}, }\n\nWebsite template borrowed from NeRFies and CLIPort.\n\n", "frontpage": true}
