{"aid": "40032649", "title": "The argument against clearing the database between tests", "url": "https://calpaterson.com/against-database-teardown.html", "domain": "calpaterson.com", "votes": 1, "user": "thunderbong", "posted_at": "2024-04-14 17:19:23", "comments": 0, "source_title": "The argument against clearing the database between tests", "source_text": "The argument against clearing the database between tests\n\n# The argument against clearing the database between tests\n\nApril 2020\n\nSome reasons why you might not want to remove data from the database between\nautomated tests: speed, correctness, data growth issues and parallelism\nadvantages\n\nI'm of the school of thought that most useful \"unit\"^1 tests should involve\nthe database. Consequently I don't mock out, fake or stub the database in\ntests that I write.\n\nOn every project, I had a small piece of test harness code that cleans the\ndatabase between tests^2:\n\n    \n    \n    @before_test def clean_session: for table in all_tables(): db.session.execute(\"truncate %s;\" % table.name) db.session.commit() return db.session\n\nThe reason for this was that it seemed obvious that each test should start\nwith a completely clean slate in order to make it a fair test. All other data\nshould be deleted so that nothing from other tests that can get conflated and\ncause the test to go wrong somehow - either spuriously passing or failing.\n\nRecently I've come to the conclusion that it can (but not always) make sense\nto run the tests with a dirty database - not only not cleaning between\nindividual tests but also not cleaning between whole test runs - and keeping\nall the old test data around on a (near)^3 permanent basis.\n\n## The required mentality change\n\nUpturning the base assumption that \"the database is clean\" in a test requires\na small adjustment to your mindset.\n\nYou can't write tests that assume that data it has created is the only thing\npresent, such as this one:\n\n    \n    \n    def test_adding_a_user(): db_session.add(user) # won't work, assumes the only user is the one added above assert db_session.query(User).count() == 1\n\nInstead each test needs to be written to assert only on the basis of data it\nhas created (or has not created). It should handle data created by others. A\ncorrected example:\n\n    \n    \n    def test_adding_a_user_2(session): user = make_user() db_session.add(user) # this is safe, doesn't assume no other users exist assert db_session.query(User).get(user.user_id) is not None\n\nThis isn't an easy change to make in existing tests. The assumption of clean\ndata is tricky to refactor away. There are reasons to consider it though.\n\n## The advantages\n\n### It's quicker\n\nTearing down data between tests or schemas between test runs is not free of\n(computational) charge.\n\nThe time taken to clean the database is usually proportional to the number of\ntables and while this cost is small to begin with it can grow over time^4.\n\nWhen you have a large number of tests this per-test overhead becomes a\nproblem. My personal experience is that the problem starts to get serious when\nyou have around one hundred tests. For the typical test suite tearing down\ndata can take anywhere from one to ten percent of total runtime, depending on\nhow efficiently it's done.\n\nThere are ways to be quicker, here are a few:\n\n  * avoid tearing down/recreating static data such as lookup tables\n  * track which tables have been touched and don't clean untouched ones\n  * turn off crash safety while testing\n\nThat all aside, the fact remains that tearing down the database is never as\nfast as not tearing it down.\n\n### More realistic data shape\n\nOne perennial problem with code that uses data is that when the volume of data\ngrows the performance can change considerably: when there are just three rows\nin a table a logarithmic time operation (fast) is indistinguishable from a\npolynomial time operation (slow) ^5.\n\nIt's a sad fact that the majority of tests and indeed most development time is\nspent with the database in an empty or nearly-empty state. As a result there\nis a loss of feedback. Without the daily experience of running with realistic\ndata sets, detecting a slow data access pattern requires thoughtful analysis\nand/or experience.\n\nWhen your tests don't clean the database your test database will slowly fill\nwith data which is, although not a complete match with the shape of\nproduction, presumably along similar lines. Sometimes you will notice problem\nqueries before finding out in production and without doing analysis.\n\nBetter yet, when your tests don't assume they're starting from a clean sheet,\nyou can run your tests with a dump from production loaded into your database.\nThis can help confirm that data access patterns you're using will work when\nthe database, as a whole, is at production size.\n\n### More realistic data\n\nTesting-by-example (as opposed to property-based testing) is based on the\nprogrammer coming up with examples of inputs and asserting that when given\nthose inputs the program produces the right outputs - or at least - the right\nsort of outputs. There is also the implicit assertion (present in all\nautomated tests) that the program does not raise an exception or crash.\n\nHopefully, at least some of the residual data your tests leave behind is\nhighly contrived and contains lots of \"bad\" data and error cases. For example:\nusers that have started the order process but who haven't gotten as far as\nentering their email address, users who have been marked as duplicates, users\nwhose names contain semi-colons and so on.\n\nRunning your tests in the presence of all this realistic wonky data can help\ntease out real bugs. If one of your tests inserts customer names based on the\nBig List of Naughty Strings then you might find more bugs in other areas when\nother tests exercise different parts of the same system.\n\nOf course - you shouldn't rely on such \"atmospheric\" bad data as an aid to\ncorrectness. Each time you find a new bug based on bad data left lying around\nfrom another test or loaded in from production a new, specific, test should be\nadded for that condition. However having a load of crap data loaded can help\nuncover issues that might not otherwise have been uncovered at the development\nstage.\n\n### Test parallelism becomes a smaller step\n\nTest parallelism is often discussed but my experience is that relatively few\nprojects ever implement it, even though quite a few would benefit.\n\nThe problem is as follows. At first your tests are fast because there aren't\nmany of them and so most teams put off parallel tests until \"later\". When\n\"later\" arrives, the build is now slow and test parallelism would help but\nit's usually not easy to adapt existing serial tests to run in parallel.\n\nThis is because tests typically manipulate state in odd ways and if each test\nassumes they are the only process manipulating state they tend to need\ncomplete isolation to be able to run in parallel - separate database\ninstances, separate S3 test doubles - even separate filesystems occasionally.\nComplete isolation is expense, hassle and more moving parts (8 SQL databases\nfor an 8-process test suite is no fun).\n\nTests written to assume the presence of irrelevant data are much easier to\nparallelise - usually it can all be done within one environment. This makes it\neasier to do and so much more likely to happen.\n\n## Disadvantages (and some mitigations)\n\n### Debugging is harder\n\nWhen a test is failing for a reason that isn't understood the debugging method\nis simple: run only that test, in isolation, and narrow it down until the\nissue is understood.\n\nThis is more difficult when the database is full of background data - you\naren't starting from a \"clean\" state. I don't think this is an insurmountable\nproblem - in this case, just change to running with a clean database until you\ncan diagnose the problem. Perhaps it's worth backing up the original dataset\nso you can refer to it later.\n\nRunning tests designed for \"dirty\" datasets with clean datasets is not a\nproblem - it's easy to switch back to using a clean dataset. Going in the\nother direction is much harder.\n\n### Precondition clashes\n\nSome tests have very different preconditions - one test might require that a\ncertain type of data is absent while other tests will add this data (or will\nhave added it in previous runs).\n\nThese types of tests are difficult to adapt but hopefully fairly rare. If they\ncan't be reworked and really are essential they can be run against a second\ninstance of the data store in question that is cleaned between test runs.\n\n## How far I've gotten\n\nI haven't used this technique in many places and have only use it myself for a\nshort time. The idea for it came to me when I saw a traditional, PHP-style,\nhand-crafted test database that wasn't being reset between tests (but which is\nreset between test runs). While I'm not a fan of that approach it got me\nthinking. I've since tried not tearing down the database on a side project, to\nsome success.\n\nI'm ready now to try this strategy in more places, bolstered by the knowledge\nthat if tests are written for a dirty database it is very easy to change them\nto running with a clean database later (usually you don't have to do\nanything).\n\n## Contact/etc\n\nPlease do send me an email about this article, especially if you disagreed\nwith it.\n\nSee other things I've written.\n\nGet an alert when I write something new, by email or RSS .\n\nI am on:\n\n  * Twitter\n  * Mastodon\n  * and Linkedin.\n\n\ud83c\uddeb\ud83c\uddee I have moved to Helsinki and am organising the Helsinki Python group. I'm\ngiving a one of the talks at the April meetup on \"bank\" python. If you know\nsomeone willing to give a talk or lend us space to meet, please do get in\ntouch. \ud83c\uddeb\ud83c\uddee\n\nIf you are feeling charitable towards me: please try out my side-project,\ncsvbase, or \"Github, but for data tables\".\n\n  1. Whatever \"unit\" means - the distinction between a unit test and an integration test is one for the philosophers at this point. \u21a9\n\n  2. It's best to do this before each test because doing it afterward can interfere with debugging failing tests. If a test fails it's quite useful to be able to browse whatever database state it had when it failed. \u21a9\n\n  3. Maybe not strictly forever - you might trim it occasionally. \u21a9\n\n  4. One handy (postgres-specific?) tip: DELETE is faster than TRUNCATE for small tables.\n\nThe real clean_session looks like this:\n\n    \n        sut: ModuleType # sut is system code @pytest.fixture(scope=\"function\") def clean_session(app, config): for table in reversed(sut.Base.metadata.sorted_tables): sut.db.session.execute(\"delete from %s;\" % table.name) sut.db.session.commit() return sut.db.session\n\n\u21a9\n\n  5. In fact, when the dataset is small, things can get very confusing: the polynomial operation can be faster. This is because with small datasets the constant factors in algorithms matter more and so slower algorithms can do better on small inputs. \u21a9\n\n", "frontpage": false}
