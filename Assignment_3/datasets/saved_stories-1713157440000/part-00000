{"aid": "40035265", "title": "Simulation to understand two kinds of measurement error in regression", "url": "https://statmodeling.stat.columbia.edu/2024/04/14/simulation-to-understand-measurement-error-in-regression/", "domain": "columbia.edu", "votes": 1, "user": "RicoElectrico", "posted_at": "2024-04-14 23:01:33", "comments": 0, "source_title": "Simulation to understand two kinds of measurement error in regression", "source_text": "Simulation to understand two kinds of measurement error in regression | Statistical Modeling, Causal Inference, and Social Science\n\nSkip to primary content\n\n# Statistical Modeling, Causal Inference, and Social Science\n\n# Simulation to understand two kinds of measurement error in regression\n\nPosted on April 14, 2024 9:07 AM by Andrew\n\nThis is all super-simple; still, it might be useful. In class today a student\nasked for some intuition as to why, when you\u2019re regressing y on x, measurement\nerror on x biases the coefficient estimate but measurement error on y does\nnot.\n\nI gave the following quick explanation: \u2013 You\u2019re already starting with the\nmodel, y_i = a + bx_i + e_i. If you add measurement error to y, call it y*_i =\ny_i + eta_i, and then you regress y* on x, you can write y* = a + bx_i + e_i +\neta_i, and as long as eta is independent of e, you can just combine them into\na single error term. \u2013 When you have measurement error in x, two things happen\nto attenuate b\u2014that is, to pull the regression coefficient toward zero. First,\nif you spreading out x but keep y unchanged, this will reduce the slope of y\non x. Second, when you add noise to x you\u2019re changing the ordering of the\ndata, which will reduce the strength of the relationship.\n\nBut that\u2019s all words (and some math). It\u2019s simpler and clearer to do a live\nsimulation, which I did right then and there in class!\n\nHere\u2019s the R code:\n\n    \n    \n    # simulation for measurement error library(\"arm\") set.seed(123) n <- 1000 x <- runif(n, 0, 10) a <- 0.2 b <- 0.3 sigma <- 0.5 y <- rnorm(n, a + b*x, sigma) fake <- data.frame(x,y) fit_1 <- lm(y ~ x, data=fake) display(fit_1) sigma_y <- 1 fake$y_star <- rnorm(n, fake$y, sigma_y) sigma_x <- 4 fake$x_star <- rnorm(n, fake$x, sigma_x) fit_2 <- lm(y_star ~ x, data=fake) display(fit_2) fit_3 <- lm(y ~ x_star, data=fake) display(fit_3) fit_4 <- lm(y_star ~ x_star, data=fake) display(fit_4) x_range <- range(fake$x, fake$x_star) y_range <- range(fake$y, fake$y_star) par(mfrow=c(2,2), mar=c(3,3,1,1), mgp=c(1.5,.5,0), tck=-.01) plot(fake$x, fake$y, xlim=x_range, ylim=y_range, bty=\"l\", pch=20, cex=.5) abline(coef(fit_1), col=\"red\", main=\"No measurement error\") plot(fake$x, fake$y_star, xlim=x_range, ylim=y_range, bty=\"l\", pch=20, cex=.5) abline(coef(fit_2), col=\"red\", main=\"Measurement error on y\") plot(fake$x_star, fake$y, xlim=x_range, ylim=y_range, bty=\"l\", pch=20, cex=.5) abline(coef(fit_3), col=\"red\", main=\"Measurement error on x\") plot(fake$x_star, fake$y_star, xlim=x_range, ylim=y_range, bty=\"l\", pch=20, cex=.5) abline(coef(fit_4), col=\"red\", main=\"Measurement error on x and y\")\n\nThe resulting plot is at the top of this post.\n\nI like this simulation for three reasons:\n\n1\\. You can look at the graph and see how the slope changes with measurement\nerror in x but not in y.\n\n2\\. This exercise shows the benefits of clear graphics, including little\nthings like making the dots small, adding the regression lines in red,\nlabeling the individual plots, and using a common axis range for all four\ngraphs.\n\n3\\. It was fast! I did it live in class, and this is an example of how\nstudents, or anyone, can answer this sort of statistical question directly,\nwith a lot more confidence and understanding than would come from a textbook\nand some formulas.\n\nP.S. As Eric Loken and I discuss in this 2017 article, everything gets more\ncomplicated if you condition on \"statistical significance.\"\n\nP.P.S. Yes, I know my R code is ugly. Think of this as an inspiration: even\nif, like me, you\u2019re a sloppy coder, you can still code up these examples for\nteaching and learning.\n\nThis entry was posted in Miscellaneous Statistics, Multilevel Modeling,\nStatistical Computing, Teaching by Andrew. Bookmark the permalink.\n\n## 12 thoughts on \u201cSimulation to understand two kinds of measurement error in\nregression\u201d\n\n  1. John R. Samborski on April 14, 2024 9:53 AM at 9:53 am said:\n\nI don\u2019t understand this sentence: \u201cIn class today a student asked for some\nintuition as to why, when you\u2019re regressing y on x, measurement error on x\nbiases the coefficient estimate by measurement error on y does not.\u201d Should\n\u201cby\u201d be \u201cbut\u201d?\n\nReply \u2193\n\n  2. Bob76 on April 14, 2024 9:56 AM at 9:56 am said:\n\nShould \u201cestimate by measurement\u201d read \u201cestimate but measurement . . .\u201d?\n\nI don\u2019t think that is such ugly code. I can mostly understand it and I don\u2019t\nuse R.\n\nBob76\n\nReply \u2193\n\n     * Andrew on April 14, 2024 10:06 AM at 10:06 am said:\n\nTypo fixed; thanks.\n\nP.S. The code is fine for my purposes, but I think it\u2019s ugly to users of\nggplot2 and tidyverse in R.\n\nReply \u2193\n\n  3. Woody on April 14, 2024 11:02 AM at 11:02 am said:\n\nI\u2019ll bet you didn\u2019t mean to include a 12.5 mB image in your post! I\u2019m\nsurprised, but it clobbered the Feedly app on my iPad.\n\nReply \u2193\n\n  4. Daniel Lakeland on April 14, 2024 12:04 PM at 12:04 pm said:\n\nA useful heuristic in many situations is to take things to the extreme. It\u2019s\neasy to see that measurement error in x when exploded to infinite standard\ndeviation, will lead to a line with zero slope. I remember a discussion on\nthis on the blog a year ago ish and I didn\u2019t get the point of the question\nuntil Carlos made me look harder at it, I finally used this heuristic of\nblowing up the x errors to very large size and it immediately made sense.\n\nWorth it to remember this trick in many applications.\n\nReply \u2193\n\n  5. Bob Carpenter on April 14, 2024 12:05 PM at 12:05 pm said:\n\nThe simulation code is very readable. I like that about R\u2019s being purpose-\nbuilt for stats. Python code for this gets cluttered up with namespace issues\n(a convention that would help R avoid the inevitable package variable/function\nnaming conflicts).\n\nI think you can remove \u201cdisplay()\u201d when writing about plotting code. I looked\nit up, and it\u2019s just a replacement for the default lm print in the arm package\nthat prints fewer statistics at lower arithmetic precision.\n\nThe plotting code is opaque to non R users because there\u2019s a function called\n\u201cplot()\u201d\u2014if this were called \u201cscatter_plot(x = , y = )\u201d it\u2019d be easier to\nunderstand what it would do without looking at the output. For people like me\nwho don\u2019t use base graphics in R much, I have no idea what all that cut-and-\npaste args are doing, like \u201cbty=\u201dl\u201d, pch=20, cex=.5\u2032\u2032 and \u201cmar=c(3,3,1,1),\nmgp=c(1.5,.5,0), tck=-.01\u201d. The way to clean this up is to write a function\nencapsulating all the redundancy and binding the repeated arguments.\n\n    \n        x_range <- ... y_range <- ... lr_plot <- function(fit, x, y, fit) { plot(x, y, xlim=x_range, ylim=y_range, bty=\"l\", pch=20, cex=.5) abline(coef(fit_1), col=\"red\", main=title) } lr_plot(fit, fake$x, fake$y, \"No measurement error\") lr_plot(fit_err_y, fake$x, fake$y_star, \"Measurement error on y\") lr_plot(fit_err_x, fake$x_star, fake$y, \"Measurement error on x\") lr_plot(fit_err_xy, fake$x_star, fake$y_star, \"Measurement error on x and y\")\n\nThis makes it clear the same thing is happening 4 times and it makes it easeir\nto see where the plots vary. It also makes it easier to check that the right\ntitles are being attached to the right graphs. I went further and renamed the\nfits to make it easier to see the alignment. Even better would be to pull the\ndata out of the fit object---I'm pretty sure you can do that because of the\neverything-and-the-kitchen-sink design of fit objects. You could go even\nfurther and name those terse args if you want to. Something like\n\"tiny_dot_plotting_character <\\- 20\" and then\n\"pch=tiny_dot_plotting_character\" (OK, I did know this one) or\n\"tick_adjustment <\\- -0.1\" and \"tck=tick_adjustment\" (in the previous\ncode)---when you name variables this way, you no longer need doc.\n\nI'd be curious what best practices in the Tidyverse would look like for this.\nI suspect that either there are built ins for lm, or you'd have to wrestle the\ndata into a dataframe, set the faceting, extract the coefficients for the\nlines, etc. I find the ggplot code pretty readable (more so than base\ngraphics), but I find all the data frame manipulation and piping very opaque.\n\nReply \u2193\n\n     * Anonymous on April 14, 2024 1:33 PM at 1:33 pm said:\n\nHere\u2019s one way:\n\nlibrary(tidyverse) set.seed(123) n <\\- 1000 a <\\- 0.2 b <\\- 0.3 sigma <\\- 0.5\n\nfake % mutate(y_star = rnorm(n, y, sigma_y), x_star = rnorm(n, x, sigma_x))\n\nbind_rows( tibble(x=fake$x, y=fake$y, name=\u201dNo measurement error\u201d),\ntibble(x=fake$x, y=fake$y_star, name=\u201dMeasurement error on y\u201d),\ntibble(x=fake$x_star, y=fake$y, name=\u201dMeasurement error on x\u201d),\ntibble(x=fake$x_star, y=fake$y_star, name=\u201dMeasurement error on x and y\u201d) )\n%>% mutate(name = fct_inorder(name)) %>% ggplot(aes(x,y)) + geom_point() +\ngeom_smooth(method=\u201dlm\u201d, fullrange=TRUE) + facet_wrap(~name)\n\nCouple of notes: geom_smooth() has a (imo) nice default of not extrapolating\nthe fitted line beyond the range of the data, hence the fullrange option to\nmatch the original. By default the plots would have appeared in alphabetical\norder by title, hence the slightly esoteric fct_inorder().\n\nAn alternative to faceting that might be more readable for non-ggplot users is\nto make individual plots and compose them using the (excellent!) patchwork.\n\nReply \u2193\n\n     * Andrew on April 14, 2024 2:07 PM at 2:07 pm said:\n\nBob:\n\nThe call to display() takes up no more space than the call to print(), and I\nprefer the display() output, so that\u2019s what I use. If you use print(), you\ndon\u2019t get standard errors for the coefficient estimates. If you use summary(),\nyou get all sorts of extra stuff like t-statistics and p-values. I wrote\ndisplay() to give me just what I want.\n\nReply \u2193\n\n     * Anonymous on April 14, 2024 3:18 PM at 3:18 pm said:\n\n> The plotting code is opaque to non R users because there\u2019s a function called\n> \u201cplot()\u201d\u2014if this were called \u201cscatter_plot(x = , y = )\u201d it\u2019d be easier to\n> understand what it would do without looking at the output.\n\nIt doesn\u2019t seem that difficult to guess what the following may do:\n\nplot(fake$x, fake$y, xlim=x_range, ylim=y_range, ...)\n\nAnyway it doesn\u2019t matter much \u2013 the objective of the code is not to make easy\nto understand what may happen when you run it but to make that happen when you\nrun it.\n\nInterestingly you called your auxiliary function lr_plot (which is invalid\nbecause of a repeated formal argument by the way) and not scatter_plot or\nlr_scatter_plot even though that would make it easier to understand what it\ndoes without looking at the output.\n\nReply \u2193\n\n  6. Dmitri on April 14, 2024 2:44 PM at 2:44 pm said:\n\nAny code done live in front of students gets an automatic aesthetic pass.\nPlus, I agree with Bob76, this seems totally fine for what it is.\n\nReply \u2193\n\n     * Andrew on April 14, 2024 3:15 PM at 3:15 pm said:\n\nDmitri:\n\nIt\u2019s partly a rhetorical trick, that I badmouth my own code and then the\ncommenters can reassure me how great it is. On the other hand, sometimes I\nbadmouth my own code and the commenters agree!\n\nReply \u2193\n\n       * Dmitri on April 14, 2024 7:13 PM at 7:13 pm said:\n\nYeah, I know that one, I use it on my family whenever I can. \u201cYou are going to\nhate this suggestion, but ...\u201d It leverages the listener\u2019s inherent\ncontrarianism in your favor. Like, they\u2019re stuck between doing what you\npredict and doing what you want!\n\nReply \u2193\n\n### Leave a Reply Cancel reply\n\n  * Art\n  * Bayesian Statistics\n  * Causal Inference\n  * Decision Analysis\n  * Economics\n  * Jobs\n  * Literature\n  * Miscellaneous Science\n  * Miscellaneous Statistics\n  * Multilevel Modeling\n  * Papers\n  * Political Science\n  * Public Health\n  * Sociology\n  * Sports\n  * Stan\n  * Statistical Computing\n  * Statistical Graphics\n  * Teaching\n  * Zombies\n\n  1. Eric Neufeld on Intelligence is whatever machines cannot (yet) doApril 14, 2024 8:15 PM\n\nMy understanding of the Chinese room problem is different. Imagine that\ninstead of the computer program (as the Chinese Room...\n\n  2. Seth Finkelstein on Intelligence is whatever machines cannot (yet) doApril 14, 2024 8:08 PM\n\nHmm, which \"people\" are referenced here in claiming \"I doubt people 50 years\nago (1974) would have said you can...\n\n  3. Dmitri on Simulation to understand two kinds of measurement error in regressionApril 14, 2024 7:13 PM\n\nYeah, I know that one, I use it on my family whenever I can. \"You are going to\nhate this...\n\n  4. Pablo Verde on How large is that treatment effect, really? (my talk at NYU economics department Thurs 18 Apr 2024, 12:30pm)April 14, 2024 6:23 PM\n\nGreat topic! Here is my two cents about it:\nhttps://pubmed.ncbi.nlm.nih.gov/30600534/ I would like to see your talk. Will\nit be...\n\n  5. Daniel Lakeland on Intelligence is whatever machines cannot (yet) doApril 14, 2024 6:04 PM\n\nWhen you feed in language and have it rewrite or comment on the language it\nmakes good sense because it...\n\n  6. Daniel Lakeland on Intelligence is whatever machines cannot (yet) doApril 14, 2024 6:01 PM\n\nThe stupid thing about the Chinese Room (as I understand it) is that Searle\npostulated that a room couldn't understand...\n\n  7. Andrew on Intelligence is whatever machines cannot (yet) doApril 14, 2024 3:58 PM\n\nYes, I remember that conversation! Or, at least, I think I do.\n\n  8. Andrew on Intelligence is whatever machines cannot (yet) doApril 14, 2024 3:56 PM\n\nAnon: I don't think Fisher was trying to ban anything, although it seems that\nhe did get pretty angry when...\n\n  9. Anoneuoid on Intelligence is whatever machines cannot (yet) doApril 14, 2024 3:21 PM\n\nThere is a long history of trying to ban it, going back to literally the first\nEuropean smoker: Rodrigo de...\n\n  10. Anonymous on Simulation to understand two kinds of measurement error in regressionApril 14, 2024 3:18 PM\n\n> The plotting code is opaque to non R users because there\u2019s a function called\n> \u201cplot()\u201d\u2014if this were called \u201cscatter_plot(x...\n\n  11. Andrew on Simulation to understand two kinds of measurement error in regressionApril 14, 2024 3:15 PM\n\nDmitri: It's partly a rhetorical trick, that I badmouth my own code and then\nthe commenters can reassure me how...\n\n  12. kj on Intelligence is whatever machines cannot (yet) doApril 14, 2024 3:00 PM\n\nMy pet theory is that a lot of argument stems from society inappropriately\nbundling concepts. With climate change for example,...\n\n  13. Dmitri on Simulation to understand two kinds of measurement error in regressionApril 14, 2024 2:44 PM\n\nAny code done live in front of students gets an automatic aesthetic pass.\nPlus, I agree with Bob76, this seems...\n\n  14. Christian Hennig on Intelligence is whatever machines cannot (yet) doApril 14, 2024 2:32 PM\n\nWhat I meant by \"naive observational approach\u201d is what I explained in the\nbeginning: \"to define intelligence by specifying a...\n\n  15. Andrew on Simulation to understand two kinds of measurement error in regressionApril 14, 2024 2:07 PM\n\nBob: The call to display() takes up no more space than the call to print(),\nand I prefer the display()...\n\n  16. Phil on Intelligence is whatever machines cannot (yet) doApril 14, 2024 1:51 PM\n\nBtw I\u2019m not claiming the ability to make a picture of a man riding a horse\nbackwards is a necessary...\n\n  17. Anonymous on Simulation to understand two kinds of measurement error in regressionApril 14, 2024 1:33 PM\n\nHere's one way: library(tidyverse) set.seed(123) n <\\- 1000 a <\\- 0.2 b <\\-\n0.3 sigma <\\- 0.5 fake % mutate(y_star...\n\n  18. Bob Carpenter on Intelligence is whatever machines cannot (yet) doApril 14, 2024 1:04 PM\n\nI think the analogy to psychics is as misguided as the rest. I have GPT teach\nme things all the...\n\n  19. Bob Carpenter on Intelligence is whatever machines cannot (yet) doApril 14, 2024 12:53 PM\n\nI'm not entirely sure what you mean by \"naive observational approach.\" The\noutside perspective was the motivation for Turing's test....\n\n  20. Bob Carpenter on Intelligence is whatever machines cannot (yet) doApril 14, 2024 12:47 PM\n\nI replied to those posts. Herb Simon, in his cognitive sci class, used to\nemphasize just how slow our brains...\n\n  21. Bob Carpenter on Intelligence is whatever machines cannot (yet) doApril 14, 2024 12:46 PM\n\nJust my poor choice of examples. Let's mine recent topics in the blog, all of\nwhich are obviously being put...\n\n  22. Bob Carpenter on Intelligence is whatever machines cannot (yet) doApril 14, 2024 12:38 PM\n\nOne can assert that \"LLMs don\u2019t do inferencing, causal or otherwise.\" and \"we\nreally actually can do causal inference\" but...\n\n  23. Ben on Intelligence is whatever machines cannot (yet) doApril 14, 2024 12:36 PM\n\nI think I'm more impressed with IBM Watson now than I was at the time it won\nJeopardy.\n\n  24. Bob Carpenter on Intelligence is whatever machines cannot (yet) doApril 14, 2024 12:34 PM\n\nThis is why semantics is so tricky. This technique of prompting is called \"in-\ncontext learning\" everywhere, even in the Wikipedia...\n\n  25. Bob Carpenter on Intelligence is whatever machines cannot (yet) doApril 14, 2024 12:25 PM\n\nYup. Very hard to get that going with Dall-E. Closest I could get after about\nten prompts was two people...\n\n  26. Bob Carpenter on Intelligence is whatever machines cannot (yet) doApril 14, 2024 12:22 PM\n\nIt wasn't me. I just grabbed a random article of the \"AI isn't here because it\ncan't do X\" variety....\n\n  27. Bob Carpenter on Intelligence is whatever machines cannot (yet) doApril 14, 2024 12:21 PM\n\nThis was my post, not Andrew's. Maybe I should be making this more clear! See\nmy earlier response about where...\n\n  28. Bob Carpenter on Intelligence is whatever machines cannot (yet) doApril 14, 2024 12:18 PM\n\nWe can think of this mulitple ways. When a user uses Dall-E to generate a\ncopyright-infringing image (or uses GPT...\n\n  29. Bob Carpenter on Intelligence is whatever machines cannot (yet) doApril 14, 2024 12:16 PM\n\nI distinctly remember asking Andrew at one point what \"happiness\" meant in a\nsurvey and he replied something like, \"whatever...\n\n  30. Bob Carpenter on Simulation to understand two kinds of measurement error in regressionApril 14, 2024 12:05 PM\n\nThe simulation code is very readable. I like that about R's being purpose-\nbuilt for stats. Python code for this gets...\n\n  31. Daniel Lakeland on Simulation to understand two kinds of measurement error in regressionApril 14, 2024 12:04 PM\n\nA useful heuristic in many situations is to take things to the extreme. It's\neasy to see that measurement error...\n\n  32. David in Tokyo on Intelligence is whatever machines cannot (yet) doApril 14, 2024 11:36 AM\n\nI've been ripped off. Plagiarized. Become a victim of intellectual theft. In\nan article* titled \"LLMs aren\u2019t very bright. Why...\n\n  33. JimV on Intelligence is whatever machines cannot (yet) doApril 14, 2024 11:36 AM\n\nSo far, as I see it, some AI programs are intelligent at certain tasks. I\nguess the outstanding example, for...\n\n  34. Clyde Schechter on Intelligence is whatever machines cannot (yet) doApril 14, 2024 11:13 AM\n\nAs best I can recall, it was a long time ago, the early discussion around\nsmoking and cancer did not...\n\n  35. Woody on Simulation to understand two kinds of measurement error in regressionApril 14, 2024 11:02 AM\n\nI'll bet you didn't mean to include a 12.5 mB image in your post! I'm\nsurprised, but it clobbered the...\n\n  36. Wonks Anonymous on Evidence, desire, supportApril 14, 2024 10:13 AM\n\nPerhaps \"ghost citing\" should be the term for when you cite a source, but you\ndidn't actually read it, and...\n\n  37. Wonks Anonymous on Evidence, desire, supportApril 14, 2024 10:06 AM\n\nI remember hearing the opposite claim: now that cellphone cameras are common,\nclaims about Bigfoot/UFOs became less common. The recent...\n\n  38. Andrew on Simulation to understand two kinds of measurement error in regressionApril 14, 2024 10:06 AM\n\nTypo fixed; thanks. P.S. The code is fine for my purposes, but I think it's\nugly to users of ggplot2...\n\n  39. Bob76 on Simulation to understand two kinds of measurement error in regressionApril 14, 2024 9:56 AM\n\nShould \"estimate by measurement\" read \"estimate but measurement . . .\"? I\ndon't think that is such ugly code. I...\n\n  40. John R. Samborski on Simulation to understand two kinds of measurement error in regressionApril 14, 2024 9:53 AM\n\nI don't understand this sentence: \"In class today a student asked for some\nintuition as to why, when you\u2019re regressing...\n\n  41. Christian Hennig on Intelligence is whatever machines cannot (yet) doApril 14, 2024 9:49 AM\n\nA major issue with the intelligence concept is that most people who use it\naren't clear about to what extent...\n\n  42. Andrew on Evidence, desire, supportApril 14, 2024 9:45 AM\n\nAnon: I believe lots of things. I believe that Australia exists, that I can\nswim, that birds can fly, that...\n\n  43. Andrew on Evidence, desire, supportApril 14, 2024 9:40 AM\n\nFixed; thanks.\n\n  44. Gregory C. Mayer on Evidence, desire, supportApril 14, 2024 9:34 AM\n\nActually, I do know where it comes from-- I cited it in a talk once: it's by\nthe astronomer Arthur...\n\n  45. Andrew on Intelligence is whatever machines cannot (yet) doApril 14, 2024 9:07 AM\n\nBob: I agree with your general point that there are problems to defining some\nthreshold of intelligence or understanding. One...\n\n  46. Anoneuoid on Intelligence is whatever machines cannot (yet) doApril 14, 2024 8:47 AM\n\nDo we think Fisher was not intelligent because he argued that smoking didn\u2019t\ncause cancer? Do we think all the...\n\n  47. Joshua on What is the prevalence of bad social science?April 14, 2024 8:31 AM\n\nThere's a woman, Susan Oliver who was on his podcast, I think a few times,\nearlier in as an expert....\n\n  48. Dale Lehman on Intelligence is whatever machines cannot (yet) doApril 14, 2024 7:55 AM\n\nAs suggested, you can turn off the machine but it does not suffer. So, I don't\nsee any meaningful way...\n\n  49. David in Tokyo on Intelligence is whatever machines cannot (yet) doApril 14, 2024 4:56 AM\n\nFor the nonce, I remain of the opinion that it's always the _user_ of the\ntechnology that's responsible. So, for...\n\n  50. Gregory C. Mayer on Evidence, desire, supportApril 14, 2024 4:50 AM\n\n\u201cI wouldn\u2019t have seen it if I hadn\u2019t believed it.\u201d There's a related saying,\nwhose origin I don't know, but...\n\nProudly powered by WordPress\n\n", "frontpage": false}
