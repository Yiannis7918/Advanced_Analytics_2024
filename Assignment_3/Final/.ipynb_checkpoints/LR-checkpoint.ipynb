{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.236:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.236:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a237dacbe0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, lower\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, SQLTransformer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import col, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+\n",
      "|     aid|         source_text|label|\n",
      "+--------+--------------------+-----+\n",
      "|39958086|Large Hadron Coll...|    0|\n",
      "|39958094|Web Mash\\n\\n<\\---...|    0|\n",
      "|39958109|Blocked\\n\\n# whoa...|    0|\n",
      "|39958127|Isaac Asimov obit...|    0|\n",
      "|39958129|Building Computin...|    0|\n",
      "+--------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('C:/Users/lenne/anaconda3/envs/AA/Advanced_Analytics/Assignment_3/spark/scripts/data_full.json')\n",
    "\n",
    "# Drop these columns for now (might use them later as predictors but doesn't seem to run for now)\n",
    "df = df.drop(\"url\", \"domain\", \"posted_at\", \"source_title\", \"title\", \"comments\", \"user\", \"votes\") #if the \"frontpage\" is already correct, can I remove posted_at? or i still need to check this to see if some are actually frontpage or not\n",
    "\n",
    "# Convert True to 1 and False to 0 in the \"frontpage\" column\n",
    "df = df.withColumn(\"label\", when(df[\"frontpage\"] == True, 1).otherwise(0))\n",
    "\n",
    "# Drop the original \"frontpage\" column\n",
    "df = df.drop(\"frontpage\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: 5747 rows, 3 columns\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.count()\n",
    "num_cols = len(df.columns)\n",
    "print(\"Shape of DataFrame: {} rows, {} columns\".format(num_rows, num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names:\n",
      "aid\n",
      "source_text\n",
      "label\n"
     ]
    }
   ],
   "source": [
    "print(\"Column Names:\")\n",
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates(['aid'])\n",
    "df = df.filter(df['source_text'] != '')\n",
    "df = df.dropDuplicates(['source_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5051"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|  846|\n",
      "|    0| 4205|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lowercase transformation \n",
    "df = df.withColumn(\"source_text\", lower(df[\"source_text\"]))\n",
    "\n",
    "# Remove newline characters (\\n), hashtags (#) and double spaces\n",
    "df = df.withColumn(\"source_text\", regexp_replace(regexp_replace(regexp_replace(\"source_text\", \"\\\\n\", \" \"), \"#\", \"\"), \"\\\\s+\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+\n",
      "|     aid|         source_text|label|\n",
      "+--------+--------------------+-----+\n",
      "|40015851|\"strong focus on ...|    1|\n",
      "|40025037|\"the tourists hav...|    1|\n",
      "|40076792|           50 รท 5 = |    0|\n",
      "|40050776| lss lock state s...|    0|\n",
      "|40087126| zen and the art ...|    0|\n",
      "+--------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a balanced datasset (class-balanced sampling - undersampling of the majority class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|  800|\n",
      "|    0|  801|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 800\n",
    "seed = 42\n",
    "\n",
    "fractions = df.groupBy(\"label\").count().withColumn(\"required_n\", n/col(\"count\")).drop(\"count\").rdd.collectAsMap()\n",
    "\n",
    "df_balanced = df.stat.sampleBy(\"label\", fractions, seed)\n",
    "df_balanced.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 1147\n",
      "Test Dataset Count: 476\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = df_balanced.randomSplit([0.7, 0.3], seed = 42)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression tokenizer: Tokenizes input text into words using a regex pattern\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"source_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# Stop words: Loads default English stop words and removes them from tokenized words\n",
    "stops = StopWordsRemover.loadDefaultStopWords('english')\n",
    "stopwordsRemover = StopWordsRemover(inputCol=regexTokenizer.getOutputCol(), outputCol=\"filtered\", stopWords = stops)\n",
    "\n",
    "# Bag of words count: Converts tokenized words into a numerical feature vector\n",
    "    # @vocabSize: max size of vocabulary; @minDF: min nr of docs in which a term should appear to be included in the vocab\n",
    "countVectors = CountVectorizer(inputCol=stopwordsRemover.getOutputCol(), outputCol=\"rawFeatures\", vocabSize=10000, minDF=10)\n",
    "\n",
    "# IDF (Inverse Document Frequency): Calculates the Inverse Document Frequency of words\n",
    "    # @inputCol: raw features (word counts); @outputCol: IDF values\n",
    "    # @minDocFreq: min nr of docs in which a term should appear to be included in IDF calculation\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=10)\n",
    "\n",
    "# @maxIter: max nr of iterations; @regParam: Regularization parameter; @elasticNetParam: 0 for L2, 1 for L1 penalty)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.3, 0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0, 0.1, 0.25]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and find the best set of parameters\n",
    "model = crossval.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RegexTokenizer_65d1ddf1203b, StopWordsRemover_50391ba4f05b, CountVectorizerModel: uid=CountVectorizer_4adbf538e7d7, vocabularySize=9809, IDFModel: uid=IDF_3d56f470cbf2, numDocs=1147, numFeatures=9809, LogisticRegressionModel: uid=LogisticRegression_5bb519a4aad6, numClasses=2, numFeatures=9809]\n"
     ]
    }
   ],
   "source": [
    "best = model.bestModel\n",
    "print(best.stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain predictions for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aid',\n",
       " 'source_text',\n",
       " 'label',\n",
       " 'words',\n",
       " 'filtered',\n",
       " 'rawFeatures',\n",
       " 'features',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.transform(testData)\n",
    "prediction.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|         probability|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|    0|[0.39364400718012...|       1.0|\n",
      "|    0|[0.44279923030141...|       1.0|\n",
      "|    1|[0.25320673271740...|       1.0|\n",
      "|    0|[0.60721402697849...|       0.0|\n",
      "|    1|[0.63915001924698...|       0.0|\n",
      "|    0|[0.87758914764345...|       0.0|\n",
      "|    1|[0.61519810514949...|       0.0|\n",
      "|    1|[0.44937622372760...|       1.0|\n",
      "|    0|[0.44664033335041...|       1.0|\n",
      "|    0|[0.50772400102630...|       0.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select('label','probability','prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5594214714901006"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5609243697478992"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = prediction.filter(prediction.label == prediction.prediction).count() / float(testData.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model locally to access later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_LR.write().overwrite().save('C:/Users/lenne/anaconda3/envs/AA/Advanced_Analytics/Assignment_3/spark/models')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
