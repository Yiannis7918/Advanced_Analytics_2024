{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the review sentiment of the game\n",
    "\n",
    "I made the script here now but I will use Google colab\n",
    "\n",
    "I will split the data again for this specific task, in such a way that there is a balanced distribution of the sentiment labels in every dataset \n",
    "\n",
    "I'm using a pre-trained CNN model VGG-16.  \n",
    "- 16 layers in total, of which 13 convolutional layers and 3 fully connected layers (dense layers)\n",
    "- the convolutional layers are composed of 3x3 filters\n",
    "- softmax layer in the end for classification\n",
    "\n",
    "The data contains 6 sentiment classes (and also missings)\n",
    "\n",
    "Inspo:\n",
    "- https://keras.io/api/applications/#usage-examples-for-image-classification-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #for data augmentation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout #the layers we need for the CNN\n",
    "from tensorflow.keras.models import Sequential #Sequential will contain all the different CNN layers\n",
    "from tensorflow.keras.applications import VGG16 #VGG-16\n",
    "\n",
    "#for the metadata\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata file for sentiment prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't need to run this again\n",
    "\n",
    "Using Gr√©goire's code, I created new metadata files because I need the sentiment labels.\n",
    "I did it for the test, train and validation sets separately "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your folder containing images\n",
    "folder_path = \"../datasets/validation\"\n",
    "\n",
    "# Path to your JSON file\n",
    "json_file_path = \"../datasets/dataset.json\"\n",
    "\n",
    "# Path to save the CSV file\n",
    "csv_file_path = \"../datasets/metadata2_validation.csv\"\n",
    "\n",
    "# Load JSON data into a dictionary\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    labels_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store image name and label pairs\n",
    "data = []\n",
    "\n",
    "# Iterate over the images in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is an image\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        # Check if the image filename is present in the labels dictionary\n",
    "        for element in labels_dict:\n",
    "            #print(element[\"screenshots\"])\n",
    "            if filename in element[\"screenshots\"]:\n",
    "                # Append image name and label to the data list\n",
    "                data.append([filename, element[\"sentiment\"], element[\"title\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write the data to a CSV file\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    # Write header\n",
    "    csv_writer.writerow(['Image Name', 'Sentiment', 'Game Name'])\n",
    "    # Write data\n",
    "    csv_writer.writerows(data)\n",
    "\n",
    "print(\"CSV file created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check out how many sentiment classes there are and if this is the same for all datasets.\n",
    "\n",
    "There are 6 sentiment classes:\n",
    "- Overwhelmingly Positive\n",
    "- Very Positive\n",
    "- Mostly Positive\n",
    "- Positive\n",
    "- Mixed\n",
    "- Mostly Negative\n",
    "- (missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes for 'Sentiment': 7\n",
      "Classes: ['Mostly Positive' nan 'Very Positive' 'Mixed' 'Positive'\n",
      " 'Overwhelmingly Positive' 'Mostly Negative']\n",
      "Number of classes for 'Sentiment': 7\n",
      "Classes: ['Mostly Positive' nan 'Very Positive' 'Mixed' 'Positive'\n",
      " 'Overwhelmingly Positive' 'Mostly Negative']\n",
      "Number of classes for 'Sentiment': 7\n",
      "Classes: ['Mostly Positive' nan 'Very Positive' 'Mixed' 'Positive'\n",
      " 'Overwhelmingly Positive' 'Mostly Negative']\n"
     ]
    }
   ],
   "source": [
    "#Load CSV files\n",
    "df_train = pd.read_csv(\"../datasets/metadata2_train.csv\")\n",
    "df_test = pd.read_csv(\"../datasets/metadata2_test.csv\")\n",
    "df_validation = pd.read_csv(\"../datasets/metadata2_validation.csv\")\n",
    "\n",
    "#Check the unique values in the 'Sentiment' column\n",
    "sentiment_classes_train = df_train['Sentiment'].unique()\n",
    "sentiment_classes_test = df_test['Sentiment'].unique()\n",
    "sentiment_classes_validation = df_validation['Sentiment'].unique()\n",
    "\n",
    "#Print the unique classes\n",
    "print(\"Number of classes for 'Sentiment':\", len(sentiment_classes_train))\n",
    "print(\"Classes:\", sentiment_classes_train)\n",
    "\n",
    "print(\"Number of classes for 'Sentiment':\", len(sentiment_classes_test))\n",
    "print(\"Classes:\", sentiment_classes_test)\n",
    "\n",
    "print(\"Number of classes for 'Sentiment':\", len(sentiment_classes_validation))\n",
    "print(\"Classes:\", sentiment_classes_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to dataset folders\n",
    "train_dir = \"../datasets/train\"\n",
    "test_dir = \"../datasets/test\"\n",
    "val_dir = \"../datasets/validation\"\n",
    "metadata_file = \"../datasets/metadata.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did some data augmentation but apparently it's better to start more simple and add complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1/255, #each pixel value in the images will be divided by 255, normalizing them to the range [0, 1]\n",
    "                            rotation_range=10, #random 10 degree rotation to the left or right\n",
    "                            width_shift_range = 0.1, #10% width shift\n",
    "                            height_shift_range=0.1, #10% height shift\n",
    "                            shear_range=0.1, #another type of transformation\n",
    "                            zoom_range=0.1, #10% zoom \n",
    "                            horizontal_flip=True) \n",
    " \n",
    "train_generator = datagen.flow_from_directory(train_dir, \n",
    "                                            target_size=(224,224), #input size for VGG-16\n",
    "                                            batch_size=4, \n",
    "                                            class_mode=\"categorical\", \n",
    "                                            subset='training') \n",
    "\n",
    "test_generator = datagen.flow_from_directory(test_dir, \n",
    "                                                target_size=(224,224), \n",
    "                                                batch_size=4, \n",
    "                                                class_mode=\"categorical\", \n",
    "                                                subset='test')\n",
    "                                                   \n",
    "validation_generator = datagen.flow_from_directory(val_dir, \n",
    "                                                target_size=(224,224), \n",
    "                                                batch_size=4, \n",
    "                                                class_mode=\"categorical\", \n",
    "                                                subset='validation')\n",
    "                                                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model \n",
    "model = Sequential() \n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', \n",
    "                 input_shape=(224, 224, 3))) \n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu')) \n",
    "\n",
    "model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu')) \n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu')) \n",
    "\n",
    "model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(Flatten()) model.add(Dense(512, activation='relu')) #convert the 2D feature maps into a 1D vector\n",
    "\n",
    "model.add(Dropout(0.5)) model.add(Dense(6, activation='softmax')) #6 sentiment classes\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001) #very small? i dont know, Adam optimizer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of the NN architecture \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG-16 model\n",
    "  \n",
    "model_vgg16 = VGG16(input_shape=(224,224,3), include_top=False, weights='imagenet') #im not sure what include_top=false does\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each layer, we set the trainable attribute to False\n",
    "#this is effectively freezing the layer's weights. \n",
    "#This means that during training, the weights of these layers will not be updated, \n",
    "#and they will retain the pre-trained weights from the ImageNet dataset.\n",
    "\n",
    "for layer in model_vgg16.layers: layer.trainable=False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG-16 architecture \n",
    "model_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a new sequential model by stacking the different layers\n",
    "model = Sequential() \n",
    "\n",
    "model.add(model_vgg16) #using the pre-trained VGG_16 mode\n",
    "\n",
    "model.add(Flatten()) \n",
    "\n",
    "model.add(Dense(512, activation='relu')) \n",
    "\n",
    "model.add(Dropout(0.5)) \n",
    "\n",
    "model.add(Dense(6, activation='softmax')) #6 sentiment classes\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I still need to check the specifications\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "\n",
    "history=model.fit(train_generator, epochs=5, validation_data=validation_generator)\n",
    "#number of epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment_VGG-16.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aavenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
